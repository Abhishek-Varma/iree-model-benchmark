#map = affine_map<(d0, d1, d2, d3) -> (d0, d1, d3)>
#map1 = affine_map<(d0, d1, d2, d3) -> (d0, d1, d2, d3)>
module @module {
  util.global private @__auto.token_embd.weight = #stream.parameter.named<"model"::"token_embd.weight"> : tensor<128256x4096xf16>
  util.global private @__auto.blk.0.attn_norm.weight = #stream.parameter.named<"model"::"blk.0.attn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.0.attn_q.weight = #stream.parameter.named<"model"::"blk.0.attn_q.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.0.attn_k.weight = #stream.parameter.named<"model"::"blk.0.attn_k.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.0.attn_v.weight = #stream.parameter.named<"model"::"blk.0.attn_v.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.0.attn_output.weight = #stream.parameter.named<"model"::"blk.0.attn_output.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.0.ffn_norm.weight = #stream.parameter.named<"model"::"blk.0.ffn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.0.ffn_gate.weight = #stream.parameter.named<"model"::"blk.0.ffn_gate.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.0.ffn_up.weight = #stream.parameter.named<"model"::"blk.0.ffn_up.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.0.ffn_down.weight = #stream.parameter.named<"model"::"blk.0.ffn_down.weight"> : tensor<4096x14336xf16>
  util.global private @__auto.blk.1.attn_norm.weight = #stream.parameter.named<"model"::"blk.1.attn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.1.attn_q.weight = #stream.parameter.named<"model"::"blk.1.attn_q.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.1.attn_k.weight = #stream.parameter.named<"model"::"blk.1.attn_k.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.1.attn_v.weight = #stream.parameter.named<"model"::"blk.1.attn_v.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.1.attn_output.weight = #stream.parameter.named<"model"::"blk.1.attn_output.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.1.ffn_norm.weight = #stream.parameter.named<"model"::"blk.1.ffn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.1.ffn_gate.weight = #stream.parameter.named<"model"::"blk.1.ffn_gate.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.1.ffn_up.weight = #stream.parameter.named<"model"::"blk.1.ffn_up.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.1.ffn_down.weight = #stream.parameter.named<"model"::"blk.1.ffn_down.weight"> : tensor<4096x14336xf16>
  util.global private @__auto.blk.2.attn_norm.weight = #stream.parameter.named<"model"::"blk.2.attn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.2.attn_q.weight = #stream.parameter.named<"model"::"blk.2.attn_q.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.2.attn_k.weight = #stream.parameter.named<"model"::"blk.2.attn_k.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.2.attn_v.weight = #stream.parameter.named<"model"::"blk.2.attn_v.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.2.attn_output.weight = #stream.parameter.named<"model"::"blk.2.attn_output.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.2.ffn_norm.weight = #stream.parameter.named<"model"::"blk.2.ffn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.2.ffn_gate.weight = #stream.parameter.named<"model"::"blk.2.ffn_gate.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.2.ffn_up.weight = #stream.parameter.named<"model"::"blk.2.ffn_up.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.2.ffn_down.weight = #stream.parameter.named<"model"::"blk.2.ffn_down.weight"> : tensor<4096x14336xf16>
  util.global private @__auto.blk.3.attn_norm.weight = #stream.parameter.named<"model"::"blk.3.attn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.3.attn_q.weight = #stream.parameter.named<"model"::"blk.3.attn_q.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.3.attn_k.weight = #stream.parameter.named<"model"::"blk.3.attn_k.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.3.attn_v.weight = #stream.parameter.named<"model"::"blk.3.attn_v.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.3.attn_output.weight = #stream.parameter.named<"model"::"blk.3.attn_output.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.3.ffn_norm.weight = #stream.parameter.named<"model"::"blk.3.ffn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.3.ffn_gate.weight = #stream.parameter.named<"model"::"blk.3.ffn_gate.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.3.ffn_up.weight = #stream.parameter.named<"model"::"blk.3.ffn_up.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.3.ffn_down.weight = #stream.parameter.named<"model"::"blk.3.ffn_down.weight"> : tensor<4096x14336xf16>
  util.global private @__auto.blk.4.attn_norm.weight = #stream.parameter.named<"model"::"blk.4.attn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.4.attn_q.weight = #stream.parameter.named<"model"::"blk.4.attn_q.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.4.attn_k.weight = #stream.parameter.named<"model"::"blk.4.attn_k.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.4.attn_v.weight = #stream.parameter.named<"model"::"blk.4.attn_v.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.4.attn_output.weight = #stream.parameter.named<"model"::"blk.4.attn_output.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.4.ffn_norm.weight = #stream.parameter.named<"model"::"blk.4.ffn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.4.ffn_gate.weight = #stream.parameter.named<"model"::"blk.4.ffn_gate.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.4.ffn_up.weight = #stream.parameter.named<"model"::"blk.4.ffn_up.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.4.ffn_down.weight = #stream.parameter.named<"model"::"blk.4.ffn_down.weight"> : tensor<4096x14336xf16>
  util.global private @__auto.blk.5.attn_norm.weight = #stream.parameter.named<"model"::"blk.5.attn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.5.attn_q.weight = #stream.parameter.named<"model"::"blk.5.attn_q.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.5.attn_k.weight = #stream.parameter.named<"model"::"blk.5.attn_k.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.5.attn_v.weight = #stream.parameter.named<"model"::"blk.5.attn_v.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.5.attn_output.weight = #stream.parameter.named<"model"::"blk.5.attn_output.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.5.ffn_norm.weight = #stream.parameter.named<"model"::"blk.5.ffn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.5.ffn_gate.weight = #stream.parameter.named<"model"::"blk.5.ffn_gate.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.5.ffn_up.weight = #stream.parameter.named<"model"::"blk.5.ffn_up.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.5.ffn_down.weight = #stream.parameter.named<"model"::"blk.5.ffn_down.weight"> : tensor<4096x14336xf16>
  util.global private @__auto.blk.6.attn_norm.weight = #stream.parameter.named<"model"::"blk.6.attn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.6.attn_q.weight = #stream.parameter.named<"model"::"blk.6.attn_q.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.6.attn_k.weight = #stream.parameter.named<"model"::"blk.6.attn_k.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.6.attn_v.weight = #stream.parameter.named<"model"::"blk.6.attn_v.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.6.attn_output.weight = #stream.parameter.named<"model"::"blk.6.attn_output.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.6.ffn_norm.weight = #stream.parameter.named<"model"::"blk.6.ffn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.6.ffn_gate.weight = #stream.parameter.named<"model"::"blk.6.ffn_gate.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.6.ffn_up.weight = #stream.parameter.named<"model"::"blk.6.ffn_up.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.6.ffn_down.weight = #stream.parameter.named<"model"::"blk.6.ffn_down.weight"> : tensor<4096x14336xf16>
  util.global private @__auto.blk.7.attn_norm.weight = #stream.parameter.named<"model"::"blk.7.attn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.7.attn_q.weight = #stream.parameter.named<"model"::"blk.7.attn_q.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.7.attn_k.weight = #stream.parameter.named<"model"::"blk.7.attn_k.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.7.attn_v.weight = #stream.parameter.named<"model"::"blk.7.attn_v.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.7.attn_output.weight = #stream.parameter.named<"model"::"blk.7.attn_output.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.7.ffn_norm.weight = #stream.parameter.named<"model"::"blk.7.ffn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.7.ffn_gate.weight = #stream.parameter.named<"model"::"blk.7.ffn_gate.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.7.ffn_up.weight = #stream.parameter.named<"model"::"blk.7.ffn_up.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.7.ffn_down.weight = #stream.parameter.named<"model"::"blk.7.ffn_down.weight"> : tensor<4096x14336xf16>
  util.global private @__auto.blk.8.attn_norm.weight = #stream.parameter.named<"model"::"blk.8.attn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.8.attn_q.weight = #stream.parameter.named<"model"::"blk.8.attn_q.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.8.attn_k.weight = #stream.parameter.named<"model"::"blk.8.attn_k.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.8.attn_v.weight = #stream.parameter.named<"model"::"blk.8.attn_v.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.8.attn_output.weight = #stream.parameter.named<"model"::"blk.8.attn_output.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.8.ffn_norm.weight = #stream.parameter.named<"model"::"blk.8.ffn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.8.ffn_gate.weight = #stream.parameter.named<"model"::"blk.8.ffn_gate.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.8.ffn_up.weight = #stream.parameter.named<"model"::"blk.8.ffn_up.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.8.ffn_down.weight = #stream.parameter.named<"model"::"blk.8.ffn_down.weight"> : tensor<4096x14336xf16>
  util.global private @__auto.blk.9.attn_norm.weight = #stream.parameter.named<"model"::"blk.9.attn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.9.attn_q.weight = #stream.parameter.named<"model"::"blk.9.attn_q.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.9.attn_k.weight = #stream.parameter.named<"model"::"blk.9.attn_k.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.9.attn_v.weight = #stream.parameter.named<"model"::"blk.9.attn_v.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.9.attn_output.weight = #stream.parameter.named<"model"::"blk.9.attn_output.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.9.ffn_norm.weight = #stream.parameter.named<"model"::"blk.9.ffn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.9.ffn_gate.weight = #stream.parameter.named<"model"::"blk.9.ffn_gate.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.9.ffn_up.weight = #stream.parameter.named<"model"::"blk.9.ffn_up.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.9.ffn_down.weight = #stream.parameter.named<"model"::"blk.9.ffn_down.weight"> : tensor<4096x14336xf16>
  util.global private @__auto.blk.10.attn_norm.weight = #stream.parameter.named<"model"::"blk.10.attn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.10.attn_q.weight = #stream.parameter.named<"model"::"blk.10.attn_q.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.10.attn_k.weight = #stream.parameter.named<"model"::"blk.10.attn_k.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.10.attn_v.weight = #stream.parameter.named<"model"::"blk.10.attn_v.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.10.attn_output.weight = #stream.parameter.named<"model"::"blk.10.attn_output.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.10.ffn_norm.weight = #stream.parameter.named<"model"::"blk.10.ffn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.10.ffn_gate.weight = #stream.parameter.named<"model"::"blk.10.ffn_gate.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.10.ffn_up.weight = #stream.parameter.named<"model"::"blk.10.ffn_up.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.10.ffn_down.weight = #stream.parameter.named<"model"::"blk.10.ffn_down.weight"> : tensor<4096x14336xf16>
  util.global private @__auto.blk.11.attn_norm.weight = #stream.parameter.named<"model"::"blk.11.attn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.11.attn_q.weight = #stream.parameter.named<"model"::"blk.11.attn_q.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.11.attn_k.weight = #stream.parameter.named<"model"::"blk.11.attn_k.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.11.attn_v.weight = #stream.parameter.named<"model"::"blk.11.attn_v.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.11.attn_output.weight = #stream.parameter.named<"model"::"blk.11.attn_output.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.11.ffn_norm.weight = #stream.parameter.named<"model"::"blk.11.ffn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.11.ffn_gate.weight = #stream.parameter.named<"model"::"blk.11.ffn_gate.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.11.ffn_up.weight = #stream.parameter.named<"model"::"blk.11.ffn_up.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.11.ffn_down.weight = #stream.parameter.named<"model"::"blk.11.ffn_down.weight"> : tensor<4096x14336xf16>
  util.global private @__auto.blk.12.attn_norm.weight = #stream.parameter.named<"model"::"blk.12.attn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.12.attn_q.weight = #stream.parameter.named<"model"::"blk.12.attn_q.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.12.attn_k.weight = #stream.parameter.named<"model"::"blk.12.attn_k.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.12.attn_v.weight = #stream.parameter.named<"model"::"blk.12.attn_v.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.12.attn_output.weight = #stream.parameter.named<"model"::"blk.12.attn_output.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.12.ffn_norm.weight = #stream.parameter.named<"model"::"blk.12.ffn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.12.ffn_gate.weight = #stream.parameter.named<"model"::"blk.12.ffn_gate.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.12.ffn_up.weight = #stream.parameter.named<"model"::"blk.12.ffn_up.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.12.ffn_down.weight = #stream.parameter.named<"model"::"blk.12.ffn_down.weight"> : tensor<4096x14336xf16>
  util.global private @__auto.blk.13.attn_norm.weight = #stream.parameter.named<"model"::"blk.13.attn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.13.attn_q.weight = #stream.parameter.named<"model"::"blk.13.attn_q.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.13.attn_k.weight = #stream.parameter.named<"model"::"blk.13.attn_k.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.13.attn_v.weight = #stream.parameter.named<"model"::"blk.13.attn_v.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.13.attn_output.weight = #stream.parameter.named<"model"::"blk.13.attn_output.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.13.ffn_norm.weight = #stream.parameter.named<"model"::"blk.13.ffn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.13.ffn_gate.weight = #stream.parameter.named<"model"::"blk.13.ffn_gate.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.13.ffn_up.weight = #stream.parameter.named<"model"::"blk.13.ffn_up.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.13.ffn_down.weight = #stream.parameter.named<"model"::"blk.13.ffn_down.weight"> : tensor<4096x14336xf16>
  util.global private @__auto.blk.14.attn_norm.weight = #stream.parameter.named<"model"::"blk.14.attn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.14.attn_q.weight = #stream.parameter.named<"model"::"blk.14.attn_q.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.14.attn_k.weight = #stream.parameter.named<"model"::"blk.14.attn_k.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.14.attn_v.weight = #stream.parameter.named<"model"::"blk.14.attn_v.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.14.attn_output.weight = #stream.parameter.named<"model"::"blk.14.attn_output.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.14.ffn_norm.weight = #stream.parameter.named<"model"::"blk.14.ffn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.14.ffn_gate.weight = #stream.parameter.named<"model"::"blk.14.ffn_gate.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.14.ffn_up.weight = #stream.parameter.named<"model"::"blk.14.ffn_up.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.14.ffn_down.weight = #stream.parameter.named<"model"::"blk.14.ffn_down.weight"> : tensor<4096x14336xf16>
  util.global private @__auto.blk.15.attn_norm.weight = #stream.parameter.named<"model"::"blk.15.attn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.15.attn_q.weight = #stream.parameter.named<"model"::"blk.15.attn_q.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.15.attn_k.weight = #stream.parameter.named<"model"::"blk.15.attn_k.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.15.attn_v.weight = #stream.parameter.named<"model"::"blk.15.attn_v.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.15.attn_output.weight = #stream.parameter.named<"model"::"blk.15.attn_output.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.15.ffn_norm.weight = #stream.parameter.named<"model"::"blk.15.ffn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.15.ffn_gate.weight = #stream.parameter.named<"model"::"blk.15.ffn_gate.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.15.ffn_up.weight = #stream.parameter.named<"model"::"blk.15.ffn_up.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.15.ffn_down.weight = #stream.parameter.named<"model"::"blk.15.ffn_down.weight"> : tensor<4096x14336xf16>
  util.global private @__auto.blk.16.attn_norm.weight = #stream.parameter.named<"model"::"blk.16.attn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.16.attn_q.weight = #stream.parameter.named<"model"::"blk.16.attn_q.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.16.attn_k.weight = #stream.parameter.named<"model"::"blk.16.attn_k.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.16.attn_v.weight = #stream.parameter.named<"model"::"blk.16.attn_v.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.16.attn_output.weight = #stream.parameter.named<"model"::"blk.16.attn_output.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.16.ffn_norm.weight = #stream.parameter.named<"model"::"blk.16.ffn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.16.ffn_gate.weight = #stream.parameter.named<"model"::"blk.16.ffn_gate.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.16.ffn_up.weight = #stream.parameter.named<"model"::"blk.16.ffn_up.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.16.ffn_down.weight = #stream.parameter.named<"model"::"blk.16.ffn_down.weight"> : tensor<4096x14336xf16>
  util.global private @__auto.blk.17.attn_norm.weight = #stream.parameter.named<"model"::"blk.17.attn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.17.attn_q.weight = #stream.parameter.named<"model"::"blk.17.attn_q.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.17.attn_k.weight = #stream.parameter.named<"model"::"blk.17.attn_k.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.17.attn_v.weight = #stream.parameter.named<"model"::"blk.17.attn_v.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.17.attn_output.weight = #stream.parameter.named<"model"::"blk.17.attn_output.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.17.ffn_norm.weight = #stream.parameter.named<"model"::"blk.17.ffn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.17.ffn_gate.weight = #stream.parameter.named<"model"::"blk.17.ffn_gate.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.17.ffn_up.weight = #stream.parameter.named<"model"::"blk.17.ffn_up.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.17.ffn_down.weight = #stream.parameter.named<"model"::"blk.17.ffn_down.weight"> : tensor<4096x14336xf16>
  util.global private @__auto.blk.18.attn_norm.weight = #stream.parameter.named<"model"::"blk.18.attn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.18.attn_q.weight = #stream.parameter.named<"model"::"blk.18.attn_q.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.18.attn_k.weight = #stream.parameter.named<"model"::"blk.18.attn_k.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.18.attn_v.weight = #stream.parameter.named<"model"::"blk.18.attn_v.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.18.attn_output.weight = #stream.parameter.named<"model"::"blk.18.attn_output.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.18.ffn_norm.weight = #stream.parameter.named<"model"::"blk.18.ffn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.18.ffn_gate.weight = #stream.parameter.named<"model"::"blk.18.ffn_gate.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.18.ffn_up.weight = #stream.parameter.named<"model"::"blk.18.ffn_up.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.18.ffn_down.weight = #stream.parameter.named<"model"::"blk.18.ffn_down.weight"> : tensor<4096x14336xf16>
  util.global private @__auto.blk.19.attn_norm.weight = #stream.parameter.named<"model"::"blk.19.attn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.19.attn_q.weight = #stream.parameter.named<"model"::"blk.19.attn_q.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.19.attn_k.weight = #stream.parameter.named<"model"::"blk.19.attn_k.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.19.attn_v.weight = #stream.parameter.named<"model"::"blk.19.attn_v.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.19.attn_output.weight = #stream.parameter.named<"model"::"blk.19.attn_output.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.19.ffn_norm.weight = #stream.parameter.named<"model"::"blk.19.ffn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.19.ffn_gate.weight = #stream.parameter.named<"model"::"blk.19.ffn_gate.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.19.ffn_up.weight = #stream.parameter.named<"model"::"blk.19.ffn_up.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.19.ffn_down.weight = #stream.parameter.named<"model"::"blk.19.ffn_down.weight"> : tensor<4096x14336xf16>
  util.global private @__auto.blk.20.attn_norm.weight = #stream.parameter.named<"model"::"blk.20.attn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.20.attn_q.weight = #stream.parameter.named<"model"::"blk.20.attn_q.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.20.attn_k.weight = #stream.parameter.named<"model"::"blk.20.attn_k.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.20.attn_v.weight = #stream.parameter.named<"model"::"blk.20.attn_v.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.20.attn_output.weight = #stream.parameter.named<"model"::"blk.20.attn_output.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.20.ffn_norm.weight = #stream.parameter.named<"model"::"blk.20.ffn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.20.ffn_gate.weight = #stream.parameter.named<"model"::"blk.20.ffn_gate.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.20.ffn_up.weight = #stream.parameter.named<"model"::"blk.20.ffn_up.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.20.ffn_down.weight = #stream.parameter.named<"model"::"blk.20.ffn_down.weight"> : tensor<4096x14336xf16>
  util.global private @__auto.blk.21.attn_norm.weight = #stream.parameter.named<"model"::"blk.21.attn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.21.attn_q.weight = #stream.parameter.named<"model"::"blk.21.attn_q.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.21.attn_k.weight = #stream.parameter.named<"model"::"blk.21.attn_k.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.21.attn_v.weight = #stream.parameter.named<"model"::"blk.21.attn_v.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.21.attn_output.weight = #stream.parameter.named<"model"::"blk.21.attn_output.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.21.ffn_norm.weight = #stream.parameter.named<"model"::"blk.21.ffn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.21.ffn_gate.weight = #stream.parameter.named<"model"::"blk.21.ffn_gate.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.21.ffn_up.weight = #stream.parameter.named<"model"::"blk.21.ffn_up.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.21.ffn_down.weight = #stream.parameter.named<"model"::"blk.21.ffn_down.weight"> : tensor<4096x14336xf16>
  util.global private @__auto.blk.22.attn_norm.weight = #stream.parameter.named<"model"::"blk.22.attn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.22.attn_q.weight = #stream.parameter.named<"model"::"blk.22.attn_q.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.22.attn_k.weight = #stream.parameter.named<"model"::"blk.22.attn_k.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.22.attn_v.weight = #stream.parameter.named<"model"::"blk.22.attn_v.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.22.attn_output.weight = #stream.parameter.named<"model"::"blk.22.attn_output.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.22.ffn_norm.weight = #stream.parameter.named<"model"::"blk.22.ffn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.22.ffn_gate.weight = #stream.parameter.named<"model"::"blk.22.ffn_gate.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.22.ffn_up.weight = #stream.parameter.named<"model"::"blk.22.ffn_up.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.22.ffn_down.weight = #stream.parameter.named<"model"::"blk.22.ffn_down.weight"> : tensor<4096x14336xf16>
  util.global private @__auto.blk.23.attn_norm.weight = #stream.parameter.named<"model"::"blk.23.attn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.23.attn_q.weight = #stream.parameter.named<"model"::"blk.23.attn_q.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.23.attn_k.weight = #stream.parameter.named<"model"::"blk.23.attn_k.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.23.attn_v.weight = #stream.parameter.named<"model"::"blk.23.attn_v.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.23.attn_output.weight = #stream.parameter.named<"model"::"blk.23.attn_output.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.23.ffn_norm.weight = #stream.parameter.named<"model"::"blk.23.ffn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.23.ffn_gate.weight = #stream.parameter.named<"model"::"blk.23.ffn_gate.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.23.ffn_up.weight = #stream.parameter.named<"model"::"blk.23.ffn_up.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.23.ffn_down.weight = #stream.parameter.named<"model"::"blk.23.ffn_down.weight"> : tensor<4096x14336xf16>
  util.global private @__auto.blk.24.attn_norm.weight = #stream.parameter.named<"model"::"blk.24.attn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.24.attn_q.weight = #stream.parameter.named<"model"::"blk.24.attn_q.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.24.attn_k.weight = #stream.parameter.named<"model"::"blk.24.attn_k.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.24.attn_v.weight = #stream.parameter.named<"model"::"blk.24.attn_v.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.24.attn_output.weight = #stream.parameter.named<"model"::"blk.24.attn_output.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.24.ffn_norm.weight = #stream.parameter.named<"model"::"blk.24.ffn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.24.ffn_gate.weight = #stream.parameter.named<"model"::"blk.24.ffn_gate.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.24.ffn_up.weight = #stream.parameter.named<"model"::"blk.24.ffn_up.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.24.ffn_down.weight = #stream.parameter.named<"model"::"blk.24.ffn_down.weight"> : tensor<4096x14336xf16>
  util.global private @__auto.blk.25.attn_norm.weight = #stream.parameter.named<"model"::"blk.25.attn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.25.attn_q.weight = #stream.parameter.named<"model"::"blk.25.attn_q.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.25.attn_k.weight = #stream.parameter.named<"model"::"blk.25.attn_k.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.25.attn_v.weight = #stream.parameter.named<"model"::"blk.25.attn_v.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.25.attn_output.weight = #stream.parameter.named<"model"::"blk.25.attn_output.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.25.ffn_norm.weight = #stream.parameter.named<"model"::"blk.25.ffn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.25.ffn_gate.weight = #stream.parameter.named<"model"::"blk.25.ffn_gate.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.25.ffn_up.weight = #stream.parameter.named<"model"::"blk.25.ffn_up.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.25.ffn_down.weight = #stream.parameter.named<"model"::"blk.25.ffn_down.weight"> : tensor<4096x14336xf16>
  util.global private @__auto.blk.26.attn_norm.weight = #stream.parameter.named<"model"::"blk.26.attn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.26.attn_q.weight = #stream.parameter.named<"model"::"blk.26.attn_q.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.26.attn_k.weight = #stream.parameter.named<"model"::"blk.26.attn_k.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.26.attn_v.weight = #stream.parameter.named<"model"::"blk.26.attn_v.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.26.attn_output.weight = #stream.parameter.named<"model"::"blk.26.attn_output.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.26.ffn_norm.weight = #stream.parameter.named<"model"::"blk.26.ffn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.26.ffn_gate.weight = #stream.parameter.named<"model"::"blk.26.ffn_gate.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.26.ffn_up.weight = #stream.parameter.named<"model"::"blk.26.ffn_up.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.26.ffn_down.weight = #stream.parameter.named<"model"::"blk.26.ffn_down.weight"> : tensor<4096x14336xf16>
  util.global private @__auto.blk.27.attn_norm.weight = #stream.parameter.named<"model"::"blk.27.attn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.27.attn_q.weight = #stream.parameter.named<"model"::"blk.27.attn_q.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.27.attn_k.weight = #stream.parameter.named<"model"::"blk.27.attn_k.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.27.attn_v.weight = #stream.parameter.named<"model"::"blk.27.attn_v.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.27.attn_output.weight = #stream.parameter.named<"model"::"blk.27.attn_output.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.27.ffn_norm.weight = #stream.parameter.named<"model"::"blk.27.ffn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.27.ffn_gate.weight = #stream.parameter.named<"model"::"blk.27.ffn_gate.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.27.ffn_up.weight = #stream.parameter.named<"model"::"blk.27.ffn_up.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.27.ffn_down.weight = #stream.parameter.named<"model"::"blk.27.ffn_down.weight"> : tensor<4096x14336xf16>
  util.global private @__auto.blk.28.attn_norm.weight = #stream.parameter.named<"model"::"blk.28.attn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.28.attn_q.weight = #stream.parameter.named<"model"::"blk.28.attn_q.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.28.attn_k.weight = #stream.parameter.named<"model"::"blk.28.attn_k.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.28.attn_v.weight = #stream.parameter.named<"model"::"blk.28.attn_v.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.28.attn_output.weight = #stream.parameter.named<"model"::"blk.28.attn_output.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.28.ffn_norm.weight = #stream.parameter.named<"model"::"blk.28.ffn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.28.ffn_gate.weight = #stream.parameter.named<"model"::"blk.28.ffn_gate.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.28.ffn_up.weight = #stream.parameter.named<"model"::"blk.28.ffn_up.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.28.ffn_down.weight = #stream.parameter.named<"model"::"blk.28.ffn_down.weight"> : tensor<4096x14336xf16>
  util.global private @__auto.blk.29.attn_norm.weight = #stream.parameter.named<"model"::"blk.29.attn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.29.attn_q.weight = #stream.parameter.named<"model"::"blk.29.attn_q.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.29.attn_k.weight = #stream.parameter.named<"model"::"blk.29.attn_k.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.29.attn_v.weight = #stream.parameter.named<"model"::"blk.29.attn_v.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.29.attn_output.weight = #stream.parameter.named<"model"::"blk.29.attn_output.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.29.ffn_norm.weight = #stream.parameter.named<"model"::"blk.29.ffn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.29.ffn_gate.weight = #stream.parameter.named<"model"::"blk.29.ffn_gate.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.29.ffn_up.weight = #stream.parameter.named<"model"::"blk.29.ffn_up.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.29.ffn_down.weight = #stream.parameter.named<"model"::"blk.29.ffn_down.weight"> : tensor<4096x14336xf16>
  util.global private @__auto.blk.30.attn_norm.weight = #stream.parameter.named<"model"::"blk.30.attn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.30.attn_q.weight = #stream.parameter.named<"model"::"blk.30.attn_q.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.30.attn_k.weight = #stream.parameter.named<"model"::"blk.30.attn_k.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.30.attn_v.weight = #stream.parameter.named<"model"::"blk.30.attn_v.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.30.attn_output.weight = #stream.parameter.named<"model"::"blk.30.attn_output.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.30.ffn_norm.weight = #stream.parameter.named<"model"::"blk.30.ffn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.30.ffn_gate.weight = #stream.parameter.named<"model"::"blk.30.ffn_gate.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.30.ffn_up.weight = #stream.parameter.named<"model"::"blk.30.ffn_up.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.30.ffn_down.weight = #stream.parameter.named<"model"::"blk.30.ffn_down.weight"> : tensor<4096x14336xf16>
  util.global private @__auto.blk.31.attn_norm.weight = #stream.parameter.named<"model"::"blk.31.attn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.31.attn_q.weight = #stream.parameter.named<"model"::"blk.31.attn_q.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.31.attn_k.weight = #stream.parameter.named<"model"::"blk.31.attn_k.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.31.attn_v.weight = #stream.parameter.named<"model"::"blk.31.attn_v.weight"> : tensor<1024x4096xf16>
  util.global private @__auto.blk.31.attn_output.weight = #stream.parameter.named<"model"::"blk.31.attn_output.weight"> : tensor<4096x4096xf16>
  util.global private @__auto.blk.31.ffn_norm.weight = #stream.parameter.named<"model"::"blk.31.ffn_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.blk.31.ffn_gate.weight = #stream.parameter.named<"model"::"blk.31.ffn_gate.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.31.ffn_up.weight = #stream.parameter.named<"model"::"blk.31.ffn_up.weight"> : tensor<14336x4096xf16>
  util.global private @__auto.blk.31.ffn_down.weight = #stream.parameter.named<"model"::"blk.31.ffn_down.weight"> : tensor<4096x14336xf16>
  util.global private @__auto.output_norm.weight = #stream.parameter.named<"model"::"output_norm.weight"> : tensor<4096xf32>
  util.global private @__auto.output.weight = #stream.parameter.named<"model"::"output.weight"> : tensor<128256x4096xf16>
  func.func @prefill_bs4(%arg0: !torch.vtensor<[4,?],si64>, %arg1: !torch.vtensor<[4],si64>, %arg2: !torch.vtensor<[4,?],si64>, %arg3: !torch.tensor<[?,2097152],f16>) -> !torch.vtensor<[4,?,128256],f16> attributes {torch.assume_strict_symbolic_shapes} {
    %__auto.token_embd.weight = util.global.load @__auto.token_embd.weight : tensor<128256x4096xf16>
    %0 = torch_c.from_builtin_tensor %__auto.token_embd.weight : tensor<128256x4096xf16> -> !torch.vtensor<[128256,4096],f16>
    %__auto.blk.0.attn_norm.weight = util.global.load @__auto.blk.0.attn_norm.weight : tensor<4096xf32>
    %1 = torch_c.from_builtin_tensor %__auto.blk.0.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.0.attn_q.weight = util.global.load @__auto.blk.0.attn_q.weight : tensor<4096x4096xf16>
    %2 = torch_c.from_builtin_tensor %__auto.blk.0.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.0.attn_k.weight = util.global.load @__auto.blk.0.attn_k.weight : tensor<1024x4096xf16>
    %3 = torch_c.from_builtin_tensor %__auto.blk.0.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.0.attn_v.weight = util.global.load @__auto.blk.0.attn_v.weight : tensor<1024x4096xf16>
    %4 = torch_c.from_builtin_tensor %__auto.blk.0.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.0.attn_output.weight = util.global.load @__auto.blk.0.attn_output.weight : tensor<4096x4096xf16>
    %5 = torch_c.from_builtin_tensor %__auto.blk.0.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.0.ffn_norm.weight = util.global.load @__auto.blk.0.ffn_norm.weight : tensor<4096xf32>
    %6 = torch_c.from_builtin_tensor %__auto.blk.0.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.0.ffn_gate.weight = util.global.load @__auto.blk.0.ffn_gate.weight : tensor<14336x4096xf16>
    %7 = torch_c.from_builtin_tensor %__auto.blk.0.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.0.ffn_up.weight = util.global.load @__auto.blk.0.ffn_up.weight : tensor<14336x4096xf16>
    %8 = torch_c.from_builtin_tensor %__auto.blk.0.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.0.ffn_down.weight = util.global.load @__auto.blk.0.ffn_down.weight : tensor<4096x14336xf16>
    %9 = torch_c.from_builtin_tensor %__auto.blk.0.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.1.attn_norm.weight = util.global.load @__auto.blk.1.attn_norm.weight : tensor<4096xf32>
    %10 = torch_c.from_builtin_tensor %__auto.blk.1.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.1.attn_q.weight = util.global.load @__auto.blk.1.attn_q.weight : tensor<4096x4096xf16>
    %11 = torch_c.from_builtin_tensor %__auto.blk.1.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.1.attn_k.weight = util.global.load @__auto.blk.1.attn_k.weight : tensor<1024x4096xf16>
    %12 = torch_c.from_builtin_tensor %__auto.blk.1.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.1.attn_v.weight = util.global.load @__auto.blk.1.attn_v.weight : tensor<1024x4096xf16>
    %13 = torch_c.from_builtin_tensor %__auto.blk.1.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.1.attn_output.weight = util.global.load @__auto.blk.1.attn_output.weight : tensor<4096x4096xf16>
    %14 = torch_c.from_builtin_tensor %__auto.blk.1.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.1.ffn_norm.weight = util.global.load @__auto.blk.1.ffn_norm.weight : tensor<4096xf32>
    %15 = torch_c.from_builtin_tensor %__auto.blk.1.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.1.ffn_gate.weight = util.global.load @__auto.blk.1.ffn_gate.weight : tensor<14336x4096xf16>
    %16 = torch_c.from_builtin_tensor %__auto.blk.1.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.1.ffn_up.weight = util.global.load @__auto.blk.1.ffn_up.weight : tensor<14336x4096xf16>
    %17 = torch_c.from_builtin_tensor %__auto.blk.1.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.1.ffn_down.weight = util.global.load @__auto.blk.1.ffn_down.weight : tensor<4096x14336xf16>
    %18 = torch_c.from_builtin_tensor %__auto.blk.1.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.2.attn_norm.weight = util.global.load @__auto.blk.2.attn_norm.weight : tensor<4096xf32>
    %19 = torch_c.from_builtin_tensor %__auto.blk.2.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.2.attn_q.weight = util.global.load @__auto.blk.2.attn_q.weight : tensor<4096x4096xf16>
    %20 = torch_c.from_builtin_tensor %__auto.blk.2.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.2.attn_k.weight = util.global.load @__auto.blk.2.attn_k.weight : tensor<1024x4096xf16>
    %21 = torch_c.from_builtin_tensor %__auto.blk.2.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.2.attn_v.weight = util.global.load @__auto.blk.2.attn_v.weight : tensor<1024x4096xf16>
    %22 = torch_c.from_builtin_tensor %__auto.blk.2.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.2.attn_output.weight = util.global.load @__auto.blk.2.attn_output.weight : tensor<4096x4096xf16>
    %23 = torch_c.from_builtin_tensor %__auto.blk.2.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.2.ffn_norm.weight = util.global.load @__auto.blk.2.ffn_norm.weight : tensor<4096xf32>
    %24 = torch_c.from_builtin_tensor %__auto.blk.2.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.2.ffn_gate.weight = util.global.load @__auto.blk.2.ffn_gate.weight : tensor<14336x4096xf16>
    %25 = torch_c.from_builtin_tensor %__auto.blk.2.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.2.ffn_up.weight = util.global.load @__auto.blk.2.ffn_up.weight : tensor<14336x4096xf16>
    %26 = torch_c.from_builtin_tensor %__auto.blk.2.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.2.ffn_down.weight = util.global.load @__auto.blk.2.ffn_down.weight : tensor<4096x14336xf16>
    %27 = torch_c.from_builtin_tensor %__auto.blk.2.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.3.attn_norm.weight = util.global.load @__auto.blk.3.attn_norm.weight : tensor<4096xf32>
    %28 = torch_c.from_builtin_tensor %__auto.blk.3.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.3.attn_q.weight = util.global.load @__auto.blk.3.attn_q.weight : tensor<4096x4096xf16>
    %29 = torch_c.from_builtin_tensor %__auto.blk.3.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.3.attn_k.weight = util.global.load @__auto.blk.3.attn_k.weight : tensor<1024x4096xf16>
    %30 = torch_c.from_builtin_tensor %__auto.blk.3.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.3.attn_v.weight = util.global.load @__auto.blk.3.attn_v.weight : tensor<1024x4096xf16>
    %31 = torch_c.from_builtin_tensor %__auto.blk.3.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.3.attn_output.weight = util.global.load @__auto.blk.3.attn_output.weight : tensor<4096x4096xf16>
    %32 = torch_c.from_builtin_tensor %__auto.blk.3.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.3.ffn_norm.weight = util.global.load @__auto.blk.3.ffn_norm.weight : tensor<4096xf32>
    %33 = torch_c.from_builtin_tensor %__auto.blk.3.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.3.ffn_gate.weight = util.global.load @__auto.blk.3.ffn_gate.weight : tensor<14336x4096xf16>
    %34 = torch_c.from_builtin_tensor %__auto.blk.3.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.3.ffn_up.weight = util.global.load @__auto.blk.3.ffn_up.weight : tensor<14336x4096xf16>
    %35 = torch_c.from_builtin_tensor %__auto.blk.3.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.3.ffn_down.weight = util.global.load @__auto.blk.3.ffn_down.weight : tensor<4096x14336xf16>
    %36 = torch_c.from_builtin_tensor %__auto.blk.3.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.4.attn_norm.weight = util.global.load @__auto.blk.4.attn_norm.weight : tensor<4096xf32>
    %37 = torch_c.from_builtin_tensor %__auto.blk.4.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.4.attn_q.weight = util.global.load @__auto.blk.4.attn_q.weight : tensor<4096x4096xf16>
    %38 = torch_c.from_builtin_tensor %__auto.blk.4.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.4.attn_k.weight = util.global.load @__auto.blk.4.attn_k.weight : tensor<1024x4096xf16>
    %39 = torch_c.from_builtin_tensor %__auto.blk.4.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.4.attn_v.weight = util.global.load @__auto.blk.4.attn_v.weight : tensor<1024x4096xf16>
    %40 = torch_c.from_builtin_tensor %__auto.blk.4.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.4.attn_output.weight = util.global.load @__auto.blk.4.attn_output.weight : tensor<4096x4096xf16>
    %41 = torch_c.from_builtin_tensor %__auto.blk.4.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.4.ffn_norm.weight = util.global.load @__auto.blk.4.ffn_norm.weight : tensor<4096xf32>
    %42 = torch_c.from_builtin_tensor %__auto.blk.4.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.4.ffn_gate.weight = util.global.load @__auto.blk.4.ffn_gate.weight : tensor<14336x4096xf16>
    %43 = torch_c.from_builtin_tensor %__auto.blk.4.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.4.ffn_up.weight = util.global.load @__auto.blk.4.ffn_up.weight : tensor<14336x4096xf16>
    %44 = torch_c.from_builtin_tensor %__auto.blk.4.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.4.ffn_down.weight = util.global.load @__auto.blk.4.ffn_down.weight : tensor<4096x14336xf16>
    %45 = torch_c.from_builtin_tensor %__auto.blk.4.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.5.attn_norm.weight = util.global.load @__auto.blk.5.attn_norm.weight : tensor<4096xf32>
    %46 = torch_c.from_builtin_tensor %__auto.blk.5.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.5.attn_q.weight = util.global.load @__auto.blk.5.attn_q.weight : tensor<4096x4096xf16>
    %47 = torch_c.from_builtin_tensor %__auto.blk.5.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.5.attn_k.weight = util.global.load @__auto.blk.5.attn_k.weight : tensor<1024x4096xf16>
    %48 = torch_c.from_builtin_tensor %__auto.blk.5.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.5.attn_v.weight = util.global.load @__auto.blk.5.attn_v.weight : tensor<1024x4096xf16>
    %49 = torch_c.from_builtin_tensor %__auto.blk.5.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.5.attn_output.weight = util.global.load @__auto.blk.5.attn_output.weight : tensor<4096x4096xf16>
    %50 = torch_c.from_builtin_tensor %__auto.blk.5.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.5.ffn_norm.weight = util.global.load @__auto.blk.5.ffn_norm.weight : tensor<4096xf32>
    %51 = torch_c.from_builtin_tensor %__auto.blk.5.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.5.ffn_gate.weight = util.global.load @__auto.blk.5.ffn_gate.weight : tensor<14336x4096xf16>
    %52 = torch_c.from_builtin_tensor %__auto.blk.5.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.5.ffn_up.weight = util.global.load @__auto.blk.5.ffn_up.weight : tensor<14336x4096xf16>
    %53 = torch_c.from_builtin_tensor %__auto.blk.5.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.5.ffn_down.weight = util.global.load @__auto.blk.5.ffn_down.weight : tensor<4096x14336xf16>
    %54 = torch_c.from_builtin_tensor %__auto.blk.5.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.6.attn_norm.weight = util.global.load @__auto.blk.6.attn_norm.weight : tensor<4096xf32>
    %55 = torch_c.from_builtin_tensor %__auto.blk.6.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.6.attn_q.weight = util.global.load @__auto.blk.6.attn_q.weight : tensor<4096x4096xf16>
    %56 = torch_c.from_builtin_tensor %__auto.blk.6.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.6.attn_k.weight = util.global.load @__auto.blk.6.attn_k.weight : tensor<1024x4096xf16>
    %57 = torch_c.from_builtin_tensor %__auto.blk.6.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.6.attn_v.weight = util.global.load @__auto.blk.6.attn_v.weight : tensor<1024x4096xf16>
    %58 = torch_c.from_builtin_tensor %__auto.blk.6.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.6.attn_output.weight = util.global.load @__auto.blk.6.attn_output.weight : tensor<4096x4096xf16>
    %59 = torch_c.from_builtin_tensor %__auto.blk.6.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.6.ffn_norm.weight = util.global.load @__auto.blk.6.ffn_norm.weight : tensor<4096xf32>
    %60 = torch_c.from_builtin_tensor %__auto.blk.6.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.6.ffn_gate.weight = util.global.load @__auto.blk.6.ffn_gate.weight : tensor<14336x4096xf16>
    %61 = torch_c.from_builtin_tensor %__auto.blk.6.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.6.ffn_up.weight = util.global.load @__auto.blk.6.ffn_up.weight : tensor<14336x4096xf16>
    %62 = torch_c.from_builtin_tensor %__auto.blk.6.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.6.ffn_down.weight = util.global.load @__auto.blk.6.ffn_down.weight : tensor<4096x14336xf16>
    %63 = torch_c.from_builtin_tensor %__auto.blk.6.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.7.attn_norm.weight = util.global.load @__auto.blk.7.attn_norm.weight : tensor<4096xf32>
    %64 = torch_c.from_builtin_tensor %__auto.blk.7.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.7.attn_q.weight = util.global.load @__auto.blk.7.attn_q.weight : tensor<4096x4096xf16>
    %65 = torch_c.from_builtin_tensor %__auto.blk.7.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.7.attn_k.weight = util.global.load @__auto.blk.7.attn_k.weight : tensor<1024x4096xf16>
    %66 = torch_c.from_builtin_tensor %__auto.blk.7.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.7.attn_v.weight = util.global.load @__auto.blk.7.attn_v.weight : tensor<1024x4096xf16>
    %67 = torch_c.from_builtin_tensor %__auto.blk.7.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.7.attn_output.weight = util.global.load @__auto.blk.7.attn_output.weight : tensor<4096x4096xf16>
    %68 = torch_c.from_builtin_tensor %__auto.blk.7.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.7.ffn_norm.weight = util.global.load @__auto.blk.7.ffn_norm.weight : tensor<4096xf32>
    %69 = torch_c.from_builtin_tensor %__auto.blk.7.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.7.ffn_gate.weight = util.global.load @__auto.blk.7.ffn_gate.weight : tensor<14336x4096xf16>
    %70 = torch_c.from_builtin_tensor %__auto.blk.7.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.7.ffn_up.weight = util.global.load @__auto.blk.7.ffn_up.weight : tensor<14336x4096xf16>
    %71 = torch_c.from_builtin_tensor %__auto.blk.7.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.7.ffn_down.weight = util.global.load @__auto.blk.7.ffn_down.weight : tensor<4096x14336xf16>
    %72 = torch_c.from_builtin_tensor %__auto.blk.7.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.8.attn_norm.weight = util.global.load @__auto.blk.8.attn_norm.weight : tensor<4096xf32>
    %73 = torch_c.from_builtin_tensor %__auto.blk.8.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.8.attn_q.weight = util.global.load @__auto.blk.8.attn_q.weight : tensor<4096x4096xf16>
    %74 = torch_c.from_builtin_tensor %__auto.blk.8.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.8.attn_k.weight = util.global.load @__auto.blk.8.attn_k.weight : tensor<1024x4096xf16>
    %75 = torch_c.from_builtin_tensor %__auto.blk.8.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.8.attn_v.weight = util.global.load @__auto.blk.8.attn_v.weight : tensor<1024x4096xf16>
    %76 = torch_c.from_builtin_tensor %__auto.blk.8.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.8.attn_output.weight = util.global.load @__auto.blk.8.attn_output.weight : tensor<4096x4096xf16>
    %77 = torch_c.from_builtin_tensor %__auto.blk.8.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.8.ffn_norm.weight = util.global.load @__auto.blk.8.ffn_norm.weight : tensor<4096xf32>
    %78 = torch_c.from_builtin_tensor %__auto.blk.8.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.8.ffn_gate.weight = util.global.load @__auto.blk.8.ffn_gate.weight : tensor<14336x4096xf16>
    %79 = torch_c.from_builtin_tensor %__auto.blk.8.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.8.ffn_up.weight = util.global.load @__auto.blk.8.ffn_up.weight : tensor<14336x4096xf16>
    %80 = torch_c.from_builtin_tensor %__auto.blk.8.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.8.ffn_down.weight = util.global.load @__auto.blk.8.ffn_down.weight : tensor<4096x14336xf16>
    %81 = torch_c.from_builtin_tensor %__auto.blk.8.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.9.attn_norm.weight = util.global.load @__auto.blk.9.attn_norm.weight : tensor<4096xf32>
    %82 = torch_c.from_builtin_tensor %__auto.blk.9.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.9.attn_q.weight = util.global.load @__auto.blk.9.attn_q.weight : tensor<4096x4096xf16>
    %83 = torch_c.from_builtin_tensor %__auto.blk.9.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.9.attn_k.weight = util.global.load @__auto.blk.9.attn_k.weight : tensor<1024x4096xf16>
    %84 = torch_c.from_builtin_tensor %__auto.blk.9.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.9.attn_v.weight = util.global.load @__auto.blk.9.attn_v.weight : tensor<1024x4096xf16>
    %85 = torch_c.from_builtin_tensor %__auto.blk.9.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.9.attn_output.weight = util.global.load @__auto.blk.9.attn_output.weight : tensor<4096x4096xf16>
    %86 = torch_c.from_builtin_tensor %__auto.blk.9.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.9.ffn_norm.weight = util.global.load @__auto.blk.9.ffn_norm.weight : tensor<4096xf32>
    %87 = torch_c.from_builtin_tensor %__auto.blk.9.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.9.ffn_gate.weight = util.global.load @__auto.blk.9.ffn_gate.weight : tensor<14336x4096xf16>
    %88 = torch_c.from_builtin_tensor %__auto.blk.9.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.9.ffn_up.weight = util.global.load @__auto.blk.9.ffn_up.weight : tensor<14336x4096xf16>
    %89 = torch_c.from_builtin_tensor %__auto.blk.9.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.9.ffn_down.weight = util.global.load @__auto.blk.9.ffn_down.weight : tensor<4096x14336xf16>
    %90 = torch_c.from_builtin_tensor %__auto.blk.9.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.10.attn_norm.weight = util.global.load @__auto.blk.10.attn_norm.weight : tensor<4096xf32>
    %91 = torch_c.from_builtin_tensor %__auto.blk.10.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.10.attn_q.weight = util.global.load @__auto.blk.10.attn_q.weight : tensor<4096x4096xf16>
    %92 = torch_c.from_builtin_tensor %__auto.blk.10.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.10.attn_k.weight = util.global.load @__auto.blk.10.attn_k.weight : tensor<1024x4096xf16>
    %93 = torch_c.from_builtin_tensor %__auto.blk.10.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.10.attn_v.weight = util.global.load @__auto.blk.10.attn_v.weight : tensor<1024x4096xf16>
    %94 = torch_c.from_builtin_tensor %__auto.blk.10.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.10.attn_output.weight = util.global.load @__auto.blk.10.attn_output.weight : tensor<4096x4096xf16>
    %95 = torch_c.from_builtin_tensor %__auto.blk.10.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.10.ffn_norm.weight = util.global.load @__auto.blk.10.ffn_norm.weight : tensor<4096xf32>
    %96 = torch_c.from_builtin_tensor %__auto.blk.10.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.10.ffn_gate.weight = util.global.load @__auto.blk.10.ffn_gate.weight : tensor<14336x4096xf16>
    %97 = torch_c.from_builtin_tensor %__auto.blk.10.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.10.ffn_up.weight = util.global.load @__auto.blk.10.ffn_up.weight : tensor<14336x4096xf16>
    %98 = torch_c.from_builtin_tensor %__auto.blk.10.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.10.ffn_down.weight = util.global.load @__auto.blk.10.ffn_down.weight : tensor<4096x14336xf16>
    %99 = torch_c.from_builtin_tensor %__auto.blk.10.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.11.attn_norm.weight = util.global.load @__auto.blk.11.attn_norm.weight : tensor<4096xf32>
    %100 = torch_c.from_builtin_tensor %__auto.blk.11.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.11.attn_q.weight = util.global.load @__auto.blk.11.attn_q.weight : tensor<4096x4096xf16>
    %101 = torch_c.from_builtin_tensor %__auto.blk.11.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.11.attn_k.weight = util.global.load @__auto.blk.11.attn_k.weight : tensor<1024x4096xf16>
    %102 = torch_c.from_builtin_tensor %__auto.blk.11.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.11.attn_v.weight = util.global.load @__auto.blk.11.attn_v.weight : tensor<1024x4096xf16>
    %103 = torch_c.from_builtin_tensor %__auto.blk.11.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.11.attn_output.weight = util.global.load @__auto.blk.11.attn_output.weight : tensor<4096x4096xf16>
    %104 = torch_c.from_builtin_tensor %__auto.blk.11.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.11.ffn_norm.weight = util.global.load @__auto.blk.11.ffn_norm.weight : tensor<4096xf32>
    %105 = torch_c.from_builtin_tensor %__auto.blk.11.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.11.ffn_gate.weight = util.global.load @__auto.blk.11.ffn_gate.weight : tensor<14336x4096xf16>
    %106 = torch_c.from_builtin_tensor %__auto.blk.11.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.11.ffn_up.weight = util.global.load @__auto.blk.11.ffn_up.weight : tensor<14336x4096xf16>
    %107 = torch_c.from_builtin_tensor %__auto.blk.11.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.11.ffn_down.weight = util.global.load @__auto.blk.11.ffn_down.weight : tensor<4096x14336xf16>
    %108 = torch_c.from_builtin_tensor %__auto.blk.11.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.12.attn_norm.weight = util.global.load @__auto.blk.12.attn_norm.weight : tensor<4096xf32>
    %109 = torch_c.from_builtin_tensor %__auto.blk.12.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.12.attn_q.weight = util.global.load @__auto.blk.12.attn_q.weight : tensor<4096x4096xf16>
    %110 = torch_c.from_builtin_tensor %__auto.blk.12.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.12.attn_k.weight = util.global.load @__auto.blk.12.attn_k.weight : tensor<1024x4096xf16>
    %111 = torch_c.from_builtin_tensor %__auto.blk.12.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.12.attn_v.weight = util.global.load @__auto.blk.12.attn_v.weight : tensor<1024x4096xf16>
    %112 = torch_c.from_builtin_tensor %__auto.blk.12.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.12.attn_output.weight = util.global.load @__auto.blk.12.attn_output.weight : tensor<4096x4096xf16>
    %113 = torch_c.from_builtin_tensor %__auto.blk.12.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.12.ffn_norm.weight = util.global.load @__auto.blk.12.ffn_norm.weight : tensor<4096xf32>
    %114 = torch_c.from_builtin_tensor %__auto.blk.12.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.12.ffn_gate.weight = util.global.load @__auto.blk.12.ffn_gate.weight : tensor<14336x4096xf16>
    %115 = torch_c.from_builtin_tensor %__auto.blk.12.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.12.ffn_up.weight = util.global.load @__auto.blk.12.ffn_up.weight : tensor<14336x4096xf16>
    %116 = torch_c.from_builtin_tensor %__auto.blk.12.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.12.ffn_down.weight = util.global.load @__auto.blk.12.ffn_down.weight : tensor<4096x14336xf16>
    %117 = torch_c.from_builtin_tensor %__auto.blk.12.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.13.attn_norm.weight = util.global.load @__auto.blk.13.attn_norm.weight : tensor<4096xf32>
    %118 = torch_c.from_builtin_tensor %__auto.blk.13.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.13.attn_q.weight = util.global.load @__auto.blk.13.attn_q.weight : tensor<4096x4096xf16>
    %119 = torch_c.from_builtin_tensor %__auto.blk.13.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.13.attn_k.weight = util.global.load @__auto.blk.13.attn_k.weight : tensor<1024x4096xf16>
    %120 = torch_c.from_builtin_tensor %__auto.blk.13.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.13.attn_v.weight = util.global.load @__auto.blk.13.attn_v.weight : tensor<1024x4096xf16>
    %121 = torch_c.from_builtin_tensor %__auto.blk.13.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.13.attn_output.weight = util.global.load @__auto.blk.13.attn_output.weight : tensor<4096x4096xf16>
    %122 = torch_c.from_builtin_tensor %__auto.blk.13.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.13.ffn_norm.weight = util.global.load @__auto.blk.13.ffn_norm.weight : tensor<4096xf32>
    %123 = torch_c.from_builtin_tensor %__auto.blk.13.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.13.ffn_gate.weight = util.global.load @__auto.blk.13.ffn_gate.weight : tensor<14336x4096xf16>
    %124 = torch_c.from_builtin_tensor %__auto.blk.13.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.13.ffn_up.weight = util.global.load @__auto.blk.13.ffn_up.weight : tensor<14336x4096xf16>
    %125 = torch_c.from_builtin_tensor %__auto.blk.13.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.13.ffn_down.weight = util.global.load @__auto.blk.13.ffn_down.weight : tensor<4096x14336xf16>
    %126 = torch_c.from_builtin_tensor %__auto.blk.13.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.14.attn_norm.weight = util.global.load @__auto.blk.14.attn_norm.weight : tensor<4096xf32>
    %127 = torch_c.from_builtin_tensor %__auto.blk.14.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.14.attn_q.weight = util.global.load @__auto.blk.14.attn_q.weight : tensor<4096x4096xf16>
    %128 = torch_c.from_builtin_tensor %__auto.blk.14.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.14.attn_k.weight = util.global.load @__auto.blk.14.attn_k.weight : tensor<1024x4096xf16>
    %129 = torch_c.from_builtin_tensor %__auto.blk.14.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.14.attn_v.weight = util.global.load @__auto.blk.14.attn_v.weight : tensor<1024x4096xf16>
    %130 = torch_c.from_builtin_tensor %__auto.blk.14.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.14.attn_output.weight = util.global.load @__auto.blk.14.attn_output.weight : tensor<4096x4096xf16>
    %131 = torch_c.from_builtin_tensor %__auto.blk.14.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.14.ffn_norm.weight = util.global.load @__auto.blk.14.ffn_norm.weight : tensor<4096xf32>
    %132 = torch_c.from_builtin_tensor %__auto.blk.14.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.14.ffn_gate.weight = util.global.load @__auto.blk.14.ffn_gate.weight : tensor<14336x4096xf16>
    %133 = torch_c.from_builtin_tensor %__auto.blk.14.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.14.ffn_up.weight = util.global.load @__auto.blk.14.ffn_up.weight : tensor<14336x4096xf16>
    %134 = torch_c.from_builtin_tensor %__auto.blk.14.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.14.ffn_down.weight = util.global.load @__auto.blk.14.ffn_down.weight : tensor<4096x14336xf16>
    %135 = torch_c.from_builtin_tensor %__auto.blk.14.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.15.attn_norm.weight = util.global.load @__auto.blk.15.attn_norm.weight : tensor<4096xf32>
    %136 = torch_c.from_builtin_tensor %__auto.blk.15.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.15.attn_q.weight = util.global.load @__auto.blk.15.attn_q.weight : tensor<4096x4096xf16>
    %137 = torch_c.from_builtin_tensor %__auto.blk.15.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.15.attn_k.weight = util.global.load @__auto.blk.15.attn_k.weight : tensor<1024x4096xf16>
    %138 = torch_c.from_builtin_tensor %__auto.blk.15.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.15.attn_v.weight = util.global.load @__auto.blk.15.attn_v.weight : tensor<1024x4096xf16>
    %139 = torch_c.from_builtin_tensor %__auto.blk.15.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.15.attn_output.weight = util.global.load @__auto.blk.15.attn_output.weight : tensor<4096x4096xf16>
    %140 = torch_c.from_builtin_tensor %__auto.blk.15.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.15.ffn_norm.weight = util.global.load @__auto.blk.15.ffn_norm.weight : tensor<4096xf32>
    %141 = torch_c.from_builtin_tensor %__auto.blk.15.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.15.ffn_gate.weight = util.global.load @__auto.blk.15.ffn_gate.weight : tensor<14336x4096xf16>
    %142 = torch_c.from_builtin_tensor %__auto.blk.15.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.15.ffn_up.weight = util.global.load @__auto.blk.15.ffn_up.weight : tensor<14336x4096xf16>
    %143 = torch_c.from_builtin_tensor %__auto.blk.15.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.15.ffn_down.weight = util.global.load @__auto.blk.15.ffn_down.weight : tensor<4096x14336xf16>
    %144 = torch_c.from_builtin_tensor %__auto.blk.15.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.16.attn_norm.weight = util.global.load @__auto.blk.16.attn_norm.weight : tensor<4096xf32>
    %145 = torch_c.from_builtin_tensor %__auto.blk.16.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.16.attn_q.weight = util.global.load @__auto.blk.16.attn_q.weight : tensor<4096x4096xf16>
    %146 = torch_c.from_builtin_tensor %__auto.blk.16.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.16.attn_k.weight = util.global.load @__auto.blk.16.attn_k.weight : tensor<1024x4096xf16>
    %147 = torch_c.from_builtin_tensor %__auto.blk.16.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.16.attn_v.weight = util.global.load @__auto.blk.16.attn_v.weight : tensor<1024x4096xf16>
    %148 = torch_c.from_builtin_tensor %__auto.blk.16.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.16.attn_output.weight = util.global.load @__auto.blk.16.attn_output.weight : tensor<4096x4096xf16>
    %149 = torch_c.from_builtin_tensor %__auto.blk.16.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.16.ffn_norm.weight = util.global.load @__auto.blk.16.ffn_norm.weight : tensor<4096xf32>
    %150 = torch_c.from_builtin_tensor %__auto.blk.16.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.16.ffn_gate.weight = util.global.load @__auto.blk.16.ffn_gate.weight : tensor<14336x4096xf16>
    %151 = torch_c.from_builtin_tensor %__auto.blk.16.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.16.ffn_up.weight = util.global.load @__auto.blk.16.ffn_up.weight : tensor<14336x4096xf16>
    %152 = torch_c.from_builtin_tensor %__auto.blk.16.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.16.ffn_down.weight = util.global.load @__auto.blk.16.ffn_down.weight : tensor<4096x14336xf16>
    %153 = torch_c.from_builtin_tensor %__auto.blk.16.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.17.attn_norm.weight = util.global.load @__auto.blk.17.attn_norm.weight : tensor<4096xf32>
    %154 = torch_c.from_builtin_tensor %__auto.blk.17.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.17.attn_q.weight = util.global.load @__auto.blk.17.attn_q.weight : tensor<4096x4096xf16>
    %155 = torch_c.from_builtin_tensor %__auto.blk.17.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.17.attn_k.weight = util.global.load @__auto.blk.17.attn_k.weight : tensor<1024x4096xf16>
    %156 = torch_c.from_builtin_tensor %__auto.blk.17.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.17.attn_v.weight = util.global.load @__auto.blk.17.attn_v.weight : tensor<1024x4096xf16>
    %157 = torch_c.from_builtin_tensor %__auto.blk.17.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.17.attn_output.weight = util.global.load @__auto.blk.17.attn_output.weight : tensor<4096x4096xf16>
    %158 = torch_c.from_builtin_tensor %__auto.blk.17.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.17.ffn_norm.weight = util.global.load @__auto.blk.17.ffn_norm.weight : tensor<4096xf32>
    %159 = torch_c.from_builtin_tensor %__auto.blk.17.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.17.ffn_gate.weight = util.global.load @__auto.blk.17.ffn_gate.weight : tensor<14336x4096xf16>
    %160 = torch_c.from_builtin_tensor %__auto.blk.17.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.17.ffn_up.weight = util.global.load @__auto.blk.17.ffn_up.weight : tensor<14336x4096xf16>
    %161 = torch_c.from_builtin_tensor %__auto.blk.17.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.17.ffn_down.weight = util.global.load @__auto.blk.17.ffn_down.weight : tensor<4096x14336xf16>
    %162 = torch_c.from_builtin_tensor %__auto.blk.17.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.18.attn_norm.weight = util.global.load @__auto.blk.18.attn_norm.weight : tensor<4096xf32>
    %163 = torch_c.from_builtin_tensor %__auto.blk.18.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.18.attn_q.weight = util.global.load @__auto.blk.18.attn_q.weight : tensor<4096x4096xf16>
    %164 = torch_c.from_builtin_tensor %__auto.blk.18.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.18.attn_k.weight = util.global.load @__auto.blk.18.attn_k.weight : tensor<1024x4096xf16>
    %165 = torch_c.from_builtin_tensor %__auto.blk.18.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.18.attn_v.weight = util.global.load @__auto.blk.18.attn_v.weight : tensor<1024x4096xf16>
    %166 = torch_c.from_builtin_tensor %__auto.blk.18.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.18.attn_output.weight = util.global.load @__auto.blk.18.attn_output.weight : tensor<4096x4096xf16>
    %167 = torch_c.from_builtin_tensor %__auto.blk.18.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.18.ffn_norm.weight = util.global.load @__auto.blk.18.ffn_norm.weight : tensor<4096xf32>
    %168 = torch_c.from_builtin_tensor %__auto.blk.18.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.18.ffn_gate.weight = util.global.load @__auto.blk.18.ffn_gate.weight : tensor<14336x4096xf16>
    %169 = torch_c.from_builtin_tensor %__auto.blk.18.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.18.ffn_up.weight = util.global.load @__auto.blk.18.ffn_up.weight : tensor<14336x4096xf16>
    %170 = torch_c.from_builtin_tensor %__auto.blk.18.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.18.ffn_down.weight = util.global.load @__auto.blk.18.ffn_down.weight : tensor<4096x14336xf16>
    %171 = torch_c.from_builtin_tensor %__auto.blk.18.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.19.attn_norm.weight = util.global.load @__auto.blk.19.attn_norm.weight : tensor<4096xf32>
    %172 = torch_c.from_builtin_tensor %__auto.blk.19.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.19.attn_q.weight = util.global.load @__auto.blk.19.attn_q.weight : tensor<4096x4096xf16>
    %173 = torch_c.from_builtin_tensor %__auto.blk.19.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.19.attn_k.weight = util.global.load @__auto.blk.19.attn_k.weight : tensor<1024x4096xf16>
    %174 = torch_c.from_builtin_tensor %__auto.blk.19.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.19.attn_v.weight = util.global.load @__auto.blk.19.attn_v.weight : tensor<1024x4096xf16>
    %175 = torch_c.from_builtin_tensor %__auto.blk.19.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.19.attn_output.weight = util.global.load @__auto.blk.19.attn_output.weight : tensor<4096x4096xf16>
    %176 = torch_c.from_builtin_tensor %__auto.blk.19.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.19.ffn_norm.weight = util.global.load @__auto.blk.19.ffn_norm.weight : tensor<4096xf32>
    %177 = torch_c.from_builtin_tensor %__auto.blk.19.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.19.ffn_gate.weight = util.global.load @__auto.blk.19.ffn_gate.weight : tensor<14336x4096xf16>
    %178 = torch_c.from_builtin_tensor %__auto.blk.19.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.19.ffn_up.weight = util.global.load @__auto.blk.19.ffn_up.weight : tensor<14336x4096xf16>
    %179 = torch_c.from_builtin_tensor %__auto.blk.19.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.19.ffn_down.weight = util.global.load @__auto.blk.19.ffn_down.weight : tensor<4096x14336xf16>
    %180 = torch_c.from_builtin_tensor %__auto.blk.19.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.20.attn_norm.weight = util.global.load @__auto.blk.20.attn_norm.weight : tensor<4096xf32>
    %181 = torch_c.from_builtin_tensor %__auto.blk.20.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.20.attn_q.weight = util.global.load @__auto.blk.20.attn_q.weight : tensor<4096x4096xf16>
    %182 = torch_c.from_builtin_tensor %__auto.blk.20.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.20.attn_k.weight = util.global.load @__auto.blk.20.attn_k.weight : tensor<1024x4096xf16>
    %183 = torch_c.from_builtin_tensor %__auto.blk.20.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.20.attn_v.weight = util.global.load @__auto.blk.20.attn_v.weight : tensor<1024x4096xf16>
    %184 = torch_c.from_builtin_tensor %__auto.blk.20.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.20.attn_output.weight = util.global.load @__auto.blk.20.attn_output.weight : tensor<4096x4096xf16>
    %185 = torch_c.from_builtin_tensor %__auto.blk.20.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.20.ffn_norm.weight = util.global.load @__auto.blk.20.ffn_norm.weight : tensor<4096xf32>
    %186 = torch_c.from_builtin_tensor %__auto.blk.20.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.20.ffn_gate.weight = util.global.load @__auto.blk.20.ffn_gate.weight : tensor<14336x4096xf16>
    %187 = torch_c.from_builtin_tensor %__auto.blk.20.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.20.ffn_up.weight = util.global.load @__auto.blk.20.ffn_up.weight : tensor<14336x4096xf16>
    %188 = torch_c.from_builtin_tensor %__auto.blk.20.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.20.ffn_down.weight = util.global.load @__auto.blk.20.ffn_down.weight : tensor<4096x14336xf16>
    %189 = torch_c.from_builtin_tensor %__auto.blk.20.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.21.attn_norm.weight = util.global.load @__auto.blk.21.attn_norm.weight : tensor<4096xf32>
    %190 = torch_c.from_builtin_tensor %__auto.blk.21.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.21.attn_q.weight = util.global.load @__auto.blk.21.attn_q.weight : tensor<4096x4096xf16>
    %191 = torch_c.from_builtin_tensor %__auto.blk.21.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.21.attn_k.weight = util.global.load @__auto.blk.21.attn_k.weight : tensor<1024x4096xf16>
    %192 = torch_c.from_builtin_tensor %__auto.blk.21.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.21.attn_v.weight = util.global.load @__auto.blk.21.attn_v.weight : tensor<1024x4096xf16>
    %193 = torch_c.from_builtin_tensor %__auto.blk.21.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.21.attn_output.weight = util.global.load @__auto.blk.21.attn_output.weight : tensor<4096x4096xf16>
    %194 = torch_c.from_builtin_tensor %__auto.blk.21.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.21.ffn_norm.weight = util.global.load @__auto.blk.21.ffn_norm.weight : tensor<4096xf32>
    %195 = torch_c.from_builtin_tensor %__auto.blk.21.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.21.ffn_gate.weight = util.global.load @__auto.blk.21.ffn_gate.weight : tensor<14336x4096xf16>
    %196 = torch_c.from_builtin_tensor %__auto.blk.21.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.21.ffn_up.weight = util.global.load @__auto.blk.21.ffn_up.weight : tensor<14336x4096xf16>
    %197 = torch_c.from_builtin_tensor %__auto.blk.21.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.21.ffn_down.weight = util.global.load @__auto.blk.21.ffn_down.weight : tensor<4096x14336xf16>
    %198 = torch_c.from_builtin_tensor %__auto.blk.21.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.22.attn_norm.weight = util.global.load @__auto.blk.22.attn_norm.weight : tensor<4096xf32>
    %199 = torch_c.from_builtin_tensor %__auto.blk.22.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.22.attn_q.weight = util.global.load @__auto.blk.22.attn_q.weight : tensor<4096x4096xf16>
    %200 = torch_c.from_builtin_tensor %__auto.blk.22.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.22.attn_k.weight = util.global.load @__auto.blk.22.attn_k.weight : tensor<1024x4096xf16>
    %201 = torch_c.from_builtin_tensor %__auto.blk.22.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.22.attn_v.weight = util.global.load @__auto.blk.22.attn_v.weight : tensor<1024x4096xf16>
    %202 = torch_c.from_builtin_tensor %__auto.blk.22.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.22.attn_output.weight = util.global.load @__auto.blk.22.attn_output.weight : tensor<4096x4096xf16>
    %203 = torch_c.from_builtin_tensor %__auto.blk.22.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.22.ffn_norm.weight = util.global.load @__auto.blk.22.ffn_norm.weight : tensor<4096xf32>
    %204 = torch_c.from_builtin_tensor %__auto.blk.22.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.22.ffn_gate.weight = util.global.load @__auto.blk.22.ffn_gate.weight : tensor<14336x4096xf16>
    %205 = torch_c.from_builtin_tensor %__auto.blk.22.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.22.ffn_up.weight = util.global.load @__auto.blk.22.ffn_up.weight : tensor<14336x4096xf16>
    %206 = torch_c.from_builtin_tensor %__auto.blk.22.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.22.ffn_down.weight = util.global.load @__auto.blk.22.ffn_down.weight : tensor<4096x14336xf16>
    %207 = torch_c.from_builtin_tensor %__auto.blk.22.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.23.attn_norm.weight = util.global.load @__auto.blk.23.attn_norm.weight : tensor<4096xf32>
    %208 = torch_c.from_builtin_tensor %__auto.blk.23.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.23.attn_q.weight = util.global.load @__auto.blk.23.attn_q.weight : tensor<4096x4096xf16>
    %209 = torch_c.from_builtin_tensor %__auto.blk.23.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.23.attn_k.weight = util.global.load @__auto.blk.23.attn_k.weight : tensor<1024x4096xf16>
    %210 = torch_c.from_builtin_tensor %__auto.blk.23.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.23.attn_v.weight = util.global.load @__auto.blk.23.attn_v.weight : tensor<1024x4096xf16>
    %211 = torch_c.from_builtin_tensor %__auto.blk.23.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.23.attn_output.weight = util.global.load @__auto.blk.23.attn_output.weight : tensor<4096x4096xf16>
    %212 = torch_c.from_builtin_tensor %__auto.blk.23.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.23.ffn_norm.weight = util.global.load @__auto.blk.23.ffn_norm.weight : tensor<4096xf32>
    %213 = torch_c.from_builtin_tensor %__auto.blk.23.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.23.ffn_gate.weight = util.global.load @__auto.blk.23.ffn_gate.weight : tensor<14336x4096xf16>
    %214 = torch_c.from_builtin_tensor %__auto.blk.23.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.23.ffn_up.weight = util.global.load @__auto.blk.23.ffn_up.weight : tensor<14336x4096xf16>
    %215 = torch_c.from_builtin_tensor %__auto.blk.23.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.23.ffn_down.weight = util.global.load @__auto.blk.23.ffn_down.weight : tensor<4096x14336xf16>
    %216 = torch_c.from_builtin_tensor %__auto.blk.23.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.24.attn_norm.weight = util.global.load @__auto.blk.24.attn_norm.weight : tensor<4096xf32>
    %217 = torch_c.from_builtin_tensor %__auto.blk.24.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.24.attn_q.weight = util.global.load @__auto.blk.24.attn_q.weight : tensor<4096x4096xf16>
    %218 = torch_c.from_builtin_tensor %__auto.blk.24.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.24.attn_k.weight = util.global.load @__auto.blk.24.attn_k.weight : tensor<1024x4096xf16>
    %219 = torch_c.from_builtin_tensor %__auto.blk.24.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.24.attn_v.weight = util.global.load @__auto.blk.24.attn_v.weight : tensor<1024x4096xf16>
    %220 = torch_c.from_builtin_tensor %__auto.blk.24.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.24.attn_output.weight = util.global.load @__auto.blk.24.attn_output.weight : tensor<4096x4096xf16>
    %221 = torch_c.from_builtin_tensor %__auto.blk.24.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.24.ffn_norm.weight = util.global.load @__auto.blk.24.ffn_norm.weight : tensor<4096xf32>
    %222 = torch_c.from_builtin_tensor %__auto.blk.24.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.24.ffn_gate.weight = util.global.load @__auto.blk.24.ffn_gate.weight : tensor<14336x4096xf16>
    %223 = torch_c.from_builtin_tensor %__auto.blk.24.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.24.ffn_up.weight = util.global.load @__auto.blk.24.ffn_up.weight : tensor<14336x4096xf16>
    %224 = torch_c.from_builtin_tensor %__auto.blk.24.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.24.ffn_down.weight = util.global.load @__auto.blk.24.ffn_down.weight : tensor<4096x14336xf16>
    %225 = torch_c.from_builtin_tensor %__auto.blk.24.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.25.attn_norm.weight = util.global.load @__auto.blk.25.attn_norm.weight : tensor<4096xf32>
    %226 = torch_c.from_builtin_tensor %__auto.blk.25.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.25.attn_q.weight = util.global.load @__auto.blk.25.attn_q.weight : tensor<4096x4096xf16>
    %227 = torch_c.from_builtin_tensor %__auto.blk.25.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.25.attn_k.weight = util.global.load @__auto.blk.25.attn_k.weight : tensor<1024x4096xf16>
    %228 = torch_c.from_builtin_tensor %__auto.blk.25.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.25.attn_v.weight = util.global.load @__auto.blk.25.attn_v.weight : tensor<1024x4096xf16>
    %229 = torch_c.from_builtin_tensor %__auto.blk.25.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.25.attn_output.weight = util.global.load @__auto.blk.25.attn_output.weight : tensor<4096x4096xf16>
    %230 = torch_c.from_builtin_tensor %__auto.blk.25.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.25.ffn_norm.weight = util.global.load @__auto.blk.25.ffn_norm.weight : tensor<4096xf32>
    %231 = torch_c.from_builtin_tensor %__auto.blk.25.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.25.ffn_gate.weight = util.global.load @__auto.blk.25.ffn_gate.weight : tensor<14336x4096xf16>
    %232 = torch_c.from_builtin_tensor %__auto.blk.25.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.25.ffn_up.weight = util.global.load @__auto.blk.25.ffn_up.weight : tensor<14336x4096xf16>
    %233 = torch_c.from_builtin_tensor %__auto.blk.25.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.25.ffn_down.weight = util.global.load @__auto.blk.25.ffn_down.weight : tensor<4096x14336xf16>
    %234 = torch_c.from_builtin_tensor %__auto.blk.25.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.26.attn_norm.weight = util.global.load @__auto.blk.26.attn_norm.weight : tensor<4096xf32>
    %235 = torch_c.from_builtin_tensor %__auto.blk.26.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.26.attn_q.weight = util.global.load @__auto.blk.26.attn_q.weight : tensor<4096x4096xf16>
    %236 = torch_c.from_builtin_tensor %__auto.blk.26.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.26.attn_k.weight = util.global.load @__auto.blk.26.attn_k.weight : tensor<1024x4096xf16>
    %237 = torch_c.from_builtin_tensor %__auto.blk.26.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.26.attn_v.weight = util.global.load @__auto.blk.26.attn_v.weight : tensor<1024x4096xf16>
    %238 = torch_c.from_builtin_tensor %__auto.blk.26.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.26.attn_output.weight = util.global.load @__auto.blk.26.attn_output.weight : tensor<4096x4096xf16>
    %239 = torch_c.from_builtin_tensor %__auto.blk.26.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.26.ffn_norm.weight = util.global.load @__auto.blk.26.ffn_norm.weight : tensor<4096xf32>
    %240 = torch_c.from_builtin_tensor %__auto.blk.26.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.26.ffn_gate.weight = util.global.load @__auto.blk.26.ffn_gate.weight : tensor<14336x4096xf16>
    %241 = torch_c.from_builtin_tensor %__auto.blk.26.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.26.ffn_up.weight = util.global.load @__auto.blk.26.ffn_up.weight : tensor<14336x4096xf16>
    %242 = torch_c.from_builtin_tensor %__auto.blk.26.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.26.ffn_down.weight = util.global.load @__auto.blk.26.ffn_down.weight : tensor<4096x14336xf16>
    %243 = torch_c.from_builtin_tensor %__auto.blk.26.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.27.attn_norm.weight = util.global.load @__auto.blk.27.attn_norm.weight : tensor<4096xf32>
    %244 = torch_c.from_builtin_tensor %__auto.blk.27.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.27.attn_q.weight = util.global.load @__auto.blk.27.attn_q.weight : tensor<4096x4096xf16>
    %245 = torch_c.from_builtin_tensor %__auto.blk.27.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.27.attn_k.weight = util.global.load @__auto.blk.27.attn_k.weight : tensor<1024x4096xf16>
    %246 = torch_c.from_builtin_tensor %__auto.blk.27.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.27.attn_v.weight = util.global.load @__auto.blk.27.attn_v.weight : tensor<1024x4096xf16>
    %247 = torch_c.from_builtin_tensor %__auto.blk.27.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.27.attn_output.weight = util.global.load @__auto.blk.27.attn_output.weight : tensor<4096x4096xf16>
    %248 = torch_c.from_builtin_tensor %__auto.blk.27.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.27.ffn_norm.weight = util.global.load @__auto.blk.27.ffn_norm.weight : tensor<4096xf32>
    %249 = torch_c.from_builtin_tensor %__auto.blk.27.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.27.ffn_gate.weight = util.global.load @__auto.blk.27.ffn_gate.weight : tensor<14336x4096xf16>
    %250 = torch_c.from_builtin_tensor %__auto.blk.27.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.27.ffn_up.weight = util.global.load @__auto.blk.27.ffn_up.weight : tensor<14336x4096xf16>
    %251 = torch_c.from_builtin_tensor %__auto.blk.27.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.27.ffn_down.weight = util.global.load @__auto.blk.27.ffn_down.weight : tensor<4096x14336xf16>
    %252 = torch_c.from_builtin_tensor %__auto.blk.27.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.28.attn_norm.weight = util.global.load @__auto.blk.28.attn_norm.weight : tensor<4096xf32>
    %253 = torch_c.from_builtin_tensor %__auto.blk.28.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.28.attn_q.weight = util.global.load @__auto.blk.28.attn_q.weight : tensor<4096x4096xf16>
    %254 = torch_c.from_builtin_tensor %__auto.blk.28.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.28.attn_k.weight = util.global.load @__auto.blk.28.attn_k.weight : tensor<1024x4096xf16>
    %255 = torch_c.from_builtin_tensor %__auto.blk.28.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.28.attn_v.weight = util.global.load @__auto.blk.28.attn_v.weight : tensor<1024x4096xf16>
    %256 = torch_c.from_builtin_tensor %__auto.blk.28.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.28.attn_output.weight = util.global.load @__auto.blk.28.attn_output.weight : tensor<4096x4096xf16>
    %257 = torch_c.from_builtin_tensor %__auto.blk.28.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.28.ffn_norm.weight = util.global.load @__auto.blk.28.ffn_norm.weight : tensor<4096xf32>
    %258 = torch_c.from_builtin_tensor %__auto.blk.28.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.28.ffn_gate.weight = util.global.load @__auto.blk.28.ffn_gate.weight : tensor<14336x4096xf16>
    %259 = torch_c.from_builtin_tensor %__auto.blk.28.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.28.ffn_up.weight = util.global.load @__auto.blk.28.ffn_up.weight : tensor<14336x4096xf16>
    %260 = torch_c.from_builtin_tensor %__auto.blk.28.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.28.ffn_down.weight = util.global.load @__auto.blk.28.ffn_down.weight : tensor<4096x14336xf16>
    %261 = torch_c.from_builtin_tensor %__auto.blk.28.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.29.attn_norm.weight = util.global.load @__auto.blk.29.attn_norm.weight : tensor<4096xf32>
    %262 = torch_c.from_builtin_tensor %__auto.blk.29.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.29.attn_q.weight = util.global.load @__auto.blk.29.attn_q.weight : tensor<4096x4096xf16>
    %263 = torch_c.from_builtin_tensor %__auto.blk.29.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.29.attn_k.weight = util.global.load @__auto.blk.29.attn_k.weight : tensor<1024x4096xf16>
    %264 = torch_c.from_builtin_tensor %__auto.blk.29.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.29.attn_v.weight = util.global.load @__auto.blk.29.attn_v.weight : tensor<1024x4096xf16>
    %265 = torch_c.from_builtin_tensor %__auto.blk.29.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.29.attn_output.weight = util.global.load @__auto.blk.29.attn_output.weight : tensor<4096x4096xf16>
    %266 = torch_c.from_builtin_tensor %__auto.blk.29.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.29.ffn_norm.weight = util.global.load @__auto.blk.29.ffn_norm.weight : tensor<4096xf32>
    %267 = torch_c.from_builtin_tensor %__auto.blk.29.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.29.ffn_gate.weight = util.global.load @__auto.blk.29.ffn_gate.weight : tensor<14336x4096xf16>
    %268 = torch_c.from_builtin_tensor %__auto.blk.29.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.29.ffn_up.weight = util.global.load @__auto.blk.29.ffn_up.weight : tensor<14336x4096xf16>
    %269 = torch_c.from_builtin_tensor %__auto.blk.29.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.29.ffn_down.weight = util.global.load @__auto.blk.29.ffn_down.weight : tensor<4096x14336xf16>
    %270 = torch_c.from_builtin_tensor %__auto.blk.29.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.30.attn_norm.weight = util.global.load @__auto.blk.30.attn_norm.weight : tensor<4096xf32>
    %271 = torch_c.from_builtin_tensor %__auto.blk.30.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.30.attn_q.weight = util.global.load @__auto.blk.30.attn_q.weight : tensor<4096x4096xf16>
    %272 = torch_c.from_builtin_tensor %__auto.blk.30.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.30.attn_k.weight = util.global.load @__auto.blk.30.attn_k.weight : tensor<1024x4096xf16>
    %273 = torch_c.from_builtin_tensor %__auto.blk.30.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.30.attn_v.weight = util.global.load @__auto.blk.30.attn_v.weight : tensor<1024x4096xf16>
    %274 = torch_c.from_builtin_tensor %__auto.blk.30.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.30.attn_output.weight = util.global.load @__auto.blk.30.attn_output.weight : tensor<4096x4096xf16>
    %275 = torch_c.from_builtin_tensor %__auto.blk.30.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.30.ffn_norm.weight = util.global.load @__auto.blk.30.ffn_norm.weight : tensor<4096xf32>
    %276 = torch_c.from_builtin_tensor %__auto.blk.30.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.30.ffn_gate.weight = util.global.load @__auto.blk.30.ffn_gate.weight : tensor<14336x4096xf16>
    %277 = torch_c.from_builtin_tensor %__auto.blk.30.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.30.ffn_up.weight = util.global.load @__auto.blk.30.ffn_up.weight : tensor<14336x4096xf16>
    %278 = torch_c.from_builtin_tensor %__auto.blk.30.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.30.ffn_down.weight = util.global.load @__auto.blk.30.ffn_down.weight : tensor<4096x14336xf16>
    %279 = torch_c.from_builtin_tensor %__auto.blk.30.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.31.attn_norm.weight = util.global.load @__auto.blk.31.attn_norm.weight : tensor<4096xf32>
    %280 = torch_c.from_builtin_tensor %__auto.blk.31.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.31.attn_q.weight = util.global.load @__auto.blk.31.attn_q.weight : tensor<4096x4096xf16>
    %281 = torch_c.from_builtin_tensor %__auto.blk.31.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.31.attn_k.weight = util.global.load @__auto.blk.31.attn_k.weight : tensor<1024x4096xf16>
    %282 = torch_c.from_builtin_tensor %__auto.blk.31.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.31.attn_v.weight = util.global.load @__auto.blk.31.attn_v.weight : tensor<1024x4096xf16>
    %283 = torch_c.from_builtin_tensor %__auto.blk.31.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.31.attn_output.weight = util.global.load @__auto.blk.31.attn_output.weight : tensor<4096x4096xf16>
    %284 = torch_c.from_builtin_tensor %__auto.blk.31.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.31.ffn_norm.weight = util.global.load @__auto.blk.31.ffn_norm.weight : tensor<4096xf32>
    %285 = torch_c.from_builtin_tensor %__auto.blk.31.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.31.ffn_gate.weight = util.global.load @__auto.blk.31.ffn_gate.weight : tensor<14336x4096xf16>
    %286 = torch_c.from_builtin_tensor %__auto.blk.31.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.31.ffn_up.weight = util.global.load @__auto.blk.31.ffn_up.weight : tensor<14336x4096xf16>
    %287 = torch_c.from_builtin_tensor %__auto.blk.31.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.31.ffn_down.weight = util.global.load @__auto.blk.31.ffn_down.weight : tensor<4096x14336xf16>
    %288 = torch_c.from_builtin_tensor %__auto.blk.31.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.output_norm.weight = util.global.load @__auto.output_norm.weight : tensor<4096xf32>
    %289 = torch_c.from_builtin_tensor %__auto.output_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.output.weight = util.global.load @__auto.output.weight : tensor<128256x4096xf16>
    %290 = torch_c.from_builtin_tensor %__auto.output.weight : tensor<128256x4096xf16> -> !torch.vtensor<[128256,4096],f16>
    %291 = torch.copy.to_vtensor %arg3 : !torch.vtensor<[?,2097152],f16>
    %292 = torch.symbolic_int "s1" {min_val = 2, max_val = 4095} : !torch.int
    %293 = torch.symbolic_int "s2" {min_val = 2, max_val = 9223372036854775806} : !torch.int
    torch.bind_symbolic_shape %arg0, [%292], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %arg2, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %291, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int-1 = torch.constant.int -1
    %false = torch.constant.bool false
    %false_0 = torch.constant.bool false
    %294 = torch.aten.embedding %0, %arg0, %int-1, %false, %false_0 : !torch.vtensor<[128256,4096],f16>, !torch.vtensor<[4,?],si64>, !torch.int, !torch.bool, !torch.bool -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %294, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6 = torch.constant.int 6
    %295 = torch.prims.convert_element_type %294, %int6 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %295, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2 = torch.constant.int 2
    %296 = torch.aten.pow.Tensor_Scalar %295, %int2 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %296, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_1 = torch.constant.int -1
    %297 = torch.prim.ListConstruct %int-1_1 : (!torch.int) -> !torch.list<int>
    %true = torch.constant.bool true
    %none = torch.constant.none
    %298 = torch.aten.mean.dim %296, %297, %true, %none : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %298, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06 = torch.constant.float 9.9999997473787516E-6
    %int1 = torch.constant.int 1
    %299 = torch.aten.add.Scalar %298, %float9.999990e-06, %int1 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %299, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %300 = torch.aten.rsqrt %299 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %300, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %301 = torch.aten.mul.Tensor %295, %300 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %301, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5 = torch.constant.int 5
    %302 = torch.prims.convert_element_type %301, %int5 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %302, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %303 = torch.aten.mul.Tensor %1, %302 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %303, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_2 = torch.constant.int 5
    %304 = torch.prims.convert_element_type %303, %int5_2 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %304, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2 = torch.constant.int -2
    %int-1_3 = torch.constant.int -1
    %305 = torch.aten.transpose.int %2, %int-2, %int-1_3 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int1_4 = torch.constant.int 1
    %306 = torch.aten.size.int %arg0, %int1_4 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.int
    %int4 = torch.constant.int 4
    %307 = torch.aten.mul.int %int4, %306 : !torch.int, !torch.int -> !torch.int
    %int4096 = torch.constant.int 4096
    %308 = torch.prim.ListConstruct %307, %int4096 : (!torch.int, !torch.int) -> !torch.list<int>
    %309 = torch.aten.view %304, %308 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %309, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %310 = torch.aten.mm %309, %305 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %310, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_5 = torch.constant.int 4
    %int4096_6 = torch.constant.int 4096
    %311 = torch.prim.ListConstruct %int4_5, %306, %int4096_6 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %312 = torch.aten.view %310, %311 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %312, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_7 = torch.constant.int -2
    %int-1_8 = torch.constant.int -1
    %313 = torch.aten.transpose.int %3, %int-2_7, %int-1_8 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_9 = torch.constant.int 4
    %314 = torch.aten.mul.int %int4_9, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_10 = torch.constant.int 4096
    %315 = torch.prim.ListConstruct %314, %int4096_10 : (!torch.int, !torch.int) -> !torch.list<int>
    %316 = torch.aten.view %304, %315 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %316, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %317 = torch.aten.mm %316, %313 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %317, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_11 = torch.constant.int 4
    %int1024 = torch.constant.int 1024
    %318 = torch.prim.ListConstruct %int4_11, %306, %int1024 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %319 = torch.aten.view %317, %318 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %319, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int-2_12 = torch.constant.int -2
    %int-1_13 = torch.constant.int -1
    %320 = torch.aten.transpose.int %4, %int-2_12, %int-1_13 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_14 = torch.constant.int 4
    %321 = torch.aten.mul.int %int4_14, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_15 = torch.constant.int 4096
    %322 = torch.prim.ListConstruct %321, %int4096_15 : (!torch.int, !torch.int) -> !torch.list<int>
    %323 = torch.aten.view %304, %322 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %323, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %324 = torch.aten.mm %323, %320 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %324, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_16 = torch.constant.int 4
    %int1024_17 = torch.constant.int 1024
    %325 = torch.prim.ListConstruct %int4_16, %306, %int1024_17 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %326 = torch.aten.view %324, %325 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %326, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int4_18 = torch.constant.int 4
    %int32 = torch.constant.int 32
    %int128 = torch.constant.int 128
    %327 = torch.prim.ListConstruct %int4_18, %306, %int32, %int128 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %328 = torch.aten.view %312, %327 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %328, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_19 = torch.constant.int 4
    %int8 = torch.constant.int 8
    %int128_20 = torch.constant.int 128
    %329 = torch.prim.ListConstruct %int4_19, %306, %int8, %int128_20 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %330 = torch.aten.view %319, %329 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %330, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int4_21 = torch.constant.int 4
    %int8_22 = torch.constant.int 8
    %int128_23 = torch.constant.int 128
    %331 = torch.prim.ListConstruct %int4_21, %306, %int8_22, %int128_23 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %332 = torch.aten.view %326, %331 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %332, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int131072 = torch.constant.int 131072
    %none_24 = torch.constant.none
    %none_25 = torch.constant.none
    %cpu = torch.constant.device "cpu"
    %false_26 = torch.constant.bool false
    %333 = torch.aten.arange %int131072, %none_24, %none_25, %cpu, %false_26 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0 = torch.constant.int 0
    %int128_27 = torch.constant.int 128
    %none_28 = torch.constant.none
    %none_29 = torch.constant.none
    %cpu_30 = torch.constant.device "cpu"
    %false_31 = torch.constant.bool false
    %334 = torch.aten.arange.start %int0, %int128_27, %none_28, %none_29, %cpu_30, %false_31 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_32 = torch.constant.int 2
    %335 = torch.aten.floor_divide.Scalar %334, %int2_32 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_33 = torch.constant.int 6
    %336 = torch.prims.convert_element_type %335, %int6_33 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_34 = torch.constant.int 128
    %337 = torch.aten.div.Scalar %336, %int128_34 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00 = torch.constant.float 2.000000e+00
    %338 = torch.aten.mul.Scalar %337, %float2.000000e00 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05 = torch.constant.float 5.000000e+05
    %339 = torch.aten.pow.Scalar %float5.000000e05, %338 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %340 = torch.aten.reciprocal %339 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00 = torch.constant.float 1.000000e+00
    %341 = torch.aten.mul.Scalar %340, %float1.000000e00 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_35 = torch.constant.int 1
    %342 = torch.aten.unsqueeze %333, %int1_35 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_36 = torch.constant.int 0
    %343 = torch.aten.unsqueeze %341, %int0_36 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %344 = torch.aten.mul.Tensor %342, %343 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_37 = torch.constant.int 1
    %345 = torch.aten.size.int %312, %int1_37 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.int
    %int0_38 = torch.constant.int 0
    %346 = torch.aten.add.int %int0_38, %345 : !torch.int, !torch.int -> !torch.int
    %int0_39 = torch.constant.int 0
    %int0_40 = torch.constant.int 0
    %int1_41 = torch.constant.int 1
    %347 = torch.aten.slice.Tensor %344, %int0_39, %int0_40, %346, %int1_41 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %347, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_42 = torch.constant.int 1
    %int0_43 = torch.constant.int 0
    %int9223372036854775807 = torch.constant.int 9223372036854775807
    %int1_44 = torch.constant.int 1
    %348 = torch.aten.slice.Tensor %347, %int1_42, %int0_43, %int9223372036854775807, %int1_44 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %348, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_45 = torch.constant.int 1
    %int0_46 = torch.constant.int 0
    %int9223372036854775807_47 = torch.constant.int 9223372036854775807
    %int1_48 = torch.constant.int 1
    %349 = torch.aten.slice.Tensor %348, %int1_45, %int0_46, %int9223372036854775807_47, %int1_48 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %349, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_49 = torch.constant.int 0
    %350 = torch.aten.unsqueeze %349, %int0_49 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %350, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_50 = torch.constant.int 1
    %int0_51 = torch.constant.int 0
    %int9223372036854775807_52 = torch.constant.int 9223372036854775807
    %int1_53 = torch.constant.int 1
    %351 = torch.aten.slice.Tensor %350, %int1_50, %int0_51, %int9223372036854775807_52, %int1_53 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %351, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_54 = torch.constant.int 2
    %int0_55 = torch.constant.int 0
    %int9223372036854775807_56 = torch.constant.int 9223372036854775807
    %int1_57 = torch.constant.int 1
    %352 = torch.aten.slice.Tensor %351, %int2_54, %int0_55, %int9223372036854775807_56, %int1_57 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %352, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_58 = torch.constant.int 4
    %int1_59 = torch.constant.int 1
    %int1_60 = torch.constant.int 1
    %353 = torch.prim.ListConstruct %int4_58, %int1_59, %int1_60 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %354 = torch.aten.repeat %352, %353 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %354, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_61 = torch.constant.int 6
    %355 = torch.prims.convert_element_type %328, %int6_61 : !torch.vtensor<[4,?,32,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %355, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %356 = torch_c.to_builtin_tensor %355 : !torch.vtensor<[4,?,32,128],f32> -> tensor<4x?x32x128xf32>
    %357 = torch_c.to_builtin_tensor %354 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %358 = util.call @sharktank_rotary_embedding_4_D_32_128_f32(%356, %357) : (tensor<4x?x32x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x32x128xf32>
    %359 = torch_c.from_builtin_tensor %358 : tensor<4x?x32x128xf32> -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %359, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %int5_62 = torch.constant.int 5
    %360 = torch.prims.convert_element_type %359, %int5_62 : !torch.vtensor<[4,?,32,128],f32>, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %360, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int131072_63 = torch.constant.int 131072
    %none_64 = torch.constant.none
    %none_65 = torch.constant.none
    %cpu_66 = torch.constant.device "cpu"
    %false_67 = torch.constant.bool false
    %361 = torch.aten.arange %int131072_63, %none_64, %none_65, %cpu_66, %false_67 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_68 = torch.constant.int 0
    %int128_69 = torch.constant.int 128
    %none_70 = torch.constant.none
    %none_71 = torch.constant.none
    %cpu_72 = torch.constant.device "cpu"
    %false_73 = torch.constant.bool false
    %362 = torch.aten.arange.start %int0_68, %int128_69, %none_70, %none_71, %cpu_72, %false_73 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_74 = torch.constant.int 2
    %363 = torch.aten.floor_divide.Scalar %362, %int2_74 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_75 = torch.constant.int 6
    %364 = torch.prims.convert_element_type %363, %int6_75 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_76 = torch.constant.int 128
    %365 = torch.aten.div.Scalar %364, %int128_76 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_77 = torch.constant.float 2.000000e+00
    %366 = torch.aten.mul.Scalar %365, %float2.000000e00_77 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_78 = torch.constant.float 5.000000e+05
    %367 = torch.aten.pow.Scalar %float5.000000e05_78, %366 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %368 = torch.aten.reciprocal %367 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_79 = torch.constant.float 1.000000e+00
    %369 = torch.aten.mul.Scalar %368, %float1.000000e00_79 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_80 = torch.constant.int 1
    %370 = torch.aten.unsqueeze %361, %int1_80 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_81 = torch.constant.int 0
    %371 = torch.aten.unsqueeze %369, %int0_81 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %372 = torch.aten.mul.Tensor %370, %371 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_82 = torch.constant.int 1
    %373 = torch.aten.size.int %319, %int1_82 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int0_83 = torch.constant.int 0
    %374 = torch.aten.add.int %int0_83, %373 : !torch.int, !torch.int -> !torch.int
    %int0_84 = torch.constant.int 0
    %int0_85 = torch.constant.int 0
    %int1_86 = torch.constant.int 1
    %375 = torch.aten.slice.Tensor %372, %int0_84, %int0_85, %374, %int1_86 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %375, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_87 = torch.constant.int 1
    %int0_88 = torch.constant.int 0
    %int9223372036854775807_89 = torch.constant.int 9223372036854775807
    %int1_90 = torch.constant.int 1
    %376 = torch.aten.slice.Tensor %375, %int1_87, %int0_88, %int9223372036854775807_89, %int1_90 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %376, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_91 = torch.constant.int 1
    %int0_92 = torch.constant.int 0
    %int9223372036854775807_93 = torch.constant.int 9223372036854775807
    %int1_94 = torch.constant.int 1
    %377 = torch.aten.slice.Tensor %376, %int1_91, %int0_92, %int9223372036854775807_93, %int1_94 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %377, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_95 = torch.constant.int 0
    %378 = torch.aten.unsqueeze %377, %int0_95 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %378, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_96 = torch.constant.int 1
    %int0_97 = torch.constant.int 0
    %int9223372036854775807_98 = torch.constant.int 9223372036854775807
    %int1_99 = torch.constant.int 1
    %379 = torch.aten.slice.Tensor %378, %int1_96, %int0_97, %int9223372036854775807_98, %int1_99 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %379, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_100 = torch.constant.int 2
    %int0_101 = torch.constant.int 0
    %int9223372036854775807_102 = torch.constant.int 9223372036854775807
    %int1_103 = torch.constant.int 1
    %380 = torch.aten.slice.Tensor %379, %int2_100, %int0_101, %int9223372036854775807_102, %int1_103 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %380, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_104 = torch.constant.int 4
    %int1_105 = torch.constant.int 1
    %int1_106 = torch.constant.int 1
    %381 = torch.prim.ListConstruct %int4_104, %int1_105, %int1_106 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %382 = torch.aten.repeat %380, %381 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %382, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_107 = torch.constant.int 6
    %383 = torch.prims.convert_element_type %330, %int6_107 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %383, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %384 = torch_c.to_builtin_tensor %383 : !torch.vtensor<[4,?,8,128],f32> -> tensor<4x?x8x128xf32>
    %385 = torch_c.to_builtin_tensor %382 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %386 = util.call @sharktank_rotary_embedding_4_D_8_128_f32(%384, %385) : (tensor<4x?x8x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x8x128xf32>
    %387 = torch_c.from_builtin_tensor %386 : tensor<4x?x8x128xf32> -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %387, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %int5_108 = torch.constant.int 5
    %388 = torch.prims.convert_element_type %387, %int5_108 : !torch.vtensor<[4,?,8,128],f32>, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %388, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_109 = torch.constant.int 0
    %389 = torch.aten.size.int %291, %int0_109 : !torch.vtensor<[?,2097152],f16>, !torch.int -> !torch.int
    %int32_110 = torch.constant.int 32
    %int2_111 = torch.constant.int 2
    %int32_112 = torch.constant.int 32
    %int8_113 = torch.constant.int 8
    %int128_114 = torch.constant.int 128
    %390 = torch.prim.ListConstruct %389, %int32_110, %int2_111, %int32_112, %int8_113, %int128_114 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %391 = torch.aten.view %291, %390 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %391, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_115 = torch.constant.int 32
    %392 = torch.aten.mul.int %389, %int32_115 : !torch.int, !torch.int -> !torch.int
    %int2_116 = torch.constant.int 2
    %393 = torch.aten.mul.int %392, %int2_116 : !torch.int, !torch.int -> !torch.int
    %int32_117 = torch.constant.int 32
    %int8_118 = torch.constant.int 8
    %int128_119 = torch.constant.int 128
    %394 = torch.prim.ListConstruct %393, %int32_117, %int8_118, %int128_119 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %395 = torch.aten.view %391, %394 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %395, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int64 = torch.constant.int 64
    %396 = torch.aten.mul.Scalar %arg2, %int64 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %396, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int0_120 = torch.constant.int 0
    %int1_121 = torch.constant.int 1
    %397 = torch.aten.add.Scalar %396, %int0_120, %int1_121 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %397, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_122 = torch.constant.int 1
    %398 = torch.aten.size.int %arg2, %int1_122 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.int
    %int4_123 = torch.constant.int 4
    %int32_124 = torch.constant.int 32
    %int8_125 = torch.constant.int 8
    %int128_126 = torch.constant.int 128
    %399 = torch.prim.ListConstruct %int4_123, %398, %int32_124, %int8_125, %int128_126 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %400 = torch.aten.view %388, %399 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %400, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_127 = torch.constant.int 4
    %401 = torch.aten.mul.int %int4_127, %398 : !torch.int, !torch.int -> !torch.int
    %int32_128 = torch.constant.int 32
    %int8_129 = torch.constant.int 8
    %int128_130 = torch.constant.int 128
    %402 = torch.prim.ListConstruct %401, %int32_128, %int8_129, %int128_130 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %403 = torch.aten.view %400, %402 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %403, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_131 = torch.constant.int 4
    %404 = torch.aten.mul.int %int4_131, %398 : !torch.int, !torch.int -> !torch.int
    %405 = torch.prim.ListConstruct %404 : (!torch.int) -> !torch.list<int>
    %406 = torch.aten.view %397, %405 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %406, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %407 = torch.prim.ListConstruct %406 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_132 = torch.constant.bool false
    %408 = torch.aten.index_put %395, %407, %403, %false_132 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %408, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_133 = torch.constant.int 32
    %int2_134 = torch.constant.int 2
    %int32_135 = torch.constant.int 32
    %int8_136 = torch.constant.int 8
    %int128_137 = torch.constant.int 128
    %409 = torch.prim.ListConstruct %389, %int32_133, %int2_134, %int32_135, %int8_136, %int128_137 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %410 = torch.aten.view %408, %409 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %410, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152 = torch.constant.int 2097152
    %411 = torch.prim.ListConstruct %389, %int2097152 : (!torch.int, !torch.int) -> !torch.list<int>
    %412 = torch.aten.view %410, %411 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %412, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_138 = torch.constant.int 32
    %int2_139 = torch.constant.int 2
    %int32_140 = torch.constant.int 32
    %int8_141 = torch.constant.int 8
    %int128_142 = torch.constant.int 128
    %413 = torch.prim.ListConstruct %389, %int32_138, %int2_139, %int32_140, %int8_141, %int128_142 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %414 = torch.aten.view %412, %413 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %414, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_143 = torch.constant.int 32
    %int8_144 = torch.constant.int 8
    %int128_145 = torch.constant.int 128
    %415 = torch.prim.ListConstruct %393, %int32_143, %int8_144, %int128_145 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %416 = torch.aten.view %414, %415 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %416, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_146 = torch.constant.int 4
    %int32_147 = torch.constant.int 32
    %int8_148 = torch.constant.int 8
    %int128_149 = torch.constant.int 128
    %417 = torch.prim.ListConstruct %int4_146, %398, %int32_147, %int8_148, %int128_149 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %418 = torch.aten.view %332, %417 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %418, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_150 = torch.constant.int 4
    %419 = torch.aten.mul.int %int4_150, %398 : !torch.int, !torch.int -> !torch.int
    %int32_151 = torch.constant.int 32
    %int8_152 = torch.constant.int 8
    %int128_153 = torch.constant.int 128
    %420 = torch.prim.ListConstruct %419, %int32_151, %int8_152, %int128_153 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %421 = torch.aten.view %418, %420 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %421, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int1_154 = torch.constant.int 1
    %int1_155 = torch.constant.int 1
    %422 = torch.aten.add.Scalar %397, %int1_154, %int1_155 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %422, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_156 = torch.constant.int 4
    %423 = torch.aten.mul.int %int4_156, %398 : !torch.int, !torch.int -> !torch.int
    %424 = torch.prim.ListConstruct %423 : (!torch.int) -> !torch.list<int>
    %425 = torch.aten.view %422, %424 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %425, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %426 = torch.prim.ListConstruct %425 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_157 = torch.constant.bool false
    %427 = torch.aten.index_put %416, %426, %421, %false_157 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %427, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_158 = torch.constant.int 32
    %int2_159 = torch.constant.int 2
    %int32_160 = torch.constant.int 32
    %int8_161 = torch.constant.int 8
    %int128_162 = torch.constant.int 128
    %428 = torch.prim.ListConstruct %389, %int32_158, %int2_159, %int32_160, %int8_161, %int128_162 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %429 = torch.aten.view %427, %428 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %429, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_163 = torch.constant.int 2097152
    %430 = torch.prim.ListConstruct %389, %int2097152_163 : (!torch.int, !torch.int) -> !torch.list<int>
    %431 = torch.aten.view %429, %430 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %431, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int-2_164 = torch.constant.int -2
    %432 = torch.aten.unsqueeze %388, %int-2_164 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %432, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int4_165 = torch.constant.int 4
    %int8_166 = torch.constant.int 8
    %int4_167 = torch.constant.int 4
    %int128_168 = torch.constant.int 128
    %433 = torch.prim.ListConstruct %int4_165, %373, %int8_166, %int4_167, %int128_168 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_169 = torch.constant.bool false
    %434 = torch.aten.expand %432, %433, %false_169 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %434, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_170 = torch.constant.int 0
    %435 = torch.aten.clone %434, %int0_170 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %435, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_171 = torch.constant.int 4
    %int32_172 = torch.constant.int 32
    %int128_173 = torch.constant.int 128
    %436 = torch.prim.ListConstruct %int4_171, %373, %int32_172, %int128_173 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %437 = torch.aten._unsafe_view %435, %436 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %437, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_174 = torch.constant.int -2
    %438 = torch.aten.unsqueeze %332, %int-2_174 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %438, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_175 = torch.constant.int 1
    %439 = torch.aten.size.int %326, %int1_175 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int4_176 = torch.constant.int 4
    %int8_177 = torch.constant.int 8
    %int4_178 = torch.constant.int 4
    %int128_179 = torch.constant.int 128
    %440 = torch.prim.ListConstruct %int4_176, %439, %int8_177, %int4_178, %int128_179 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_180 = torch.constant.bool false
    %441 = torch.aten.expand %438, %440, %false_180 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %441, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_181 = torch.constant.int 0
    %442 = torch.aten.clone %441, %int0_181 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %442, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_182 = torch.constant.int 4
    %int32_183 = torch.constant.int 32
    %int128_184 = torch.constant.int 128
    %443 = torch.prim.ListConstruct %int4_182, %439, %int32_183, %int128_184 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %444 = torch.aten._unsafe_view %442, %443 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %444, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_185 = torch.constant.int 1
    %int2_186 = torch.constant.int 2
    %445 = torch.aten.transpose.int %360, %int1_185, %int2_186 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %445, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_187 = torch.constant.int 1
    %int2_188 = torch.constant.int 2
    %446 = torch.aten.transpose.int %437, %int1_187, %int2_188 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %446, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_189 = torch.constant.int 1
    %int2_190 = torch.constant.int 2
    %447 = torch.aten.transpose.int %444, %int1_189, %int2_190 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %447, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00 = torch.constant.float 0.000000e+00
    %true_191 = torch.constant.bool true
    %none_192 = torch.constant.none
    %none_193 = torch.constant.none
    %448:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%445, %446, %447, %float0.000000e00, %true_191, %none_192, %none_193) : (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?],f32>) 
    torch.bind_symbolic_shape %448#0, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_194 = torch.constant.int 1
    %int2_195 = torch.constant.int 2
    %449 = torch.aten.transpose.int %448#0, %int1_194, %int2_195 : !torch.vtensor<[4,32,?,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %449, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_196 = torch.constant.int 4
    %int4096_197 = torch.constant.int 4096
    %450 = torch.prim.ListConstruct %int4_196, %345, %int4096_197 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %451 = torch.aten.view %449, %450 : !torch.vtensor<[4,?,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %451, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_198 = torch.constant.int -2
    %int-1_199 = torch.constant.int -1
    %452 = torch.aten.transpose.int %5, %int-2_198, %int-1_199 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_200 = torch.constant.int 4
    %453 = torch.aten.mul.int %int4_200, %345 : !torch.int, !torch.int -> !torch.int
    %int4096_201 = torch.constant.int 4096
    %454 = torch.prim.ListConstruct %453, %int4096_201 : (!torch.int, !torch.int) -> !torch.list<int>
    %455 = torch.aten.view %451, %454 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %455, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %456 = torch.aten.mm %455, %452 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %456, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_202 = torch.constant.int 4
    %int4096_203 = torch.constant.int 4096
    %457 = torch.prim.ListConstruct %int4_202, %345, %int4096_203 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %458 = torch.aten.view %456, %457 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %458, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_204 = torch.constant.int 1
    %459 = torch.aten.add.Tensor %294, %458, %int1_204 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %459, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_205 = torch.constant.int 6
    %460 = torch.prims.convert_element_type %459, %int6_205 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %460, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_206 = torch.constant.int 2
    %461 = torch.aten.pow.Tensor_Scalar %460, %int2_206 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %461, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_207 = torch.constant.int -1
    %462 = torch.prim.ListConstruct %int-1_207 : (!torch.int) -> !torch.list<int>
    %true_208 = torch.constant.bool true
    %none_209 = torch.constant.none
    %463 = torch.aten.mean.dim %461, %462, %true_208, %none_209 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %463, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_210 = torch.constant.float 9.9999997473787516E-6
    %int1_211 = torch.constant.int 1
    %464 = torch.aten.add.Scalar %463, %float9.999990e-06_210, %int1_211 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %464, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %465 = torch.aten.rsqrt %464 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %465, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %466 = torch.aten.mul.Tensor %460, %465 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %466, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_212 = torch.constant.int 5
    %467 = torch.prims.convert_element_type %466, %int5_212 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %467, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %468 = torch.aten.mul.Tensor %6, %467 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %468, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_213 = torch.constant.int 5
    %469 = torch.prims.convert_element_type %468, %int5_213 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %469, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_214 = torch.constant.int -2
    %int-1_215 = torch.constant.int -1
    %470 = torch.aten.transpose.int %7, %int-2_214, %int-1_215 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_216 = torch.constant.int 4
    %471 = torch.aten.mul.int %int4_216, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_217 = torch.constant.int 4096
    %472 = torch.prim.ListConstruct %471, %int4096_217 : (!torch.int, !torch.int) -> !torch.list<int>
    %473 = torch.aten.view %469, %472 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %473, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %474 = torch.aten.mm %473, %470 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %474, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_218 = torch.constant.int 4
    %int14336 = torch.constant.int 14336
    %475 = torch.prim.ListConstruct %int4_218, %306, %int14336 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %476 = torch.aten.view %474, %475 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %476, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %477 = torch.aten.silu %476 : !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %477, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_219 = torch.constant.int -2
    %int-1_220 = torch.constant.int -1
    %478 = torch.aten.transpose.int %8, %int-2_219, %int-1_220 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_221 = torch.constant.int 4
    %479 = torch.aten.mul.int %int4_221, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_222 = torch.constant.int 4096
    %480 = torch.prim.ListConstruct %479, %int4096_222 : (!torch.int, !torch.int) -> !torch.list<int>
    %481 = torch.aten.view %469, %480 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %481, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %482 = torch.aten.mm %481, %478 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %482, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_223 = torch.constant.int 4
    %int14336_224 = torch.constant.int 14336
    %483 = torch.prim.ListConstruct %int4_223, %306, %int14336_224 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %484 = torch.aten.view %482, %483 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %484, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %485 = torch.aten.mul.Tensor %477, %484 : !torch.vtensor<[4,?,14336],f16>, !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %485, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_225 = torch.constant.int -2
    %int-1_226 = torch.constant.int -1
    %486 = torch.aten.transpose.int %9, %int-2_225, %int-1_226 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int1_227 = torch.constant.int 1
    %487 = torch.aten.size.int %476, %int1_227 : !torch.vtensor<[4,?,14336],f16>, !torch.int -> !torch.int
    %int4_228 = torch.constant.int 4
    %488 = torch.aten.mul.int %int4_228, %487 : !torch.int, !torch.int -> !torch.int
    %int14336_229 = torch.constant.int 14336
    %489 = torch.prim.ListConstruct %488, %int14336_229 : (!torch.int, !torch.int) -> !torch.list<int>
    %490 = torch.aten.view %485, %489 : !torch.vtensor<[4,?,14336],f16>, !torch.list<int> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %490, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %491 = torch.aten.mm %490, %486 : !torch.vtensor<[?,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %491, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_230 = torch.constant.int 4
    %int4096_231 = torch.constant.int 4096
    %492 = torch.prim.ListConstruct %int4_230, %487, %int4096_231 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %493 = torch.aten.view %491, %492 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %493, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_232 = torch.constant.int 1
    %494 = torch.aten.add.Tensor %459, %493, %int1_232 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %494, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_233 = torch.constant.int 6
    %495 = torch.prims.convert_element_type %494, %int6_233 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %495, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_234 = torch.constant.int 2
    %496 = torch.aten.pow.Tensor_Scalar %495, %int2_234 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %496, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_235 = torch.constant.int -1
    %497 = torch.prim.ListConstruct %int-1_235 : (!torch.int) -> !torch.list<int>
    %true_236 = torch.constant.bool true
    %none_237 = torch.constant.none
    %498 = torch.aten.mean.dim %496, %497, %true_236, %none_237 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %498, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_238 = torch.constant.float 9.9999997473787516E-6
    %int1_239 = torch.constant.int 1
    %499 = torch.aten.add.Scalar %498, %float9.999990e-06_238, %int1_239 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %499, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %500 = torch.aten.rsqrt %499 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %500, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %501 = torch.aten.mul.Tensor %495, %500 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %501, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_240 = torch.constant.int 5
    %502 = torch.prims.convert_element_type %501, %int5_240 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %502, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %503 = torch.aten.mul.Tensor %10, %502 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %503, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_241 = torch.constant.int 5
    %504 = torch.prims.convert_element_type %503, %int5_241 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %504, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_242 = torch.constant.int -2
    %int-1_243 = torch.constant.int -1
    %505 = torch.aten.transpose.int %11, %int-2_242, %int-1_243 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_244 = torch.constant.int 4
    %506 = torch.aten.mul.int %int4_244, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_245 = torch.constant.int 4096
    %507 = torch.prim.ListConstruct %506, %int4096_245 : (!torch.int, !torch.int) -> !torch.list<int>
    %508 = torch.aten.view %504, %507 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %508, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %509 = torch.aten.mm %508, %505 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %509, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_246 = torch.constant.int 4
    %int4096_247 = torch.constant.int 4096
    %510 = torch.prim.ListConstruct %int4_246, %306, %int4096_247 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %511 = torch.aten.view %509, %510 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %511, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_248 = torch.constant.int -2
    %int-1_249 = torch.constant.int -1
    %512 = torch.aten.transpose.int %12, %int-2_248, %int-1_249 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_250 = torch.constant.int 4
    %513 = torch.aten.mul.int %int4_250, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_251 = torch.constant.int 4096
    %514 = torch.prim.ListConstruct %513, %int4096_251 : (!torch.int, !torch.int) -> !torch.list<int>
    %515 = torch.aten.view %504, %514 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %515, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %516 = torch.aten.mm %515, %512 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %516, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_252 = torch.constant.int 4
    %int1024_253 = torch.constant.int 1024
    %517 = torch.prim.ListConstruct %int4_252, %306, %int1024_253 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %518 = torch.aten.view %516, %517 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %518, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int-2_254 = torch.constant.int -2
    %int-1_255 = torch.constant.int -1
    %519 = torch.aten.transpose.int %13, %int-2_254, %int-1_255 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_256 = torch.constant.int 4
    %520 = torch.aten.mul.int %int4_256, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_257 = torch.constant.int 4096
    %521 = torch.prim.ListConstruct %520, %int4096_257 : (!torch.int, !torch.int) -> !torch.list<int>
    %522 = torch.aten.view %504, %521 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %522, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %523 = torch.aten.mm %522, %519 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %523, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_258 = torch.constant.int 4
    %int1024_259 = torch.constant.int 1024
    %524 = torch.prim.ListConstruct %int4_258, %306, %int1024_259 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %525 = torch.aten.view %523, %524 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %525, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int4_260 = torch.constant.int 4
    %int32_261 = torch.constant.int 32
    %int128_262 = torch.constant.int 128
    %526 = torch.prim.ListConstruct %int4_260, %306, %int32_261, %int128_262 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %527 = torch.aten.view %511, %526 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %527, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_263 = torch.constant.int 4
    %int8_264 = torch.constant.int 8
    %int128_265 = torch.constant.int 128
    %528 = torch.prim.ListConstruct %int4_263, %306, %int8_264, %int128_265 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %529 = torch.aten.view %518, %528 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %529, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int4_266 = torch.constant.int 4
    %int8_267 = torch.constant.int 8
    %int128_268 = torch.constant.int 128
    %530 = torch.prim.ListConstruct %int4_266, %306, %int8_267, %int128_268 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %531 = torch.aten.view %525, %530 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %531, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int131072_269 = torch.constant.int 131072
    %none_270 = torch.constant.none
    %none_271 = torch.constant.none
    %cpu_272 = torch.constant.device "cpu"
    %false_273 = torch.constant.bool false
    %532 = torch.aten.arange %int131072_269, %none_270, %none_271, %cpu_272, %false_273 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_274 = torch.constant.int 0
    %int128_275 = torch.constant.int 128
    %none_276 = torch.constant.none
    %none_277 = torch.constant.none
    %cpu_278 = torch.constant.device "cpu"
    %false_279 = torch.constant.bool false
    %533 = torch.aten.arange.start %int0_274, %int128_275, %none_276, %none_277, %cpu_278, %false_279 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_280 = torch.constant.int 2
    %534 = torch.aten.floor_divide.Scalar %533, %int2_280 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_281 = torch.constant.int 6
    %535 = torch.prims.convert_element_type %534, %int6_281 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_282 = torch.constant.int 128
    %536 = torch.aten.div.Scalar %535, %int128_282 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_283 = torch.constant.float 2.000000e+00
    %537 = torch.aten.mul.Scalar %536, %float2.000000e00_283 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_284 = torch.constant.float 5.000000e+05
    %538 = torch.aten.pow.Scalar %float5.000000e05_284, %537 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %539 = torch.aten.reciprocal %538 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_285 = torch.constant.float 1.000000e+00
    %540 = torch.aten.mul.Scalar %539, %float1.000000e00_285 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_286 = torch.constant.int 1
    %541 = torch.aten.unsqueeze %532, %int1_286 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_287 = torch.constant.int 0
    %542 = torch.aten.unsqueeze %540, %int0_287 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %543 = torch.aten.mul.Tensor %541, %542 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_288 = torch.constant.int 1
    %544 = torch.aten.size.int %511, %int1_288 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.int
    %int0_289 = torch.constant.int 0
    %545 = torch.aten.add.int %int0_289, %544 : !torch.int, !torch.int -> !torch.int
    %int0_290 = torch.constant.int 0
    %int0_291 = torch.constant.int 0
    %int1_292 = torch.constant.int 1
    %546 = torch.aten.slice.Tensor %543, %int0_290, %int0_291, %545, %int1_292 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %546, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_293 = torch.constant.int 1
    %int0_294 = torch.constant.int 0
    %int9223372036854775807_295 = torch.constant.int 9223372036854775807
    %int1_296 = torch.constant.int 1
    %547 = torch.aten.slice.Tensor %546, %int1_293, %int0_294, %int9223372036854775807_295, %int1_296 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %547, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_297 = torch.constant.int 1
    %int0_298 = torch.constant.int 0
    %int9223372036854775807_299 = torch.constant.int 9223372036854775807
    %int1_300 = torch.constant.int 1
    %548 = torch.aten.slice.Tensor %547, %int1_297, %int0_298, %int9223372036854775807_299, %int1_300 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %548, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_301 = torch.constant.int 0
    %549 = torch.aten.unsqueeze %548, %int0_301 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %549, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_302 = torch.constant.int 1
    %int0_303 = torch.constant.int 0
    %int9223372036854775807_304 = torch.constant.int 9223372036854775807
    %int1_305 = torch.constant.int 1
    %550 = torch.aten.slice.Tensor %549, %int1_302, %int0_303, %int9223372036854775807_304, %int1_305 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %550, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_306 = torch.constant.int 2
    %int0_307 = torch.constant.int 0
    %int9223372036854775807_308 = torch.constant.int 9223372036854775807
    %int1_309 = torch.constant.int 1
    %551 = torch.aten.slice.Tensor %550, %int2_306, %int0_307, %int9223372036854775807_308, %int1_309 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %551, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_310 = torch.constant.int 4
    %int1_311 = torch.constant.int 1
    %int1_312 = torch.constant.int 1
    %552 = torch.prim.ListConstruct %int4_310, %int1_311, %int1_312 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %553 = torch.aten.repeat %551, %552 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %553, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_313 = torch.constant.int 6
    %554 = torch.prims.convert_element_type %527, %int6_313 : !torch.vtensor<[4,?,32,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %554, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %555 = torch_c.to_builtin_tensor %554 : !torch.vtensor<[4,?,32,128],f32> -> tensor<4x?x32x128xf32>
    %556 = torch_c.to_builtin_tensor %553 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %557 = util.call @sharktank_rotary_embedding_4_D_32_128_f32(%555, %556) : (tensor<4x?x32x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x32x128xf32>
    %558 = torch_c.from_builtin_tensor %557 : tensor<4x?x32x128xf32> -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %558, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %int5_314 = torch.constant.int 5
    %559 = torch.prims.convert_element_type %558, %int5_314 : !torch.vtensor<[4,?,32,128],f32>, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %559, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int131072_315 = torch.constant.int 131072
    %none_316 = torch.constant.none
    %none_317 = torch.constant.none
    %cpu_318 = torch.constant.device "cpu"
    %false_319 = torch.constant.bool false
    %560 = torch.aten.arange %int131072_315, %none_316, %none_317, %cpu_318, %false_319 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_320 = torch.constant.int 0
    %int128_321 = torch.constant.int 128
    %none_322 = torch.constant.none
    %none_323 = torch.constant.none
    %cpu_324 = torch.constant.device "cpu"
    %false_325 = torch.constant.bool false
    %561 = torch.aten.arange.start %int0_320, %int128_321, %none_322, %none_323, %cpu_324, %false_325 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_326 = torch.constant.int 2
    %562 = torch.aten.floor_divide.Scalar %561, %int2_326 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_327 = torch.constant.int 6
    %563 = torch.prims.convert_element_type %562, %int6_327 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_328 = torch.constant.int 128
    %564 = torch.aten.div.Scalar %563, %int128_328 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_329 = torch.constant.float 2.000000e+00
    %565 = torch.aten.mul.Scalar %564, %float2.000000e00_329 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_330 = torch.constant.float 5.000000e+05
    %566 = torch.aten.pow.Scalar %float5.000000e05_330, %565 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %567 = torch.aten.reciprocal %566 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_331 = torch.constant.float 1.000000e+00
    %568 = torch.aten.mul.Scalar %567, %float1.000000e00_331 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_332 = torch.constant.int 1
    %569 = torch.aten.unsqueeze %560, %int1_332 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_333 = torch.constant.int 0
    %570 = torch.aten.unsqueeze %568, %int0_333 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %571 = torch.aten.mul.Tensor %569, %570 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_334 = torch.constant.int 1
    %572 = torch.aten.size.int %518, %int1_334 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int0_335 = torch.constant.int 0
    %573 = torch.aten.add.int %int0_335, %572 : !torch.int, !torch.int -> !torch.int
    %int0_336 = torch.constant.int 0
    %int0_337 = torch.constant.int 0
    %int1_338 = torch.constant.int 1
    %574 = torch.aten.slice.Tensor %571, %int0_336, %int0_337, %573, %int1_338 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %574, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_339 = torch.constant.int 1
    %int0_340 = torch.constant.int 0
    %int9223372036854775807_341 = torch.constant.int 9223372036854775807
    %int1_342 = torch.constant.int 1
    %575 = torch.aten.slice.Tensor %574, %int1_339, %int0_340, %int9223372036854775807_341, %int1_342 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %575, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_343 = torch.constant.int 1
    %int0_344 = torch.constant.int 0
    %int9223372036854775807_345 = torch.constant.int 9223372036854775807
    %int1_346 = torch.constant.int 1
    %576 = torch.aten.slice.Tensor %575, %int1_343, %int0_344, %int9223372036854775807_345, %int1_346 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %576, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_347 = torch.constant.int 0
    %577 = torch.aten.unsqueeze %576, %int0_347 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %577, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_348 = torch.constant.int 1
    %int0_349 = torch.constant.int 0
    %int9223372036854775807_350 = torch.constant.int 9223372036854775807
    %int1_351 = torch.constant.int 1
    %578 = torch.aten.slice.Tensor %577, %int1_348, %int0_349, %int9223372036854775807_350, %int1_351 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %578, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_352 = torch.constant.int 2
    %int0_353 = torch.constant.int 0
    %int9223372036854775807_354 = torch.constant.int 9223372036854775807
    %int1_355 = torch.constant.int 1
    %579 = torch.aten.slice.Tensor %578, %int2_352, %int0_353, %int9223372036854775807_354, %int1_355 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %579, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_356 = torch.constant.int 4
    %int1_357 = torch.constant.int 1
    %int1_358 = torch.constant.int 1
    %580 = torch.prim.ListConstruct %int4_356, %int1_357, %int1_358 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %581 = torch.aten.repeat %579, %580 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %581, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_359 = torch.constant.int 6
    %582 = torch.prims.convert_element_type %529, %int6_359 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %582, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %583 = torch_c.to_builtin_tensor %582 : !torch.vtensor<[4,?,8,128],f32> -> tensor<4x?x8x128xf32>
    %584 = torch_c.to_builtin_tensor %581 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %585 = util.call @sharktank_rotary_embedding_4_D_8_128_f32(%583, %584) : (tensor<4x?x8x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x8x128xf32>
    %586 = torch_c.from_builtin_tensor %585 : tensor<4x?x8x128xf32> -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %586, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %int5_360 = torch.constant.int 5
    %587 = torch.prims.convert_element_type %586, %int5_360 : !torch.vtensor<[4,?,8,128],f32>, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %587, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int64_361 = torch.constant.int 64
    %588 = torch.aten.mul.Scalar %arg2, %int64_361 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %588, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_362 = torch.constant.int 2
    %int1_363 = torch.constant.int 1
    %589 = torch.aten.add.Scalar %588, %int2_362, %int1_363 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %589, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_364 = torch.constant.int 4
    %int32_365 = torch.constant.int 32
    %int8_366 = torch.constant.int 8
    %int128_367 = torch.constant.int 128
    %590 = torch.prim.ListConstruct %int4_364, %398, %int32_365, %int8_366, %int128_367 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %591 = torch.aten.view %587, %590 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %591, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_368 = torch.constant.int 4
    %592 = torch.aten.mul.int %int4_368, %398 : !torch.int, !torch.int -> !torch.int
    %int32_369 = torch.constant.int 32
    %int8_370 = torch.constant.int 8
    %int128_371 = torch.constant.int 128
    %593 = torch.prim.ListConstruct %592, %int32_369, %int8_370, %int128_371 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %594 = torch.aten.view %591, %593 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %594, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_372 = torch.constant.int 4
    %595 = torch.aten.mul.int %int4_372, %398 : !torch.int, !torch.int -> !torch.int
    %596 = torch.prim.ListConstruct %595 : (!torch.int) -> !torch.list<int>
    %597 = torch.aten.view %589, %596 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %597, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_373 = torch.constant.int 32
    %int2_374 = torch.constant.int 2
    %int32_375 = torch.constant.int 32
    %int8_376 = torch.constant.int 8
    %int128_377 = torch.constant.int 128
    %598 = torch.prim.ListConstruct %389, %int32_373, %int2_374, %int32_375, %int8_376, %int128_377 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %599 = torch.aten.view %431, %598 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %599, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_378 = torch.constant.int 32
    %600 = torch.aten.mul.int %389, %int32_378 : !torch.int, !torch.int -> !torch.int
    %int2_379 = torch.constant.int 2
    %601 = torch.aten.mul.int %600, %int2_379 : !torch.int, !torch.int -> !torch.int
    %int32_380 = torch.constant.int 32
    %int8_381 = torch.constant.int 8
    %int128_382 = torch.constant.int 128
    %602 = torch.prim.ListConstruct %601, %int32_380, %int8_381, %int128_382 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %603 = torch.aten.view %599, %602 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %603, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %604 = torch.prim.ListConstruct %597 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_383 = torch.constant.bool false
    %605 = torch.aten.index_put %603, %604, %594, %false_383 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %605, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_384 = torch.constant.int 32
    %int2_385 = torch.constant.int 2
    %int32_386 = torch.constant.int 32
    %int8_387 = torch.constant.int 8
    %int128_388 = torch.constant.int 128
    %606 = torch.prim.ListConstruct %389, %int32_384, %int2_385, %int32_386, %int8_387, %int128_388 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %607 = torch.aten.view %605, %606 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %607, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_389 = torch.constant.int 2097152
    %608 = torch.prim.ListConstruct %389, %int2097152_389 : (!torch.int, !torch.int) -> !torch.list<int>
    %609 = torch.aten.view %607, %608 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %609, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_390 = torch.constant.int 32
    %int2_391 = torch.constant.int 2
    %int32_392 = torch.constant.int 32
    %int8_393 = torch.constant.int 8
    %int128_394 = torch.constant.int 128
    %610 = torch.prim.ListConstruct %389, %int32_390, %int2_391, %int32_392, %int8_393, %int128_394 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %611 = torch.aten.view %609, %610 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %611, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_395 = torch.constant.int 32
    %int8_396 = torch.constant.int 8
    %int128_397 = torch.constant.int 128
    %612 = torch.prim.ListConstruct %601, %int32_395, %int8_396, %int128_397 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %613 = torch.aten.view %611, %612 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %613, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_398 = torch.constant.int 4
    %int32_399 = torch.constant.int 32
    %int8_400 = torch.constant.int 8
    %int128_401 = torch.constant.int 128
    %614 = torch.prim.ListConstruct %int4_398, %398, %int32_399, %int8_400, %int128_401 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %615 = torch.aten.view %531, %614 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %615, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_402 = torch.constant.int 4
    %616 = torch.aten.mul.int %int4_402, %398 : !torch.int, !torch.int -> !torch.int
    %int32_403 = torch.constant.int 32
    %int8_404 = torch.constant.int 8
    %int128_405 = torch.constant.int 128
    %617 = torch.prim.ListConstruct %616, %int32_403, %int8_404, %int128_405 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %618 = torch.aten.view %615, %617 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %618, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int1_406 = torch.constant.int 1
    %int1_407 = torch.constant.int 1
    %619 = torch.aten.add.Scalar %589, %int1_406, %int1_407 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %619, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_408 = torch.constant.int 4
    %620 = torch.aten.mul.int %int4_408, %398 : !torch.int, !torch.int -> !torch.int
    %621 = torch.prim.ListConstruct %620 : (!torch.int) -> !torch.list<int>
    %622 = torch.aten.view %619, %621 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %622, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %623 = torch.prim.ListConstruct %622 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_409 = torch.constant.bool false
    %624 = torch.aten.index_put %613, %623, %618, %false_409 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %624, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_410 = torch.constant.int 32
    %int2_411 = torch.constant.int 2
    %int32_412 = torch.constant.int 32
    %int8_413 = torch.constant.int 8
    %int128_414 = torch.constant.int 128
    %625 = torch.prim.ListConstruct %389, %int32_410, %int2_411, %int32_412, %int8_413, %int128_414 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %626 = torch.aten.view %624, %625 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %626, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_415 = torch.constant.int 2097152
    %627 = torch.prim.ListConstruct %389, %int2097152_415 : (!torch.int, !torch.int) -> !torch.list<int>
    %628 = torch.aten.view %626, %627 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %628, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int-2_416 = torch.constant.int -2
    %629 = torch.aten.unsqueeze %587, %int-2_416 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %629, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int4_417 = torch.constant.int 4
    %int8_418 = torch.constant.int 8
    %int4_419 = torch.constant.int 4
    %int128_420 = torch.constant.int 128
    %630 = torch.prim.ListConstruct %int4_417, %572, %int8_418, %int4_419, %int128_420 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_421 = torch.constant.bool false
    %631 = torch.aten.expand %629, %630, %false_421 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %631, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_422 = torch.constant.int 0
    %632 = torch.aten.clone %631, %int0_422 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %632, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_423 = torch.constant.int 4
    %int32_424 = torch.constant.int 32
    %int128_425 = torch.constant.int 128
    %633 = torch.prim.ListConstruct %int4_423, %572, %int32_424, %int128_425 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %634 = torch.aten._unsafe_view %632, %633 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %634, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_426 = torch.constant.int -2
    %635 = torch.aten.unsqueeze %531, %int-2_426 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %635, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_427 = torch.constant.int 1
    %636 = torch.aten.size.int %525, %int1_427 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int4_428 = torch.constant.int 4
    %int8_429 = torch.constant.int 8
    %int4_430 = torch.constant.int 4
    %int128_431 = torch.constant.int 128
    %637 = torch.prim.ListConstruct %int4_428, %636, %int8_429, %int4_430, %int128_431 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_432 = torch.constant.bool false
    %638 = torch.aten.expand %635, %637, %false_432 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %638, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_433 = torch.constant.int 0
    %639 = torch.aten.clone %638, %int0_433 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %639, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_434 = torch.constant.int 4
    %int32_435 = torch.constant.int 32
    %int128_436 = torch.constant.int 128
    %640 = torch.prim.ListConstruct %int4_434, %636, %int32_435, %int128_436 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %641 = torch.aten._unsafe_view %639, %640 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %641, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_437 = torch.constant.int 1
    %int2_438 = torch.constant.int 2
    %642 = torch.aten.transpose.int %559, %int1_437, %int2_438 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %642, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_439 = torch.constant.int 1
    %int2_440 = torch.constant.int 2
    %643 = torch.aten.transpose.int %634, %int1_439, %int2_440 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %643, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_441 = torch.constant.int 1
    %int2_442 = torch.constant.int 2
    %644 = torch.aten.transpose.int %641, %int1_441, %int2_442 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %644, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_443 = torch.constant.float 0.000000e+00
    %true_444 = torch.constant.bool true
    %none_445 = torch.constant.none
    %none_446 = torch.constant.none
    %645:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%642, %643, %644, %float0.000000e00_443, %true_444, %none_445, %none_446) : (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?],f32>) 
    torch.bind_symbolic_shape %645#0, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_447 = torch.constant.int 1
    %int2_448 = torch.constant.int 2
    %646 = torch.aten.transpose.int %645#0, %int1_447, %int2_448 : !torch.vtensor<[4,32,?,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %646, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_449 = torch.constant.int 4
    %int4096_450 = torch.constant.int 4096
    %647 = torch.prim.ListConstruct %int4_449, %544, %int4096_450 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %648 = torch.aten.view %646, %647 : !torch.vtensor<[4,?,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %648, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_451 = torch.constant.int -2
    %int-1_452 = torch.constant.int -1
    %649 = torch.aten.transpose.int %14, %int-2_451, %int-1_452 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_453 = torch.constant.int 4
    %650 = torch.aten.mul.int %int4_453, %544 : !torch.int, !torch.int -> !torch.int
    %int4096_454 = torch.constant.int 4096
    %651 = torch.prim.ListConstruct %650, %int4096_454 : (!torch.int, !torch.int) -> !torch.list<int>
    %652 = torch.aten.view %648, %651 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %652, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %653 = torch.aten.mm %652, %649 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %653, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_455 = torch.constant.int 4
    %int4096_456 = torch.constant.int 4096
    %654 = torch.prim.ListConstruct %int4_455, %544, %int4096_456 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %655 = torch.aten.view %653, %654 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %655, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_457 = torch.constant.int 1
    %656 = torch.aten.add.Tensor %494, %655, %int1_457 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %656, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_458 = torch.constant.int 6
    %657 = torch.prims.convert_element_type %656, %int6_458 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %657, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_459 = torch.constant.int 2
    %658 = torch.aten.pow.Tensor_Scalar %657, %int2_459 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %658, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_460 = torch.constant.int -1
    %659 = torch.prim.ListConstruct %int-1_460 : (!torch.int) -> !torch.list<int>
    %true_461 = torch.constant.bool true
    %none_462 = torch.constant.none
    %660 = torch.aten.mean.dim %658, %659, %true_461, %none_462 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %660, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_463 = torch.constant.float 9.9999997473787516E-6
    %int1_464 = torch.constant.int 1
    %661 = torch.aten.add.Scalar %660, %float9.999990e-06_463, %int1_464 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %661, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %662 = torch.aten.rsqrt %661 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %662, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %663 = torch.aten.mul.Tensor %657, %662 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %663, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_465 = torch.constant.int 5
    %664 = torch.prims.convert_element_type %663, %int5_465 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %664, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %665 = torch.aten.mul.Tensor %15, %664 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %665, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_466 = torch.constant.int 5
    %666 = torch.prims.convert_element_type %665, %int5_466 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %666, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_467 = torch.constant.int -2
    %int-1_468 = torch.constant.int -1
    %667 = torch.aten.transpose.int %16, %int-2_467, %int-1_468 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_469 = torch.constant.int 4
    %668 = torch.aten.mul.int %int4_469, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_470 = torch.constant.int 4096
    %669 = torch.prim.ListConstruct %668, %int4096_470 : (!torch.int, !torch.int) -> !torch.list<int>
    %670 = torch.aten.view %666, %669 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %670, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %671 = torch.aten.mm %670, %667 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %671, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_471 = torch.constant.int 4
    %int14336_472 = torch.constant.int 14336
    %672 = torch.prim.ListConstruct %int4_471, %306, %int14336_472 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %673 = torch.aten.view %671, %672 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %673, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %674 = torch.aten.silu %673 : !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %674, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_473 = torch.constant.int -2
    %int-1_474 = torch.constant.int -1
    %675 = torch.aten.transpose.int %17, %int-2_473, %int-1_474 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_475 = torch.constant.int 4
    %676 = torch.aten.mul.int %int4_475, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_476 = torch.constant.int 4096
    %677 = torch.prim.ListConstruct %676, %int4096_476 : (!torch.int, !torch.int) -> !torch.list<int>
    %678 = torch.aten.view %666, %677 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %678, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %679 = torch.aten.mm %678, %675 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %679, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_477 = torch.constant.int 4
    %int14336_478 = torch.constant.int 14336
    %680 = torch.prim.ListConstruct %int4_477, %306, %int14336_478 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %681 = torch.aten.view %679, %680 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %681, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %682 = torch.aten.mul.Tensor %674, %681 : !torch.vtensor<[4,?,14336],f16>, !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %682, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_479 = torch.constant.int -2
    %int-1_480 = torch.constant.int -1
    %683 = torch.aten.transpose.int %18, %int-2_479, %int-1_480 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int1_481 = torch.constant.int 1
    %684 = torch.aten.size.int %673, %int1_481 : !torch.vtensor<[4,?,14336],f16>, !torch.int -> !torch.int
    %int4_482 = torch.constant.int 4
    %685 = torch.aten.mul.int %int4_482, %684 : !torch.int, !torch.int -> !torch.int
    %int14336_483 = torch.constant.int 14336
    %686 = torch.prim.ListConstruct %685, %int14336_483 : (!torch.int, !torch.int) -> !torch.list<int>
    %687 = torch.aten.view %682, %686 : !torch.vtensor<[4,?,14336],f16>, !torch.list<int> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %687, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %688 = torch.aten.mm %687, %683 : !torch.vtensor<[?,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %688, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_484 = torch.constant.int 4
    %int4096_485 = torch.constant.int 4096
    %689 = torch.prim.ListConstruct %int4_484, %684, %int4096_485 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %690 = torch.aten.view %688, %689 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %690, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_486 = torch.constant.int 1
    %691 = torch.aten.add.Tensor %656, %690, %int1_486 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %691, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_487 = torch.constant.int 6
    %692 = torch.prims.convert_element_type %691, %int6_487 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %692, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_488 = torch.constant.int 2
    %693 = torch.aten.pow.Tensor_Scalar %692, %int2_488 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %693, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_489 = torch.constant.int -1
    %694 = torch.prim.ListConstruct %int-1_489 : (!torch.int) -> !torch.list<int>
    %true_490 = torch.constant.bool true
    %none_491 = torch.constant.none
    %695 = torch.aten.mean.dim %693, %694, %true_490, %none_491 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %695, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_492 = torch.constant.float 9.9999997473787516E-6
    %int1_493 = torch.constant.int 1
    %696 = torch.aten.add.Scalar %695, %float9.999990e-06_492, %int1_493 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %696, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %697 = torch.aten.rsqrt %696 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %697, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %698 = torch.aten.mul.Tensor %692, %697 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %698, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_494 = torch.constant.int 5
    %699 = torch.prims.convert_element_type %698, %int5_494 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %699, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %700 = torch.aten.mul.Tensor %19, %699 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %700, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_495 = torch.constant.int 5
    %701 = torch.prims.convert_element_type %700, %int5_495 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %701, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_496 = torch.constant.int -2
    %int-1_497 = torch.constant.int -1
    %702 = torch.aten.transpose.int %20, %int-2_496, %int-1_497 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_498 = torch.constant.int 4
    %703 = torch.aten.mul.int %int4_498, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_499 = torch.constant.int 4096
    %704 = torch.prim.ListConstruct %703, %int4096_499 : (!torch.int, !torch.int) -> !torch.list<int>
    %705 = torch.aten.view %701, %704 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %705, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %706 = torch.aten.mm %705, %702 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %706, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_500 = torch.constant.int 4
    %int4096_501 = torch.constant.int 4096
    %707 = torch.prim.ListConstruct %int4_500, %306, %int4096_501 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %708 = torch.aten.view %706, %707 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %708, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_502 = torch.constant.int -2
    %int-1_503 = torch.constant.int -1
    %709 = torch.aten.transpose.int %21, %int-2_502, %int-1_503 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_504 = torch.constant.int 4
    %710 = torch.aten.mul.int %int4_504, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_505 = torch.constant.int 4096
    %711 = torch.prim.ListConstruct %710, %int4096_505 : (!torch.int, !torch.int) -> !torch.list<int>
    %712 = torch.aten.view %701, %711 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %712, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %713 = torch.aten.mm %712, %709 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %713, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_506 = torch.constant.int 4
    %int1024_507 = torch.constant.int 1024
    %714 = torch.prim.ListConstruct %int4_506, %306, %int1024_507 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %715 = torch.aten.view %713, %714 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %715, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int-2_508 = torch.constant.int -2
    %int-1_509 = torch.constant.int -1
    %716 = torch.aten.transpose.int %22, %int-2_508, %int-1_509 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_510 = torch.constant.int 4
    %717 = torch.aten.mul.int %int4_510, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_511 = torch.constant.int 4096
    %718 = torch.prim.ListConstruct %717, %int4096_511 : (!torch.int, !torch.int) -> !torch.list<int>
    %719 = torch.aten.view %701, %718 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %719, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %720 = torch.aten.mm %719, %716 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %720, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_512 = torch.constant.int 4
    %int1024_513 = torch.constant.int 1024
    %721 = torch.prim.ListConstruct %int4_512, %306, %int1024_513 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %722 = torch.aten.view %720, %721 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %722, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int4_514 = torch.constant.int 4
    %int32_515 = torch.constant.int 32
    %int128_516 = torch.constant.int 128
    %723 = torch.prim.ListConstruct %int4_514, %306, %int32_515, %int128_516 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %724 = torch.aten.view %708, %723 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %724, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_517 = torch.constant.int 4
    %int8_518 = torch.constant.int 8
    %int128_519 = torch.constant.int 128
    %725 = torch.prim.ListConstruct %int4_517, %306, %int8_518, %int128_519 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %726 = torch.aten.view %715, %725 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %726, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int4_520 = torch.constant.int 4
    %int8_521 = torch.constant.int 8
    %int128_522 = torch.constant.int 128
    %727 = torch.prim.ListConstruct %int4_520, %306, %int8_521, %int128_522 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %728 = torch.aten.view %722, %727 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %728, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int131072_523 = torch.constant.int 131072
    %none_524 = torch.constant.none
    %none_525 = torch.constant.none
    %cpu_526 = torch.constant.device "cpu"
    %false_527 = torch.constant.bool false
    %729 = torch.aten.arange %int131072_523, %none_524, %none_525, %cpu_526, %false_527 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_528 = torch.constant.int 0
    %int128_529 = torch.constant.int 128
    %none_530 = torch.constant.none
    %none_531 = torch.constant.none
    %cpu_532 = torch.constant.device "cpu"
    %false_533 = torch.constant.bool false
    %730 = torch.aten.arange.start %int0_528, %int128_529, %none_530, %none_531, %cpu_532, %false_533 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_534 = torch.constant.int 2
    %731 = torch.aten.floor_divide.Scalar %730, %int2_534 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_535 = torch.constant.int 6
    %732 = torch.prims.convert_element_type %731, %int6_535 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_536 = torch.constant.int 128
    %733 = torch.aten.div.Scalar %732, %int128_536 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_537 = torch.constant.float 2.000000e+00
    %734 = torch.aten.mul.Scalar %733, %float2.000000e00_537 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_538 = torch.constant.float 5.000000e+05
    %735 = torch.aten.pow.Scalar %float5.000000e05_538, %734 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %736 = torch.aten.reciprocal %735 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_539 = torch.constant.float 1.000000e+00
    %737 = torch.aten.mul.Scalar %736, %float1.000000e00_539 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_540 = torch.constant.int 1
    %738 = torch.aten.unsqueeze %729, %int1_540 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_541 = torch.constant.int 0
    %739 = torch.aten.unsqueeze %737, %int0_541 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %740 = torch.aten.mul.Tensor %738, %739 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_542 = torch.constant.int 1
    %741 = torch.aten.size.int %708, %int1_542 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.int
    %int0_543 = torch.constant.int 0
    %742 = torch.aten.add.int %int0_543, %741 : !torch.int, !torch.int -> !torch.int
    %int0_544 = torch.constant.int 0
    %int0_545 = torch.constant.int 0
    %int1_546 = torch.constant.int 1
    %743 = torch.aten.slice.Tensor %740, %int0_544, %int0_545, %742, %int1_546 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %743, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_547 = torch.constant.int 1
    %int0_548 = torch.constant.int 0
    %int9223372036854775807_549 = torch.constant.int 9223372036854775807
    %int1_550 = torch.constant.int 1
    %744 = torch.aten.slice.Tensor %743, %int1_547, %int0_548, %int9223372036854775807_549, %int1_550 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %744, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_551 = torch.constant.int 1
    %int0_552 = torch.constant.int 0
    %int9223372036854775807_553 = torch.constant.int 9223372036854775807
    %int1_554 = torch.constant.int 1
    %745 = torch.aten.slice.Tensor %744, %int1_551, %int0_552, %int9223372036854775807_553, %int1_554 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %745, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_555 = torch.constant.int 0
    %746 = torch.aten.unsqueeze %745, %int0_555 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %746, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_556 = torch.constant.int 1
    %int0_557 = torch.constant.int 0
    %int9223372036854775807_558 = torch.constant.int 9223372036854775807
    %int1_559 = torch.constant.int 1
    %747 = torch.aten.slice.Tensor %746, %int1_556, %int0_557, %int9223372036854775807_558, %int1_559 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %747, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_560 = torch.constant.int 2
    %int0_561 = torch.constant.int 0
    %int9223372036854775807_562 = torch.constant.int 9223372036854775807
    %int1_563 = torch.constant.int 1
    %748 = torch.aten.slice.Tensor %747, %int2_560, %int0_561, %int9223372036854775807_562, %int1_563 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %748, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_564 = torch.constant.int 4
    %int1_565 = torch.constant.int 1
    %int1_566 = torch.constant.int 1
    %749 = torch.prim.ListConstruct %int4_564, %int1_565, %int1_566 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %750 = torch.aten.repeat %748, %749 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %750, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_567 = torch.constant.int 6
    %751 = torch.prims.convert_element_type %724, %int6_567 : !torch.vtensor<[4,?,32,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %751, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %752 = torch_c.to_builtin_tensor %751 : !torch.vtensor<[4,?,32,128],f32> -> tensor<4x?x32x128xf32>
    %753 = torch_c.to_builtin_tensor %750 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %754 = util.call @sharktank_rotary_embedding_4_D_32_128_f32(%752, %753) : (tensor<4x?x32x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x32x128xf32>
    %755 = torch_c.from_builtin_tensor %754 : tensor<4x?x32x128xf32> -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %755, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %int5_568 = torch.constant.int 5
    %756 = torch.prims.convert_element_type %755, %int5_568 : !torch.vtensor<[4,?,32,128],f32>, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %756, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int131072_569 = torch.constant.int 131072
    %none_570 = torch.constant.none
    %none_571 = torch.constant.none
    %cpu_572 = torch.constant.device "cpu"
    %false_573 = torch.constant.bool false
    %757 = torch.aten.arange %int131072_569, %none_570, %none_571, %cpu_572, %false_573 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_574 = torch.constant.int 0
    %int128_575 = torch.constant.int 128
    %none_576 = torch.constant.none
    %none_577 = torch.constant.none
    %cpu_578 = torch.constant.device "cpu"
    %false_579 = torch.constant.bool false
    %758 = torch.aten.arange.start %int0_574, %int128_575, %none_576, %none_577, %cpu_578, %false_579 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_580 = torch.constant.int 2
    %759 = torch.aten.floor_divide.Scalar %758, %int2_580 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_581 = torch.constant.int 6
    %760 = torch.prims.convert_element_type %759, %int6_581 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_582 = torch.constant.int 128
    %761 = torch.aten.div.Scalar %760, %int128_582 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_583 = torch.constant.float 2.000000e+00
    %762 = torch.aten.mul.Scalar %761, %float2.000000e00_583 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_584 = torch.constant.float 5.000000e+05
    %763 = torch.aten.pow.Scalar %float5.000000e05_584, %762 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %764 = torch.aten.reciprocal %763 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_585 = torch.constant.float 1.000000e+00
    %765 = torch.aten.mul.Scalar %764, %float1.000000e00_585 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_586 = torch.constant.int 1
    %766 = torch.aten.unsqueeze %757, %int1_586 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_587 = torch.constant.int 0
    %767 = torch.aten.unsqueeze %765, %int0_587 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %768 = torch.aten.mul.Tensor %766, %767 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_588 = torch.constant.int 1
    %769 = torch.aten.size.int %715, %int1_588 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int0_589 = torch.constant.int 0
    %770 = torch.aten.add.int %int0_589, %769 : !torch.int, !torch.int -> !torch.int
    %int0_590 = torch.constant.int 0
    %int0_591 = torch.constant.int 0
    %int1_592 = torch.constant.int 1
    %771 = torch.aten.slice.Tensor %768, %int0_590, %int0_591, %770, %int1_592 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %771, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_593 = torch.constant.int 1
    %int0_594 = torch.constant.int 0
    %int9223372036854775807_595 = torch.constant.int 9223372036854775807
    %int1_596 = torch.constant.int 1
    %772 = torch.aten.slice.Tensor %771, %int1_593, %int0_594, %int9223372036854775807_595, %int1_596 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %772, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_597 = torch.constant.int 1
    %int0_598 = torch.constant.int 0
    %int9223372036854775807_599 = torch.constant.int 9223372036854775807
    %int1_600 = torch.constant.int 1
    %773 = torch.aten.slice.Tensor %772, %int1_597, %int0_598, %int9223372036854775807_599, %int1_600 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %773, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_601 = torch.constant.int 0
    %774 = torch.aten.unsqueeze %773, %int0_601 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %774, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_602 = torch.constant.int 1
    %int0_603 = torch.constant.int 0
    %int9223372036854775807_604 = torch.constant.int 9223372036854775807
    %int1_605 = torch.constant.int 1
    %775 = torch.aten.slice.Tensor %774, %int1_602, %int0_603, %int9223372036854775807_604, %int1_605 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %775, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_606 = torch.constant.int 2
    %int0_607 = torch.constant.int 0
    %int9223372036854775807_608 = torch.constant.int 9223372036854775807
    %int1_609 = torch.constant.int 1
    %776 = torch.aten.slice.Tensor %775, %int2_606, %int0_607, %int9223372036854775807_608, %int1_609 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %776, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_610 = torch.constant.int 4
    %int1_611 = torch.constant.int 1
    %int1_612 = torch.constant.int 1
    %777 = torch.prim.ListConstruct %int4_610, %int1_611, %int1_612 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %778 = torch.aten.repeat %776, %777 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %778, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_613 = torch.constant.int 6
    %779 = torch.prims.convert_element_type %726, %int6_613 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %779, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %780 = torch_c.to_builtin_tensor %779 : !torch.vtensor<[4,?,8,128],f32> -> tensor<4x?x8x128xf32>
    %781 = torch_c.to_builtin_tensor %778 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %782 = util.call @sharktank_rotary_embedding_4_D_8_128_f32(%780, %781) : (tensor<4x?x8x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x8x128xf32>
    %783 = torch_c.from_builtin_tensor %782 : tensor<4x?x8x128xf32> -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %783, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %int5_614 = torch.constant.int 5
    %784 = torch.prims.convert_element_type %783, %int5_614 : !torch.vtensor<[4,?,8,128],f32>, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %784, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int64_615 = torch.constant.int 64
    %785 = torch.aten.mul.Scalar %arg2, %int64_615 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %785, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_616 = torch.constant.int 4
    %int1_617 = torch.constant.int 1
    %786 = torch.aten.add.Scalar %785, %int4_616, %int1_617 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %786, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_618 = torch.constant.int 4
    %int32_619 = torch.constant.int 32
    %int8_620 = torch.constant.int 8
    %int128_621 = torch.constant.int 128
    %787 = torch.prim.ListConstruct %int4_618, %398, %int32_619, %int8_620, %int128_621 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %788 = torch.aten.view %784, %787 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %788, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_622 = torch.constant.int 4
    %789 = torch.aten.mul.int %int4_622, %398 : !torch.int, !torch.int -> !torch.int
    %int32_623 = torch.constant.int 32
    %int8_624 = torch.constant.int 8
    %int128_625 = torch.constant.int 128
    %790 = torch.prim.ListConstruct %789, %int32_623, %int8_624, %int128_625 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %791 = torch.aten.view %788, %790 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %791, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_626 = torch.constant.int 4
    %792 = torch.aten.mul.int %int4_626, %398 : !torch.int, !torch.int -> !torch.int
    %793 = torch.prim.ListConstruct %792 : (!torch.int) -> !torch.list<int>
    %794 = torch.aten.view %786, %793 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %794, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_627 = torch.constant.int 32
    %int2_628 = torch.constant.int 2
    %int32_629 = torch.constant.int 32
    %int8_630 = torch.constant.int 8
    %int128_631 = torch.constant.int 128
    %795 = torch.prim.ListConstruct %389, %int32_627, %int2_628, %int32_629, %int8_630, %int128_631 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %796 = torch.aten.view %628, %795 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %796, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_632 = torch.constant.int 32
    %797 = torch.aten.mul.int %389, %int32_632 : !torch.int, !torch.int -> !torch.int
    %int2_633 = torch.constant.int 2
    %798 = torch.aten.mul.int %797, %int2_633 : !torch.int, !torch.int -> !torch.int
    %int32_634 = torch.constant.int 32
    %int8_635 = torch.constant.int 8
    %int128_636 = torch.constant.int 128
    %799 = torch.prim.ListConstruct %798, %int32_634, %int8_635, %int128_636 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %800 = torch.aten.view %796, %799 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %800, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %801 = torch.prim.ListConstruct %794 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_637 = torch.constant.bool false
    %802 = torch.aten.index_put %800, %801, %791, %false_637 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %802, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_638 = torch.constant.int 32
    %int2_639 = torch.constant.int 2
    %int32_640 = torch.constant.int 32
    %int8_641 = torch.constant.int 8
    %int128_642 = torch.constant.int 128
    %803 = torch.prim.ListConstruct %389, %int32_638, %int2_639, %int32_640, %int8_641, %int128_642 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %804 = torch.aten.view %802, %803 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %804, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_643 = torch.constant.int 2097152
    %805 = torch.prim.ListConstruct %389, %int2097152_643 : (!torch.int, !torch.int) -> !torch.list<int>
    %806 = torch.aten.view %804, %805 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %806, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_644 = torch.constant.int 32
    %int2_645 = torch.constant.int 2
    %int32_646 = torch.constant.int 32
    %int8_647 = torch.constant.int 8
    %int128_648 = torch.constant.int 128
    %807 = torch.prim.ListConstruct %389, %int32_644, %int2_645, %int32_646, %int8_647, %int128_648 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %808 = torch.aten.view %806, %807 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %808, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_649 = torch.constant.int 32
    %int8_650 = torch.constant.int 8
    %int128_651 = torch.constant.int 128
    %809 = torch.prim.ListConstruct %798, %int32_649, %int8_650, %int128_651 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %810 = torch.aten.view %808, %809 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %810, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_652 = torch.constant.int 4
    %int32_653 = torch.constant.int 32
    %int8_654 = torch.constant.int 8
    %int128_655 = torch.constant.int 128
    %811 = torch.prim.ListConstruct %int4_652, %398, %int32_653, %int8_654, %int128_655 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %812 = torch.aten.view %728, %811 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %812, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_656 = torch.constant.int 4
    %813 = torch.aten.mul.int %int4_656, %398 : !torch.int, !torch.int -> !torch.int
    %int32_657 = torch.constant.int 32
    %int8_658 = torch.constant.int 8
    %int128_659 = torch.constant.int 128
    %814 = torch.prim.ListConstruct %813, %int32_657, %int8_658, %int128_659 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %815 = torch.aten.view %812, %814 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %815, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int1_660 = torch.constant.int 1
    %int1_661 = torch.constant.int 1
    %816 = torch.aten.add.Scalar %786, %int1_660, %int1_661 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %816, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_662 = torch.constant.int 4
    %817 = torch.aten.mul.int %int4_662, %398 : !torch.int, !torch.int -> !torch.int
    %818 = torch.prim.ListConstruct %817 : (!torch.int) -> !torch.list<int>
    %819 = torch.aten.view %816, %818 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %819, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %820 = torch.prim.ListConstruct %819 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_663 = torch.constant.bool false
    %821 = torch.aten.index_put %810, %820, %815, %false_663 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %821, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_664 = torch.constant.int 32
    %int2_665 = torch.constant.int 2
    %int32_666 = torch.constant.int 32
    %int8_667 = torch.constant.int 8
    %int128_668 = torch.constant.int 128
    %822 = torch.prim.ListConstruct %389, %int32_664, %int2_665, %int32_666, %int8_667, %int128_668 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %823 = torch.aten.view %821, %822 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %823, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_669 = torch.constant.int 2097152
    %824 = torch.prim.ListConstruct %389, %int2097152_669 : (!torch.int, !torch.int) -> !torch.list<int>
    %825 = torch.aten.view %823, %824 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %825, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int-2_670 = torch.constant.int -2
    %826 = torch.aten.unsqueeze %784, %int-2_670 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %826, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int4_671 = torch.constant.int 4
    %int8_672 = torch.constant.int 8
    %int4_673 = torch.constant.int 4
    %int128_674 = torch.constant.int 128
    %827 = torch.prim.ListConstruct %int4_671, %769, %int8_672, %int4_673, %int128_674 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_675 = torch.constant.bool false
    %828 = torch.aten.expand %826, %827, %false_675 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %828, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_676 = torch.constant.int 0
    %829 = torch.aten.clone %828, %int0_676 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %829, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_677 = torch.constant.int 4
    %int32_678 = torch.constant.int 32
    %int128_679 = torch.constant.int 128
    %830 = torch.prim.ListConstruct %int4_677, %769, %int32_678, %int128_679 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %831 = torch.aten._unsafe_view %829, %830 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %831, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_680 = torch.constant.int -2
    %832 = torch.aten.unsqueeze %728, %int-2_680 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %832, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_681 = torch.constant.int 1
    %833 = torch.aten.size.int %722, %int1_681 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int4_682 = torch.constant.int 4
    %int8_683 = torch.constant.int 8
    %int4_684 = torch.constant.int 4
    %int128_685 = torch.constant.int 128
    %834 = torch.prim.ListConstruct %int4_682, %833, %int8_683, %int4_684, %int128_685 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_686 = torch.constant.bool false
    %835 = torch.aten.expand %832, %834, %false_686 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %835, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_687 = torch.constant.int 0
    %836 = torch.aten.clone %835, %int0_687 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %836, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_688 = torch.constant.int 4
    %int32_689 = torch.constant.int 32
    %int128_690 = torch.constant.int 128
    %837 = torch.prim.ListConstruct %int4_688, %833, %int32_689, %int128_690 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %838 = torch.aten._unsafe_view %836, %837 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %838, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_691 = torch.constant.int 1
    %int2_692 = torch.constant.int 2
    %839 = torch.aten.transpose.int %756, %int1_691, %int2_692 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %839, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_693 = torch.constant.int 1
    %int2_694 = torch.constant.int 2
    %840 = torch.aten.transpose.int %831, %int1_693, %int2_694 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %840, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_695 = torch.constant.int 1
    %int2_696 = torch.constant.int 2
    %841 = torch.aten.transpose.int %838, %int1_695, %int2_696 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %841, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_697 = torch.constant.float 0.000000e+00
    %true_698 = torch.constant.bool true
    %none_699 = torch.constant.none
    %none_700 = torch.constant.none
    %842:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%839, %840, %841, %float0.000000e00_697, %true_698, %none_699, %none_700) : (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?],f32>) 
    torch.bind_symbolic_shape %842#0, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_701 = torch.constant.int 1
    %int2_702 = torch.constant.int 2
    %843 = torch.aten.transpose.int %842#0, %int1_701, %int2_702 : !torch.vtensor<[4,32,?,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %843, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_703 = torch.constant.int 4
    %int4096_704 = torch.constant.int 4096
    %844 = torch.prim.ListConstruct %int4_703, %741, %int4096_704 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %845 = torch.aten.view %843, %844 : !torch.vtensor<[4,?,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %845, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_705 = torch.constant.int -2
    %int-1_706 = torch.constant.int -1
    %846 = torch.aten.transpose.int %23, %int-2_705, %int-1_706 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_707 = torch.constant.int 4
    %847 = torch.aten.mul.int %int4_707, %741 : !torch.int, !torch.int -> !torch.int
    %int4096_708 = torch.constant.int 4096
    %848 = torch.prim.ListConstruct %847, %int4096_708 : (!torch.int, !torch.int) -> !torch.list<int>
    %849 = torch.aten.view %845, %848 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %849, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %850 = torch.aten.mm %849, %846 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %850, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_709 = torch.constant.int 4
    %int4096_710 = torch.constant.int 4096
    %851 = torch.prim.ListConstruct %int4_709, %741, %int4096_710 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %852 = torch.aten.view %850, %851 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %852, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_711 = torch.constant.int 1
    %853 = torch.aten.add.Tensor %691, %852, %int1_711 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %853, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_712 = torch.constant.int 6
    %854 = torch.prims.convert_element_type %853, %int6_712 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %854, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_713 = torch.constant.int 2
    %855 = torch.aten.pow.Tensor_Scalar %854, %int2_713 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %855, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_714 = torch.constant.int -1
    %856 = torch.prim.ListConstruct %int-1_714 : (!torch.int) -> !torch.list<int>
    %true_715 = torch.constant.bool true
    %none_716 = torch.constant.none
    %857 = torch.aten.mean.dim %855, %856, %true_715, %none_716 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %857, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_717 = torch.constant.float 9.9999997473787516E-6
    %int1_718 = torch.constant.int 1
    %858 = torch.aten.add.Scalar %857, %float9.999990e-06_717, %int1_718 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %858, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %859 = torch.aten.rsqrt %858 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %859, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %860 = torch.aten.mul.Tensor %854, %859 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %860, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_719 = torch.constant.int 5
    %861 = torch.prims.convert_element_type %860, %int5_719 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %861, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %862 = torch.aten.mul.Tensor %24, %861 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %862, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_720 = torch.constant.int 5
    %863 = torch.prims.convert_element_type %862, %int5_720 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %863, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_721 = torch.constant.int -2
    %int-1_722 = torch.constant.int -1
    %864 = torch.aten.transpose.int %25, %int-2_721, %int-1_722 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_723 = torch.constant.int 4
    %865 = torch.aten.mul.int %int4_723, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_724 = torch.constant.int 4096
    %866 = torch.prim.ListConstruct %865, %int4096_724 : (!torch.int, !torch.int) -> !torch.list<int>
    %867 = torch.aten.view %863, %866 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %867, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %868 = torch.aten.mm %867, %864 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %868, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_725 = torch.constant.int 4
    %int14336_726 = torch.constant.int 14336
    %869 = torch.prim.ListConstruct %int4_725, %306, %int14336_726 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %870 = torch.aten.view %868, %869 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %870, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %871 = torch.aten.silu %870 : !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %871, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_727 = torch.constant.int -2
    %int-1_728 = torch.constant.int -1
    %872 = torch.aten.transpose.int %26, %int-2_727, %int-1_728 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_729 = torch.constant.int 4
    %873 = torch.aten.mul.int %int4_729, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_730 = torch.constant.int 4096
    %874 = torch.prim.ListConstruct %873, %int4096_730 : (!torch.int, !torch.int) -> !torch.list<int>
    %875 = torch.aten.view %863, %874 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %875, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %876 = torch.aten.mm %875, %872 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %876, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_731 = torch.constant.int 4
    %int14336_732 = torch.constant.int 14336
    %877 = torch.prim.ListConstruct %int4_731, %306, %int14336_732 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %878 = torch.aten.view %876, %877 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %878, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %879 = torch.aten.mul.Tensor %871, %878 : !torch.vtensor<[4,?,14336],f16>, !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %879, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_733 = torch.constant.int -2
    %int-1_734 = torch.constant.int -1
    %880 = torch.aten.transpose.int %27, %int-2_733, %int-1_734 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int1_735 = torch.constant.int 1
    %881 = torch.aten.size.int %870, %int1_735 : !torch.vtensor<[4,?,14336],f16>, !torch.int -> !torch.int
    %int4_736 = torch.constant.int 4
    %882 = torch.aten.mul.int %int4_736, %881 : !torch.int, !torch.int -> !torch.int
    %int14336_737 = torch.constant.int 14336
    %883 = torch.prim.ListConstruct %882, %int14336_737 : (!torch.int, !torch.int) -> !torch.list<int>
    %884 = torch.aten.view %879, %883 : !torch.vtensor<[4,?,14336],f16>, !torch.list<int> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %884, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %885 = torch.aten.mm %884, %880 : !torch.vtensor<[?,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %885, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_738 = torch.constant.int 4
    %int4096_739 = torch.constant.int 4096
    %886 = torch.prim.ListConstruct %int4_738, %881, %int4096_739 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %887 = torch.aten.view %885, %886 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %887, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_740 = torch.constant.int 1
    %888 = torch.aten.add.Tensor %853, %887, %int1_740 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %888, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_741 = torch.constant.int 6
    %889 = torch.prims.convert_element_type %888, %int6_741 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %889, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_742 = torch.constant.int 2
    %890 = torch.aten.pow.Tensor_Scalar %889, %int2_742 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %890, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_743 = torch.constant.int -1
    %891 = torch.prim.ListConstruct %int-1_743 : (!torch.int) -> !torch.list<int>
    %true_744 = torch.constant.bool true
    %none_745 = torch.constant.none
    %892 = torch.aten.mean.dim %890, %891, %true_744, %none_745 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %892, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_746 = torch.constant.float 9.9999997473787516E-6
    %int1_747 = torch.constant.int 1
    %893 = torch.aten.add.Scalar %892, %float9.999990e-06_746, %int1_747 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %893, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %894 = torch.aten.rsqrt %893 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %894, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %895 = torch.aten.mul.Tensor %889, %894 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %895, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_748 = torch.constant.int 5
    %896 = torch.prims.convert_element_type %895, %int5_748 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %896, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %897 = torch.aten.mul.Tensor %28, %896 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %897, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_749 = torch.constant.int 5
    %898 = torch.prims.convert_element_type %897, %int5_749 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %898, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_750 = torch.constant.int -2
    %int-1_751 = torch.constant.int -1
    %899 = torch.aten.transpose.int %29, %int-2_750, %int-1_751 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_752 = torch.constant.int 4
    %900 = torch.aten.mul.int %int4_752, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_753 = torch.constant.int 4096
    %901 = torch.prim.ListConstruct %900, %int4096_753 : (!torch.int, !torch.int) -> !torch.list<int>
    %902 = torch.aten.view %898, %901 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %902, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %903 = torch.aten.mm %902, %899 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %903, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_754 = torch.constant.int 4
    %int4096_755 = torch.constant.int 4096
    %904 = torch.prim.ListConstruct %int4_754, %306, %int4096_755 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %905 = torch.aten.view %903, %904 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %905, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_756 = torch.constant.int -2
    %int-1_757 = torch.constant.int -1
    %906 = torch.aten.transpose.int %30, %int-2_756, %int-1_757 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_758 = torch.constant.int 4
    %907 = torch.aten.mul.int %int4_758, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_759 = torch.constant.int 4096
    %908 = torch.prim.ListConstruct %907, %int4096_759 : (!torch.int, !torch.int) -> !torch.list<int>
    %909 = torch.aten.view %898, %908 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %909, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %910 = torch.aten.mm %909, %906 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %910, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_760 = torch.constant.int 4
    %int1024_761 = torch.constant.int 1024
    %911 = torch.prim.ListConstruct %int4_760, %306, %int1024_761 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %912 = torch.aten.view %910, %911 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %912, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int-2_762 = torch.constant.int -2
    %int-1_763 = torch.constant.int -1
    %913 = torch.aten.transpose.int %31, %int-2_762, %int-1_763 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_764 = torch.constant.int 4
    %914 = torch.aten.mul.int %int4_764, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_765 = torch.constant.int 4096
    %915 = torch.prim.ListConstruct %914, %int4096_765 : (!torch.int, !torch.int) -> !torch.list<int>
    %916 = torch.aten.view %898, %915 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %916, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %917 = torch.aten.mm %916, %913 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %917, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_766 = torch.constant.int 4
    %int1024_767 = torch.constant.int 1024
    %918 = torch.prim.ListConstruct %int4_766, %306, %int1024_767 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %919 = torch.aten.view %917, %918 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %919, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int4_768 = torch.constant.int 4
    %int32_769 = torch.constant.int 32
    %int128_770 = torch.constant.int 128
    %920 = torch.prim.ListConstruct %int4_768, %306, %int32_769, %int128_770 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %921 = torch.aten.view %905, %920 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %921, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_771 = torch.constant.int 4
    %int8_772 = torch.constant.int 8
    %int128_773 = torch.constant.int 128
    %922 = torch.prim.ListConstruct %int4_771, %306, %int8_772, %int128_773 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %923 = torch.aten.view %912, %922 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %923, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int4_774 = torch.constant.int 4
    %int8_775 = torch.constant.int 8
    %int128_776 = torch.constant.int 128
    %924 = torch.prim.ListConstruct %int4_774, %306, %int8_775, %int128_776 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %925 = torch.aten.view %919, %924 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %925, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int131072_777 = torch.constant.int 131072
    %none_778 = torch.constant.none
    %none_779 = torch.constant.none
    %cpu_780 = torch.constant.device "cpu"
    %false_781 = torch.constant.bool false
    %926 = torch.aten.arange %int131072_777, %none_778, %none_779, %cpu_780, %false_781 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_782 = torch.constant.int 0
    %int128_783 = torch.constant.int 128
    %none_784 = torch.constant.none
    %none_785 = torch.constant.none
    %cpu_786 = torch.constant.device "cpu"
    %false_787 = torch.constant.bool false
    %927 = torch.aten.arange.start %int0_782, %int128_783, %none_784, %none_785, %cpu_786, %false_787 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_788 = torch.constant.int 2
    %928 = torch.aten.floor_divide.Scalar %927, %int2_788 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_789 = torch.constant.int 6
    %929 = torch.prims.convert_element_type %928, %int6_789 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_790 = torch.constant.int 128
    %930 = torch.aten.div.Scalar %929, %int128_790 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_791 = torch.constant.float 2.000000e+00
    %931 = torch.aten.mul.Scalar %930, %float2.000000e00_791 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_792 = torch.constant.float 5.000000e+05
    %932 = torch.aten.pow.Scalar %float5.000000e05_792, %931 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %933 = torch.aten.reciprocal %932 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_793 = torch.constant.float 1.000000e+00
    %934 = torch.aten.mul.Scalar %933, %float1.000000e00_793 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_794 = torch.constant.int 1
    %935 = torch.aten.unsqueeze %926, %int1_794 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_795 = torch.constant.int 0
    %936 = torch.aten.unsqueeze %934, %int0_795 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %937 = torch.aten.mul.Tensor %935, %936 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_796 = torch.constant.int 1
    %938 = torch.aten.size.int %905, %int1_796 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.int
    %int0_797 = torch.constant.int 0
    %939 = torch.aten.add.int %int0_797, %938 : !torch.int, !torch.int -> !torch.int
    %int0_798 = torch.constant.int 0
    %int0_799 = torch.constant.int 0
    %int1_800 = torch.constant.int 1
    %940 = torch.aten.slice.Tensor %937, %int0_798, %int0_799, %939, %int1_800 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %940, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_801 = torch.constant.int 1
    %int0_802 = torch.constant.int 0
    %int9223372036854775807_803 = torch.constant.int 9223372036854775807
    %int1_804 = torch.constant.int 1
    %941 = torch.aten.slice.Tensor %940, %int1_801, %int0_802, %int9223372036854775807_803, %int1_804 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %941, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_805 = torch.constant.int 1
    %int0_806 = torch.constant.int 0
    %int9223372036854775807_807 = torch.constant.int 9223372036854775807
    %int1_808 = torch.constant.int 1
    %942 = torch.aten.slice.Tensor %941, %int1_805, %int0_806, %int9223372036854775807_807, %int1_808 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %942, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_809 = torch.constant.int 0
    %943 = torch.aten.unsqueeze %942, %int0_809 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %943, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_810 = torch.constant.int 1
    %int0_811 = torch.constant.int 0
    %int9223372036854775807_812 = torch.constant.int 9223372036854775807
    %int1_813 = torch.constant.int 1
    %944 = torch.aten.slice.Tensor %943, %int1_810, %int0_811, %int9223372036854775807_812, %int1_813 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %944, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_814 = torch.constant.int 2
    %int0_815 = torch.constant.int 0
    %int9223372036854775807_816 = torch.constant.int 9223372036854775807
    %int1_817 = torch.constant.int 1
    %945 = torch.aten.slice.Tensor %944, %int2_814, %int0_815, %int9223372036854775807_816, %int1_817 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %945, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_818 = torch.constant.int 4
    %int1_819 = torch.constant.int 1
    %int1_820 = torch.constant.int 1
    %946 = torch.prim.ListConstruct %int4_818, %int1_819, %int1_820 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %947 = torch.aten.repeat %945, %946 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %947, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_821 = torch.constant.int 6
    %948 = torch.prims.convert_element_type %921, %int6_821 : !torch.vtensor<[4,?,32,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %948, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %949 = torch_c.to_builtin_tensor %948 : !torch.vtensor<[4,?,32,128],f32> -> tensor<4x?x32x128xf32>
    %950 = torch_c.to_builtin_tensor %947 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %951 = util.call @sharktank_rotary_embedding_4_D_32_128_f32(%949, %950) : (tensor<4x?x32x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x32x128xf32>
    %952 = torch_c.from_builtin_tensor %951 : tensor<4x?x32x128xf32> -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %952, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %int5_822 = torch.constant.int 5
    %953 = torch.prims.convert_element_type %952, %int5_822 : !torch.vtensor<[4,?,32,128],f32>, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %953, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int131072_823 = torch.constant.int 131072
    %none_824 = torch.constant.none
    %none_825 = torch.constant.none
    %cpu_826 = torch.constant.device "cpu"
    %false_827 = torch.constant.bool false
    %954 = torch.aten.arange %int131072_823, %none_824, %none_825, %cpu_826, %false_827 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_828 = torch.constant.int 0
    %int128_829 = torch.constant.int 128
    %none_830 = torch.constant.none
    %none_831 = torch.constant.none
    %cpu_832 = torch.constant.device "cpu"
    %false_833 = torch.constant.bool false
    %955 = torch.aten.arange.start %int0_828, %int128_829, %none_830, %none_831, %cpu_832, %false_833 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_834 = torch.constant.int 2
    %956 = torch.aten.floor_divide.Scalar %955, %int2_834 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_835 = torch.constant.int 6
    %957 = torch.prims.convert_element_type %956, %int6_835 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_836 = torch.constant.int 128
    %958 = torch.aten.div.Scalar %957, %int128_836 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_837 = torch.constant.float 2.000000e+00
    %959 = torch.aten.mul.Scalar %958, %float2.000000e00_837 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_838 = torch.constant.float 5.000000e+05
    %960 = torch.aten.pow.Scalar %float5.000000e05_838, %959 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %961 = torch.aten.reciprocal %960 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_839 = torch.constant.float 1.000000e+00
    %962 = torch.aten.mul.Scalar %961, %float1.000000e00_839 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_840 = torch.constant.int 1
    %963 = torch.aten.unsqueeze %954, %int1_840 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_841 = torch.constant.int 0
    %964 = torch.aten.unsqueeze %962, %int0_841 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %965 = torch.aten.mul.Tensor %963, %964 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_842 = torch.constant.int 1
    %966 = torch.aten.size.int %912, %int1_842 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int0_843 = torch.constant.int 0
    %967 = torch.aten.add.int %int0_843, %966 : !torch.int, !torch.int -> !torch.int
    %int0_844 = torch.constant.int 0
    %int0_845 = torch.constant.int 0
    %int1_846 = torch.constant.int 1
    %968 = torch.aten.slice.Tensor %965, %int0_844, %int0_845, %967, %int1_846 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %968, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_847 = torch.constant.int 1
    %int0_848 = torch.constant.int 0
    %int9223372036854775807_849 = torch.constant.int 9223372036854775807
    %int1_850 = torch.constant.int 1
    %969 = torch.aten.slice.Tensor %968, %int1_847, %int0_848, %int9223372036854775807_849, %int1_850 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %969, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_851 = torch.constant.int 1
    %int0_852 = torch.constant.int 0
    %int9223372036854775807_853 = torch.constant.int 9223372036854775807
    %int1_854 = torch.constant.int 1
    %970 = torch.aten.slice.Tensor %969, %int1_851, %int0_852, %int9223372036854775807_853, %int1_854 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %970, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_855 = torch.constant.int 0
    %971 = torch.aten.unsqueeze %970, %int0_855 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %971, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_856 = torch.constant.int 1
    %int0_857 = torch.constant.int 0
    %int9223372036854775807_858 = torch.constant.int 9223372036854775807
    %int1_859 = torch.constant.int 1
    %972 = torch.aten.slice.Tensor %971, %int1_856, %int0_857, %int9223372036854775807_858, %int1_859 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %972, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_860 = torch.constant.int 2
    %int0_861 = torch.constant.int 0
    %int9223372036854775807_862 = torch.constant.int 9223372036854775807
    %int1_863 = torch.constant.int 1
    %973 = torch.aten.slice.Tensor %972, %int2_860, %int0_861, %int9223372036854775807_862, %int1_863 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %973, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_864 = torch.constant.int 4
    %int1_865 = torch.constant.int 1
    %int1_866 = torch.constant.int 1
    %974 = torch.prim.ListConstruct %int4_864, %int1_865, %int1_866 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %975 = torch.aten.repeat %973, %974 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %975, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_867 = torch.constant.int 6
    %976 = torch.prims.convert_element_type %923, %int6_867 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %976, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %977 = torch_c.to_builtin_tensor %976 : !torch.vtensor<[4,?,8,128],f32> -> tensor<4x?x8x128xf32>
    %978 = torch_c.to_builtin_tensor %975 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %979 = util.call @sharktank_rotary_embedding_4_D_8_128_f32(%977, %978) : (tensor<4x?x8x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x8x128xf32>
    %980 = torch_c.from_builtin_tensor %979 : tensor<4x?x8x128xf32> -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %980, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %int5_868 = torch.constant.int 5
    %981 = torch.prims.convert_element_type %980, %int5_868 : !torch.vtensor<[4,?,8,128],f32>, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %981, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int64_869 = torch.constant.int 64
    %982 = torch.aten.mul.Scalar %arg2, %int64_869 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %982, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int6_870 = torch.constant.int 6
    %int1_871 = torch.constant.int 1
    %983 = torch.aten.add.Scalar %982, %int6_870, %int1_871 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %983, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_872 = torch.constant.int 4
    %int32_873 = torch.constant.int 32
    %int8_874 = torch.constant.int 8
    %int128_875 = torch.constant.int 128
    %984 = torch.prim.ListConstruct %int4_872, %398, %int32_873, %int8_874, %int128_875 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %985 = torch.aten.view %981, %984 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %985, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_876 = torch.constant.int 4
    %986 = torch.aten.mul.int %int4_876, %398 : !torch.int, !torch.int -> !torch.int
    %int32_877 = torch.constant.int 32
    %int8_878 = torch.constant.int 8
    %int128_879 = torch.constant.int 128
    %987 = torch.prim.ListConstruct %986, %int32_877, %int8_878, %int128_879 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %988 = torch.aten.view %985, %987 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %988, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_880 = torch.constant.int 4
    %989 = torch.aten.mul.int %int4_880, %398 : !torch.int, !torch.int -> !torch.int
    %990 = torch.prim.ListConstruct %989 : (!torch.int) -> !torch.list<int>
    %991 = torch.aten.view %983, %990 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %991, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_881 = torch.constant.int 32
    %int2_882 = torch.constant.int 2
    %int32_883 = torch.constant.int 32
    %int8_884 = torch.constant.int 8
    %int128_885 = torch.constant.int 128
    %992 = torch.prim.ListConstruct %389, %int32_881, %int2_882, %int32_883, %int8_884, %int128_885 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %993 = torch.aten.view %825, %992 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %993, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_886 = torch.constant.int 32
    %994 = torch.aten.mul.int %389, %int32_886 : !torch.int, !torch.int -> !torch.int
    %int2_887 = torch.constant.int 2
    %995 = torch.aten.mul.int %994, %int2_887 : !torch.int, !torch.int -> !torch.int
    %int32_888 = torch.constant.int 32
    %int8_889 = torch.constant.int 8
    %int128_890 = torch.constant.int 128
    %996 = torch.prim.ListConstruct %995, %int32_888, %int8_889, %int128_890 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %997 = torch.aten.view %993, %996 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %997, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %998 = torch.prim.ListConstruct %991 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_891 = torch.constant.bool false
    %999 = torch.aten.index_put %997, %998, %988, %false_891 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %999, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_892 = torch.constant.int 32
    %int2_893 = torch.constant.int 2
    %int32_894 = torch.constant.int 32
    %int8_895 = torch.constant.int 8
    %int128_896 = torch.constant.int 128
    %1000 = torch.prim.ListConstruct %389, %int32_892, %int2_893, %int32_894, %int8_895, %int128_896 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1001 = torch.aten.view %999, %1000 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1001, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_897 = torch.constant.int 2097152
    %1002 = torch.prim.ListConstruct %389, %int2097152_897 : (!torch.int, !torch.int) -> !torch.list<int>
    %1003 = torch.aten.view %1001, %1002 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %1003, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_898 = torch.constant.int 32
    %int2_899 = torch.constant.int 2
    %int32_900 = torch.constant.int 32
    %int8_901 = torch.constant.int 8
    %int128_902 = torch.constant.int 128
    %1004 = torch.prim.ListConstruct %389, %int32_898, %int2_899, %int32_900, %int8_901, %int128_902 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1005 = torch.aten.view %1003, %1004 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1005, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_903 = torch.constant.int 32
    %int8_904 = torch.constant.int 8
    %int128_905 = torch.constant.int 128
    %1006 = torch.prim.ListConstruct %995, %int32_903, %int8_904, %int128_905 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1007 = torch.aten.view %1005, %1006 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %1007, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_906 = torch.constant.int 4
    %int32_907 = torch.constant.int 32
    %int8_908 = torch.constant.int 8
    %int128_909 = torch.constant.int 128
    %1008 = torch.prim.ListConstruct %int4_906, %398, %int32_907, %int8_908, %int128_909 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1009 = torch.aten.view %925, %1008 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1009, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_910 = torch.constant.int 4
    %1010 = torch.aten.mul.int %int4_910, %398 : !torch.int, !torch.int -> !torch.int
    %int32_911 = torch.constant.int 32
    %int8_912 = torch.constant.int 8
    %int128_913 = torch.constant.int 128
    %1011 = torch.prim.ListConstruct %1010, %int32_911, %int8_912, %int128_913 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1012 = torch.aten.view %1009, %1011 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %1012, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int1_914 = torch.constant.int 1
    %int1_915 = torch.constant.int 1
    %1013 = torch.aten.add.Scalar %983, %int1_914, %int1_915 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1013, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_916 = torch.constant.int 4
    %1014 = torch.aten.mul.int %int4_916, %398 : !torch.int, !torch.int -> !torch.int
    %1015 = torch.prim.ListConstruct %1014 : (!torch.int) -> !torch.list<int>
    %1016 = torch.aten.view %1013, %1015 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1016, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %1017 = torch.prim.ListConstruct %1016 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_917 = torch.constant.bool false
    %1018 = torch.aten.index_put %1007, %1017, %1012, %false_917 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %1018, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_918 = torch.constant.int 32
    %int2_919 = torch.constant.int 2
    %int32_920 = torch.constant.int 32
    %int8_921 = torch.constant.int 8
    %int128_922 = torch.constant.int 128
    %1019 = torch.prim.ListConstruct %389, %int32_918, %int2_919, %int32_920, %int8_921, %int128_922 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1020 = torch.aten.view %1018, %1019 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1020, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_923 = torch.constant.int 2097152
    %1021 = torch.prim.ListConstruct %389, %int2097152_923 : (!torch.int, !torch.int) -> !torch.list<int>
    %1022 = torch.aten.view %1020, %1021 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %1022, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int-2_924 = torch.constant.int -2
    %1023 = torch.aten.unsqueeze %981, %int-2_924 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %1023, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int4_925 = torch.constant.int 4
    %int8_926 = torch.constant.int 8
    %int4_927 = torch.constant.int 4
    %int128_928 = torch.constant.int 128
    %1024 = torch.prim.ListConstruct %int4_925, %966, %int8_926, %int4_927, %int128_928 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_929 = torch.constant.bool false
    %1025 = torch.aten.expand %1023, %1024, %false_929 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1025, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_930 = torch.constant.int 0
    %1026 = torch.aten.clone %1025, %int0_930 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1026, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_931 = torch.constant.int 4
    %int32_932 = torch.constant.int 32
    %int128_933 = torch.constant.int 128
    %1027 = torch.prim.ListConstruct %int4_931, %966, %int32_932, %int128_933 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1028 = torch.aten._unsafe_view %1026, %1027 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %1028, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_934 = torch.constant.int -2
    %1029 = torch.aten.unsqueeze %925, %int-2_934 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %1029, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_935 = torch.constant.int 1
    %1030 = torch.aten.size.int %919, %int1_935 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int4_936 = torch.constant.int 4
    %int8_937 = torch.constant.int 8
    %int4_938 = torch.constant.int 4
    %int128_939 = torch.constant.int 128
    %1031 = torch.prim.ListConstruct %int4_936, %1030, %int8_937, %int4_938, %int128_939 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_940 = torch.constant.bool false
    %1032 = torch.aten.expand %1029, %1031, %false_940 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1032, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_941 = torch.constant.int 0
    %1033 = torch.aten.clone %1032, %int0_941 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1033, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_942 = torch.constant.int 4
    %int32_943 = torch.constant.int 32
    %int128_944 = torch.constant.int 128
    %1034 = torch.prim.ListConstruct %int4_942, %1030, %int32_943, %int128_944 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1035 = torch.aten._unsafe_view %1033, %1034 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %1035, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_945 = torch.constant.int 1
    %int2_946 = torch.constant.int 2
    %1036 = torch.aten.transpose.int %953, %int1_945, %int2_946 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %1036, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_947 = torch.constant.int 1
    %int2_948 = torch.constant.int 2
    %1037 = torch.aten.transpose.int %1028, %int1_947, %int2_948 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %1037, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_949 = torch.constant.int 1
    %int2_950 = torch.constant.int 2
    %1038 = torch.aten.transpose.int %1035, %int1_949, %int2_950 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %1038, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_951 = torch.constant.float 0.000000e+00
    %true_952 = torch.constant.bool true
    %none_953 = torch.constant.none
    %none_954 = torch.constant.none
    %1039:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1036, %1037, %1038, %float0.000000e00_951, %true_952, %none_953, %none_954) : (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?],f32>) 
    torch.bind_symbolic_shape %1039#0, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_955 = torch.constant.int 1
    %int2_956 = torch.constant.int 2
    %1040 = torch.aten.transpose.int %1039#0, %int1_955, %int2_956 : !torch.vtensor<[4,32,?,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %1040, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_957 = torch.constant.int 4
    %int4096_958 = torch.constant.int 4096
    %1041 = torch.prim.ListConstruct %int4_957, %938, %int4096_958 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1042 = torch.aten.view %1040, %1041 : !torch.vtensor<[4,?,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1042, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_959 = torch.constant.int -2
    %int-1_960 = torch.constant.int -1
    %1043 = torch.aten.transpose.int %32, %int-2_959, %int-1_960 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_961 = torch.constant.int 4
    %1044 = torch.aten.mul.int %int4_961, %938 : !torch.int, !torch.int -> !torch.int
    %int4096_962 = torch.constant.int 4096
    %1045 = torch.prim.ListConstruct %1044, %int4096_962 : (!torch.int, !torch.int) -> !torch.list<int>
    %1046 = torch.aten.view %1042, %1045 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1046, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %1047 = torch.aten.mm %1046, %1043 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1047, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_963 = torch.constant.int 4
    %int4096_964 = torch.constant.int 4096
    %1048 = torch.prim.ListConstruct %int4_963, %938, %int4096_964 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1049 = torch.aten.view %1047, %1048 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1049, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_965 = torch.constant.int 1
    %1050 = torch.aten.add.Tensor %888, %1049, %int1_965 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1050, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_966 = torch.constant.int 6
    %1051 = torch.prims.convert_element_type %1050, %int6_966 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1051, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_967 = torch.constant.int 2
    %1052 = torch.aten.pow.Tensor_Scalar %1051, %int2_967 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1052, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_968 = torch.constant.int -1
    %1053 = torch.prim.ListConstruct %int-1_968 : (!torch.int) -> !torch.list<int>
    %true_969 = torch.constant.bool true
    %none_970 = torch.constant.none
    %1054 = torch.aten.mean.dim %1052, %1053, %true_969, %none_970 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1054, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_971 = torch.constant.float 9.9999997473787516E-6
    %int1_972 = torch.constant.int 1
    %1055 = torch.aten.add.Scalar %1054, %float9.999990e-06_971, %int1_972 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1055, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %1056 = torch.aten.rsqrt %1055 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1056, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %1057 = torch.aten.mul.Tensor %1051, %1056 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1057, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_973 = torch.constant.int 5
    %1058 = torch.prims.convert_element_type %1057, %int5_973 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1058, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %1059 = torch.aten.mul.Tensor %33, %1058 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1059, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_974 = torch.constant.int 5
    %1060 = torch.prims.convert_element_type %1059, %int5_974 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1060, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_975 = torch.constant.int -2
    %int-1_976 = torch.constant.int -1
    %1061 = torch.aten.transpose.int %34, %int-2_975, %int-1_976 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_977 = torch.constant.int 4
    %1062 = torch.aten.mul.int %int4_977, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_978 = torch.constant.int 4096
    %1063 = torch.prim.ListConstruct %1062, %int4096_978 : (!torch.int, !torch.int) -> !torch.list<int>
    %1064 = torch.aten.view %1060, %1063 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1064, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %1065 = torch.aten.mm %1064, %1061 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %1065, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_979 = torch.constant.int 4
    %int14336_980 = torch.constant.int 14336
    %1066 = torch.prim.ListConstruct %int4_979, %306, %int14336_980 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1067 = torch.aten.view %1065, %1066 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %1067, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %1068 = torch.aten.silu %1067 : !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %1068, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_981 = torch.constant.int -2
    %int-1_982 = torch.constant.int -1
    %1069 = torch.aten.transpose.int %35, %int-2_981, %int-1_982 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_983 = torch.constant.int 4
    %1070 = torch.aten.mul.int %int4_983, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_984 = torch.constant.int 4096
    %1071 = torch.prim.ListConstruct %1070, %int4096_984 : (!torch.int, !torch.int) -> !torch.list<int>
    %1072 = torch.aten.view %1060, %1071 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1072, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %1073 = torch.aten.mm %1072, %1069 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %1073, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_985 = torch.constant.int 4
    %int14336_986 = torch.constant.int 14336
    %1074 = torch.prim.ListConstruct %int4_985, %306, %int14336_986 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1075 = torch.aten.view %1073, %1074 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %1075, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %1076 = torch.aten.mul.Tensor %1068, %1075 : !torch.vtensor<[4,?,14336],f16>, !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %1076, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_987 = torch.constant.int -2
    %int-1_988 = torch.constant.int -1
    %1077 = torch.aten.transpose.int %36, %int-2_987, %int-1_988 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int1_989 = torch.constant.int 1
    %1078 = torch.aten.size.int %1067, %int1_989 : !torch.vtensor<[4,?,14336],f16>, !torch.int -> !torch.int
    %int4_990 = torch.constant.int 4
    %1079 = torch.aten.mul.int %int4_990, %1078 : !torch.int, !torch.int -> !torch.int
    %int14336_991 = torch.constant.int 14336
    %1080 = torch.prim.ListConstruct %1079, %int14336_991 : (!torch.int, !torch.int) -> !torch.list<int>
    %1081 = torch.aten.view %1076, %1080 : !torch.vtensor<[4,?,14336],f16>, !torch.list<int> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %1081, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %1082 = torch.aten.mm %1081, %1077 : !torch.vtensor<[?,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1082, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_992 = torch.constant.int 4
    %int4096_993 = torch.constant.int 4096
    %1083 = torch.prim.ListConstruct %int4_992, %1078, %int4096_993 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1084 = torch.aten.view %1082, %1083 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1084, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_994 = torch.constant.int 1
    %1085 = torch.aten.add.Tensor %1050, %1084, %int1_994 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1085, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_995 = torch.constant.int 6
    %1086 = torch.prims.convert_element_type %1085, %int6_995 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1086, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_996 = torch.constant.int 2
    %1087 = torch.aten.pow.Tensor_Scalar %1086, %int2_996 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1087, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_997 = torch.constant.int -1
    %1088 = torch.prim.ListConstruct %int-1_997 : (!torch.int) -> !torch.list<int>
    %true_998 = torch.constant.bool true
    %none_999 = torch.constant.none
    %1089 = torch.aten.mean.dim %1087, %1088, %true_998, %none_999 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1089, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_1000 = torch.constant.float 9.9999997473787516E-6
    %int1_1001 = torch.constant.int 1
    %1090 = torch.aten.add.Scalar %1089, %float9.999990e-06_1000, %int1_1001 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1090, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %1091 = torch.aten.rsqrt %1090 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1091, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %1092 = torch.aten.mul.Tensor %1086, %1091 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1092, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_1002 = torch.constant.int 5
    %1093 = torch.prims.convert_element_type %1092, %int5_1002 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1093, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %1094 = torch.aten.mul.Tensor %37, %1093 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1094, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_1003 = torch.constant.int 5
    %1095 = torch.prims.convert_element_type %1094, %int5_1003 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1095, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_1004 = torch.constant.int -2
    %int-1_1005 = torch.constant.int -1
    %1096 = torch.aten.transpose.int %38, %int-2_1004, %int-1_1005 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_1006 = torch.constant.int 4
    %1097 = torch.aten.mul.int %int4_1006, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_1007 = torch.constant.int 4096
    %1098 = torch.prim.ListConstruct %1097, %int4096_1007 : (!torch.int, !torch.int) -> !torch.list<int>
    %1099 = torch.aten.view %1095, %1098 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1099, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %1100 = torch.aten.mm %1099, %1096 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1100, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_1008 = torch.constant.int 4
    %int4096_1009 = torch.constant.int 4096
    %1101 = torch.prim.ListConstruct %int4_1008, %306, %int4096_1009 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1102 = torch.aten.view %1100, %1101 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1102, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_1010 = torch.constant.int -2
    %int-1_1011 = torch.constant.int -1
    %1103 = torch.aten.transpose.int %39, %int-2_1010, %int-1_1011 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_1012 = torch.constant.int 4
    %1104 = torch.aten.mul.int %int4_1012, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_1013 = torch.constant.int 4096
    %1105 = torch.prim.ListConstruct %1104, %int4096_1013 : (!torch.int, !torch.int) -> !torch.list<int>
    %1106 = torch.aten.view %1095, %1105 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1106, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %1107 = torch.aten.mm %1106, %1103 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %1107, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_1014 = torch.constant.int 4
    %int1024_1015 = torch.constant.int 1024
    %1108 = torch.prim.ListConstruct %int4_1014, %306, %int1024_1015 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1109 = torch.aten.view %1107, %1108 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %1109, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int-2_1016 = torch.constant.int -2
    %int-1_1017 = torch.constant.int -1
    %1110 = torch.aten.transpose.int %40, %int-2_1016, %int-1_1017 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_1018 = torch.constant.int 4
    %1111 = torch.aten.mul.int %int4_1018, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_1019 = torch.constant.int 4096
    %1112 = torch.prim.ListConstruct %1111, %int4096_1019 : (!torch.int, !torch.int) -> !torch.list<int>
    %1113 = torch.aten.view %1095, %1112 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1113, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %1114 = torch.aten.mm %1113, %1110 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %1114, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_1020 = torch.constant.int 4
    %int1024_1021 = torch.constant.int 1024
    %1115 = torch.prim.ListConstruct %int4_1020, %306, %int1024_1021 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1116 = torch.aten.view %1114, %1115 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %1116, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int4_1022 = torch.constant.int 4
    %int32_1023 = torch.constant.int 32
    %int128_1024 = torch.constant.int 128
    %1117 = torch.prim.ListConstruct %int4_1022, %306, %int32_1023, %int128_1024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1118 = torch.aten.view %1102, %1117 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %1118, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_1025 = torch.constant.int 4
    %int8_1026 = torch.constant.int 8
    %int128_1027 = torch.constant.int 128
    %1119 = torch.prim.ListConstruct %int4_1025, %306, %int8_1026, %int128_1027 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1120 = torch.aten.view %1109, %1119 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %1120, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int4_1028 = torch.constant.int 4
    %int8_1029 = torch.constant.int 8
    %int128_1030 = torch.constant.int 128
    %1121 = torch.prim.ListConstruct %int4_1028, %306, %int8_1029, %int128_1030 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1122 = torch.aten.view %1116, %1121 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %1122, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int131072_1031 = torch.constant.int 131072
    %none_1032 = torch.constant.none
    %none_1033 = torch.constant.none
    %cpu_1034 = torch.constant.device "cpu"
    %false_1035 = torch.constant.bool false
    %1123 = torch.aten.arange %int131072_1031, %none_1032, %none_1033, %cpu_1034, %false_1035 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_1036 = torch.constant.int 0
    %int128_1037 = torch.constant.int 128
    %none_1038 = torch.constant.none
    %none_1039 = torch.constant.none
    %cpu_1040 = torch.constant.device "cpu"
    %false_1041 = torch.constant.bool false
    %1124 = torch.aten.arange.start %int0_1036, %int128_1037, %none_1038, %none_1039, %cpu_1040, %false_1041 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_1042 = torch.constant.int 2
    %1125 = torch.aten.floor_divide.Scalar %1124, %int2_1042 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_1043 = torch.constant.int 6
    %1126 = torch.prims.convert_element_type %1125, %int6_1043 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_1044 = torch.constant.int 128
    %1127 = torch.aten.div.Scalar %1126, %int128_1044 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_1045 = torch.constant.float 2.000000e+00
    %1128 = torch.aten.mul.Scalar %1127, %float2.000000e00_1045 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_1046 = torch.constant.float 5.000000e+05
    %1129 = torch.aten.pow.Scalar %float5.000000e05_1046, %1128 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %1130 = torch.aten.reciprocal %1129 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_1047 = torch.constant.float 1.000000e+00
    %1131 = torch.aten.mul.Scalar %1130, %float1.000000e00_1047 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_1048 = torch.constant.int 1
    %1132 = torch.aten.unsqueeze %1123, %int1_1048 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_1049 = torch.constant.int 0
    %1133 = torch.aten.unsqueeze %1131, %int0_1049 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %1134 = torch.aten.mul.Tensor %1132, %1133 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_1050 = torch.constant.int 1
    %1135 = torch.aten.size.int %1102, %int1_1050 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.int
    %int0_1051 = torch.constant.int 0
    %1136 = torch.aten.add.int %int0_1051, %1135 : !torch.int, !torch.int -> !torch.int
    %int0_1052 = torch.constant.int 0
    %int0_1053 = torch.constant.int 0
    %int1_1054 = torch.constant.int 1
    %1137 = torch.aten.slice.Tensor %1134, %int0_1052, %int0_1053, %1136, %int1_1054 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %1137, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_1055 = torch.constant.int 1
    %int0_1056 = torch.constant.int 0
    %int9223372036854775807_1057 = torch.constant.int 9223372036854775807
    %int1_1058 = torch.constant.int 1
    %1138 = torch.aten.slice.Tensor %1137, %int1_1055, %int0_1056, %int9223372036854775807_1057, %int1_1058 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %1138, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_1059 = torch.constant.int 1
    %int0_1060 = torch.constant.int 0
    %int9223372036854775807_1061 = torch.constant.int 9223372036854775807
    %int1_1062 = torch.constant.int 1
    %1139 = torch.aten.slice.Tensor %1138, %int1_1059, %int0_1060, %int9223372036854775807_1061, %int1_1062 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %1139, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_1063 = torch.constant.int 0
    %1140 = torch.aten.unsqueeze %1139, %int0_1063 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %1140, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_1064 = torch.constant.int 1
    %int0_1065 = torch.constant.int 0
    %int9223372036854775807_1066 = torch.constant.int 9223372036854775807
    %int1_1067 = torch.constant.int 1
    %1141 = torch.aten.slice.Tensor %1140, %int1_1064, %int0_1065, %int9223372036854775807_1066, %int1_1067 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %1141, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_1068 = torch.constant.int 2
    %int0_1069 = torch.constant.int 0
    %int9223372036854775807_1070 = torch.constant.int 9223372036854775807
    %int1_1071 = torch.constant.int 1
    %1142 = torch.aten.slice.Tensor %1141, %int2_1068, %int0_1069, %int9223372036854775807_1070, %int1_1071 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %1142, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_1072 = torch.constant.int 4
    %int1_1073 = torch.constant.int 1
    %int1_1074 = torch.constant.int 1
    %1143 = torch.prim.ListConstruct %int4_1072, %int1_1073, %int1_1074 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1144 = torch.aten.repeat %1142, %1143 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %1144, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_1075 = torch.constant.int 6
    %1145 = torch.prims.convert_element_type %1118, %int6_1075 : !torch.vtensor<[4,?,32,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %1145, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %1146 = torch_c.to_builtin_tensor %1145 : !torch.vtensor<[4,?,32,128],f32> -> tensor<4x?x32x128xf32>
    %1147 = torch_c.to_builtin_tensor %1144 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %1148 = util.call @sharktank_rotary_embedding_4_D_32_128_f32(%1146, %1147) : (tensor<4x?x32x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x32x128xf32>
    %1149 = torch_c.from_builtin_tensor %1148 : tensor<4x?x32x128xf32> -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %1149, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %int5_1076 = torch.constant.int 5
    %1150 = torch.prims.convert_element_type %1149, %int5_1076 : !torch.vtensor<[4,?,32,128],f32>, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %1150, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int131072_1077 = torch.constant.int 131072
    %none_1078 = torch.constant.none
    %none_1079 = torch.constant.none
    %cpu_1080 = torch.constant.device "cpu"
    %false_1081 = torch.constant.bool false
    %1151 = torch.aten.arange %int131072_1077, %none_1078, %none_1079, %cpu_1080, %false_1081 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_1082 = torch.constant.int 0
    %int128_1083 = torch.constant.int 128
    %none_1084 = torch.constant.none
    %none_1085 = torch.constant.none
    %cpu_1086 = torch.constant.device "cpu"
    %false_1087 = torch.constant.bool false
    %1152 = torch.aten.arange.start %int0_1082, %int128_1083, %none_1084, %none_1085, %cpu_1086, %false_1087 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_1088 = torch.constant.int 2
    %1153 = torch.aten.floor_divide.Scalar %1152, %int2_1088 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_1089 = torch.constant.int 6
    %1154 = torch.prims.convert_element_type %1153, %int6_1089 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_1090 = torch.constant.int 128
    %1155 = torch.aten.div.Scalar %1154, %int128_1090 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_1091 = torch.constant.float 2.000000e+00
    %1156 = torch.aten.mul.Scalar %1155, %float2.000000e00_1091 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_1092 = torch.constant.float 5.000000e+05
    %1157 = torch.aten.pow.Scalar %float5.000000e05_1092, %1156 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %1158 = torch.aten.reciprocal %1157 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_1093 = torch.constant.float 1.000000e+00
    %1159 = torch.aten.mul.Scalar %1158, %float1.000000e00_1093 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_1094 = torch.constant.int 1
    %1160 = torch.aten.unsqueeze %1151, %int1_1094 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_1095 = torch.constant.int 0
    %1161 = torch.aten.unsqueeze %1159, %int0_1095 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %1162 = torch.aten.mul.Tensor %1160, %1161 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_1096 = torch.constant.int 1
    %1163 = torch.aten.size.int %1109, %int1_1096 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int0_1097 = torch.constant.int 0
    %1164 = torch.aten.add.int %int0_1097, %1163 : !torch.int, !torch.int -> !torch.int
    %int0_1098 = torch.constant.int 0
    %int0_1099 = torch.constant.int 0
    %int1_1100 = torch.constant.int 1
    %1165 = torch.aten.slice.Tensor %1162, %int0_1098, %int0_1099, %1164, %int1_1100 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %1165, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_1101 = torch.constant.int 1
    %int0_1102 = torch.constant.int 0
    %int9223372036854775807_1103 = torch.constant.int 9223372036854775807
    %int1_1104 = torch.constant.int 1
    %1166 = torch.aten.slice.Tensor %1165, %int1_1101, %int0_1102, %int9223372036854775807_1103, %int1_1104 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %1166, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_1105 = torch.constant.int 1
    %int0_1106 = torch.constant.int 0
    %int9223372036854775807_1107 = torch.constant.int 9223372036854775807
    %int1_1108 = torch.constant.int 1
    %1167 = torch.aten.slice.Tensor %1166, %int1_1105, %int0_1106, %int9223372036854775807_1107, %int1_1108 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %1167, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_1109 = torch.constant.int 0
    %1168 = torch.aten.unsqueeze %1167, %int0_1109 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %1168, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_1110 = torch.constant.int 1
    %int0_1111 = torch.constant.int 0
    %int9223372036854775807_1112 = torch.constant.int 9223372036854775807
    %int1_1113 = torch.constant.int 1
    %1169 = torch.aten.slice.Tensor %1168, %int1_1110, %int0_1111, %int9223372036854775807_1112, %int1_1113 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %1169, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_1114 = torch.constant.int 2
    %int0_1115 = torch.constant.int 0
    %int9223372036854775807_1116 = torch.constant.int 9223372036854775807
    %int1_1117 = torch.constant.int 1
    %1170 = torch.aten.slice.Tensor %1169, %int2_1114, %int0_1115, %int9223372036854775807_1116, %int1_1117 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %1170, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_1118 = torch.constant.int 4
    %int1_1119 = torch.constant.int 1
    %int1_1120 = torch.constant.int 1
    %1171 = torch.prim.ListConstruct %int4_1118, %int1_1119, %int1_1120 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1172 = torch.aten.repeat %1170, %1171 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %1172, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_1121 = torch.constant.int 6
    %1173 = torch.prims.convert_element_type %1120, %int6_1121 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %1173, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %1174 = torch_c.to_builtin_tensor %1173 : !torch.vtensor<[4,?,8,128],f32> -> tensor<4x?x8x128xf32>
    %1175 = torch_c.to_builtin_tensor %1172 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %1176 = util.call @sharktank_rotary_embedding_4_D_8_128_f32(%1174, %1175) : (tensor<4x?x8x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x8x128xf32>
    %1177 = torch_c.from_builtin_tensor %1176 : tensor<4x?x8x128xf32> -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %1177, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %int5_1122 = torch.constant.int 5
    %1178 = torch.prims.convert_element_type %1177, %int5_1122 : !torch.vtensor<[4,?,8,128],f32>, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %1178, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int64_1123 = torch.constant.int 64
    %1179 = torch.aten.mul.Scalar %arg2, %int64_1123 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1179, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int8_1124 = torch.constant.int 8
    %int1_1125 = torch.constant.int 1
    %1180 = torch.aten.add.Scalar %1179, %int8_1124, %int1_1125 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1180, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_1126 = torch.constant.int 4
    %int32_1127 = torch.constant.int 32
    %int8_1128 = torch.constant.int 8
    %int128_1129 = torch.constant.int 128
    %1181 = torch.prim.ListConstruct %int4_1126, %398, %int32_1127, %int8_1128, %int128_1129 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1182 = torch.aten.view %1178, %1181 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1182, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_1130 = torch.constant.int 4
    %1183 = torch.aten.mul.int %int4_1130, %398 : !torch.int, !torch.int -> !torch.int
    %int32_1131 = torch.constant.int 32
    %int8_1132 = torch.constant.int 8
    %int128_1133 = torch.constant.int 128
    %1184 = torch.prim.ListConstruct %1183, %int32_1131, %int8_1132, %int128_1133 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1185 = torch.aten.view %1182, %1184 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %1185, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_1134 = torch.constant.int 4
    %1186 = torch.aten.mul.int %int4_1134, %398 : !torch.int, !torch.int -> !torch.int
    %1187 = torch.prim.ListConstruct %1186 : (!torch.int) -> !torch.list<int>
    %1188 = torch.aten.view %1180, %1187 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1188, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_1135 = torch.constant.int 32
    %int2_1136 = torch.constant.int 2
    %int32_1137 = torch.constant.int 32
    %int8_1138 = torch.constant.int 8
    %int128_1139 = torch.constant.int 128
    %1189 = torch.prim.ListConstruct %389, %int32_1135, %int2_1136, %int32_1137, %int8_1138, %int128_1139 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1190 = torch.aten.view %1022, %1189 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1190, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_1140 = torch.constant.int 32
    %1191 = torch.aten.mul.int %389, %int32_1140 : !torch.int, !torch.int -> !torch.int
    %int2_1141 = torch.constant.int 2
    %1192 = torch.aten.mul.int %1191, %int2_1141 : !torch.int, !torch.int -> !torch.int
    %int32_1142 = torch.constant.int 32
    %int8_1143 = torch.constant.int 8
    %int128_1144 = torch.constant.int 128
    %1193 = torch.prim.ListConstruct %1192, %int32_1142, %int8_1143, %int128_1144 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1194 = torch.aten.view %1190, %1193 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %1194, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %1195 = torch.prim.ListConstruct %1188 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_1145 = torch.constant.bool false
    %1196 = torch.aten.index_put %1194, %1195, %1185, %false_1145 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %1196, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_1146 = torch.constant.int 32
    %int2_1147 = torch.constant.int 2
    %int32_1148 = torch.constant.int 32
    %int8_1149 = torch.constant.int 8
    %int128_1150 = torch.constant.int 128
    %1197 = torch.prim.ListConstruct %389, %int32_1146, %int2_1147, %int32_1148, %int8_1149, %int128_1150 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1198 = torch.aten.view %1196, %1197 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1198, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_1151 = torch.constant.int 2097152
    %1199 = torch.prim.ListConstruct %389, %int2097152_1151 : (!torch.int, !torch.int) -> !torch.list<int>
    %1200 = torch.aten.view %1198, %1199 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %1200, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_1152 = torch.constant.int 32
    %int2_1153 = torch.constant.int 2
    %int32_1154 = torch.constant.int 32
    %int8_1155 = torch.constant.int 8
    %int128_1156 = torch.constant.int 128
    %1201 = torch.prim.ListConstruct %389, %int32_1152, %int2_1153, %int32_1154, %int8_1155, %int128_1156 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1202 = torch.aten.view %1200, %1201 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1202, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_1157 = torch.constant.int 32
    %int8_1158 = torch.constant.int 8
    %int128_1159 = torch.constant.int 128
    %1203 = torch.prim.ListConstruct %1192, %int32_1157, %int8_1158, %int128_1159 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1204 = torch.aten.view %1202, %1203 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %1204, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_1160 = torch.constant.int 4
    %int32_1161 = torch.constant.int 32
    %int8_1162 = torch.constant.int 8
    %int128_1163 = torch.constant.int 128
    %1205 = torch.prim.ListConstruct %int4_1160, %398, %int32_1161, %int8_1162, %int128_1163 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1206 = torch.aten.view %1122, %1205 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1206, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_1164 = torch.constant.int 4
    %1207 = torch.aten.mul.int %int4_1164, %398 : !torch.int, !torch.int -> !torch.int
    %int32_1165 = torch.constant.int 32
    %int8_1166 = torch.constant.int 8
    %int128_1167 = torch.constant.int 128
    %1208 = torch.prim.ListConstruct %1207, %int32_1165, %int8_1166, %int128_1167 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1209 = torch.aten.view %1206, %1208 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %1209, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int1_1168 = torch.constant.int 1
    %int1_1169 = torch.constant.int 1
    %1210 = torch.aten.add.Scalar %1180, %int1_1168, %int1_1169 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1210, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_1170 = torch.constant.int 4
    %1211 = torch.aten.mul.int %int4_1170, %398 : !torch.int, !torch.int -> !torch.int
    %1212 = torch.prim.ListConstruct %1211 : (!torch.int) -> !torch.list<int>
    %1213 = torch.aten.view %1210, %1212 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1213, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %1214 = torch.prim.ListConstruct %1213 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_1171 = torch.constant.bool false
    %1215 = torch.aten.index_put %1204, %1214, %1209, %false_1171 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %1215, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_1172 = torch.constant.int 32
    %int2_1173 = torch.constant.int 2
    %int32_1174 = torch.constant.int 32
    %int8_1175 = torch.constant.int 8
    %int128_1176 = torch.constant.int 128
    %1216 = torch.prim.ListConstruct %389, %int32_1172, %int2_1173, %int32_1174, %int8_1175, %int128_1176 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1217 = torch.aten.view %1215, %1216 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1217, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_1177 = torch.constant.int 2097152
    %1218 = torch.prim.ListConstruct %389, %int2097152_1177 : (!torch.int, !torch.int) -> !torch.list<int>
    %1219 = torch.aten.view %1217, %1218 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %1219, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int-2_1178 = torch.constant.int -2
    %1220 = torch.aten.unsqueeze %1178, %int-2_1178 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %1220, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int4_1179 = torch.constant.int 4
    %int8_1180 = torch.constant.int 8
    %int4_1181 = torch.constant.int 4
    %int128_1182 = torch.constant.int 128
    %1221 = torch.prim.ListConstruct %int4_1179, %1163, %int8_1180, %int4_1181, %int128_1182 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_1183 = torch.constant.bool false
    %1222 = torch.aten.expand %1220, %1221, %false_1183 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1222, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_1184 = torch.constant.int 0
    %1223 = torch.aten.clone %1222, %int0_1184 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1223, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_1185 = torch.constant.int 4
    %int32_1186 = torch.constant.int 32
    %int128_1187 = torch.constant.int 128
    %1224 = torch.prim.ListConstruct %int4_1185, %1163, %int32_1186, %int128_1187 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1225 = torch.aten._unsafe_view %1223, %1224 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %1225, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_1188 = torch.constant.int -2
    %1226 = torch.aten.unsqueeze %1122, %int-2_1188 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %1226, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_1189 = torch.constant.int 1
    %1227 = torch.aten.size.int %1116, %int1_1189 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int4_1190 = torch.constant.int 4
    %int8_1191 = torch.constant.int 8
    %int4_1192 = torch.constant.int 4
    %int128_1193 = torch.constant.int 128
    %1228 = torch.prim.ListConstruct %int4_1190, %1227, %int8_1191, %int4_1192, %int128_1193 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_1194 = torch.constant.bool false
    %1229 = torch.aten.expand %1226, %1228, %false_1194 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1229, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_1195 = torch.constant.int 0
    %1230 = torch.aten.clone %1229, %int0_1195 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1230, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_1196 = torch.constant.int 4
    %int32_1197 = torch.constant.int 32
    %int128_1198 = torch.constant.int 128
    %1231 = torch.prim.ListConstruct %int4_1196, %1227, %int32_1197, %int128_1198 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1232 = torch.aten._unsafe_view %1230, %1231 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %1232, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_1199 = torch.constant.int 1
    %int2_1200 = torch.constant.int 2
    %1233 = torch.aten.transpose.int %1150, %int1_1199, %int2_1200 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %1233, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_1201 = torch.constant.int 1
    %int2_1202 = torch.constant.int 2
    %1234 = torch.aten.transpose.int %1225, %int1_1201, %int2_1202 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %1234, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_1203 = torch.constant.int 1
    %int2_1204 = torch.constant.int 2
    %1235 = torch.aten.transpose.int %1232, %int1_1203, %int2_1204 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %1235, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_1205 = torch.constant.float 0.000000e+00
    %true_1206 = torch.constant.bool true
    %none_1207 = torch.constant.none
    %none_1208 = torch.constant.none
    %1236:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1233, %1234, %1235, %float0.000000e00_1205, %true_1206, %none_1207, %none_1208) : (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?],f32>) 
    torch.bind_symbolic_shape %1236#0, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_1209 = torch.constant.int 1
    %int2_1210 = torch.constant.int 2
    %1237 = torch.aten.transpose.int %1236#0, %int1_1209, %int2_1210 : !torch.vtensor<[4,32,?,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %1237, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_1211 = torch.constant.int 4
    %int4096_1212 = torch.constant.int 4096
    %1238 = torch.prim.ListConstruct %int4_1211, %1135, %int4096_1212 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1239 = torch.aten.view %1237, %1238 : !torch.vtensor<[4,?,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1239, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_1213 = torch.constant.int -2
    %int-1_1214 = torch.constant.int -1
    %1240 = torch.aten.transpose.int %41, %int-2_1213, %int-1_1214 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_1215 = torch.constant.int 4
    %1241 = torch.aten.mul.int %int4_1215, %1135 : !torch.int, !torch.int -> !torch.int
    %int4096_1216 = torch.constant.int 4096
    %1242 = torch.prim.ListConstruct %1241, %int4096_1216 : (!torch.int, !torch.int) -> !torch.list<int>
    %1243 = torch.aten.view %1239, %1242 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1243, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %1244 = torch.aten.mm %1243, %1240 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1244, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_1217 = torch.constant.int 4
    %int4096_1218 = torch.constant.int 4096
    %1245 = torch.prim.ListConstruct %int4_1217, %1135, %int4096_1218 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1246 = torch.aten.view %1244, %1245 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1246, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_1219 = torch.constant.int 1
    %1247 = torch.aten.add.Tensor %1085, %1246, %int1_1219 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1247, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_1220 = torch.constant.int 6
    %1248 = torch.prims.convert_element_type %1247, %int6_1220 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1248, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_1221 = torch.constant.int 2
    %1249 = torch.aten.pow.Tensor_Scalar %1248, %int2_1221 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1249, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_1222 = torch.constant.int -1
    %1250 = torch.prim.ListConstruct %int-1_1222 : (!torch.int) -> !torch.list<int>
    %true_1223 = torch.constant.bool true
    %none_1224 = torch.constant.none
    %1251 = torch.aten.mean.dim %1249, %1250, %true_1223, %none_1224 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1251, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_1225 = torch.constant.float 9.9999997473787516E-6
    %int1_1226 = torch.constant.int 1
    %1252 = torch.aten.add.Scalar %1251, %float9.999990e-06_1225, %int1_1226 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1252, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %1253 = torch.aten.rsqrt %1252 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1253, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %1254 = torch.aten.mul.Tensor %1248, %1253 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1254, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_1227 = torch.constant.int 5
    %1255 = torch.prims.convert_element_type %1254, %int5_1227 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1255, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %1256 = torch.aten.mul.Tensor %42, %1255 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1256, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_1228 = torch.constant.int 5
    %1257 = torch.prims.convert_element_type %1256, %int5_1228 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1257, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_1229 = torch.constant.int -2
    %int-1_1230 = torch.constant.int -1
    %1258 = torch.aten.transpose.int %43, %int-2_1229, %int-1_1230 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_1231 = torch.constant.int 4
    %1259 = torch.aten.mul.int %int4_1231, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_1232 = torch.constant.int 4096
    %1260 = torch.prim.ListConstruct %1259, %int4096_1232 : (!torch.int, !torch.int) -> !torch.list<int>
    %1261 = torch.aten.view %1257, %1260 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1261, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %1262 = torch.aten.mm %1261, %1258 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %1262, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_1233 = torch.constant.int 4
    %int14336_1234 = torch.constant.int 14336
    %1263 = torch.prim.ListConstruct %int4_1233, %306, %int14336_1234 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1264 = torch.aten.view %1262, %1263 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %1264, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %1265 = torch.aten.silu %1264 : !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %1265, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_1235 = torch.constant.int -2
    %int-1_1236 = torch.constant.int -1
    %1266 = torch.aten.transpose.int %44, %int-2_1235, %int-1_1236 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_1237 = torch.constant.int 4
    %1267 = torch.aten.mul.int %int4_1237, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_1238 = torch.constant.int 4096
    %1268 = torch.prim.ListConstruct %1267, %int4096_1238 : (!torch.int, !torch.int) -> !torch.list<int>
    %1269 = torch.aten.view %1257, %1268 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1269, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %1270 = torch.aten.mm %1269, %1266 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %1270, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_1239 = torch.constant.int 4
    %int14336_1240 = torch.constant.int 14336
    %1271 = torch.prim.ListConstruct %int4_1239, %306, %int14336_1240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1272 = torch.aten.view %1270, %1271 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %1272, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %1273 = torch.aten.mul.Tensor %1265, %1272 : !torch.vtensor<[4,?,14336],f16>, !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %1273, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_1241 = torch.constant.int -2
    %int-1_1242 = torch.constant.int -1
    %1274 = torch.aten.transpose.int %45, %int-2_1241, %int-1_1242 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int1_1243 = torch.constant.int 1
    %1275 = torch.aten.size.int %1264, %int1_1243 : !torch.vtensor<[4,?,14336],f16>, !torch.int -> !torch.int
    %int4_1244 = torch.constant.int 4
    %1276 = torch.aten.mul.int %int4_1244, %1275 : !torch.int, !torch.int -> !torch.int
    %int14336_1245 = torch.constant.int 14336
    %1277 = torch.prim.ListConstruct %1276, %int14336_1245 : (!torch.int, !torch.int) -> !torch.list<int>
    %1278 = torch.aten.view %1273, %1277 : !torch.vtensor<[4,?,14336],f16>, !torch.list<int> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %1278, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %1279 = torch.aten.mm %1278, %1274 : !torch.vtensor<[?,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1279, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_1246 = torch.constant.int 4
    %int4096_1247 = torch.constant.int 4096
    %1280 = torch.prim.ListConstruct %int4_1246, %1275, %int4096_1247 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1281 = torch.aten.view %1279, %1280 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1281, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_1248 = torch.constant.int 1
    %1282 = torch.aten.add.Tensor %1247, %1281, %int1_1248 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1282, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_1249 = torch.constant.int 6
    %1283 = torch.prims.convert_element_type %1282, %int6_1249 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1283, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_1250 = torch.constant.int 2
    %1284 = torch.aten.pow.Tensor_Scalar %1283, %int2_1250 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1284, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_1251 = torch.constant.int -1
    %1285 = torch.prim.ListConstruct %int-1_1251 : (!torch.int) -> !torch.list<int>
    %true_1252 = torch.constant.bool true
    %none_1253 = torch.constant.none
    %1286 = torch.aten.mean.dim %1284, %1285, %true_1252, %none_1253 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1286, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_1254 = torch.constant.float 9.9999997473787516E-6
    %int1_1255 = torch.constant.int 1
    %1287 = torch.aten.add.Scalar %1286, %float9.999990e-06_1254, %int1_1255 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1287, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %1288 = torch.aten.rsqrt %1287 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1288, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %1289 = torch.aten.mul.Tensor %1283, %1288 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1289, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_1256 = torch.constant.int 5
    %1290 = torch.prims.convert_element_type %1289, %int5_1256 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1290, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %1291 = torch.aten.mul.Tensor %46, %1290 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1291, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_1257 = torch.constant.int 5
    %1292 = torch.prims.convert_element_type %1291, %int5_1257 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1292, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_1258 = torch.constant.int -2
    %int-1_1259 = torch.constant.int -1
    %1293 = torch.aten.transpose.int %47, %int-2_1258, %int-1_1259 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_1260 = torch.constant.int 4
    %1294 = torch.aten.mul.int %int4_1260, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_1261 = torch.constant.int 4096
    %1295 = torch.prim.ListConstruct %1294, %int4096_1261 : (!torch.int, !torch.int) -> !torch.list<int>
    %1296 = torch.aten.view %1292, %1295 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1296, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %1297 = torch.aten.mm %1296, %1293 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1297, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_1262 = torch.constant.int 4
    %int4096_1263 = torch.constant.int 4096
    %1298 = torch.prim.ListConstruct %int4_1262, %306, %int4096_1263 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1299 = torch.aten.view %1297, %1298 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1299, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_1264 = torch.constant.int -2
    %int-1_1265 = torch.constant.int -1
    %1300 = torch.aten.transpose.int %48, %int-2_1264, %int-1_1265 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_1266 = torch.constant.int 4
    %1301 = torch.aten.mul.int %int4_1266, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_1267 = torch.constant.int 4096
    %1302 = torch.prim.ListConstruct %1301, %int4096_1267 : (!torch.int, !torch.int) -> !torch.list<int>
    %1303 = torch.aten.view %1292, %1302 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1303, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %1304 = torch.aten.mm %1303, %1300 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %1304, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_1268 = torch.constant.int 4
    %int1024_1269 = torch.constant.int 1024
    %1305 = torch.prim.ListConstruct %int4_1268, %306, %int1024_1269 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1306 = torch.aten.view %1304, %1305 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %1306, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int-2_1270 = torch.constant.int -2
    %int-1_1271 = torch.constant.int -1
    %1307 = torch.aten.transpose.int %49, %int-2_1270, %int-1_1271 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_1272 = torch.constant.int 4
    %1308 = torch.aten.mul.int %int4_1272, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_1273 = torch.constant.int 4096
    %1309 = torch.prim.ListConstruct %1308, %int4096_1273 : (!torch.int, !torch.int) -> !torch.list<int>
    %1310 = torch.aten.view %1292, %1309 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1310, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %1311 = torch.aten.mm %1310, %1307 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %1311, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_1274 = torch.constant.int 4
    %int1024_1275 = torch.constant.int 1024
    %1312 = torch.prim.ListConstruct %int4_1274, %306, %int1024_1275 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1313 = torch.aten.view %1311, %1312 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %1313, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int4_1276 = torch.constant.int 4
    %int32_1277 = torch.constant.int 32
    %int128_1278 = torch.constant.int 128
    %1314 = torch.prim.ListConstruct %int4_1276, %306, %int32_1277, %int128_1278 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1315 = torch.aten.view %1299, %1314 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %1315, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_1279 = torch.constant.int 4
    %int8_1280 = torch.constant.int 8
    %int128_1281 = torch.constant.int 128
    %1316 = torch.prim.ListConstruct %int4_1279, %306, %int8_1280, %int128_1281 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1317 = torch.aten.view %1306, %1316 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %1317, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int4_1282 = torch.constant.int 4
    %int8_1283 = torch.constant.int 8
    %int128_1284 = torch.constant.int 128
    %1318 = torch.prim.ListConstruct %int4_1282, %306, %int8_1283, %int128_1284 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1319 = torch.aten.view %1313, %1318 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %1319, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int131072_1285 = torch.constant.int 131072
    %none_1286 = torch.constant.none
    %none_1287 = torch.constant.none
    %cpu_1288 = torch.constant.device "cpu"
    %false_1289 = torch.constant.bool false
    %1320 = torch.aten.arange %int131072_1285, %none_1286, %none_1287, %cpu_1288, %false_1289 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_1290 = torch.constant.int 0
    %int128_1291 = torch.constant.int 128
    %none_1292 = torch.constant.none
    %none_1293 = torch.constant.none
    %cpu_1294 = torch.constant.device "cpu"
    %false_1295 = torch.constant.bool false
    %1321 = torch.aten.arange.start %int0_1290, %int128_1291, %none_1292, %none_1293, %cpu_1294, %false_1295 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_1296 = torch.constant.int 2
    %1322 = torch.aten.floor_divide.Scalar %1321, %int2_1296 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_1297 = torch.constant.int 6
    %1323 = torch.prims.convert_element_type %1322, %int6_1297 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_1298 = torch.constant.int 128
    %1324 = torch.aten.div.Scalar %1323, %int128_1298 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_1299 = torch.constant.float 2.000000e+00
    %1325 = torch.aten.mul.Scalar %1324, %float2.000000e00_1299 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_1300 = torch.constant.float 5.000000e+05
    %1326 = torch.aten.pow.Scalar %float5.000000e05_1300, %1325 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %1327 = torch.aten.reciprocal %1326 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_1301 = torch.constant.float 1.000000e+00
    %1328 = torch.aten.mul.Scalar %1327, %float1.000000e00_1301 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_1302 = torch.constant.int 1
    %1329 = torch.aten.unsqueeze %1320, %int1_1302 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_1303 = torch.constant.int 0
    %1330 = torch.aten.unsqueeze %1328, %int0_1303 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %1331 = torch.aten.mul.Tensor %1329, %1330 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_1304 = torch.constant.int 1
    %1332 = torch.aten.size.int %1299, %int1_1304 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.int
    %int0_1305 = torch.constant.int 0
    %1333 = torch.aten.add.int %int0_1305, %1332 : !torch.int, !torch.int -> !torch.int
    %int0_1306 = torch.constant.int 0
    %int0_1307 = torch.constant.int 0
    %int1_1308 = torch.constant.int 1
    %1334 = torch.aten.slice.Tensor %1331, %int0_1306, %int0_1307, %1333, %int1_1308 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %1334, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_1309 = torch.constant.int 1
    %int0_1310 = torch.constant.int 0
    %int9223372036854775807_1311 = torch.constant.int 9223372036854775807
    %int1_1312 = torch.constant.int 1
    %1335 = torch.aten.slice.Tensor %1334, %int1_1309, %int0_1310, %int9223372036854775807_1311, %int1_1312 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %1335, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_1313 = torch.constant.int 1
    %int0_1314 = torch.constant.int 0
    %int9223372036854775807_1315 = torch.constant.int 9223372036854775807
    %int1_1316 = torch.constant.int 1
    %1336 = torch.aten.slice.Tensor %1335, %int1_1313, %int0_1314, %int9223372036854775807_1315, %int1_1316 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %1336, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_1317 = torch.constant.int 0
    %1337 = torch.aten.unsqueeze %1336, %int0_1317 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %1337, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_1318 = torch.constant.int 1
    %int0_1319 = torch.constant.int 0
    %int9223372036854775807_1320 = torch.constant.int 9223372036854775807
    %int1_1321 = torch.constant.int 1
    %1338 = torch.aten.slice.Tensor %1337, %int1_1318, %int0_1319, %int9223372036854775807_1320, %int1_1321 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %1338, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_1322 = torch.constant.int 2
    %int0_1323 = torch.constant.int 0
    %int9223372036854775807_1324 = torch.constant.int 9223372036854775807
    %int1_1325 = torch.constant.int 1
    %1339 = torch.aten.slice.Tensor %1338, %int2_1322, %int0_1323, %int9223372036854775807_1324, %int1_1325 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %1339, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_1326 = torch.constant.int 4
    %int1_1327 = torch.constant.int 1
    %int1_1328 = torch.constant.int 1
    %1340 = torch.prim.ListConstruct %int4_1326, %int1_1327, %int1_1328 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1341 = torch.aten.repeat %1339, %1340 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %1341, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_1329 = torch.constant.int 6
    %1342 = torch.prims.convert_element_type %1315, %int6_1329 : !torch.vtensor<[4,?,32,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %1342, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %1343 = torch_c.to_builtin_tensor %1342 : !torch.vtensor<[4,?,32,128],f32> -> tensor<4x?x32x128xf32>
    %1344 = torch_c.to_builtin_tensor %1341 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %1345 = util.call @sharktank_rotary_embedding_4_D_32_128_f32(%1343, %1344) : (tensor<4x?x32x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x32x128xf32>
    %1346 = torch_c.from_builtin_tensor %1345 : tensor<4x?x32x128xf32> -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %1346, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %int5_1330 = torch.constant.int 5
    %1347 = torch.prims.convert_element_type %1346, %int5_1330 : !torch.vtensor<[4,?,32,128],f32>, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %1347, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int131072_1331 = torch.constant.int 131072
    %none_1332 = torch.constant.none
    %none_1333 = torch.constant.none
    %cpu_1334 = torch.constant.device "cpu"
    %false_1335 = torch.constant.bool false
    %1348 = torch.aten.arange %int131072_1331, %none_1332, %none_1333, %cpu_1334, %false_1335 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_1336 = torch.constant.int 0
    %int128_1337 = torch.constant.int 128
    %none_1338 = torch.constant.none
    %none_1339 = torch.constant.none
    %cpu_1340 = torch.constant.device "cpu"
    %false_1341 = torch.constant.bool false
    %1349 = torch.aten.arange.start %int0_1336, %int128_1337, %none_1338, %none_1339, %cpu_1340, %false_1341 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_1342 = torch.constant.int 2
    %1350 = torch.aten.floor_divide.Scalar %1349, %int2_1342 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_1343 = torch.constant.int 6
    %1351 = torch.prims.convert_element_type %1350, %int6_1343 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_1344 = torch.constant.int 128
    %1352 = torch.aten.div.Scalar %1351, %int128_1344 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_1345 = torch.constant.float 2.000000e+00
    %1353 = torch.aten.mul.Scalar %1352, %float2.000000e00_1345 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_1346 = torch.constant.float 5.000000e+05
    %1354 = torch.aten.pow.Scalar %float5.000000e05_1346, %1353 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %1355 = torch.aten.reciprocal %1354 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_1347 = torch.constant.float 1.000000e+00
    %1356 = torch.aten.mul.Scalar %1355, %float1.000000e00_1347 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_1348 = torch.constant.int 1
    %1357 = torch.aten.unsqueeze %1348, %int1_1348 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_1349 = torch.constant.int 0
    %1358 = torch.aten.unsqueeze %1356, %int0_1349 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %1359 = torch.aten.mul.Tensor %1357, %1358 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_1350 = torch.constant.int 1
    %1360 = torch.aten.size.int %1306, %int1_1350 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int0_1351 = torch.constant.int 0
    %1361 = torch.aten.add.int %int0_1351, %1360 : !torch.int, !torch.int -> !torch.int
    %int0_1352 = torch.constant.int 0
    %int0_1353 = torch.constant.int 0
    %int1_1354 = torch.constant.int 1
    %1362 = torch.aten.slice.Tensor %1359, %int0_1352, %int0_1353, %1361, %int1_1354 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %1362, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_1355 = torch.constant.int 1
    %int0_1356 = torch.constant.int 0
    %int9223372036854775807_1357 = torch.constant.int 9223372036854775807
    %int1_1358 = torch.constant.int 1
    %1363 = torch.aten.slice.Tensor %1362, %int1_1355, %int0_1356, %int9223372036854775807_1357, %int1_1358 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %1363, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_1359 = torch.constant.int 1
    %int0_1360 = torch.constant.int 0
    %int9223372036854775807_1361 = torch.constant.int 9223372036854775807
    %int1_1362 = torch.constant.int 1
    %1364 = torch.aten.slice.Tensor %1363, %int1_1359, %int0_1360, %int9223372036854775807_1361, %int1_1362 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %1364, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_1363 = torch.constant.int 0
    %1365 = torch.aten.unsqueeze %1364, %int0_1363 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %1365, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_1364 = torch.constant.int 1
    %int0_1365 = torch.constant.int 0
    %int9223372036854775807_1366 = torch.constant.int 9223372036854775807
    %int1_1367 = torch.constant.int 1
    %1366 = torch.aten.slice.Tensor %1365, %int1_1364, %int0_1365, %int9223372036854775807_1366, %int1_1367 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %1366, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_1368 = torch.constant.int 2
    %int0_1369 = torch.constant.int 0
    %int9223372036854775807_1370 = torch.constant.int 9223372036854775807
    %int1_1371 = torch.constant.int 1
    %1367 = torch.aten.slice.Tensor %1366, %int2_1368, %int0_1369, %int9223372036854775807_1370, %int1_1371 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %1367, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_1372 = torch.constant.int 4
    %int1_1373 = torch.constant.int 1
    %int1_1374 = torch.constant.int 1
    %1368 = torch.prim.ListConstruct %int4_1372, %int1_1373, %int1_1374 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1369 = torch.aten.repeat %1367, %1368 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %1369, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_1375 = torch.constant.int 6
    %1370 = torch.prims.convert_element_type %1317, %int6_1375 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %1370, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %1371 = torch_c.to_builtin_tensor %1370 : !torch.vtensor<[4,?,8,128],f32> -> tensor<4x?x8x128xf32>
    %1372 = torch_c.to_builtin_tensor %1369 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %1373 = util.call @sharktank_rotary_embedding_4_D_8_128_f32(%1371, %1372) : (tensor<4x?x8x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x8x128xf32>
    %1374 = torch_c.from_builtin_tensor %1373 : tensor<4x?x8x128xf32> -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %1374, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %int5_1376 = torch.constant.int 5
    %1375 = torch.prims.convert_element_type %1374, %int5_1376 : !torch.vtensor<[4,?,8,128],f32>, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %1375, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int64_1377 = torch.constant.int 64
    %1376 = torch.aten.mul.Scalar %arg2, %int64_1377 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1376, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int10 = torch.constant.int 10
    %int1_1378 = torch.constant.int 1
    %1377 = torch.aten.add.Scalar %1376, %int10, %int1_1378 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1377, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_1379 = torch.constant.int 4
    %int32_1380 = torch.constant.int 32
    %int8_1381 = torch.constant.int 8
    %int128_1382 = torch.constant.int 128
    %1378 = torch.prim.ListConstruct %int4_1379, %398, %int32_1380, %int8_1381, %int128_1382 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1379 = torch.aten.view %1375, %1378 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1379, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_1383 = torch.constant.int 4
    %1380 = torch.aten.mul.int %int4_1383, %398 : !torch.int, !torch.int -> !torch.int
    %int32_1384 = torch.constant.int 32
    %int8_1385 = torch.constant.int 8
    %int128_1386 = torch.constant.int 128
    %1381 = torch.prim.ListConstruct %1380, %int32_1384, %int8_1385, %int128_1386 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1382 = torch.aten.view %1379, %1381 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %1382, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_1387 = torch.constant.int 4
    %1383 = torch.aten.mul.int %int4_1387, %398 : !torch.int, !torch.int -> !torch.int
    %1384 = torch.prim.ListConstruct %1383 : (!torch.int) -> !torch.list<int>
    %1385 = torch.aten.view %1377, %1384 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1385, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_1388 = torch.constant.int 32
    %int2_1389 = torch.constant.int 2
    %int32_1390 = torch.constant.int 32
    %int8_1391 = torch.constant.int 8
    %int128_1392 = torch.constant.int 128
    %1386 = torch.prim.ListConstruct %389, %int32_1388, %int2_1389, %int32_1390, %int8_1391, %int128_1392 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1387 = torch.aten.view %1219, %1386 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1387, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_1393 = torch.constant.int 32
    %1388 = torch.aten.mul.int %389, %int32_1393 : !torch.int, !torch.int -> !torch.int
    %int2_1394 = torch.constant.int 2
    %1389 = torch.aten.mul.int %1388, %int2_1394 : !torch.int, !torch.int -> !torch.int
    %int32_1395 = torch.constant.int 32
    %int8_1396 = torch.constant.int 8
    %int128_1397 = torch.constant.int 128
    %1390 = torch.prim.ListConstruct %1389, %int32_1395, %int8_1396, %int128_1397 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1391 = torch.aten.view %1387, %1390 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %1391, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %1392 = torch.prim.ListConstruct %1385 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_1398 = torch.constant.bool false
    %1393 = torch.aten.index_put %1391, %1392, %1382, %false_1398 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %1393, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_1399 = torch.constant.int 32
    %int2_1400 = torch.constant.int 2
    %int32_1401 = torch.constant.int 32
    %int8_1402 = torch.constant.int 8
    %int128_1403 = torch.constant.int 128
    %1394 = torch.prim.ListConstruct %389, %int32_1399, %int2_1400, %int32_1401, %int8_1402, %int128_1403 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1395 = torch.aten.view %1393, %1394 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1395, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_1404 = torch.constant.int 2097152
    %1396 = torch.prim.ListConstruct %389, %int2097152_1404 : (!torch.int, !torch.int) -> !torch.list<int>
    %1397 = torch.aten.view %1395, %1396 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %1397, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_1405 = torch.constant.int 32
    %int2_1406 = torch.constant.int 2
    %int32_1407 = torch.constant.int 32
    %int8_1408 = torch.constant.int 8
    %int128_1409 = torch.constant.int 128
    %1398 = torch.prim.ListConstruct %389, %int32_1405, %int2_1406, %int32_1407, %int8_1408, %int128_1409 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1399 = torch.aten.view %1397, %1398 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1399, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_1410 = torch.constant.int 32
    %int8_1411 = torch.constant.int 8
    %int128_1412 = torch.constant.int 128
    %1400 = torch.prim.ListConstruct %1389, %int32_1410, %int8_1411, %int128_1412 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1401 = torch.aten.view %1399, %1400 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %1401, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_1413 = torch.constant.int 4
    %int32_1414 = torch.constant.int 32
    %int8_1415 = torch.constant.int 8
    %int128_1416 = torch.constant.int 128
    %1402 = torch.prim.ListConstruct %int4_1413, %398, %int32_1414, %int8_1415, %int128_1416 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1403 = torch.aten.view %1319, %1402 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1403, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_1417 = torch.constant.int 4
    %1404 = torch.aten.mul.int %int4_1417, %398 : !torch.int, !torch.int -> !torch.int
    %int32_1418 = torch.constant.int 32
    %int8_1419 = torch.constant.int 8
    %int128_1420 = torch.constant.int 128
    %1405 = torch.prim.ListConstruct %1404, %int32_1418, %int8_1419, %int128_1420 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1406 = torch.aten.view %1403, %1405 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %1406, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int1_1421 = torch.constant.int 1
    %int1_1422 = torch.constant.int 1
    %1407 = torch.aten.add.Scalar %1377, %int1_1421, %int1_1422 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1407, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_1423 = torch.constant.int 4
    %1408 = torch.aten.mul.int %int4_1423, %398 : !torch.int, !torch.int -> !torch.int
    %1409 = torch.prim.ListConstruct %1408 : (!torch.int) -> !torch.list<int>
    %1410 = torch.aten.view %1407, %1409 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1410, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %1411 = torch.prim.ListConstruct %1410 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_1424 = torch.constant.bool false
    %1412 = torch.aten.index_put %1401, %1411, %1406, %false_1424 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %1412, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_1425 = torch.constant.int 32
    %int2_1426 = torch.constant.int 2
    %int32_1427 = torch.constant.int 32
    %int8_1428 = torch.constant.int 8
    %int128_1429 = torch.constant.int 128
    %1413 = torch.prim.ListConstruct %389, %int32_1425, %int2_1426, %int32_1427, %int8_1428, %int128_1429 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1414 = torch.aten.view %1412, %1413 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1414, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_1430 = torch.constant.int 2097152
    %1415 = torch.prim.ListConstruct %389, %int2097152_1430 : (!torch.int, !torch.int) -> !torch.list<int>
    %1416 = torch.aten.view %1414, %1415 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %1416, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int-2_1431 = torch.constant.int -2
    %1417 = torch.aten.unsqueeze %1375, %int-2_1431 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %1417, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int4_1432 = torch.constant.int 4
    %int8_1433 = torch.constant.int 8
    %int4_1434 = torch.constant.int 4
    %int128_1435 = torch.constant.int 128
    %1418 = torch.prim.ListConstruct %int4_1432, %1360, %int8_1433, %int4_1434, %int128_1435 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_1436 = torch.constant.bool false
    %1419 = torch.aten.expand %1417, %1418, %false_1436 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1419, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_1437 = torch.constant.int 0
    %1420 = torch.aten.clone %1419, %int0_1437 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1420, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_1438 = torch.constant.int 4
    %int32_1439 = torch.constant.int 32
    %int128_1440 = torch.constant.int 128
    %1421 = torch.prim.ListConstruct %int4_1438, %1360, %int32_1439, %int128_1440 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1422 = torch.aten._unsafe_view %1420, %1421 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %1422, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_1441 = torch.constant.int -2
    %1423 = torch.aten.unsqueeze %1319, %int-2_1441 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %1423, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_1442 = torch.constant.int 1
    %1424 = torch.aten.size.int %1313, %int1_1442 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int4_1443 = torch.constant.int 4
    %int8_1444 = torch.constant.int 8
    %int4_1445 = torch.constant.int 4
    %int128_1446 = torch.constant.int 128
    %1425 = torch.prim.ListConstruct %int4_1443, %1424, %int8_1444, %int4_1445, %int128_1446 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_1447 = torch.constant.bool false
    %1426 = torch.aten.expand %1423, %1425, %false_1447 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1426, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_1448 = torch.constant.int 0
    %1427 = torch.aten.clone %1426, %int0_1448 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1427, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_1449 = torch.constant.int 4
    %int32_1450 = torch.constant.int 32
    %int128_1451 = torch.constant.int 128
    %1428 = torch.prim.ListConstruct %int4_1449, %1424, %int32_1450, %int128_1451 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1429 = torch.aten._unsafe_view %1427, %1428 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %1429, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_1452 = torch.constant.int 1
    %int2_1453 = torch.constant.int 2
    %1430 = torch.aten.transpose.int %1347, %int1_1452, %int2_1453 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %1430, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_1454 = torch.constant.int 1
    %int2_1455 = torch.constant.int 2
    %1431 = torch.aten.transpose.int %1422, %int1_1454, %int2_1455 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %1431, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_1456 = torch.constant.int 1
    %int2_1457 = torch.constant.int 2
    %1432 = torch.aten.transpose.int %1429, %int1_1456, %int2_1457 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %1432, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_1458 = torch.constant.float 0.000000e+00
    %true_1459 = torch.constant.bool true
    %none_1460 = torch.constant.none
    %none_1461 = torch.constant.none
    %1433:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1430, %1431, %1432, %float0.000000e00_1458, %true_1459, %none_1460, %none_1461) : (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?],f32>) 
    torch.bind_symbolic_shape %1433#0, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_1462 = torch.constant.int 1
    %int2_1463 = torch.constant.int 2
    %1434 = torch.aten.transpose.int %1433#0, %int1_1462, %int2_1463 : !torch.vtensor<[4,32,?,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %1434, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_1464 = torch.constant.int 4
    %int4096_1465 = torch.constant.int 4096
    %1435 = torch.prim.ListConstruct %int4_1464, %1332, %int4096_1465 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1436 = torch.aten.view %1434, %1435 : !torch.vtensor<[4,?,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1436, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_1466 = torch.constant.int -2
    %int-1_1467 = torch.constant.int -1
    %1437 = torch.aten.transpose.int %50, %int-2_1466, %int-1_1467 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_1468 = torch.constant.int 4
    %1438 = torch.aten.mul.int %int4_1468, %1332 : !torch.int, !torch.int -> !torch.int
    %int4096_1469 = torch.constant.int 4096
    %1439 = torch.prim.ListConstruct %1438, %int4096_1469 : (!torch.int, !torch.int) -> !torch.list<int>
    %1440 = torch.aten.view %1436, %1439 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1440, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %1441 = torch.aten.mm %1440, %1437 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1441, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_1470 = torch.constant.int 4
    %int4096_1471 = torch.constant.int 4096
    %1442 = torch.prim.ListConstruct %int4_1470, %1332, %int4096_1471 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1443 = torch.aten.view %1441, %1442 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1443, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_1472 = torch.constant.int 1
    %1444 = torch.aten.add.Tensor %1282, %1443, %int1_1472 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1444, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_1473 = torch.constant.int 6
    %1445 = torch.prims.convert_element_type %1444, %int6_1473 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1445, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_1474 = torch.constant.int 2
    %1446 = torch.aten.pow.Tensor_Scalar %1445, %int2_1474 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1446, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_1475 = torch.constant.int -1
    %1447 = torch.prim.ListConstruct %int-1_1475 : (!torch.int) -> !torch.list<int>
    %true_1476 = torch.constant.bool true
    %none_1477 = torch.constant.none
    %1448 = torch.aten.mean.dim %1446, %1447, %true_1476, %none_1477 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1448, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_1478 = torch.constant.float 9.9999997473787516E-6
    %int1_1479 = torch.constant.int 1
    %1449 = torch.aten.add.Scalar %1448, %float9.999990e-06_1478, %int1_1479 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1449, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %1450 = torch.aten.rsqrt %1449 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1450, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %1451 = torch.aten.mul.Tensor %1445, %1450 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1451, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_1480 = torch.constant.int 5
    %1452 = torch.prims.convert_element_type %1451, %int5_1480 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1452, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %1453 = torch.aten.mul.Tensor %51, %1452 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1453, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_1481 = torch.constant.int 5
    %1454 = torch.prims.convert_element_type %1453, %int5_1481 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1454, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_1482 = torch.constant.int -2
    %int-1_1483 = torch.constant.int -1
    %1455 = torch.aten.transpose.int %52, %int-2_1482, %int-1_1483 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_1484 = torch.constant.int 4
    %1456 = torch.aten.mul.int %int4_1484, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_1485 = torch.constant.int 4096
    %1457 = torch.prim.ListConstruct %1456, %int4096_1485 : (!torch.int, !torch.int) -> !torch.list<int>
    %1458 = torch.aten.view %1454, %1457 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1458, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %1459 = torch.aten.mm %1458, %1455 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %1459, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_1486 = torch.constant.int 4
    %int14336_1487 = torch.constant.int 14336
    %1460 = torch.prim.ListConstruct %int4_1486, %306, %int14336_1487 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1461 = torch.aten.view %1459, %1460 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %1461, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %1462 = torch.aten.silu %1461 : !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %1462, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_1488 = torch.constant.int -2
    %int-1_1489 = torch.constant.int -1
    %1463 = torch.aten.transpose.int %53, %int-2_1488, %int-1_1489 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_1490 = torch.constant.int 4
    %1464 = torch.aten.mul.int %int4_1490, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_1491 = torch.constant.int 4096
    %1465 = torch.prim.ListConstruct %1464, %int4096_1491 : (!torch.int, !torch.int) -> !torch.list<int>
    %1466 = torch.aten.view %1454, %1465 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1466, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %1467 = torch.aten.mm %1466, %1463 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %1467, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_1492 = torch.constant.int 4
    %int14336_1493 = torch.constant.int 14336
    %1468 = torch.prim.ListConstruct %int4_1492, %306, %int14336_1493 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1469 = torch.aten.view %1467, %1468 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %1469, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %1470 = torch.aten.mul.Tensor %1462, %1469 : !torch.vtensor<[4,?,14336],f16>, !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %1470, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_1494 = torch.constant.int -2
    %int-1_1495 = torch.constant.int -1
    %1471 = torch.aten.transpose.int %54, %int-2_1494, %int-1_1495 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int1_1496 = torch.constant.int 1
    %1472 = torch.aten.size.int %1461, %int1_1496 : !torch.vtensor<[4,?,14336],f16>, !torch.int -> !torch.int
    %int4_1497 = torch.constant.int 4
    %1473 = torch.aten.mul.int %int4_1497, %1472 : !torch.int, !torch.int -> !torch.int
    %int14336_1498 = torch.constant.int 14336
    %1474 = torch.prim.ListConstruct %1473, %int14336_1498 : (!torch.int, !torch.int) -> !torch.list<int>
    %1475 = torch.aten.view %1470, %1474 : !torch.vtensor<[4,?,14336],f16>, !torch.list<int> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %1475, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %1476 = torch.aten.mm %1475, %1471 : !torch.vtensor<[?,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1476, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_1499 = torch.constant.int 4
    %int4096_1500 = torch.constant.int 4096
    %1477 = torch.prim.ListConstruct %int4_1499, %1472, %int4096_1500 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1478 = torch.aten.view %1476, %1477 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1478, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_1501 = torch.constant.int 1
    %1479 = torch.aten.add.Tensor %1444, %1478, %int1_1501 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1479, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_1502 = torch.constant.int 6
    %1480 = torch.prims.convert_element_type %1479, %int6_1502 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1480, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_1503 = torch.constant.int 2
    %1481 = torch.aten.pow.Tensor_Scalar %1480, %int2_1503 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1481, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_1504 = torch.constant.int -1
    %1482 = torch.prim.ListConstruct %int-1_1504 : (!torch.int) -> !torch.list<int>
    %true_1505 = torch.constant.bool true
    %none_1506 = torch.constant.none
    %1483 = torch.aten.mean.dim %1481, %1482, %true_1505, %none_1506 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1483, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_1507 = torch.constant.float 9.9999997473787516E-6
    %int1_1508 = torch.constant.int 1
    %1484 = torch.aten.add.Scalar %1483, %float9.999990e-06_1507, %int1_1508 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1484, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %1485 = torch.aten.rsqrt %1484 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1485, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %1486 = torch.aten.mul.Tensor %1480, %1485 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1486, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_1509 = torch.constant.int 5
    %1487 = torch.prims.convert_element_type %1486, %int5_1509 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1487, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %1488 = torch.aten.mul.Tensor %55, %1487 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1488, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_1510 = torch.constant.int 5
    %1489 = torch.prims.convert_element_type %1488, %int5_1510 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1489, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_1511 = torch.constant.int -2
    %int-1_1512 = torch.constant.int -1
    %1490 = torch.aten.transpose.int %56, %int-2_1511, %int-1_1512 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_1513 = torch.constant.int 4
    %1491 = torch.aten.mul.int %int4_1513, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_1514 = torch.constant.int 4096
    %1492 = torch.prim.ListConstruct %1491, %int4096_1514 : (!torch.int, !torch.int) -> !torch.list<int>
    %1493 = torch.aten.view %1489, %1492 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1493, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %1494 = torch.aten.mm %1493, %1490 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1494, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_1515 = torch.constant.int 4
    %int4096_1516 = torch.constant.int 4096
    %1495 = torch.prim.ListConstruct %int4_1515, %306, %int4096_1516 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1496 = torch.aten.view %1494, %1495 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1496, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_1517 = torch.constant.int -2
    %int-1_1518 = torch.constant.int -1
    %1497 = torch.aten.transpose.int %57, %int-2_1517, %int-1_1518 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_1519 = torch.constant.int 4
    %1498 = torch.aten.mul.int %int4_1519, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_1520 = torch.constant.int 4096
    %1499 = torch.prim.ListConstruct %1498, %int4096_1520 : (!torch.int, !torch.int) -> !torch.list<int>
    %1500 = torch.aten.view %1489, %1499 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1500, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %1501 = torch.aten.mm %1500, %1497 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %1501, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_1521 = torch.constant.int 4
    %int1024_1522 = torch.constant.int 1024
    %1502 = torch.prim.ListConstruct %int4_1521, %306, %int1024_1522 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1503 = torch.aten.view %1501, %1502 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %1503, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int-2_1523 = torch.constant.int -2
    %int-1_1524 = torch.constant.int -1
    %1504 = torch.aten.transpose.int %58, %int-2_1523, %int-1_1524 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_1525 = torch.constant.int 4
    %1505 = torch.aten.mul.int %int4_1525, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_1526 = torch.constant.int 4096
    %1506 = torch.prim.ListConstruct %1505, %int4096_1526 : (!torch.int, !torch.int) -> !torch.list<int>
    %1507 = torch.aten.view %1489, %1506 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1507, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %1508 = torch.aten.mm %1507, %1504 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %1508, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_1527 = torch.constant.int 4
    %int1024_1528 = torch.constant.int 1024
    %1509 = torch.prim.ListConstruct %int4_1527, %306, %int1024_1528 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1510 = torch.aten.view %1508, %1509 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %1510, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int4_1529 = torch.constant.int 4
    %int32_1530 = torch.constant.int 32
    %int128_1531 = torch.constant.int 128
    %1511 = torch.prim.ListConstruct %int4_1529, %306, %int32_1530, %int128_1531 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1512 = torch.aten.view %1496, %1511 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %1512, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_1532 = torch.constant.int 4
    %int8_1533 = torch.constant.int 8
    %int128_1534 = torch.constant.int 128
    %1513 = torch.prim.ListConstruct %int4_1532, %306, %int8_1533, %int128_1534 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1514 = torch.aten.view %1503, %1513 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %1514, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int4_1535 = torch.constant.int 4
    %int8_1536 = torch.constant.int 8
    %int128_1537 = torch.constant.int 128
    %1515 = torch.prim.ListConstruct %int4_1535, %306, %int8_1536, %int128_1537 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1516 = torch.aten.view %1510, %1515 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %1516, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int131072_1538 = torch.constant.int 131072
    %none_1539 = torch.constant.none
    %none_1540 = torch.constant.none
    %cpu_1541 = torch.constant.device "cpu"
    %false_1542 = torch.constant.bool false
    %1517 = torch.aten.arange %int131072_1538, %none_1539, %none_1540, %cpu_1541, %false_1542 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_1543 = torch.constant.int 0
    %int128_1544 = torch.constant.int 128
    %none_1545 = torch.constant.none
    %none_1546 = torch.constant.none
    %cpu_1547 = torch.constant.device "cpu"
    %false_1548 = torch.constant.bool false
    %1518 = torch.aten.arange.start %int0_1543, %int128_1544, %none_1545, %none_1546, %cpu_1547, %false_1548 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_1549 = torch.constant.int 2
    %1519 = torch.aten.floor_divide.Scalar %1518, %int2_1549 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_1550 = torch.constant.int 6
    %1520 = torch.prims.convert_element_type %1519, %int6_1550 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_1551 = torch.constant.int 128
    %1521 = torch.aten.div.Scalar %1520, %int128_1551 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_1552 = torch.constant.float 2.000000e+00
    %1522 = torch.aten.mul.Scalar %1521, %float2.000000e00_1552 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_1553 = torch.constant.float 5.000000e+05
    %1523 = torch.aten.pow.Scalar %float5.000000e05_1553, %1522 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %1524 = torch.aten.reciprocal %1523 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_1554 = torch.constant.float 1.000000e+00
    %1525 = torch.aten.mul.Scalar %1524, %float1.000000e00_1554 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_1555 = torch.constant.int 1
    %1526 = torch.aten.unsqueeze %1517, %int1_1555 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_1556 = torch.constant.int 0
    %1527 = torch.aten.unsqueeze %1525, %int0_1556 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %1528 = torch.aten.mul.Tensor %1526, %1527 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_1557 = torch.constant.int 1
    %1529 = torch.aten.size.int %1496, %int1_1557 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.int
    %int0_1558 = torch.constant.int 0
    %1530 = torch.aten.add.int %int0_1558, %1529 : !torch.int, !torch.int -> !torch.int
    %int0_1559 = torch.constant.int 0
    %int0_1560 = torch.constant.int 0
    %int1_1561 = torch.constant.int 1
    %1531 = torch.aten.slice.Tensor %1528, %int0_1559, %int0_1560, %1530, %int1_1561 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %1531, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_1562 = torch.constant.int 1
    %int0_1563 = torch.constant.int 0
    %int9223372036854775807_1564 = torch.constant.int 9223372036854775807
    %int1_1565 = torch.constant.int 1
    %1532 = torch.aten.slice.Tensor %1531, %int1_1562, %int0_1563, %int9223372036854775807_1564, %int1_1565 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %1532, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_1566 = torch.constant.int 1
    %int0_1567 = torch.constant.int 0
    %int9223372036854775807_1568 = torch.constant.int 9223372036854775807
    %int1_1569 = torch.constant.int 1
    %1533 = torch.aten.slice.Tensor %1532, %int1_1566, %int0_1567, %int9223372036854775807_1568, %int1_1569 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %1533, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_1570 = torch.constant.int 0
    %1534 = torch.aten.unsqueeze %1533, %int0_1570 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %1534, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_1571 = torch.constant.int 1
    %int0_1572 = torch.constant.int 0
    %int9223372036854775807_1573 = torch.constant.int 9223372036854775807
    %int1_1574 = torch.constant.int 1
    %1535 = torch.aten.slice.Tensor %1534, %int1_1571, %int0_1572, %int9223372036854775807_1573, %int1_1574 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %1535, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_1575 = torch.constant.int 2
    %int0_1576 = torch.constant.int 0
    %int9223372036854775807_1577 = torch.constant.int 9223372036854775807
    %int1_1578 = torch.constant.int 1
    %1536 = torch.aten.slice.Tensor %1535, %int2_1575, %int0_1576, %int9223372036854775807_1577, %int1_1578 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %1536, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_1579 = torch.constant.int 4
    %int1_1580 = torch.constant.int 1
    %int1_1581 = torch.constant.int 1
    %1537 = torch.prim.ListConstruct %int4_1579, %int1_1580, %int1_1581 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1538 = torch.aten.repeat %1536, %1537 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %1538, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_1582 = torch.constant.int 6
    %1539 = torch.prims.convert_element_type %1512, %int6_1582 : !torch.vtensor<[4,?,32,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %1539, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %1540 = torch_c.to_builtin_tensor %1539 : !torch.vtensor<[4,?,32,128],f32> -> tensor<4x?x32x128xf32>
    %1541 = torch_c.to_builtin_tensor %1538 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %1542 = util.call @sharktank_rotary_embedding_4_D_32_128_f32(%1540, %1541) : (tensor<4x?x32x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x32x128xf32>
    %1543 = torch_c.from_builtin_tensor %1542 : tensor<4x?x32x128xf32> -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %1543, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %int5_1583 = torch.constant.int 5
    %1544 = torch.prims.convert_element_type %1543, %int5_1583 : !torch.vtensor<[4,?,32,128],f32>, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %1544, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int131072_1584 = torch.constant.int 131072
    %none_1585 = torch.constant.none
    %none_1586 = torch.constant.none
    %cpu_1587 = torch.constant.device "cpu"
    %false_1588 = torch.constant.bool false
    %1545 = torch.aten.arange %int131072_1584, %none_1585, %none_1586, %cpu_1587, %false_1588 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_1589 = torch.constant.int 0
    %int128_1590 = torch.constant.int 128
    %none_1591 = torch.constant.none
    %none_1592 = torch.constant.none
    %cpu_1593 = torch.constant.device "cpu"
    %false_1594 = torch.constant.bool false
    %1546 = torch.aten.arange.start %int0_1589, %int128_1590, %none_1591, %none_1592, %cpu_1593, %false_1594 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_1595 = torch.constant.int 2
    %1547 = torch.aten.floor_divide.Scalar %1546, %int2_1595 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_1596 = torch.constant.int 6
    %1548 = torch.prims.convert_element_type %1547, %int6_1596 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_1597 = torch.constant.int 128
    %1549 = torch.aten.div.Scalar %1548, %int128_1597 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_1598 = torch.constant.float 2.000000e+00
    %1550 = torch.aten.mul.Scalar %1549, %float2.000000e00_1598 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_1599 = torch.constant.float 5.000000e+05
    %1551 = torch.aten.pow.Scalar %float5.000000e05_1599, %1550 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %1552 = torch.aten.reciprocal %1551 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_1600 = torch.constant.float 1.000000e+00
    %1553 = torch.aten.mul.Scalar %1552, %float1.000000e00_1600 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_1601 = torch.constant.int 1
    %1554 = torch.aten.unsqueeze %1545, %int1_1601 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_1602 = torch.constant.int 0
    %1555 = torch.aten.unsqueeze %1553, %int0_1602 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %1556 = torch.aten.mul.Tensor %1554, %1555 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_1603 = torch.constant.int 1
    %1557 = torch.aten.size.int %1503, %int1_1603 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int0_1604 = torch.constant.int 0
    %1558 = torch.aten.add.int %int0_1604, %1557 : !torch.int, !torch.int -> !torch.int
    %int0_1605 = torch.constant.int 0
    %int0_1606 = torch.constant.int 0
    %int1_1607 = torch.constant.int 1
    %1559 = torch.aten.slice.Tensor %1556, %int0_1605, %int0_1606, %1558, %int1_1607 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %1559, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_1608 = torch.constant.int 1
    %int0_1609 = torch.constant.int 0
    %int9223372036854775807_1610 = torch.constant.int 9223372036854775807
    %int1_1611 = torch.constant.int 1
    %1560 = torch.aten.slice.Tensor %1559, %int1_1608, %int0_1609, %int9223372036854775807_1610, %int1_1611 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %1560, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_1612 = torch.constant.int 1
    %int0_1613 = torch.constant.int 0
    %int9223372036854775807_1614 = torch.constant.int 9223372036854775807
    %int1_1615 = torch.constant.int 1
    %1561 = torch.aten.slice.Tensor %1560, %int1_1612, %int0_1613, %int9223372036854775807_1614, %int1_1615 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %1561, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_1616 = torch.constant.int 0
    %1562 = torch.aten.unsqueeze %1561, %int0_1616 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %1562, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_1617 = torch.constant.int 1
    %int0_1618 = torch.constant.int 0
    %int9223372036854775807_1619 = torch.constant.int 9223372036854775807
    %int1_1620 = torch.constant.int 1
    %1563 = torch.aten.slice.Tensor %1562, %int1_1617, %int0_1618, %int9223372036854775807_1619, %int1_1620 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %1563, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_1621 = torch.constant.int 2
    %int0_1622 = torch.constant.int 0
    %int9223372036854775807_1623 = torch.constant.int 9223372036854775807
    %int1_1624 = torch.constant.int 1
    %1564 = torch.aten.slice.Tensor %1563, %int2_1621, %int0_1622, %int9223372036854775807_1623, %int1_1624 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %1564, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_1625 = torch.constant.int 4
    %int1_1626 = torch.constant.int 1
    %int1_1627 = torch.constant.int 1
    %1565 = torch.prim.ListConstruct %int4_1625, %int1_1626, %int1_1627 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1566 = torch.aten.repeat %1564, %1565 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %1566, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_1628 = torch.constant.int 6
    %1567 = torch.prims.convert_element_type %1514, %int6_1628 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %1567, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %1568 = torch_c.to_builtin_tensor %1567 : !torch.vtensor<[4,?,8,128],f32> -> tensor<4x?x8x128xf32>
    %1569 = torch_c.to_builtin_tensor %1566 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %1570 = util.call @sharktank_rotary_embedding_4_D_8_128_f32(%1568, %1569) : (tensor<4x?x8x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x8x128xf32>
    %1571 = torch_c.from_builtin_tensor %1570 : tensor<4x?x8x128xf32> -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %1571, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %int5_1629 = torch.constant.int 5
    %1572 = torch.prims.convert_element_type %1571, %int5_1629 : !torch.vtensor<[4,?,8,128],f32>, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %1572, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int64_1630 = torch.constant.int 64
    %1573 = torch.aten.mul.Scalar %arg2, %int64_1630 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1573, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int12 = torch.constant.int 12
    %int1_1631 = torch.constant.int 1
    %1574 = torch.aten.add.Scalar %1573, %int12, %int1_1631 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1574, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_1632 = torch.constant.int 4
    %int32_1633 = torch.constant.int 32
    %int8_1634 = torch.constant.int 8
    %int128_1635 = torch.constant.int 128
    %1575 = torch.prim.ListConstruct %int4_1632, %398, %int32_1633, %int8_1634, %int128_1635 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1576 = torch.aten.view %1572, %1575 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1576, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_1636 = torch.constant.int 4
    %1577 = torch.aten.mul.int %int4_1636, %398 : !torch.int, !torch.int -> !torch.int
    %int32_1637 = torch.constant.int 32
    %int8_1638 = torch.constant.int 8
    %int128_1639 = torch.constant.int 128
    %1578 = torch.prim.ListConstruct %1577, %int32_1637, %int8_1638, %int128_1639 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1579 = torch.aten.view %1576, %1578 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %1579, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_1640 = torch.constant.int 4
    %1580 = torch.aten.mul.int %int4_1640, %398 : !torch.int, !torch.int -> !torch.int
    %1581 = torch.prim.ListConstruct %1580 : (!torch.int) -> !torch.list<int>
    %1582 = torch.aten.view %1574, %1581 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1582, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_1641 = torch.constant.int 32
    %int2_1642 = torch.constant.int 2
    %int32_1643 = torch.constant.int 32
    %int8_1644 = torch.constant.int 8
    %int128_1645 = torch.constant.int 128
    %1583 = torch.prim.ListConstruct %389, %int32_1641, %int2_1642, %int32_1643, %int8_1644, %int128_1645 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1584 = torch.aten.view %1416, %1583 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1584, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_1646 = torch.constant.int 32
    %1585 = torch.aten.mul.int %389, %int32_1646 : !torch.int, !torch.int -> !torch.int
    %int2_1647 = torch.constant.int 2
    %1586 = torch.aten.mul.int %1585, %int2_1647 : !torch.int, !torch.int -> !torch.int
    %int32_1648 = torch.constant.int 32
    %int8_1649 = torch.constant.int 8
    %int128_1650 = torch.constant.int 128
    %1587 = torch.prim.ListConstruct %1586, %int32_1648, %int8_1649, %int128_1650 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1588 = torch.aten.view %1584, %1587 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %1588, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %1589 = torch.prim.ListConstruct %1582 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_1651 = torch.constant.bool false
    %1590 = torch.aten.index_put %1588, %1589, %1579, %false_1651 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %1590, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_1652 = torch.constant.int 32
    %int2_1653 = torch.constant.int 2
    %int32_1654 = torch.constant.int 32
    %int8_1655 = torch.constant.int 8
    %int128_1656 = torch.constant.int 128
    %1591 = torch.prim.ListConstruct %389, %int32_1652, %int2_1653, %int32_1654, %int8_1655, %int128_1656 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1592 = torch.aten.view %1590, %1591 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1592, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_1657 = torch.constant.int 2097152
    %1593 = torch.prim.ListConstruct %389, %int2097152_1657 : (!torch.int, !torch.int) -> !torch.list<int>
    %1594 = torch.aten.view %1592, %1593 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %1594, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_1658 = torch.constant.int 32
    %int2_1659 = torch.constant.int 2
    %int32_1660 = torch.constant.int 32
    %int8_1661 = torch.constant.int 8
    %int128_1662 = torch.constant.int 128
    %1595 = torch.prim.ListConstruct %389, %int32_1658, %int2_1659, %int32_1660, %int8_1661, %int128_1662 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1596 = torch.aten.view %1594, %1595 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1596, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_1663 = torch.constant.int 32
    %int8_1664 = torch.constant.int 8
    %int128_1665 = torch.constant.int 128
    %1597 = torch.prim.ListConstruct %1586, %int32_1663, %int8_1664, %int128_1665 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1598 = torch.aten.view %1596, %1597 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %1598, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_1666 = torch.constant.int 4
    %int32_1667 = torch.constant.int 32
    %int8_1668 = torch.constant.int 8
    %int128_1669 = torch.constant.int 128
    %1599 = torch.prim.ListConstruct %int4_1666, %398, %int32_1667, %int8_1668, %int128_1669 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1600 = torch.aten.view %1516, %1599 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1600, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_1670 = torch.constant.int 4
    %1601 = torch.aten.mul.int %int4_1670, %398 : !torch.int, !torch.int -> !torch.int
    %int32_1671 = torch.constant.int 32
    %int8_1672 = torch.constant.int 8
    %int128_1673 = torch.constant.int 128
    %1602 = torch.prim.ListConstruct %1601, %int32_1671, %int8_1672, %int128_1673 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1603 = torch.aten.view %1600, %1602 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %1603, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int1_1674 = torch.constant.int 1
    %int1_1675 = torch.constant.int 1
    %1604 = torch.aten.add.Scalar %1574, %int1_1674, %int1_1675 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1604, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_1676 = torch.constant.int 4
    %1605 = torch.aten.mul.int %int4_1676, %398 : !torch.int, !torch.int -> !torch.int
    %1606 = torch.prim.ListConstruct %1605 : (!torch.int) -> !torch.list<int>
    %1607 = torch.aten.view %1604, %1606 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1607, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %1608 = torch.prim.ListConstruct %1607 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_1677 = torch.constant.bool false
    %1609 = torch.aten.index_put %1598, %1608, %1603, %false_1677 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %1609, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_1678 = torch.constant.int 32
    %int2_1679 = torch.constant.int 2
    %int32_1680 = torch.constant.int 32
    %int8_1681 = torch.constant.int 8
    %int128_1682 = torch.constant.int 128
    %1610 = torch.prim.ListConstruct %389, %int32_1678, %int2_1679, %int32_1680, %int8_1681, %int128_1682 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1611 = torch.aten.view %1609, %1610 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1611, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_1683 = torch.constant.int 2097152
    %1612 = torch.prim.ListConstruct %389, %int2097152_1683 : (!torch.int, !torch.int) -> !torch.list<int>
    %1613 = torch.aten.view %1611, %1612 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %1613, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int-2_1684 = torch.constant.int -2
    %1614 = torch.aten.unsqueeze %1572, %int-2_1684 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %1614, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int4_1685 = torch.constant.int 4
    %int8_1686 = torch.constant.int 8
    %int4_1687 = torch.constant.int 4
    %int128_1688 = torch.constant.int 128
    %1615 = torch.prim.ListConstruct %int4_1685, %1557, %int8_1686, %int4_1687, %int128_1688 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_1689 = torch.constant.bool false
    %1616 = torch.aten.expand %1614, %1615, %false_1689 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1616, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_1690 = torch.constant.int 0
    %1617 = torch.aten.clone %1616, %int0_1690 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1617, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_1691 = torch.constant.int 4
    %int32_1692 = torch.constant.int 32
    %int128_1693 = torch.constant.int 128
    %1618 = torch.prim.ListConstruct %int4_1691, %1557, %int32_1692, %int128_1693 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1619 = torch.aten._unsafe_view %1617, %1618 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %1619, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_1694 = torch.constant.int -2
    %1620 = torch.aten.unsqueeze %1516, %int-2_1694 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %1620, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_1695 = torch.constant.int 1
    %1621 = torch.aten.size.int %1510, %int1_1695 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int4_1696 = torch.constant.int 4
    %int8_1697 = torch.constant.int 8
    %int4_1698 = torch.constant.int 4
    %int128_1699 = torch.constant.int 128
    %1622 = torch.prim.ListConstruct %int4_1696, %1621, %int8_1697, %int4_1698, %int128_1699 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_1700 = torch.constant.bool false
    %1623 = torch.aten.expand %1620, %1622, %false_1700 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1623, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_1701 = torch.constant.int 0
    %1624 = torch.aten.clone %1623, %int0_1701 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1624, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_1702 = torch.constant.int 4
    %int32_1703 = torch.constant.int 32
    %int128_1704 = torch.constant.int 128
    %1625 = torch.prim.ListConstruct %int4_1702, %1621, %int32_1703, %int128_1704 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1626 = torch.aten._unsafe_view %1624, %1625 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %1626, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_1705 = torch.constant.int 1
    %int2_1706 = torch.constant.int 2
    %1627 = torch.aten.transpose.int %1544, %int1_1705, %int2_1706 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %1627, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_1707 = torch.constant.int 1
    %int2_1708 = torch.constant.int 2
    %1628 = torch.aten.transpose.int %1619, %int1_1707, %int2_1708 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %1628, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_1709 = torch.constant.int 1
    %int2_1710 = torch.constant.int 2
    %1629 = torch.aten.transpose.int %1626, %int1_1709, %int2_1710 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %1629, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_1711 = torch.constant.float 0.000000e+00
    %true_1712 = torch.constant.bool true
    %none_1713 = torch.constant.none
    %none_1714 = torch.constant.none
    %1630:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1627, %1628, %1629, %float0.000000e00_1711, %true_1712, %none_1713, %none_1714) : (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?],f32>) 
    torch.bind_symbolic_shape %1630#0, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_1715 = torch.constant.int 1
    %int2_1716 = torch.constant.int 2
    %1631 = torch.aten.transpose.int %1630#0, %int1_1715, %int2_1716 : !torch.vtensor<[4,32,?,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %1631, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_1717 = torch.constant.int 4
    %int4096_1718 = torch.constant.int 4096
    %1632 = torch.prim.ListConstruct %int4_1717, %1529, %int4096_1718 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1633 = torch.aten.view %1631, %1632 : !torch.vtensor<[4,?,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1633, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_1719 = torch.constant.int -2
    %int-1_1720 = torch.constant.int -1
    %1634 = torch.aten.transpose.int %59, %int-2_1719, %int-1_1720 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_1721 = torch.constant.int 4
    %1635 = torch.aten.mul.int %int4_1721, %1529 : !torch.int, !torch.int -> !torch.int
    %int4096_1722 = torch.constant.int 4096
    %1636 = torch.prim.ListConstruct %1635, %int4096_1722 : (!torch.int, !torch.int) -> !torch.list<int>
    %1637 = torch.aten.view %1633, %1636 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1637, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %1638 = torch.aten.mm %1637, %1634 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1638, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_1723 = torch.constant.int 4
    %int4096_1724 = torch.constant.int 4096
    %1639 = torch.prim.ListConstruct %int4_1723, %1529, %int4096_1724 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1640 = torch.aten.view %1638, %1639 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1640, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_1725 = torch.constant.int 1
    %1641 = torch.aten.add.Tensor %1479, %1640, %int1_1725 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1641, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_1726 = torch.constant.int 6
    %1642 = torch.prims.convert_element_type %1641, %int6_1726 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1642, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_1727 = torch.constant.int 2
    %1643 = torch.aten.pow.Tensor_Scalar %1642, %int2_1727 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1643, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_1728 = torch.constant.int -1
    %1644 = torch.prim.ListConstruct %int-1_1728 : (!torch.int) -> !torch.list<int>
    %true_1729 = torch.constant.bool true
    %none_1730 = torch.constant.none
    %1645 = torch.aten.mean.dim %1643, %1644, %true_1729, %none_1730 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1645, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_1731 = torch.constant.float 9.9999997473787516E-6
    %int1_1732 = torch.constant.int 1
    %1646 = torch.aten.add.Scalar %1645, %float9.999990e-06_1731, %int1_1732 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1646, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %1647 = torch.aten.rsqrt %1646 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1647, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %1648 = torch.aten.mul.Tensor %1642, %1647 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1648, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_1733 = torch.constant.int 5
    %1649 = torch.prims.convert_element_type %1648, %int5_1733 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1649, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %1650 = torch.aten.mul.Tensor %60, %1649 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1650, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_1734 = torch.constant.int 5
    %1651 = torch.prims.convert_element_type %1650, %int5_1734 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1651, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_1735 = torch.constant.int -2
    %int-1_1736 = torch.constant.int -1
    %1652 = torch.aten.transpose.int %61, %int-2_1735, %int-1_1736 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_1737 = torch.constant.int 4
    %1653 = torch.aten.mul.int %int4_1737, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_1738 = torch.constant.int 4096
    %1654 = torch.prim.ListConstruct %1653, %int4096_1738 : (!torch.int, !torch.int) -> !torch.list<int>
    %1655 = torch.aten.view %1651, %1654 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1655, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %1656 = torch.aten.mm %1655, %1652 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %1656, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_1739 = torch.constant.int 4
    %int14336_1740 = torch.constant.int 14336
    %1657 = torch.prim.ListConstruct %int4_1739, %306, %int14336_1740 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1658 = torch.aten.view %1656, %1657 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %1658, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %1659 = torch.aten.silu %1658 : !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %1659, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_1741 = torch.constant.int -2
    %int-1_1742 = torch.constant.int -1
    %1660 = torch.aten.transpose.int %62, %int-2_1741, %int-1_1742 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_1743 = torch.constant.int 4
    %1661 = torch.aten.mul.int %int4_1743, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_1744 = torch.constant.int 4096
    %1662 = torch.prim.ListConstruct %1661, %int4096_1744 : (!torch.int, !torch.int) -> !torch.list<int>
    %1663 = torch.aten.view %1651, %1662 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1663, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %1664 = torch.aten.mm %1663, %1660 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %1664, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_1745 = torch.constant.int 4
    %int14336_1746 = torch.constant.int 14336
    %1665 = torch.prim.ListConstruct %int4_1745, %306, %int14336_1746 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1666 = torch.aten.view %1664, %1665 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %1666, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %1667 = torch.aten.mul.Tensor %1659, %1666 : !torch.vtensor<[4,?,14336],f16>, !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %1667, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_1747 = torch.constant.int -2
    %int-1_1748 = torch.constant.int -1
    %1668 = torch.aten.transpose.int %63, %int-2_1747, %int-1_1748 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int1_1749 = torch.constant.int 1
    %1669 = torch.aten.size.int %1658, %int1_1749 : !torch.vtensor<[4,?,14336],f16>, !torch.int -> !torch.int
    %int4_1750 = torch.constant.int 4
    %1670 = torch.aten.mul.int %int4_1750, %1669 : !torch.int, !torch.int -> !torch.int
    %int14336_1751 = torch.constant.int 14336
    %1671 = torch.prim.ListConstruct %1670, %int14336_1751 : (!torch.int, !torch.int) -> !torch.list<int>
    %1672 = torch.aten.view %1667, %1671 : !torch.vtensor<[4,?,14336],f16>, !torch.list<int> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %1672, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %1673 = torch.aten.mm %1672, %1668 : !torch.vtensor<[?,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1673, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_1752 = torch.constant.int 4
    %int4096_1753 = torch.constant.int 4096
    %1674 = torch.prim.ListConstruct %int4_1752, %1669, %int4096_1753 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1675 = torch.aten.view %1673, %1674 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1675, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_1754 = torch.constant.int 1
    %1676 = torch.aten.add.Tensor %1641, %1675, %int1_1754 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1676, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_1755 = torch.constant.int 6
    %1677 = torch.prims.convert_element_type %1676, %int6_1755 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1677, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_1756 = torch.constant.int 2
    %1678 = torch.aten.pow.Tensor_Scalar %1677, %int2_1756 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1678, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_1757 = torch.constant.int -1
    %1679 = torch.prim.ListConstruct %int-1_1757 : (!torch.int) -> !torch.list<int>
    %true_1758 = torch.constant.bool true
    %none_1759 = torch.constant.none
    %1680 = torch.aten.mean.dim %1678, %1679, %true_1758, %none_1759 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1680, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_1760 = torch.constant.float 9.9999997473787516E-6
    %int1_1761 = torch.constant.int 1
    %1681 = torch.aten.add.Scalar %1680, %float9.999990e-06_1760, %int1_1761 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1681, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %1682 = torch.aten.rsqrt %1681 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1682, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %1683 = torch.aten.mul.Tensor %1677, %1682 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1683, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_1762 = torch.constant.int 5
    %1684 = torch.prims.convert_element_type %1683, %int5_1762 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1684, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %1685 = torch.aten.mul.Tensor %64, %1684 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1685, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_1763 = torch.constant.int 5
    %1686 = torch.prims.convert_element_type %1685, %int5_1763 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1686, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_1764 = torch.constant.int -2
    %int-1_1765 = torch.constant.int -1
    %1687 = torch.aten.transpose.int %65, %int-2_1764, %int-1_1765 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_1766 = torch.constant.int 4
    %1688 = torch.aten.mul.int %int4_1766, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_1767 = torch.constant.int 4096
    %1689 = torch.prim.ListConstruct %1688, %int4096_1767 : (!torch.int, !torch.int) -> !torch.list<int>
    %1690 = torch.aten.view %1686, %1689 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1690, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %1691 = torch.aten.mm %1690, %1687 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1691, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_1768 = torch.constant.int 4
    %int4096_1769 = torch.constant.int 4096
    %1692 = torch.prim.ListConstruct %int4_1768, %306, %int4096_1769 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1693 = torch.aten.view %1691, %1692 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1693, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_1770 = torch.constant.int -2
    %int-1_1771 = torch.constant.int -1
    %1694 = torch.aten.transpose.int %66, %int-2_1770, %int-1_1771 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_1772 = torch.constant.int 4
    %1695 = torch.aten.mul.int %int4_1772, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_1773 = torch.constant.int 4096
    %1696 = torch.prim.ListConstruct %1695, %int4096_1773 : (!torch.int, !torch.int) -> !torch.list<int>
    %1697 = torch.aten.view %1686, %1696 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1697, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %1698 = torch.aten.mm %1697, %1694 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %1698, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_1774 = torch.constant.int 4
    %int1024_1775 = torch.constant.int 1024
    %1699 = torch.prim.ListConstruct %int4_1774, %306, %int1024_1775 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1700 = torch.aten.view %1698, %1699 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %1700, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int-2_1776 = torch.constant.int -2
    %int-1_1777 = torch.constant.int -1
    %1701 = torch.aten.transpose.int %67, %int-2_1776, %int-1_1777 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_1778 = torch.constant.int 4
    %1702 = torch.aten.mul.int %int4_1778, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_1779 = torch.constant.int 4096
    %1703 = torch.prim.ListConstruct %1702, %int4096_1779 : (!torch.int, !torch.int) -> !torch.list<int>
    %1704 = torch.aten.view %1686, %1703 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1704, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %1705 = torch.aten.mm %1704, %1701 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %1705, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_1780 = torch.constant.int 4
    %int1024_1781 = torch.constant.int 1024
    %1706 = torch.prim.ListConstruct %int4_1780, %306, %int1024_1781 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1707 = torch.aten.view %1705, %1706 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %1707, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int4_1782 = torch.constant.int 4
    %int32_1783 = torch.constant.int 32
    %int128_1784 = torch.constant.int 128
    %1708 = torch.prim.ListConstruct %int4_1782, %306, %int32_1783, %int128_1784 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1709 = torch.aten.view %1693, %1708 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %1709, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_1785 = torch.constant.int 4
    %int8_1786 = torch.constant.int 8
    %int128_1787 = torch.constant.int 128
    %1710 = torch.prim.ListConstruct %int4_1785, %306, %int8_1786, %int128_1787 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1711 = torch.aten.view %1700, %1710 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %1711, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int4_1788 = torch.constant.int 4
    %int8_1789 = torch.constant.int 8
    %int128_1790 = torch.constant.int 128
    %1712 = torch.prim.ListConstruct %int4_1788, %306, %int8_1789, %int128_1790 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1713 = torch.aten.view %1707, %1712 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %1713, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int131072_1791 = torch.constant.int 131072
    %none_1792 = torch.constant.none
    %none_1793 = torch.constant.none
    %cpu_1794 = torch.constant.device "cpu"
    %false_1795 = torch.constant.bool false
    %1714 = torch.aten.arange %int131072_1791, %none_1792, %none_1793, %cpu_1794, %false_1795 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_1796 = torch.constant.int 0
    %int128_1797 = torch.constant.int 128
    %none_1798 = torch.constant.none
    %none_1799 = torch.constant.none
    %cpu_1800 = torch.constant.device "cpu"
    %false_1801 = torch.constant.bool false
    %1715 = torch.aten.arange.start %int0_1796, %int128_1797, %none_1798, %none_1799, %cpu_1800, %false_1801 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_1802 = torch.constant.int 2
    %1716 = torch.aten.floor_divide.Scalar %1715, %int2_1802 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_1803 = torch.constant.int 6
    %1717 = torch.prims.convert_element_type %1716, %int6_1803 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_1804 = torch.constant.int 128
    %1718 = torch.aten.div.Scalar %1717, %int128_1804 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_1805 = torch.constant.float 2.000000e+00
    %1719 = torch.aten.mul.Scalar %1718, %float2.000000e00_1805 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_1806 = torch.constant.float 5.000000e+05
    %1720 = torch.aten.pow.Scalar %float5.000000e05_1806, %1719 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %1721 = torch.aten.reciprocal %1720 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_1807 = torch.constant.float 1.000000e+00
    %1722 = torch.aten.mul.Scalar %1721, %float1.000000e00_1807 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_1808 = torch.constant.int 1
    %1723 = torch.aten.unsqueeze %1714, %int1_1808 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_1809 = torch.constant.int 0
    %1724 = torch.aten.unsqueeze %1722, %int0_1809 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %1725 = torch.aten.mul.Tensor %1723, %1724 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_1810 = torch.constant.int 1
    %1726 = torch.aten.size.int %1693, %int1_1810 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.int
    %int0_1811 = torch.constant.int 0
    %1727 = torch.aten.add.int %int0_1811, %1726 : !torch.int, !torch.int -> !torch.int
    %int0_1812 = torch.constant.int 0
    %int0_1813 = torch.constant.int 0
    %int1_1814 = torch.constant.int 1
    %1728 = torch.aten.slice.Tensor %1725, %int0_1812, %int0_1813, %1727, %int1_1814 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %1728, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_1815 = torch.constant.int 1
    %int0_1816 = torch.constant.int 0
    %int9223372036854775807_1817 = torch.constant.int 9223372036854775807
    %int1_1818 = torch.constant.int 1
    %1729 = torch.aten.slice.Tensor %1728, %int1_1815, %int0_1816, %int9223372036854775807_1817, %int1_1818 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %1729, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_1819 = torch.constant.int 1
    %int0_1820 = torch.constant.int 0
    %int9223372036854775807_1821 = torch.constant.int 9223372036854775807
    %int1_1822 = torch.constant.int 1
    %1730 = torch.aten.slice.Tensor %1729, %int1_1819, %int0_1820, %int9223372036854775807_1821, %int1_1822 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %1730, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_1823 = torch.constant.int 0
    %1731 = torch.aten.unsqueeze %1730, %int0_1823 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %1731, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_1824 = torch.constant.int 1
    %int0_1825 = torch.constant.int 0
    %int9223372036854775807_1826 = torch.constant.int 9223372036854775807
    %int1_1827 = torch.constant.int 1
    %1732 = torch.aten.slice.Tensor %1731, %int1_1824, %int0_1825, %int9223372036854775807_1826, %int1_1827 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %1732, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_1828 = torch.constant.int 2
    %int0_1829 = torch.constant.int 0
    %int9223372036854775807_1830 = torch.constant.int 9223372036854775807
    %int1_1831 = torch.constant.int 1
    %1733 = torch.aten.slice.Tensor %1732, %int2_1828, %int0_1829, %int9223372036854775807_1830, %int1_1831 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %1733, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_1832 = torch.constant.int 4
    %int1_1833 = torch.constant.int 1
    %int1_1834 = torch.constant.int 1
    %1734 = torch.prim.ListConstruct %int4_1832, %int1_1833, %int1_1834 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1735 = torch.aten.repeat %1733, %1734 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %1735, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_1835 = torch.constant.int 6
    %1736 = torch.prims.convert_element_type %1709, %int6_1835 : !torch.vtensor<[4,?,32,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %1736, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %1737 = torch_c.to_builtin_tensor %1736 : !torch.vtensor<[4,?,32,128],f32> -> tensor<4x?x32x128xf32>
    %1738 = torch_c.to_builtin_tensor %1735 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %1739 = util.call @sharktank_rotary_embedding_4_D_32_128_f32(%1737, %1738) : (tensor<4x?x32x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x32x128xf32>
    %1740 = torch_c.from_builtin_tensor %1739 : tensor<4x?x32x128xf32> -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %1740, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %int5_1836 = torch.constant.int 5
    %1741 = torch.prims.convert_element_type %1740, %int5_1836 : !torch.vtensor<[4,?,32,128],f32>, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %1741, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int131072_1837 = torch.constant.int 131072
    %none_1838 = torch.constant.none
    %none_1839 = torch.constant.none
    %cpu_1840 = torch.constant.device "cpu"
    %false_1841 = torch.constant.bool false
    %1742 = torch.aten.arange %int131072_1837, %none_1838, %none_1839, %cpu_1840, %false_1841 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_1842 = torch.constant.int 0
    %int128_1843 = torch.constant.int 128
    %none_1844 = torch.constant.none
    %none_1845 = torch.constant.none
    %cpu_1846 = torch.constant.device "cpu"
    %false_1847 = torch.constant.bool false
    %1743 = torch.aten.arange.start %int0_1842, %int128_1843, %none_1844, %none_1845, %cpu_1846, %false_1847 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_1848 = torch.constant.int 2
    %1744 = torch.aten.floor_divide.Scalar %1743, %int2_1848 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_1849 = torch.constant.int 6
    %1745 = torch.prims.convert_element_type %1744, %int6_1849 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_1850 = torch.constant.int 128
    %1746 = torch.aten.div.Scalar %1745, %int128_1850 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_1851 = torch.constant.float 2.000000e+00
    %1747 = torch.aten.mul.Scalar %1746, %float2.000000e00_1851 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_1852 = torch.constant.float 5.000000e+05
    %1748 = torch.aten.pow.Scalar %float5.000000e05_1852, %1747 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %1749 = torch.aten.reciprocal %1748 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_1853 = torch.constant.float 1.000000e+00
    %1750 = torch.aten.mul.Scalar %1749, %float1.000000e00_1853 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_1854 = torch.constant.int 1
    %1751 = torch.aten.unsqueeze %1742, %int1_1854 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_1855 = torch.constant.int 0
    %1752 = torch.aten.unsqueeze %1750, %int0_1855 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %1753 = torch.aten.mul.Tensor %1751, %1752 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_1856 = torch.constant.int 1
    %1754 = torch.aten.size.int %1700, %int1_1856 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int0_1857 = torch.constant.int 0
    %1755 = torch.aten.add.int %int0_1857, %1754 : !torch.int, !torch.int -> !torch.int
    %int0_1858 = torch.constant.int 0
    %int0_1859 = torch.constant.int 0
    %int1_1860 = torch.constant.int 1
    %1756 = torch.aten.slice.Tensor %1753, %int0_1858, %int0_1859, %1755, %int1_1860 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %1756, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_1861 = torch.constant.int 1
    %int0_1862 = torch.constant.int 0
    %int9223372036854775807_1863 = torch.constant.int 9223372036854775807
    %int1_1864 = torch.constant.int 1
    %1757 = torch.aten.slice.Tensor %1756, %int1_1861, %int0_1862, %int9223372036854775807_1863, %int1_1864 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %1757, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_1865 = torch.constant.int 1
    %int0_1866 = torch.constant.int 0
    %int9223372036854775807_1867 = torch.constant.int 9223372036854775807
    %int1_1868 = torch.constant.int 1
    %1758 = torch.aten.slice.Tensor %1757, %int1_1865, %int0_1866, %int9223372036854775807_1867, %int1_1868 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %1758, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_1869 = torch.constant.int 0
    %1759 = torch.aten.unsqueeze %1758, %int0_1869 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %1759, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_1870 = torch.constant.int 1
    %int0_1871 = torch.constant.int 0
    %int9223372036854775807_1872 = torch.constant.int 9223372036854775807
    %int1_1873 = torch.constant.int 1
    %1760 = torch.aten.slice.Tensor %1759, %int1_1870, %int0_1871, %int9223372036854775807_1872, %int1_1873 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %1760, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_1874 = torch.constant.int 2
    %int0_1875 = torch.constant.int 0
    %int9223372036854775807_1876 = torch.constant.int 9223372036854775807
    %int1_1877 = torch.constant.int 1
    %1761 = torch.aten.slice.Tensor %1760, %int2_1874, %int0_1875, %int9223372036854775807_1876, %int1_1877 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %1761, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_1878 = torch.constant.int 4
    %int1_1879 = torch.constant.int 1
    %int1_1880 = torch.constant.int 1
    %1762 = torch.prim.ListConstruct %int4_1878, %int1_1879, %int1_1880 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1763 = torch.aten.repeat %1761, %1762 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %1763, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_1881 = torch.constant.int 6
    %1764 = torch.prims.convert_element_type %1711, %int6_1881 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %1764, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %1765 = torch_c.to_builtin_tensor %1764 : !torch.vtensor<[4,?,8,128],f32> -> tensor<4x?x8x128xf32>
    %1766 = torch_c.to_builtin_tensor %1763 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %1767 = util.call @sharktank_rotary_embedding_4_D_8_128_f32(%1765, %1766) : (tensor<4x?x8x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x8x128xf32>
    %1768 = torch_c.from_builtin_tensor %1767 : tensor<4x?x8x128xf32> -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %1768, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %int5_1882 = torch.constant.int 5
    %1769 = torch.prims.convert_element_type %1768, %int5_1882 : !torch.vtensor<[4,?,8,128],f32>, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %1769, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int64_1883 = torch.constant.int 64
    %1770 = torch.aten.mul.Scalar %arg2, %int64_1883 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1770, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int14 = torch.constant.int 14
    %int1_1884 = torch.constant.int 1
    %1771 = torch.aten.add.Scalar %1770, %int14, %int1_1884 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1771, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_1885 = torch.constant.int 4
    %int32_1886 = torch.constant.int 32
    %int8_1887 = torch.constant.int 8
    %int128_1888 = torch.constant.int 128
    %1772 = torch.prim.ListConstruct %int4_1885, %398, %int32_1886, %int8_1887, %int128_1888 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1773 = torch.aten.view %1769, %1772 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1773, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_1889 = torch.constant.int 4
    %1774 = torch.aten.mul.int %int4_1889, %398 : !torch.int, !torch.int -> !torch.int
    %int32_1890 = torch.constant.int 32
    %int8_1891 = torch.constant.int 8
    %int128_1892 = torch.constant.int 128
    %1775 = torch.prim.ListConstruct %1774, %int32_1890, %int8_1891, %int128_1892 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1776 = torch.aten.view %1773, %1775 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %1776, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_1893 = torch.constant.int 4
    %1777 = torch.aten.mul.int %int4_1893, %398 : !torch.int, !torch.int -> !torch.int
    %1778 = torch.prim.ListConstruct %1777 : (!torch.int) -> !torch.list<int>
    %1779 = torch.aten.view %1771, %1778 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1779, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_1894 = torch.constant.int 32
    %int2_1895 = torch.constant.int 2
    %int32_1896 = torch.constant.int 32
    %int8_1897 = torch.constant.int 8
    %int128_1898 = torch.constant.int 128
    %1780 = torch.prim.ListConstruct %389, %int32_1894, %int2_1895, %int32_1896, %int8_1897, %int128_1898 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1781 = torch.aten.view %1613, %1780 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1781, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_1899 = torch.constant.int 32
    %1782 = torch.aten.mul.int %389, %int32_1899 : !torch.int, !torch.int -> !torch.int
    %int2_1900 = torch.constant.int 2
    %1783 = torch.aten.mul.int %1782, %int2_1900 : !torch.int, !torch.int -> !torch.int
    %int32_1901 = torch.constant.int 32
    %int8_1902 = torch.constant.int 8
    %int128_1903 = torch.constant.int 128
    %1784 = torch.prim.ListConstruct %1783, %int32_1901, %int8_1902, %int128_1903 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1785 = torch.aten.view %1781, %1784 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %1785, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %1786 = torch.prim.ListConstruct %1779 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_1904 = torch.constant.bool false
    %1787 = torch.aten.index_put %1785, %1786, %1776, %false_1904 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %1787, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_1905 = torch.constant.int 32
    %int2_1906 = torch.constant.int 2
    %int32_1907 = torch.constant.int 32
    %int8_1908 = torch.constant.int 8
    %int128_1909 = torch.constant.int 128
    %1788 = torch.prim.ListConstruct %389, %int32_1905, %int2_1906, %int32_1907, %int8_1908, %int128_1909 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1789 = torch.aten.view %1787, %1788 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1789, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_1910 = torch.constant.int 2097152
    %1790 = torch.prim.ListConstruct %389, %int2097152_1910 : (!torch.int, !torch.int) -> !torch.list<int>
    %1791 = torch.aten.view %1789, %1790 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %1791, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_1911 = torch.constant.int 32
    %int2_1912 = torch.constant.int 2
    %int32_1913 = torch.constant.int 32
    %int8_1914 = torch.constant.int 8
    %int128_1915 = torch.constant.int 128
    %1792 = torch.prim.ListConstruct %389, %int32_1911, %int2_1912, %int32_1913, %int8_1914, %int128_1915 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1793 = torch.aten.view %1791, %1792 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1793, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_1916 = torch.constant.int 32
    %int8_1917 = torch.constant.int 8
    %int128_1918 = torch.constant.int 128
    %1794 = torch.prim.ListConstruct %1783, %int32_1916, %int8_1917, %int128_1918 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1795 = torch.aten.view %1793, %1794 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %1795, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_1919 = torch.constant.int 4
    %int32_1920 = torch.constant.int 32
    %int8_1921 = torch.constant.int 8
    %int128_1922 = torch.constant.int 128
    %1796 = torch.prim.ListConstruct %int4_1919, %398, %int32_1920, %int8_1921, %int128_1922 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1797 = torch.aten.view %1713, %1796 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1797, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_1923 = torch.constant.int 4
    %1798 = torch.aten.mul.int %int4_1923, %398 : !torch.int, !torch.int -> !torch.int
    %int32_1924 = torch.constant.int 32
    %int8_1925 = torch.constant.int 8
    %int128_1926 = torch.constant.int 128
    %1799 = torch.prim.ListConstruct %1798, %int32_1924, %int8_1925, %int128_1926 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1800 = torch.aten.view %1797, %1799 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %1800, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int1_1927 = torch.constant.int 1
    %int1_1928 = torch.constant.int 1
    %1801 = torch.aten.add.Scalar %1771, %int1_1927, %int1_1928 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1801, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_1929 = torch.constant.int 4
    %1802 = torch.aten.mul.int %int4_1929, %398 : !torch.int, !torch.int -> !torch.int
    %1803 = torch.prim.ListConstruct %1802 : (!torch.int) -> !torch.list<int>
    %1804 = torch.aten.view %1801, %1803 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1804, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %1805 = torch.prim.ListConstruct %1804 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_1930 = torch.constant.bool false
    %1806 = torch.aten.index_put %1795, %1805, %1800, %false_1930 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %1806, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_1931 = torch.constant.int 32
    %int2_1932 = torch.constant.int 2
    %int32_1933 = torch.constant.int 32
    %int8_1934 = torch.constant.int 8
    %int128_1935 = torch.constant.int 128
    %1807 = torch.prim.ListConstruct %389, %int32_1931, %int2_1932, %int32_1933, %int8_1934, %int128_1935 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1808 = torch.aten.view %1806, %1807 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1808, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_1936 = torch.constant.int 2097152
    %1809 = torch.prim.ListConstruct %389, %int2097152_1936 : (!torch.int, !torch.int) -> !torch.list<int>
    %1810 = torch.aten.view %1808, %1809 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %1810, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int-2_1937 = torch.constant.int -2
    %1811 = torch.aten.unsqueeze %1769, %int-2_1937 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %1811, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int4_1938 = torch.constant.int 4
    %int8_1939 = torch.constant.int 8
    %int4_1940 = torch.constant.int 4
    %int128_1941 = torch.constant.int 128
    %1812 = torch.prim.ListConstruct %int4_1938, %1754, %int8_1939, %int4_1940, %int128_1941 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_1942 = torch.constant.bool false
    %1813 = torch.aten.expand %1811, %1812, %false_1942 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1813, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_1943 = torch.constant.int 0
    %1814 = torch.aten.clone %1813, %int0_1943 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1814, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_1944 = torch.constant.int 4
    %int32_1945 = torch.constant.int 32
    %int128_1946 = torch.constant.int 128
    %1815 = torch.prim.ListConstruct %int4_1944, %1754, %int32_1945, %int128_1946 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1816 = torch.aten._unsafe_view %1814, %1815 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %1816, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_1947 = torch.constant.int -2
    %1817 = torch.aten.unsqueeze %1713, %int-2_1947 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %1817, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_1948 = torch.constant.int 1
    %1818 = torch.aten.size.int %1707, %int1_1948 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int4_1949 = torch.constant.int 4
    %int8_1950 = torch.constant.int 8
    %int4_1951 = torch.constant.int 4
    %int128_1952 = torch.constant.int 128
    %1819 = torch.prim.ListConstruct %int4_1949, %1818, %int8_1950, %int4_1951, %int128_1952 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_1953 = torch.constant.bool false
    %1820 = torch.aten.expand %1817, %1819, %false_1953 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1820, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_1954 = torch.constant.int 0
    %1821 = torch.aten.clone %1820, %int0_1954 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1821, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_1955 = torch.constant.int 4
    %int32_1956 = torch.constant.int 32
    %int128_1957 = torch.constant.int 128
    %1822 = torch.prim.ListConstruct %int4_1955, %1818, %int32_1956, %int128_1957 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1823 = torch.aten._unsafe_view %1821, %1822 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %1823, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_1958 = torch.constant.int 1
    %int2_1959 = torch.constant.int 2
    %1824 = torch.aten.transpose.int %1741, %int1_1958, %int2_1959 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %1824, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_1960 = torch.constant.int 1
    %int2_1961 = torch.constant.int 2
    %1825 = torch.aten.transpose.int %1816, %int1_1960, %int2_1961 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %1825, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_1962 = torch.constant.int 1
    %int2_1963 = torch.constant.int 2
    %1826 = torch.aten.transpose.int %1823, %int1_1962, %int2_1963 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %1826, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_1964 = torch.constant.float 0.000000e+00
    %true_1965 = torch.constant.bool true
    %none_1966 = torch.constant.none
    %none_1967 = torch.constant.none
    %1827:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1824, %1825, %1826, %float0.000000e00_1964, %true_1965, %none_1966, %none_1967) : (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?],f32>) 
    torch.bind_symbolic_shape %1827#0, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_1968 = torch.constant.int 1
    %int2_1969 = torch.constant.int 2
    %1828 = torch.aten.transpose.int %1827#0, %int1_1968, %int2_1969 : !torch.vtensor<[4,32,?,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %1828, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_1970 = torch.constant.int 4
    %int4096_1971 = torch.constant.int 4096
    %1829 = torch.prim.ListConstruct %int4_1970, %1726, %int4096_1971 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1830 = torch.aten.view %1828, %1829 : !torch.vtensor<[4,?,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1830, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_1972 = torch.constant.int -2
    %int-1_1973 = torch.constant.int -1
    %1831 = torch.aten.transpose.int %68, %int-2_1972, %int-1_1973 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_1974 = torch.constant.int 4
    %1832 = torch.aten.mul.int %int4_1974, %1726 : !torch.int, !torch.int -> !torch.int
    %int4096_1975 = torch.constant.int 4096
    %1833 = torch.prim.ListConstruct %1832, %int4096_1975 : (!torch.int, !torch.int) -> !torch.list<int>
    %1834 = torch.aten.view %1830, %1833 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1834, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %1835 = torch.aten.mm %1834, %1831 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1835, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_1976 = torch.constant.int 4
    %int4096_1977 = torch.constant.int 4096
    %1836 = torch.prim.ListConstruct %int4_1976, %1726, %int4096_1977 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1837 = torch.aten.view %1835, %1836 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1837, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_1978 = torch.constant.int 1
    %1838 = torch.aten.add.Tensor %1676, %1837, %int1_1978 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1838, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_1979 = torch.constant.int 6
    %1839 = torch.prims.convert_element_type %1838, %int6_1979 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1839, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_1980 = torch.constant.int 2
    %1840 = torch.aten.pow.Tensor_Scalar %1839, %int2_1980 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1840, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_1981 = torch.constant.int -1
    %1841 = torch.prim.ListConstruct %int-1_1981 : (!torch.int) -> !torch.list<int>
    %true_1982 = torch.constant.bool true
    %none_1983 = torch.constant.none
    %1842 = torch.aten.mean.dim %1840, %1841, %true_1982, %none_1983 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1842, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_1984 = torch.constant.float 9.9999997473787516E-6
    %int1_1985 = torch.constant.int 1
    %1843 = torch.aten.add.Scalar %1842, %float9.999990e-06_1984, %int1_1985 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1843, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %1844 = torch.aten.rsqrt %1843 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1844, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %1845 = torch.aten.mul.Tensor %1839, %1844 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1845, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_1986 = torch.constant.int 5
    %1846 = torch.prims.convert_element_type %1845, %int5_1986 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1846, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %1847 = torch.aten.mul.Tensor %69, %1846 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1847, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_1987 = torch.constant.int 5
    %1848 = torch.prims.convert_element_type %1847, %int5_1987 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1848, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_1988 = torch.constant.int -2
    %int-1_1989 = torch.constant.int -1
    %1849 = torch.aten.transpose.int %70, %int-2_1988, %int-1_1989 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_1990 = torch.constant.int 4
    %1850 = torch.aten.mul.int %int4_1990, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_1991 = torch.constant.int 4096
    %1851 = torch.prim.ListConstruct %1850, %int4096_1991 : (!torch.int, !torch.int) -> !torch.list<int>
    %1852 = torch.aten.view %1848, %1851 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1852, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %1853 = torch.aten.mm %1852, %1849 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %1853, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_1992 = torch.constant.int 4
    %int14336_1993 = torch.constant.int 14336
    %1854 = torch.prim.ListConstruct %int4_1992, %306, %int14336_1993 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1855 = torch.aten.view %1853, %1854 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %1855, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %1856 = torch.aten.silu %1855 : !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %1856, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_1994 = torch.constant.int -2
    %int-1_1995 = torch.constant.int -1
    %1857 = torch.aten.transpose.int %71, %int-2_1994, %int-1_1995 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_1996 = torch.constant.int 4
    %1858 = torch.aten.mul.int %int4_1996, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_1997 = torch.constant.int 4096
    %1859 = torch.prim.ListConstruct %1858, %int4096_1997 : (!torch.int, !torch.int) -> !torch.list<int>
    %1860 = torch.aten.view %1848, %1859 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1860, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %1861 = torch.aten.mm %1860, %1857 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %1861, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_1998 = torch.constant.int 4
    %int14336_1999 = torch.constant.int 14336
    %1862 = torch.prim.ListConstruct %int4_1998, %306, %int14336_1999 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1863 = torch.aten.view %1861, %1862 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %1863, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %1864 = torch.aten.mul.Tensor %1856, %1863 : !torch.vtensor<[4,?,14336],f16>, !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %1864, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_2000 = torch.constant.int -2
    %int-1_2001 = torch.constant.int -1
    %1865 = torch.aten.transpose.int %72, %int-2_2000, %int-1_2001 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int1_2002 = torch.constant.int 1
    %1866 = torch.aten.size.int %1855, %int1_2002 : !torch.vtensor<[4,?,14336],f16>, !torch.int -> !torch.int
    %int4_2003 = torch.constant.int 4
    %1867 = torch.aten.mul.int %int4_2003, %1866 : !torch.int, !torch.int -> !torch.int
    %int14336_2004 = torch.constant.int 14336
    %1868 = torch.prim.ListConstruct %1867, %int14336_2004 : (!torch.int, !torch.int) -> !torch.list<int>
    %1869 = torch.aten.view %1864, %1868 : !torch.vtensor<[4,?,14336],f16>, !torch.list<int> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %1869, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %1870 = torch.aten.mm %1869, %1865 : !torch.vtensor<[?,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1870, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_2005 = torch.constant.int 4
    %int4096_2006 = torch.constant.int 4096
    %1871 = torch.prim.ListConstruct %int4_2005, %1866, %int4096_2006 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1872 = torch.aten.view %1870, %1871 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1872, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_2007 = torch.constant.int 1
    %1873 = torch.aten.add.Tensor %1838, %1872, %int1_2007 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1873, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_2008 = torch.constant.int 6
    %1874 = torch.prims.convert_element_type %1873, %int6_2008 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1874, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_2009 = torch.constant.int 2
    %1875 = torch.aten.pow.Tensor_Scalar %1874, %int2_2009 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1875, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_2010 = torch.constant.int -1
    %1876 = torch.prim.ListConstruct %int-1_2010 : (!torch.int) -> !torch.list<int>
    %true_2011 = torch.constant.bool true
    %none_2012 = torch.constant.none
    %1877 = torch.aten.mean.dim %1875, %1876, %true_2011, %none_2012 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1877, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_2013 = torch.constant.float 9.9999997473787516E-6
    %int1_2014 = torch.constant.int 1
    %1878 = torch.aten.add.Scalar %1877, %float9.999990e-06_2013, %int1_2014 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1878, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %1879 = torch.aten.rsqrt %1878 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %1879, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %1880 = torch.aten.mul.Tensor %1874, %1879 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1880, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_2015 = torch.constant.int 5
    %1881 = torch.prims.convert_element_type %1880, %int5_2015 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1881, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %1882 = torch.aten.mul.Tensor %73, %1881 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %1882, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_2016 = torch.constant.int 5
    %1883 = torch.prims.convert_element_type %1882, %int5_2016 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1883, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_2017 = torch.constant.int -2
    %int-1_2018 = torch.constant.int -1
    %1884 = torch.aten.transpose.int %74, %int-2_2017, %int-1_2018 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_2019 = torch.constant.int 4
    %1885 = torch.aten.mul.int %int4_2019, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_2020 = torch.constant.int 4096
    %1886 = torch.prim.ListConstruct %1885, %int4096_2020 : (!torch.int, !torch.int) -> !torch.list<int>
    %1887 = torch.aten.view %1883, %1886 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1887, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %1888 = torch.aten.mm %1887, %1884 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1888, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_2021 = torch.constant.int 4
    %int4096_2022 = torch.constant.int 4096
    %1889 = torch.prim.ListConstruct %int4_2021, %306, %int4096_2022 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1890 = torch.aten.view %1888, %1889 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %1890, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_2023 = torch.constant.int -2
    %int-1_2024 = torch.constant.int -1
    %1891 = torch.aten.transpose.int %75, %int-2_2023, %int-1_2024 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_2025 = torch.constant.int 4
    %1892 = torch.aten.mul.int %int4_2025, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_2026 = torch.constant.int 4096
    %1893 = torch.prim.ListConstruct %1892, %int4096_2026 : (!torch.int, !torch.int) -> !torch.list<int>
    %1894 = torch.aten.view %1883, %1893 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1894, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %1895 = torch.aten.mm %1894, %1891 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %1895, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_2027 = torch.constant.int 4
    %int1024_2028 = torch.constant.int 1024
    %1896 = torch.prim.ListConstruct %int4_2027, %306, %int1024_2028 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1897 = torch.aten.view %1895, %1896 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %1897, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int-2_2029 = torch.constant.int -2
    %int-1_2030 = torch.constant.int -1
    %1898 = torch.aten.transpose.int %76, %int-2_2029, %int-1_2030 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_2031 = torch.constant.int 4
    %1899 = torch.aten.mul.int %int4_2031, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_2032 = torch.constant.int 4096
    %1900 = torch.prim.ListConstruct %1899, %int4096_2032 : (!torch.int, !torch.int) -> !torch.list<int>
    %1901 = torch.aten.view %1883, %1900 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %1901, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %1902 = torch.aten.mm %1901, %1898 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %1902, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_2033 = torch.constant.int 4
    %int1024_2034 = torch.constant.int 1024
    %1903 = torch.prim.ListConstruct %int4_2033, %306, %int1024_2034 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1904 = torch.aten.view %1902, %1903 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %1904, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int4_2035 = torch.constant.int 4
    %int32_2036 = torch.constant.int 32
    %int128_2037 = torch.constant.int 128
    %1905 = torch.prim.ListConstruct %int4_2035, %306, %int32_2036, %int128_2037 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1906 = torch.aten.view %1890, %1905 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %1906, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_2038 = torch.constant.int 4
    %int8_2039 = torch.constant.int 8
    %int128_2040 = torch.constant.int 128
    %1907 = torch.prim.ListConstruct %int4_2038, %306, %int8_2039, %int128_2040 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1908 = torch.aten.view %1897, %1907 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %1908, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int4_2041 = torch.constant.int 4
    %int8_2042 = torch.constant.int 8
    %int128_2043 = torch.constant.int 128
    %1909 = torch.prim.ListConstruct %int4_2041, %306, %int8_2042, %int128_2043 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1910 = torch.aten.view %1904, %1909 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %1910, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int131072_2044 = torch.constant.int 131072
    %none_2045 = torch.constant.none
    %none_2046 = torch.constant.none
    %cpu_2047 = torch.constant.device "cpu"
    %false_2048 = torch.constant.bool false
    %1911 = torch.aten.arange %int131072_2044, %none_2045, %none_2046, %cpu_2047, %false_2048 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_2049 = torch.constant.int 0
    %int128_2050 = torch.constant.int 128
    %none_2051 = torch.constant.none
    %none_2052 = torch.constant.none
    %cpu_2053 = torch.constant.device "cpu"
    %false_2054 = torch.constant.bool false
    %1912 = torch.aten.arange.start %int0_2049, %int128_2050, %none_2051, %none_2052, %cpu_2053, %false_2054 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_2055 = torch.constant.int 2
    %1913 = torch.aten.floor_divide.Scalar %1912, %int2_2055 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_2056 = torch.constant.int 6
    %1914 = torch.prims.convert_element_type %1913, %int6_2056 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_2057 = torch.constant.int 128
    %1915 = torch.aten.div.Scalar %1914, %int128_2057 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_2058 = torch.constant.float 2.000000e+00
    %1916 = torch.aten.mul.Scalar %1915, %float2.000000e00_2058 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_2059 = torch.constant.float 5.000000e+05
    %1917 = torch.aten.pow.Scalar %float5.000000e05_2059, %1916 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %1918 = torch.aten.reciprocal %1917 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_2060 = torch.constant.float 1.000000e+00
    %1919 = torch.aten.mul.Scalar %1918, %float1.000000e00_2060 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_2061 = torch.constant.int 1
    %1920 = torch.aten.unsqueeze %1911, %int1_2061 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_2062 = torch.constant.int 0
    %1921 = torch.aten.unsqueeze %1919, %int0_2062 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %1922 = torch.aten.mul.Tensor %1920, %1921 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_2063 = torch.constant.int 1
    %1923 = torch.aten.size.int %1890, %int1_2063 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.int
    %int0_2064 = torch.constant.int 0
    %1924 = torch.aten.add.int %int0_2064, %1923 : !torch.int, !torch.int -> !torch.int
    %int0_2065 = torch.constant.int 0
    %int0_2066 = torch.constant.int 0
    %int1_2067 = torch.constant.int 1
    %1925 = torch.aten.slice.Tensor %1922, %int0_2065, %int0_2066, %1924, %int1_2067 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %1925, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_2068 = torch.constant.int 1
    %int0_2069 = torch.constant.int 0
    %int9223372036854775807_2070 = torch.constant.int 9223372036854775807
    %int1_2071 = torch.constant.int 1
    %1926 = torch.aten.slice.Tensor %1925, %int1_2068, %int0_2069, %int9223372036854775807_2070, %int1_2071 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %1926, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_2072 = torch.constant.int 1
    %int0_2073 = torch.constant.int 0
    %int9223372036854775807_2074 = torch.constant.int 9223372036854775807
    %int1_2075 = torch.constant.int 1
    %1927 = torch.aten.slice.Tensor %1926, %int1_2072, %int0_2073, %int9223372036854775807_2074, %int1_2075 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %1927, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_2076 = torch.constant.int 0
    %1928 = torch.aten.unsqueeze %1927, %int0_2076 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %1928, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_2077 = torch.constant.int 1
    %int0_2078 = torch.constant.int 0
    %int9223372036854775807_2079 = torch.constant.int 9223372036854775807
    %int1_2080 = torch.constant.int 1
    %1929 = torch.aten.slice.Tensor %1928, %int1_2077, %int0_2078, %int9223372036854775807_2079, %int1_2080 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %1929, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_2081 = torch.constant.int 2
    %int0_2082 = torch.constant.int 0
    %int9223372036854775807_2083 = torch.constant.int 9223372036854775807
    %int1_2084 = torch.constant.int 1
    %1930 = torch.aten.slice.Tensor %1929, %int2_2081, %int0_2082, %int9223372036854775807_2083, %int1_2084 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %1930, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_2085 = torch.constant.int 4
    %int1_2086 = torch.constant.int 1
    %int1_2087 = torch.constant.int 1
    %1931 = torch.prim.ListConstruct %int4_2085, %int1_2086, %int1_2087 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1932 = torch.aten.repeat %1930, %1931 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %1932, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_2088 = torch.constant.int 6
    %1933 = torch.prims.convert_element_type %1906, %int6_2088 : !torch.vtensor<[4,?,32,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %1933, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %1934 = torch_c.to_builtin_tensor %1933 : !torch.vtensor<[4,?,32,128],f32> -> tensor<4x?x32x128xf32>
    %1935 = torch_c.to_builtin_tensor %1932 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %1936 = util.call @sharktank_rotary_embedding_4_D_32_128_f32(%1934, %1935) : (tensor<4x?x32x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x32x128xf32>
    %1937 = torch_c.from_builtin_tensor %1936 : tensor<4x?x32x128xf32> -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %1937, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %int5_2089 = torch.constant.int 5
    %1938 = torch.prims.convert_element_type %1937, %int5_2089 : !torch.vtensor<[4,?,32,128],f32>, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %1938, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int131072_2090 = torch.constant.int 131072
    %none_2091 = torch.constant.none
    %none_2092 = torch.constant.none
    %cpu_2093 = torch.constant.device "cpu"
    %false_2094 = torch.constant.bool false
    %1939 = torch.aten.arange %int131072_2090, %none_2091, %none_2092, %cpu_2093, %false_2094 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_2095 = torch.constant.int 0
    %int128_2096 = torch.constant.int 128
    %none_2097 = torch.constant.none
    %none_2098 = torch.constant.none
    %cpu_2099 = torch.constant.device "cpu"
    %false_2100 = torch.constant.bool false
    %1940 = torch.aten.arange.start %int0_2095, %int128_2096, %none_2097, %none_2098, %cpu_2099, %false_2100 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_2101 = torch.constant.int 2
    %1941 = torch.aten.floor_divide.Scalar %1940, %int2_2101 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_2102 = torch.constant.int 6
    %1942 = torch.prims.convert_element_type %1941, %int6_2102 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_2103 = torch.constant.int 128
    %1943 = torch.aten.div.Scalar %1942, %int128_2103 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_2104 = torch.constant.float 2.000000e+00
    %1944 = torch.aten.mul.Scalar %1943, %float2.000000e00_2104 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_2105 = torch.constant.float 5.000000e+05
    %1945 = torch.aten.pow.Scalar %float5.000000e05_2105, %1944 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %1946 = torch.aten.reciprocal %1945 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_2106 = torch.constant.float 1.000000e+00
    %1947 = torch.aten.mul.Scalar %1946, %float1.000000e00_2106 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_2107 = torch.constant.int 1
    %1948 = torch.aten.unsqueeze %1939, %int1_2107 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_2108 = torch.constant.int 0
    %1949 = torch.aten.unsqueeze %1947, %int0_2108 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %1950 = torch.aten.mul.Tensor %1948, %1949 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_2109 = torch.constant.int 1
    %1951 = torch.aten.size.int %1897, %int1_2109 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int0_2110 = torch.constant.int 0
    %1952 = torch.aten.add.int %int0_2110, %1951 : !torch.int, !torch.int -> !torch.int
    %int0_2111 = torch.constant.int 0
    %int0_2112 = torch.constant.int 0
    %int1_2113 = torch.constant.int 1
    %1953 = torch.aten.slice.Tensor %1950, %int0_2111, %int0_2112, %1952, %int1_2113 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %1953, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_2114 = torch.constant.int 1
    %int0_2115 = torch.constant.int 0
    %int9223372036854775807_2116 = torch.constant.int 9223372036854775807
    %int1_2117 = torch.constant.int 1
    %1954 = torch.aten.slice.Tensor %1953, %int1_2114, %int0_2115, %int9223372036854775807_2116, %int1_2117 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %1954, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_2118 = torch.constant.int 1
    %int0_2119 = torch.constant.int 0
    %int9223372036854775807_2120 = torch.constant.int 9223372036854775807
    %int1_2121 = torch.constant.int 1
    %1955 = torch.aten.slice.Tensor %1954, %int1_2118, %int0_2119, %int9223372036854775807_2120, %int1_2121 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %1955, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_2122 = torch.constant.int 0
    %1956 = torch.aten.unsqueeze %1955, %int0_2122 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %1956, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_2123 = torch.constant.int 1
    %int0_2124 = torch.constant.int 0
    %int9223372036854775807_2125 = torch.constant.int 9223372036854775807
    %int1_2126 = torch.constant.int 1
    %1957 = torch.aten.slice.Tensor %1956, %int1_2123, %int0_2124, %int9223372036854775807_2125, %int1_2126 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %1957, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_2127 = torch.constant.int 2
    %int0_2128 = torch.constant.int 0
    %int9223372036854775807_2129 = torch.constant.int 9223372036854775807
    %int1_2130 = torch.constant.int 1
    %1958 = torch.aten.slice.Tensor %1957, %int2_2127, %int0_2128, %int9223372036854775807_2129, %int1_2130 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %1958, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_2131 = torch.constant.int 4
    %int1_2132 = torch.constant.int 1
    %int1_2133 = torch.constant.int 1
    %1959 = torch.prim.ListConstruct %int4_2131, %int1_2132, %int1_2133 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1960 = torch.aten.repeat %1958, %1959 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %1960, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_2134 = torch.constant.int 6
    %1961 = torch.prims.convert_element_type %1908, %int6_2134 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %1961, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %1962 = torch_c.to_builtin_tensor %1961 : !torch.vtensor<[4,?,8,128],f32> -> tensor<4x?x8x128xf32>
    %1963 = torch_c.to_builtin_tensor %1960 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %1964 = util.call @sharktank_rotary_embedding_4_D_8_128_f32(%1962, %1963) : (tensor<4x?x8x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x8x128xf32>
    %1965 = torch_c.from_builtin_tensor %1964 : tensor<4x?x8x128xf32> -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %1965, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %int5_2135 = torch.constant.int 5
    %1966 = torch.prims.convert_element_type %1965, %int5_2135 : !torch.vtensor<[4,?,8,128],f32>, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %1966, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int64_2136 = torch.constant.int 64
    %1967 = torch.aten.mul.Scalar %arg2, %int64_2136 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1967, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int16 = torch.constant.int 16
    %int1_2137 = torch.constant.int 1
    %1968 = torch.aten.add.Scalar %1967, %int16, %int1_2137 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1968, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_2138 = torch.constant.int 4
    %int32_2139 = torch.constant.int 32
    %int8_2140 = torch.constant.int 8
    %int128_2141 = torch.constant.int 128
    %1969 = torch.prim.ListConstruct %int4_2138, %398, %int32_2139, %int8_2140, %int128_2141 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1970 = torch.aten.view %1966, %1969 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1970, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_2142 = torch.constant.int 4
    %1971 = torch.aten.mul.int %int4_2142, %398 : !torch.int, !torch.int -> !torch.int
    %int32_2143 = torch.constant.int 32
    %int8_2144 = torch.constant.int 8
    %int128_2145 = torch.constant.int 128
    %1972 = torch.prim.ListConstruct %1971, %int32_2143, %int8_2144, %int128_2145 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1973 = torch.aten.view %1970, %1972 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %1973, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_2146 = torch.constant.int 4
    %1974 = torch.aten.mul.int %int4_2146, %398 : !torch.int, !torch.int -> !torch.int
    %1975 = torch.prim.ListConstruct %1974 : (!torch.int) -> !torch.list<int>
    %1976 = torch.aten.view %1968, %1975 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1976, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_2147 = torch.constant.int 32
    %int2_2148 = torch.constant.int 2
    %int32_2149 = torch.constant.int 32
    %int8_2150 = torch.constant.int 8
    %int128_2151 = torch.constant.int 128
    %1977 = torch.prim.ListConstruct %389, %int32_2147, %int2_2148, %int32_2149, %int8_2150, %int128_2151 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1978 = torch.aten.view %1810, %1977 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1978, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_2152 = torch.constant.int 32
    %1979 = torch.aten.mul.int %389, %int32_2152 : !torch.int, !torch.int -> !torch.int
    %int2_2153 = torch.constant.int 2
    %1980 = torch.aten.mul.int %1979, %int2_2153 : !torch.int, !torch.int -> !torch.int
    %int32_2154 = torch.constant.int 32
    %int8_2155 = torch.constant.int 8
    %int128_2156 = torch.constant.int 128
    %1981 = torch.prim.ListConstruct %1980, %int32_2154, %int8_2155, %int128_2156 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1982 = torch.aten.view %1978, %1981 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %1982, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %1983 = torch.prim.ListConstruct %1976 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_2157 = torch.constant.bool false
    %1984 = torch.aten.index_put %1982, %1983, %1973, %false_2157 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %1984, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_2158 = torch.constant.int 32
    %int2_2159 = torch.constant.int 2
    %int32_2160 = torch.constant.int 32
    %int8_2161 = torch.constant.int 8
    %int128_2162 = torch.constant.int 128
    %1985 = torch.prim.ListConstruct %389, %int32_2158, %int2_2159, %int32_2160, %int8_2161, %int128_2162 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1986 = torch.aten.view %1984, %1985 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1986, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_2163 = torch.constant.int 2097152
    %1987 = torch.prim.ListConstruct %389, %int2097152_2163 : (!torch.int, !torch.int) -> !torch.list<int>
    %1988 = torch.aten.view %1986, %1987 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %1988, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_2164 = torch.constant.int 32
    %int2_2165 = torch.constant.int 2
    %int32_2166 = torch.constant.int 32
    %int8_2167 = torch.constant.int 8
    %int128_2168 = torch.constant.int 128
    %1989 = torch.prim.ListConstruct %389, %int32_2164, %int2_2165, %int32_2166, %int8_2167, %int128_2168 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1990 = torch.aten.view %1988, %1989 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1990, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_2169 = torch.constant.int 32
    %int8_2170 = torch.constant.int 8
    %int128_2171 = torch.constant.int 128
    %1991 = torch.prim.ListConstruct %1980, %int32_2169, %int8_2170, %int128_2171 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1992 = torch.aten.view %1990, %1991 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %1992, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_2172 = torch.constant.int 4
    %int32_2173 = torch.constant.int 32
    %int8_2174 = torch.constant.int 8
    %int128_2175 = torch.constant.int 128
    %1993 = torch.prim.ListConstruct %int4_2172, %398, %int32_2173, %int8_2174, %int128_2175 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1994 = torch.aten.view %1910, %1993 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1994, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_2176 = torch.constant.int 4
    %1995 = torch.aten.mul.int %int4_2176, %398 : !torch.int, !torch.int -> !torch.int
    %int32_2177 = torch.constant.int 32
    %int8_2178 = torch.constant.int 8
    %int128_2179 = torch.constant.int 128
    %1996 = torch.prim.ListConstruct %1995, %int32_2177, %int8_2178, %int128_2179 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1997 = torch.aten.view %1994, %1996 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %1997, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int1_2180 = torch.constant.int 1
    %int1_2181 = torch.constant.int 1
    %1998 = torch.aten.add.Scalar %1968, %int1_2180, %int1_2181 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1998, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_2182 = torch.constant.int 4
    %1999 = torch.aten.mul.int %int4_2182, %398 : !torch.int, !torch.int -> !torch.int
    %2000 = torch.prim.ListConstruct %1999 : (!torch.int) -> !torch.list<int>
    %2001 = torch.aten.view %1998, %2000 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2001, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %2002 = torch.prim.ListConstruct %2001 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_2183 = torch.constant.bool false
    %2003 = torch.aten.index_put %1992, %2002, %1997, %false_2183 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %2003, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_2184 = torch.constant.int 32
    %int2_2185 = torch.constant.int 2
    %int32_2186 = torch.constant.int 32
    %int8_2187 = torch.constant.int 8
    %int128_2188 = torch.constant.int 128
    %2004 = torch.prim.ListConstruct %389, %int32_2184, %int2_2185, %int32_2186, %int8_2187, %int128_2188 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2005 = torch.aten.view %2003, %2004 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2005, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_2189 = torch.constant.int 2097152
    %2006 = torch.prim.ListConstruct %389, %int2097152_2189 : (!torch.int, !torch.int) -> !torch.list<int>
    %2007 = torch.aten.view %2005, %2006 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %2007, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int-2_2190 = torch.constant.int -2
    %2008 = torch.aten.unsqueeze %1966, %int-2_2190 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %2008, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int4_2191 = torch.constant.int 4
    %int8_2192 = torch.constant.int 8
    %int4_2193 = torch.constant.int 4
    %int128_2194 = torch.constant.int 128
    %2009 = torch.prim.ListConstruct %int4_2191, %1951, %int8_2192, %int4_2193, %int128_2194 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_2195 = torch.constant.bool false
    %2010 = torch.aten.expand %2008, %2009, %false_2195 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2010, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_2196 = torch.constant.int 0
    %2011 = torch.aten.clone %2010, %int0_2196 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2011, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_2197 = torch.constant.int 4
    %int32_2198 = torch.constant.int 32
    %int128_2199 = torch.constant.int 128
    %2012 = torch.prim.ListConstruct %int4_2197, %1951, %int32_2198, %int128_2199 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2013 = torch.aten._unsafe_view %2011, %2012 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2013, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_2200 = torch.constant.int -2
    %2014 = torch.aten.unsqueeze %1910, %int-2_2200 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %2014, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_2201 = torch.constant.int 1
    %2015 = torch.aten.size.int %1904, %int1_2201 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int4_2202 = torch.constant.int 4
    %int8_2203 = torch.constant.int 8
    %int4_2204 = torch.constant.int 4
    %int128_2205 = torch.constant.int 128
    %2016 = torch.prim.ListConstruct %int4_2202, %2015, %int8_2203, %int4_2204, %int128_2205 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_2206 = torch.constant.bool false
    %2017 = torch.aten.expand %2014, %2016, %false_2206 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2017, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_2207 = torch.constant.int 0
    %2018 = torch.aten.clone %2017, %int0_2207 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2018, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_2208 = torch.constant.int 4
    %int32_2209 = torch.constant.int 32
    %int128_2210 = torch.constant.int 128
    %2019 = torch.prim.ListConstruct %int4_2208, %2015, %int32_2209, %int128_2210 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2020 = torch.aten._unsafe_view %2018, %2019 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2020, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_2211 = torch.constant.int 1
    %int2_2212 = torch.constant.int 2
    %2021 = torch.aten.transpose.int %1938, %int1_2211, %int2_2212 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %2021, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_2213 = torch.constant.int 1
    %int2_2214 = torch.constant.int 2
    %2022 = torch.aten.transpose.int %2013, %int1_2213, %int2_2214 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %2022, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_2215 = torch.constant.int 1
    %int2_2216 = torch.constant.int 2
    %2023 = torch.aten.transpose.int %2020, %int1_2215, %int2_2216 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %2023, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_2217 = torch.constant.float 0.000000e+00
    %true_2218 = torch.constant.bool true
    %none_2219 = torch.constant.none
    %none_2220 = torch.constant.none
    %2024:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2021, %2022, %2023, %float0.000000e00_2217, %true_2218, %none_2219, %none_2220) : (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?],f32>) 
    torch.bind_symbolic_shape %2024#0, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_2221 = torch.constant.int 1
    %int2_2222 = torch.constant.int 2
    %2025 = torch.aten.transpose.int %2024#0, %int1_2221, %int2_2222 : !torch.vtensor<[4,32,?,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2025, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_2223 = torch.constant.int 4
    %int4096_2224 = torch.constant.int 4096
    %2026 = torch.prim.ListConstruct %int4_2223, %1923, %int4096_2224 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2027 = torch.aten.view %2025, %2026 : !torch.vtensor<[4,?,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2027, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_2225 = torch.constant.int -2
    %int-1_2226 = torch.constant.int -1
    %2028 = torch.aten.transpose.int %77, %int-2_2225, %int-1_2226 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_2227 = torch.constant.int 4
    %2029 = torch.aten.mul.int %int4_2227, %1923 : !torch.int, !torch.int -> !torch.int
    %int4096_2228 = torch.constant.int 4096
    %2030 = torch.prim.ListConstruct %2029, %int4096_2228 : (!torch.int, !torch.int) -> !torch.list<int>
    %2031 = torch.aten.view %2027, %2030 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2031, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %2032 = torch.aten.mm %2031, %2028 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2032, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_2229 = torch.constant.int 4
    %int4096_2230 = torch.constant.int 4096
    %2033 = torch.prim.ListConstruct %int4_2229, %1923, %int4096_2230 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2034 = torch.aten.view %2032, %2033 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2034, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_2231 = torch.constant.int 1
    %2035 = torch.aten.add.Tensor %1873, %2034, %int1_2231 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2035, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_2232 = torch.constant.int 6
    %2036 = torch.prims.convert_element_type %2035, %int6_2232 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2036, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_2233 = torch.constant.int 2
    %2037 = torch.aten.pow.Tensor_Scalar %2036, %int2_2233 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2037, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_2234 = torch.constant.int -1
    %2038 = torch.prim.ListConstruct %int-1_2234 : (!torch.int) -> !torch.list<int>
    %true_2235 = torch.constant.bool true
    %none_2236 = torch.constant.none
    %2039 = torch.aten.mean.dim %2037, %2038, %true_2235, %none_2236 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2039, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_2237 = torch.constant.float 9.9999997473787516E-6
    %int1_2238 = torch.constant.int 1
    %2040 = torch.aten.add.Scalar %2039, %float9.999990e-06_2237, %int1_2238 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2040, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %2041 = torch.aten.rsqrt %2040 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2041, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %2042 = torch.aten.mul.Tensor %2036, %2041 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2042, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_2239 = torch.constant.int 5
    %2043 = torch.prims.convert_element_type %2042, %int5_2239 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2043, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %2044 = torch.aten.mul.Tensor %78, %2043 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2044, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_2240 = torch.constant.int 5
    %2045 = torch.prims.convert_element_type %2044, %int5_2240 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2045, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_2241 = torch.constant.int -2
    %int-1_2242 = torch.constant.int -1
    %2046 = torch.aten.transpose.int %79, %int-2_2241, %int-1_2242 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_2243 = torch.constant.int 4
    %2047 = torch.aten.mul.int %int4_2243, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_2244 = torch.constant.int 4096
    %2048 = torch.prim.ListConstruct %2047, %int4096_2244 : (!torch.int, !torch.int) -> !torch.list<int>
    %2049 = torch.aten.view %2045, %2048 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2049, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %2050 = torch.aten.mm %2049, %2046 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %2050, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_2245 = torch.constant.int 4
    %int14336_2246 = torch.constant.int 14336
    %2051 = torch.prim.ListConstruct %int4_2245, %306, %int14336_2246 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2052 = torch.aten.view %2050, %2051 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %2052, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %2053 = torch.aten.silu %2052 : !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %2053, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_2247 = torch.constant.int -2
    %int-1_2248 = torch.constant.int -1
    %2054 = torch.aten.transpose.int %80, %int-2_2247, %int-1_2248 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_2249 = torch.constant.int 4
    %2055 = torch.aten.mul.int %int4_2249, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_2250 = torch.constant.int 4096
    %2056 = torch.prim.ListConstruct %2055, %int4096_2250 : (!torch.int, !torch.int) -> !torch.list<int>
    %2057 = torch.aten.view %2045, %2056 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2057, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %2058 = torch.aten.mm %2057, %2054 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %2058, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_2251 = torch.constant.int 4
    %int14336_2252 = torch.constant.int 14336
    %2059 = torch.prim.ListConstruct %int4_2251, %306, %int14336_2252 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2060 = torch.aten.view %2058, %2059 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %2060, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %2061 = torch.aten.mul.Tensor %2053, %2060 : !torch.vtensor<[4,?,14336],f16>, !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %2061, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_2253 = torch.constant.int -2
    %int-1_2254 = torch.constant.int -1
    %2062 = torch.aten.transpose.int %81, %int-2_2253, %int-1_2254 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int1_2255 = torch.constant.int 1
    %2063 = torch.aten.size.int %2052, %int1_2255 : !torch.vtensor<[4,?,14336],f16>, !torch.int -> !torch.int
    %int4_2256 = torch.constant.int 4
    %2064 = torch.aten.mul.int %int4_2256, %2063 : !torch.int, !torch.int -> !torch.int
    %int14336_2257 = torch.constant.int 14336
    %2065 = torch.prim.ListConstruct %2064, %int14336_2257 : (!torch.int, !torch.int) -> !torch.list<int>
    %2066 = torch.aten.view %2061, %2065 : !torch.vtensor<[4,?,14336],f16>, !torch.list<int> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %2066, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %2067 = torch.aten.mm %2066, %2062 : !torch.vtensor<[?,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2067, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_2258 = torch.constant.int 4
    %int4096_2259 = torch.constant.int 4096
    %2068 = torch.prim.ListConstruct %int4_2258, %2063, %int4096_2259 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2069 = torch.aten.view %2067, %2068 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2069, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_2260 = torch.constant.int 1
    %2070 = torch.aten.add.Tensor %2035, %2069, %int1_2260 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2070, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_2261 = torch.constant.int 6
    %2071 = torch.prims.convert_element_type %2070, %int6_2261 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2071, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_2262 = torch.constant.int 2
    %2072 = torch.aten.pow.Tensor_Scalar %2071, %int2_2262 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2072, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_2263 = torch.constant.int -1
    %2073 = torch.prim.ListConstruct %int-1_2263 : (!torch.int) -> !torch.list<int>
    %true_2264 = torch.constant.bool true
    %none_2265 = torch.constant.none
    %2074 = torch.aten.mean.dim %2072, %2073, %true_2264, %none_2265 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2074, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_2266 = torch.constant.float 9.9999997473787516E-6
    %int1_2267 = torch.constant.int 1
    %2075 = torch.aten.add.Scalar %2074, %float9.999990e-06_2266, %int1_2267 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2075, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %2076 = torch.aten.rsqrt %2075 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2076, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %2077 = torch.aten.mul.Tensor %2071, %2076 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2077, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_2268 = torch.constant.int 5
    %2078 = torch.prims.convert_element_type %2077, %int5_2268 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2078, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %2079 = torch.aten.mul.Tensor %82, %2078 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2079, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_2269 = torch.constant.int 5
    %2080 = torch.prims.convert_element_type %2079, %int5_2269 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2080, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_2270 = torch.constant.int -2
    %int-1_2271 = torch.constant.int -1
    %2081 = torch.aten.transpose.int %83, %int-2_2270, %int-1_2271 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_2272 = torch.constant.int 4
    %2082 = torch.aten.mul.int %int4_2272, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_2273 = torch.constant.int 4096
    %2083 = torch.prim.ListConstruct %2082, %int4096_2273 : (!torch.int, !torch.int) -> !torch.list<int>
    %2084 = torch.aten.view %2080, %2083 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2084, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %2085 = torch.aten.mm %2084, %2081 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2085, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_2274 = torch.constant.int 4
    %int4096_2275 = torch.constant.int 4096
    %2086 = torch.prim.ListConstruct %int4_2274, %306, %int4096_2275 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2087 = torch.aten.view %2085, %2086 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2087, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_2276 = torch.constant.int -2
    %int-1_2277 = torch.constant.int -1
    %2088 = torch.aten.transpose.int %84, %int-2_2276, %int-1_2277 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_2278 = torch.constant.int 4
    %2089 = torch.aten.mul.int %int4_2278, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_2279 = torch.constant.int 4096
    %2090 = torch.prim.ListConstruct %2089, %int4096_2279 : (!torch.int, !torch.int) -> !torch.list<int>
    %2091 = torch.aten.view %2080, %2090 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2091, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %2092 = torch.aten.mm %2091, %2088 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %2092, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_2280 = torch.constant.int 4
    %int1024_2281 = torch.constant.int 1024
    %2093 = torch.prim.ListConstruct %int4_2280, %306, %int1024_2281 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2094 = torch.aten.view %2092, %2093 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %2094, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int-2_2282 = torch.constant.int -2
    %int-1_2283 = torch.constant.int -1
    %2095 = torch.aten.transpose.int %85, %int-2_2282, %int-1_2283 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_2284 = torch.constant.int 4
    %2096 = torch.aten.mul.int %int4_2284, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_2285 = torch.constant.int 4096
    %2097 = torch.prim.ListConstruct %2096, %int4096_2285 : (!torch.int, !torch.int) -> !torch.list<int>
    %2098 = torch.aten.view %2080, %2097 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2098, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %2099 = torch.aten.mm %2098, %2095 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %2099, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_2286 = torch.constant.int 4
    %int1024_2287 = torch.constant.int 1024
    %2100 = torch.prim.ListConstruct %int4_2286, %306, %int1024_2287 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2101 = torch.aten.view %2099, %2100 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %2101, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int4_2288 = torch.constant.int 4
    %int32_2289 = torch.constant.int 32
    %int128_2290 = torch.constant.int 128
    %2102 = torch.prim.ListConstruct %int4_2288, %306, %int32_2289, %int128_2290 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2103 = torch.aten.view %2087, %2102 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2103, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_2291 = torch.constant.int 4
    %int8_2292 = torch.constant.int 8
    %int128_2293 = torch.constant.int 128
    %2104 = torch.prim.ListConstruct %int4_2291, %306, %int8_2292, %int128_2293 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2105 = torch.aten.view %2094, %2104 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %2105, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int4_2294 = torch.constant.int 4
    %int8_2295 = torch.constant.int 8
    %int128_2296 = torch.constant.int 128
    %2106 = torch.prim.ListConstruct %int4_2294, %306, %int8_2295, %int128_2296 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2107 = torch.aten.view %2101, %2106 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %2107, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int131072_2297 = torch.constant.int 131072
    %none_2298 = torch.constant.none
    %none_2299 = torch.constant.none
    %cpu_2300 = torch.constant.device "cpu"
    %false_2301 = torch.constant.bool false
    %2108 = torch.aten.arange %int131072_2297, %none_2298, %none_2299, %cpu_2300, %false_2301 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_2302 = torch.constant.int 0
    %int128_2303 = torch.constant.int 128
    %none_2304 = torch.constant.none
    %none_2305 = torch.constant.none
    %cpu_2306 = torch.constant.device "cpu"
    %false_2307 = torch.constant.bool false
    %2109 = torch.aten.arange.start %int0_2302, %int128_2303, %none_2304, %none_2305, %cpu_2306, %false_2307 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_2308 = torch.constant.int 2
    %2110 = torch.aten.floor_divide.Scalar %2109, %int2_2308 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_2309 = torch.constant.int 6
    %2111 = torch.prims.convert_element_type %2110, %int6_2309 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_2310 = torch.constant.int 128
    %2112 = torch.aten.div.Scalar %2111, %int128_2310 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_2311 = torch.constant.float 2.000000e+00
    %2113 = torch.aten.mul.Scalar %2112, %float2.000000e00_2311 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_2312 = torch.constant.float 5.000000e+05
    %2114 = torch.aten.pow.Scalar %float5.000000e05_2312, %2113 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %2115 = torch.aten.reciprocal %2114 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_2313 = torch.constant.float 1.000000e+00
    %2116 = torch.aten.mul.Scalar %2115, %float1.000000e00_2313 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_2314 = torch.constant.int 1
    %2117 = torch.aten.unsqueeze %2108, %int1_2314 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_2315 = torch.constant.int 0
    %2118 = torch.aten.unsqueeze %2116, %int0_2315 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %2119 = torch.aten.mul.Tensor %2117, %2118 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_2316 = torch.constant.int 1
    %2120 = torch.aten.size.int %2087, %int1_2316 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.int
    %int0_2317 = torch.constant.int 0
    %2121 = torch.aten.add.int %int0_2317, %2120 : !torch.int, !torch.int -> !torch.int
    %int0_2318 = torch.constant.int 0
    %int0_2319 = torch.constant.int 0
    %int1_2320 = torch.constant.int 1
    %2122 = torch.aten.slice.Tensor %2119, %int0_2318, %int0_2319, %2121, %int1_2320 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %2122, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_2321 = torch.constant.int 1
    %int0_2322 = torch.constant.int 0
    %int9223372036854775807_2323 = torch.constant.int 9223372036854775807
    %int1_2324 = torch.constant.int 1
    %2123 = torch.aten.slice.Tensor %2122, %int1_2321, %int0_2322, %int9223372036854775807_2323, %int1_2324 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %2123, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_2325 = torch.constant.int 1
    %int0_2326 = torch.constant.int 0
    %int9223372036854775807_2327 = torch.constant.int 9223372036854775807
    %int1_2328 = torch.constant.int 1
    %2124 = torch.aten.slice.Tensor %2123, %int1_2325, %int0_2326, %int9223372036854775807_2327, %int1_2328 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %2124, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_2329 = torch.constant.int 0
    %2125 = torch.aten.unsqueeze %2124, %int0_2329 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %2125, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_2330 = torch.constant.int 1
    %int0_2331 = torch.constant.int 0
    %int9223372036854775807_2332 = torch.constant.int 9223372036854775807
    %int1_2333 = torch.constant.int 1
    %2126 = torch.aten.slice.Tensor %2125, %int1_2330, %int0_2331, %int9223372036854775807_2332, %int1_2333 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %2126, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_2334 = torch.constant.int 2
    %int0_2335 = torch.constant.int 0
    %int9223372036854775807_2336 = torch.constant.int 9223372036854775807
    %int1_2337 = torch.constant.int 1
    %2127 = torch.aten.slice.Tensor %2126, %int2_2334, %int0_2335, %int9223372036854775807_2336, %int1_2337 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %2127, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_2338 = torch.constant.int 4
    %int1_2339 = torch.constant.int 1
    %int1_2340 = torch.constant.int 1
    %2128 = torch.prim.ListConstruct %int4_2338, %int1_2339, %int1_2340 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2129 = torch.aten.repeat %2127, %2128 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %2129, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_2341 = torch.constant.int 6
    %2130 = torch.prims.convert_element_type %2103, %int6_2341 : !torch.vtensor<[4,?,32,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %2130, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %2131 = torch_c.to_builtin_tensor %2130 : !torch.vtensor<[4,?,32,128],f32> -> tensor<4x?x32x128xf32>
    %2132 = torch_c.to_builtin_tensor %2129 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %2133 = util.call @sharktank_rotary_embedding_4_D_32_128_f32(%2131, %2132) : (tensor<4x?x32x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x32x128xf32>
    %2134 = torch_c.from_builtin_tensor %2133 : tensor<4x?x32x128xf32> -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %2134, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %int5_2342 = torch.constant.int 5
    %2135 = torch.prims.convert_element_type %2134, %int5_2342 : !torch.vtensor<[4,?,32,128],f32>, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2135, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int131072_2343 = torch.constant.int 131072
    %none_2344 = torch.constant.none
    %none_2345 = torch.constant.none
    %cpu_2346 = torch.constant.device "cpu"
    %false_2347 = torch.constant.bool false
    %2136 = torch.aten.arange %int131072_2343, %none_2344, %none_2345, %cpu_2346, %false_2347 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_2348 = torch.constant.int 0
    %int128_2349 = torch.constant.int 128
    %none_2350 = torch.constant.none
    %none_2351 = torch.constant.none
    %cpu_2352 = torch.constant.device "cpu"
    %false_2353 = torch.constant.bool false
    %2137 = torch.aten.arange.start %int0_2348, %int128_2349, %none_2350, %none_2351, %cpu_2352, %false_2353 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_2354 = torch.constant.int 2
    %2138 = torch.aten.floor_divide.Scalar %2137, %int2_2354 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_2355 = torch.constant.int 6
    %2139 = torch.prims.convert_element_type %2138, %int6_2355 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_2356 = torch.constant.int 128
    %2140 = torch.aten.div.Scalar %2139, %int128_2356 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_2357 = torch.constant.float 2.000000e+00
    %2141 = torch.aten.mul.Scalar %2140, %float2.000000e00_2357 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_2358 = torch.constant.float 5.000000e+05
    %2142 = torch.aten.pow.Scalar %float5.000000e05_2358, %2141 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %2143 = torch.aten.reciprocal %2142 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_2359 = torch.constant.float 1.000000e+00
    %2144 = torch.aten.mul.Scalar %2143, %float1.000000e00_2359 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_2360 = torch.constant.int 1
    %2145 = torch.aten.unsqueeze %2136, %int1_2360 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_2361 = torch.constant.int 0
    %2146 = torch.aten.unsqueeze %2144, %int0_2361 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %2147 = torch.aten.mul.Tensor %2145, %2146 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_2362 = torch.constant.int 1
    %2148 = torch.aten.size.int %2094, %int1_2362 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int0_2363 = torch.constant.int 0
    %2149 = torch.aten.add.int %int0_2363, %2148 : !torch.int, !torch.int -> !torch.int
    %int0_2364 = torch.constant.int 0
    %int0_2365 = torch.constant.int 0
    %int1_2366 = torch.constant.int 1
    %2150 = torch.aten.slice.Tensor %2147, %int0_2364, %int0_2365, %2149, %int1_2366 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %2150, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_2367 = torch.constant.int 1
    %int0_2368 = torch.constant.int 0
    %int9223372036854775807_2369 = torch.constant.int 9223372036854775807
    %int1_2370 = torch.constant.int 1
    %2151 = torch.aten.slice.Tensor %2150, %int1_2367, %int0_2368, %int9223372036854775807_2369, %int1_2370 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %2151, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_2371 = torch.constant.int 1
    %int0_2372 = torch.constant.int 0
    %int9223372036854775807_2373 = torch.constant.int 9223372036854775807
    %int1_2374 = torch.constant.int 1
    %2152 = torch.aten.slice.Tensor %2151, %int1_2371, %int0_2372, %int9223372036854775807_2373, %int1_2374 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %2152, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_2375 = torch.constant.int 0
    %2153 = torch.aten.unsqueeze %2152, %int0_2375 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %2153, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_2376 = torch.constant.int 1
    %int0_2377 = torch.constant.int 0
    %int9223372036854775807_2378 = torch.constant.int 9223372036854775807
    %int1_2379 = torch.constant.int 1
    %2154 = torch.aten.slice.Tensor %2153, %int1_2376, %int0_2377, %int9223372036854775807_2378, %int1_2379 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %2154, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_2380 = torch.constant.int 2
    %int0_2381 = torch.constant.int 0
    %int9223372036854775807_2382 = torch.constant.int 9223372036854775807
    %int1_2383 = torch.constant.int 1
    %2155 = torch.aten.slice.Tensor %2154, %int2_2380, %int0_2381, %int9223372036854775807_2382, %int1_2383 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %2155, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_2384 = torch.constant.int 4
    %int1_2385 = torch.constant.int 1
    %int1_2386 = torch.constant.int 1
    %2156 = torch.prim.ListConstruct %int4_2384, %int1_2385, %int1_2386 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2157 = torch.aten.repeat %2155, %2156 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %2157, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_2387 = torch.constant.int 6
    %2158 = torch.prims.convert_element_type %2105, %int6_2387 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %2158, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %2159 = torch_c.to_builtin_tensor %2158 : !torch.vtensor<[4,?,8,128],f32> -> tensor<4x?x8x128xf32>
    %2160 = torch_c.to_builtin_tensor %2157 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %2161 = util.call @sharktank_rotary_embedding_4_D_8_128_f32(%2159, %2160) : (tensor<4x?x8x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x8x128xf32>
    %2162 = torch_c.from_builtin_tensor %2161 : tensor<4x?x8x128xf32> -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %2162, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %int5_2388 = torch.constant.int 5
    %2163 = torch.prims.convert_element_type %2162, %int5_2388 : !torch.vtensor<[4,?,8,128],f32>, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %2163, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int64_2389 = torch.constant.int 64
    %2164 = torch.aten.mul.Scalar %arg2, %int64_2389 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2164, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int18 = torch.constant.int 18
    %int1_2390 = torch.constant.int 1
    %2165 = torch.aten.add.Scalar %2164, %int18, %int1_2390 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2165, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_2391 = torch.constant.int 4
    %int32_2392 = torch.constant.int 32
    %int8_2393 = torch.constant.int 8
    %int128_2394 = torch.constant.int 128
    %2166 = torch.prim.ListConstruct %int4_2391, %398, %int32_2392, %int8_2393, %int128_2394 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2167 = torch.aten.view %2163, %2166 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2167, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_2395 = torch.constant.int 4
    %2168 = torch.aten.mul.int %int4_2395, %398 : !torch.int, !torch.int -> !torch.int
    %int32_2396 = torch.constant.int 32
    %int8_2397 = torch.constant.int 8
    %int128_2398 = torch.constant.int 128
    %2169 = torch.prim.ListConstruct %2168, %int32_2396, %int8_2397, %int128_2398 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2170 = torch.aten.view %2167, %2169 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %2170, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_2399 = torch.constant.int 4
    %2171 = torch.aten.mul.int %int4_2399, %398 : !torch.int, !torch.int -> !torch.int
    %2172 = torch.prim.ListConstruct %2171 : (!torch.int) -> !torch.list<int>
    %2173 = torch.aten.view %2165, %2172 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2173, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_2400 = torch.constant.int 32
    %int2_2401 = torch.constant.int 2
    %int32_2402 = torch.constant.int 32
    %int8_2403 = torch.constant.int 8
    %int128_2404 = torch.constant.int 128
    %2174 = torch.prim.ListConstruct %389, %int32_2400, %int2_2401, %int32_2402, %int8_2403, %int128_2404 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2175 = torch.aten.view %2007, %2174 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2175, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_2405 = torch.constant.int 32
    %2176 = torch.aten.mul.int %389, %int32_2405 : !torch.int, !torch.int -> !torch.int
    %int2_2406 = torch.constant.int 2
    %2177 = torch.aten.mul.int %2176, %int2_2406 : !torch.int, !torch.int -> !torch.int
    %int32_2407 = torch.constant.int 32
    %int8_2408 = torch.constant.int 8
    %int128_2409 = torch.constant.int 128
    %2178 = torch.prim.ListConstruct %2177, %int32_2407, %int8_2408, %int128_2409 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2179 = torch.aten.view %2175, %2178 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %2179, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %2180 = torch.prim.ListConstruct %2173 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_2410 = torch.constant.bool false
    %2181 = torch.aten.index_put %2179, %2180, %2170, %false_2410 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %2181, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_2411 = torch.constant.int 32
    %int2_2412 = torch.constant.int 2
    %int32_2413 = torch.constant.int 32
    %int8_2414 = torch.constant.int 8
    %int128_2415 = torch.constant.int 128
    %2182 = torch.prim.ListConstruct %389, %int32_2411, %int2_2412, %int32_2413, %int8_2414, %int128_2415 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2183 = torch.aten.view %2181, %2182 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2183, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_2416 = torch.constant.int 2097152
    %2184 = torch.prim.ListConstruct %389, %int2097152_2416 : (!torch.int, !torch.int) -> !torch.list<int>
    %2185 = torch.aten.view %2183, %2184 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %2185, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_2417 = torch.constant.int 32
    %int2_2418 = torch.constant.int 2
    %int32_2419 = torch.constant.int 32
    %int8_2420 = torch.constant.int 8
    %int128_2421 = torch.constant.int 128
    %2186 = torch.prim.ListConstruct %389, %int32_2417, %int2_2418, %int32_2419, %int8_2420, %int128_2421 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2187 = torch.aten.view %2185, %2186 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2187, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_2422 = torch.constant.int 32
    %int8_2423 = torch.constant.int 8
    %int128_2424 = torch.constant.int 128
    %2188 = torch.prim.ListConstruct %2177, %int32_2422, %int8_2423, %int128_2424 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2189 = torch.aten.view %2187, %2188 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %2189, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_2425 = torch.constant.int 4
    %int32_2426 = torch.constant.int 32
    %int8_2427 = torch.constant.int 8
    %int128_2428 = torch.constant.int 128
    %2190 = torch.prim.ListConstruct %int4_2425, %398, %int32_2426, %int8_2427, %int128_2428 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2191 = torch.aten.view %2107, %2190 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2191, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_2429 = torch.constant.int 4
    %2192 = torch.aten.mul.int %int4_2429, %398 : !torch.int, !torch.int -> !torch.int
    %int32_2430 = torch.constant.int 32
    %int8_2431 = torch.constant.int 8
    %int128_2432 = torch.constant.int 128
    %2193 = torch.prim.ListConstruct %2192, %int32_2430, %int8_2431, %int128_2432 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2194 = torch.aten.view %2191, %2193 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %2194, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int1_2433 = torch.constant.int 1
    %int1_2434 = torch.constant.int 1
    %2195 = torch.aten.add.Scalar %2165, %int1_2433, %int1_2434 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2195, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_2435 = torch.constant.int 4
    %2196 = torch.aten.mul.int %int4_2435, %398 : !torch.int, !torch.int -> !torch.int
    %2197 = torch.prim.ListConstruct %2196 : (!torch.int) -> !torch.list<int>
    %2198 = torch.aten.view %2195, %2197 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2198, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %2199 = torch.prim.ListConstruct %2198 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_2436 = torch.constant.bool false
    %2200 = torch.aten.index_put %2189, %2199, %2194, %false_2436 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %2200, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_2437 = torch.constant.int 32
    %int2_2438 = torch.constant.int 2
    %int32_2439 = torch.constant.int 32
    %int8_2440 = torch.constant.int 8
    %int128_2441 = torch.constant.int 128
    %2201 = torch.prim.ListConstruct %389, %int32_2437, %int2_2438, %int32_2439, %int8_2440, %int128_2441 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2202 = torch.aten.view %2200, %2201 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2202, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_2442 = torch.constant.int 2097152
    %2203 = torch.prim.ListConstruct %389, %int2097152_2442 : (!torch.int, !torch.int) -> !torch.list<int>
    %2204 = torch.aten.view %2202, %2203 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %2204, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int-2_2443 = torch.constant.int -2
    %2205 = torch.aten.unsqueeze %2163, %int-2_2443 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %2205, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int4_2444 = torch.constant.int 4
    %int8_2445 = torch.constant.int 8
    %int4_2446 = torch.constant.int 4
    %int128_2447 = torch.constant.int 128
    %2206 = torch.prim.ListConstruct %int4_2444, %2148, %int8_2445, %int4_2446, %int128_2447 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_2448 = torch.constant.bool false
    %2207 = torch.aten.expand %2205, %2206, %false_2448 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2207, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_2449 = torch.constant.int 0
    %2208 = torch.aten.clone %2207, %int0_2449 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2208, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_2450 = torch.constant.int 4
    %int32_2451 = torch.constant.int 32
    %int128_2452 = torch.constant.int 128
    %2209 = torch.prim.ListConstruct %int4_2450, %2148, %int32_2451, %int128_2452 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2210 = torch.aten._unsafe_view %2208, %2209 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2210, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_2453 = torch.constant.int -2
    %2211 = torch.aten.unsqueeze %2107, %int-2_2453 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %2211, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_2454 = torch.constant.int 1
    %2212 = torch.aten.size.int %2101, %int1_2454 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int4_2455 = torch.constant.int 4
    %int8_2456 = torch.constant.int 8
    %int4_2457 = torch.constant.int 4
    %int128_2458 = torch.constant.int 128
    %2213 = torch.prim.ListConstruct %int4_2455, %2212, %int8_2456, %int4_2457, %int128_2458 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_2459 = torch.constant.bool false
    %2214 = torch.aten.expand %2211, %2213, %false_2459 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2214, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_2460 = torch.constant.int 0
    %2215 = torch.aten.clone %2214, %int0_2460 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2215, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_2461 = torch.constant.int 4
    %int32_2462 = torch.constant.int 32
    %int128_2463 = torch.constant.int 128
    %2216 = torch.prim.ListConstruct %int4_2461, %2212, %int32_2462, %int128_2463 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2217 = torch.aten._unsafe_view %2215, %2216 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2217, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_2464 = torch.constant.int 1
    %int2_2465 = torch.constant.int 2
    %2218 = torch.aten.transpose.int %2135, %int1_2464, %int2_2465 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %2218, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_2466 = torch.constant.int 1
    %int2_2467 = torch.constant.int 2
    %2219 = torch.aten.transpose.int %2210, %int1_2466, %int2_2467 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %2219, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_2468 = torch.constant.int 1
    %int2_2469 = torch.constant.int 2
    %2220 = torch.aten.transpose.int %2217, %int1_2468, %int2_2469 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %2220, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_2470 = torch.constant.float 0.000000e+00
    %true_2471 = torch.constant.bool true
    %none_2472 = torch.constant.none
    %none_2473 = torch.constant.none
    %2221:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2218, %2219, %2220, %float0.000000e00_2470, %true_2471, %none_2472, %none_2473) : (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?],f32>) 
    torch.bind_symbolic_shape %2221#0, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_2474 = torch.constant.int 1
    %int2_2475 = torch.constant.int 2
    %2222 = torch.aten.transpose.int %2221#0, %int1_2474, %int2_2475 : !torch.vtensor<[4,32,?,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2222, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_2476 = torch.constant.int 4
    %int4096_2477 = torch.constant.int 4096
    %2223 = torch.prim.ListConstruct %int4_2476, %2120, %int4096_2477 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2224 = torch.aten.view %2222, %2223 : !torch.vtensor<[4,?,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2224, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_2478 = torch.constant.int -2
    %int-1_2479 = torch.constant.int -1
    %2225 = torch.aten.transpose.int %86, %int-2_2478, %int-1_2479 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_2480 = torch.constant.int 4
    %2226 = torch.aten.mul.int %int4_2480, %2120 : !torch.int, !torch.int -> !torch.int
    %int4096_2481 = torch.constant.int 4096
    %2227 = torch.prim.ListConstruct %2226, %int4096_2481 : (!torch.int, !torch.int) -> !torch.list<int>
    %2228 = torch.aten.view %2224, %2227 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2228, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %2229 = torch.aten.mm %2228, %2225 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2229, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_2482 = torch.constant.int 4
    %int4096_2483 = torch.constant.int 4096
    %2230 = torch.prim.ListConstruct %int4_2482, %2120, %int4096_2483 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2231 = torch.aten.view %2229, %2230 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2231, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_2484 = torch.constant.int 1
    %2232 = torch.aten.add.Tensor %2070, %2231, %int1_2484 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2232, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_2485 = torch.constant.int 6
    %2233 = torch.prims.convert_element_type %2232, %int6_2485 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2233, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_2486 = torch.constant.int 2
    %2234 = torch.aten.pow.Tensor_Scalar %2233, %int2_2486 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2234, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_2487 = torch.constant.int -1
    %2235 = torch.prim.ListConstruct %int-1_2487 : (!torch.int) -> !torch.list<int>
    %true_2488 = torch.constant.bool true
    %none_2489 = torch.constant.none
    %2236 = torch.aten.mean.dim %2234, %2235, %true_2488, %none_2489 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2236, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_2490 = torch.constant.float 9.9999997473787516E-6
    %int1_2491 = torch.constant.int 1
    %2237 = torch.aten.add.Scalar %2236, %float9.999990e-06_2490, %int1_2491 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2237, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %2238 = torch.aten.rsqrt %2237 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2238, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %2239 = torch.aten.mul.Tensor %2233, %2238 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2239, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_2492 = torch.constant.int 5
    %2240 = torch.prims.convert_element_type %2239, %int5_2492 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2240, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %2241 = torch.aten.mul.Tensor %87, %2240 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2241, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_2493 = torch.constant.int 5
    %2242 = torch.prims.convert_element_type %2241, %int5_2493 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2242, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_2494 = torch.constant.int -2
    %int-1_2495 = torch.constant.int -1
    %2243 = torch.aten.transpose.int %88, %int-2_2494, %int-1_2495 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_2496 = torch.constant.int 4
    %2244 = torch.aten.mul.int %int4_2496, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_2497 = torch.constant.int 4096
    %2245 = torch.prim.ListConstruct %2244, %int4096_2497 : (!torch.int, !torch.int) -> !torch.list<int>
    %2246 = torch.aten.view %2242, %2245 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2246, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %2247 = torch.aten.mm %2246, %2243 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %2247, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_2498 = torch.constant.int 4
    %int14336_2499 = torch.constant.int 14336
    %2248 = torch.prim.ListConstruct %int4_2498, %306, %int14336_2499 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2249 = torch.aten.view %2247, %2248 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %2249, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %2250 = torch.aten.silu %2249 : !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %2250, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_2500 = torch.constant.int -2
    %int-1_2501 = torch.constant.int -1
    %2251 = torch.aten.transpose.int %89, %int-2_2500, %int-1_2501 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_2502 = torch.constant.int 4
    %2252 = torch.aten.mul.int %int4_2502, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_2503 = torch.constant.int 4096
    %2253 = torch.prim.ListConstruct %2252, %int4096_2503 : (!torch.int, !torch.int) -> !torch.list<int>
    %2254 = torch.aten.view %2242, %2253 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2254, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %2255 = torch.aten.mm %2254, %2251 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %2255, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_2504 = torch.constant.int 4
    %int14336_2505 = torch.constant.int 14336
    %2256 = torch.prim.ListConstruct %int4_2504, %306, %int14336_2505 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2257 = torch.aten.view %2255, %2256 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %2257, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %2258 = torch.aten.mul.Tensor %2250, %2257 : !torch.vtensor<[4,?,14336],f16>, !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %2258, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_2506 = torch.constant.int -2
    %int-1_2507 = torch.constant.int -1
    %2259 = torch.aten.transpose.int %90, %int-2_2506, %int-1_2507 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int1_2508 = torch.constant.int 1
    %2260 = torch.aten.size.int %2249, %int1_2508 : !torch.vtensor<[4,?,14336],f16>, !torch.int -> !torch.int
    %int4_2509 = torch.constant.int 4
    %2261 = torch.aten.mul.int %int4_2509, %2260 : !torch.int, !torch.int -> !torch.int
    %int14336_2510 = torch.constant.int 14336
    %2262 = torch.prim.ListConstruct %2261, %int14336_2510 : (!torch.int, !torch.int) -> !torch.list<int>
    %2263 = torch.aten.view %2258, %2262 : !torch.vtensor<[4,?,14336],f16>, !torch.list<int> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %2263, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %2264 = torch.aten.mm %2263, %2259 : !torch.vtensor<[?,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2264, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_2511 = torch.constant.int 4
    %int4096_2512 = torch.constant.int 4096
    %2265 = torch.prim.ListConstruct %int4_2511, %2260, %int4096_2512 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2266 = torch.aten.view %2264, %2265 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2266, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_2513 = torch.constant.int 1
    %2267 = torch.aten.add.Tensor %2232, %2266, %int1_2513 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2267, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_2514 = torch.constant.int 6
    %2268 = torch.prims.convert_element_type %2267, %int6_2514 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2268, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_2515 = torch.constant.int 2
    %2269 = torch.aten.pow.Tensor_Scalar %2268, %int2_2515 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2269, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_2516 = torch.constant.int -1
    %2270 = torch.prim.ListConstruct %int-1_2516 : (!torch.int) -> !torch.list<int>
    %true_2517 = torch.constant.bool true
    %none_2518 = torch.constant.none
    %2271 = torch.aten.mean.dim %2269, %2270, %true_2517, %none_2518 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2271, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_2519 = torch.constant.float 9.9999997473787516E-6
    %int1_2520 = torch.constant.int 1
    %2272 = torch.aten.add.Scalar %2271, %float9.999990e-06_2519, %int1_2520 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2272, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %2273 = torch.aten.rsqrt %2272 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2273, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %2274 = torch.aten.mul.Tensor %2268, %2273 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2274, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_2521 = torch.constant.int 5
    %2275 = torch.prims.convert_element_type %2274, %int5_2521 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2275, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %2276 = torch.aten.mul.Tensor %91, %2275 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2276, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_2522 = torch.constant.int 5
    %2277 = torch.prims.convert_element_type %2276, %int5_2522 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2277, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_2523 = torch.constant.int -2
    %int-1_2524 = torch.constant.int -1
    %2278 = torch.aten.transpose.int %92, %int-2_2523, %int-1_2524 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_2525 = torch.constant.int 4
    %2279 = torch.aten.mul.int %int4_2525, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_2526 = torch.constant.int 4096
    %2280 = torch.prim.ListConstruct %2279, %int4096_2526 : (!torch.int, !torch.int) -> !torch.list<int>
    %2281 = torch.aten.view %2277, %2280 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2281, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %2282 = torch.aten.mm %2281, %2278 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2282, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_2527 = torch.constant.int 4
    %int4096_2528 = torch.constant.int 4096
    %2283 = torch.prim.ListConstruct %int4_2527, %306, %int4096_2528 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2284 = torch.aten.view %2282, %2283 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2284, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_2529 = torch.constant.int -2
    %int-1_2530 = torch.constant.int -1
    %2285 = torch.aten.transpose.int %93, %int-2_2529, %int-1_2530 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_2531 = torch.constant.int 4
    %2286 = torch.aten.mul.int %int4_2531, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_2532 = torch.constant.int 4096
    %2287 = torch.prim.ListConstruct %2286, %int4096_2532 : (!torch.int, !torch.int) -> !torch.list<int>
    %2288 = torch.aten.view %2277, %2287 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2288, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %2289 = torch.aten.mm %2288, %2285 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %2289, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_2533 = torch.constant.int 4
    %int1024_2534 = torch.constant.int 1024
    %2290 = torch.prim.ListConstruct %int4_2533, %306, %int1024_2534 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2291 = torch.aten.view %2289, %2290 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %2291, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int-2_2535 = torch.constant.int -2
    %int-1_2536 = torch.constant.int -1
    %2292 = torch.aten.transpose.int %94, %int-2_2535, %int-1_2536 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_2537 = torch.constant.int 4
    %2293 = torch.aten.mul.int %int4_2537, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_2538 = torch.constant.int 4096
    %2294 = torch.prim.ListConstruct %2293, %int4096_2538 : (!torch.int, !torch.int) -> !torch.list<int>
    %2295 = torch.aten.view %2277, %2294 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2295, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %2296 = torch.aten.mm %2295, %2292 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %2296, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_2539 = torch.constant.int 4
    %int1024_2540 = torch.constant.int 1024
    %2297 = torch.prim.ListConstruct %int4_2539, %306, %int1024_2540 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2298 = torch.aten.view %2296, %2297 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %2298, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int4_2541 = torch.constant.int 4
    %int32_2542 = torch.constant.int 32
    %int128_2543 = torch.constant.int 128
    %2299 = torch.prim.ListConstruct %int4_2541, %306, %int32_2542, %int128_2543 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2300 = torch.aten.view %2284, %2299 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2300, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_2544 = torch.constant.int 4
    %int8_2545 = torch.constant.int 8
    %int128_2546 = torch.constant.int 128
    %2301 = torch.prim.ListConstruct %int4_2544, %306, %int8_2545, %int128_2546 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2302 = torch.aten.view %2291, %2301 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %2302, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int4_2547 = torch.constant.int 4
    %int8_2548 = torch.constant.int 8
    %int128_2549 = torch.constant.int 128
    %2303 = torch.prim.ListConstruct %int4_2547, %306, %int8_2548, %int128_2549 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2304 = torch.aten.view %2298, %2303 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %2304, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int131072_2550 = torch.constant.int 131072
    %none_2551 = torch.constant.none
    %none_2552 = torch.constant.none
    %cpu_2553 = torch.constant.device "cpu"
    %false_2554 = torch.constant.bool false
    %2305 = torch.aten.arange %int131072_2550, %none_2551, %none_2552, %cpu_2553, %false_2554 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_2555 = torch.constant.int 0
    %int128_2556 = torch.constant.int 128
    %none_2557 = torch.constant.none
    %none_2558 = torch.constant.none
    %cpu_2559 = torch.constant.device "cpu"
    %false_2560 = torch.constant.bool false
    %2306 = torch.aten.arange.start %int0_2555, %int128_2556, %none_2557, %none_2558, %cpu_2559, %false_2560 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_2561 = torch.constant.int 2
    %2307 = torch.aten.floor_divide.Scalar %2306, %int2_2561 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_2562 = torch.constant.int 6
    %2308 = torch.prims.convert_element_type %2307, %int6_2562 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_2563 = torch.constant.int 128
    %2309 = torch.aten.div.Scalar %2308, %int128_2563 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_2564 = torch.constant.float 2.000000e+00
    %2310 = torch.aten.mul.Scalar %2309, %float2.000000e00_2564 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_2565 = torch.constant.float 5.000000e+05
    %2311 = torch.aten.pow.Scalar %float5.000000e05_2565, %2310 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %2312 = torch.aten.reciprocal %2311 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_2566 = torch.constant.float 1.000000e+00
    %2313 = torch.aten.mul.Scalar %2312, %float1.000000e00_2566 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_2567 = torch.constant.int 1
    %2314 = torch.aten.unsqueeze %2305, %int1_2567 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_2568 = torch.constant.int 0
    %2315 = torch.aten.unsqueeze %2313, %int0_2568 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %2316 = torch.aten.mul.Tensor %2314, %2315 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_2569 = torch.constant.int 1
    %2317 = torch.aten.size.int %2284, %int1_2569 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.int
    %int0_2570 = torch.constant.int 0
    %2318 = torch.aten.add.int %int0_2570, %2317 : !torch.int, !torch.int -> !torch.int
    %int0_2571 = torch.constant.int 0
    %int0_2572 = torch.constant.int 0
    %int1_2573 = torch.constant.int 1
    %2319 = torch.aten.slice.Tensor %2316, %int0_2571, %int0_2572, %2318, %int1_2573 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %2319, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_2574 = torch.constant.int 1
    %int0_2575 = torch.constant.int 0
    %int9223372036854775807_2576 = torch.constant.int 9223372036854775807
    %int1_2577 = torch.constant.int 1
    %2320 = torch.aten.slice.Tensor %2319, %int1_2574, %int0_2575, %int9223372036854775807_2576, %int1_2577 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %2320, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_2578 = torch.constant.int 1
    %int0_2579 = torch.constant.int 0
    %int9223372036854775807_2580 = torch.constant.int 9223372036854775807
    %int1_2581 = torch.constant.int 1
    %2321 = torch.aten.slice.Tensor %2320, %int1_2578, %int0_2579, %int9223372036854775807_2580, %int1_2581 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %2321, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_2582 = torch.constant.int 0
    %2322 = torch.aten.unsqueeze %2321, %int0_2582 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %2322, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_2583 = torch.constant.int 1
    %int0_2584 = torch.constant.int 0
    %int9223372036854775807_2585 = torch.constant.int 9223372036854775807
    %int1_2586 = torch.constant.int 1
    %2323 = torch.aten.slice.Tensor %2322, %int1_2583, %int0_2584, %int9223372036854775807_2585, %int1_2586 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %2323, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_2587 = torch.constant.int 2
    %int0_2588 = torch.constant.int 0
    %int9223372036854775807_2589 = torch.constant.int 9223372036854775807
    %int1_2590 = torch.constant.int 1
    %2324 = torch.aten.slice.Tensor %2323, %int2_2587, %int0_2588, %int9223372036854775807_2589, %int1_2590 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %2324, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_2591 = torch.constant.int 4
    %int1_2592 = torch.constant.int 1
    %int1_2593 = torch.constant.int 1
    %2325 = torch.prim.ListConstruct %int4_2591, %int1_2592, %int1_2593 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2326 = torch.aten.repeat %2324, %2325 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %2326, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_2594 = torch.constant.int 6
    %2327 = torch.prims.convert_element_type %2300, %int6_2594 : !torch.vtensor<[4,?,32,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %2327, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %2328 = torch_c.to_builtin_tensor %2327 : !torch.vtensor<[4,?,32,128],f32> -> tensor<4x?x32x128xf32>
    %2329 = torch_c.to_builtin_tensor %2326 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %2330 = util.call @sharktank_rotary_embedding_4_D_32_128_f32(%2328, %2329) : (tensor<4x?x32x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x32x128xf32>
    %2331 = torch_c.from_builtin_tensor %2330 : tensor<4x?x32x128xf32> -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %2331, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %int5_2595 = torch.constant.int 5
    %2332 = torch.prims.convert_element_type %2331, %int5_2595 : !torch.vtensor<[4,?,32,128],f32>, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2332, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int131072_2596 = torch.constant.int 131072
    %none_2597 = torch.constant.none
    %none_2598 = torch.constant.none
    %cpu_2599 = torch.constant.device "cpu"
    %false_2600 = torch.constant.bool false
    %2333 = torch.aten.arange %int131072_2596, %none_2597, %none_2598, %cpu_2599, %false_2600 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_2601 = torch.constant.int 0
    %int128_2602 = torch.constant.int 128
    %none_2603 = torch.constant.none
    %none_2604 = torch.constant.none
    %cpu_2605 = torch.constant.device "cpu"
    %false_2606 = torch.constant.bool false
    %2334 = torch.aten.arange.start %int0_2601, %int128_2602, %none_2603, %none_2604, %cpu_2605, %false_2606 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_2607 = torch.constant.int 2
    %2335 = torch.aten.floor_divide.Scalar %2334, %int2_2607 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_2608 = torch.constant.int 6
    %2336 = torch.prims.convert_element_type %2335, %int6_2608 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_2609 = torch.constant.int 128
    %2337 = torch.aten.div.Scalar %2336, %int128_2609 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_2610 = torch.constant.float 2.000000e+00
    %2338 = torch.aten.mul.Scalar %2337, %float2.000000e00_2610 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_2611 = torch.constant.float 5.000000e+05
    %2339 = torch.aten.pow.Scalar %float5.000000e05_2611, %2338 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %2340 = torch.aten.reciprocal %2339 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_2612 = torch.constant.float 1.000000e+00
    %2341 = torch.aten.mul.Scalar %2340, %float1.000000e00_2612 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_2613 = torch.constant.int 1
    %2342 = torch.aten.unsqueeze %2333, %int1_2613 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_2614 = torch.constant.int 0
    %2343 = torch.aten.unsqueeze %2341, %int0_2614 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %2344 = torch.aten.mul.Tensor %2342, %2343 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_2615 = torch.constant.int 1
    %2345 = torch.aten.size.int %2291, %int1_2615 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int0_2616 = torch.constant.int 0
    %2346 = torch.aten.add.int %int0_2616, %2345 : !torch.int, !torch.int -> !torch.int
    %int0_2617 = torch.constant.int 0
    %int0_2618 = torch.constant.int 0
    %int1_2619 = torch.constant.int 1
    %2347 = torch.aten.slice.Tensor %2344, %int0_2617, %int0_2618, %2346, %int1_2619 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %2347, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_2620 = torch.constant.int 1
    %int0_2621 = torch.constant.int 0
    %int9223372036854775807_2622 = torch.constant.int 9223372036854775807
    %int1_2623 = torch.constant.int 1
    %2348 = torch.aten.slice.Tensor %2347, %int1_2620, %int0_2621, %int9223372036854775807_2622, %int1_2623 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %2348, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_2624 = torch.constant.int 1
    %int0_2625 = torch.constant.int 0
    %int9223372036854775807_2626 = torch.constant.int 9223372036854775807
    %int1_2627 = torch.constant.int 1
    %2349 = torch.aten.slice.Tensor %2348, %int1_2624, %int0_2625, %int9223372036854775807_2626, %int1_2627 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %2349, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_2628 = torch.constant.int 0
    %2350 = torch.aten.unsqueeze %2349, %int0_2628 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %2350, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_2629 = torch.constant.int 1
    %int0_2630 = torch.constant.int 0
    %int9223372036854775807_2631 = torch.constant.int 9223372036854775807
    %int1_2632 = torch.constant.int 1
    %2351 = torch.aten.slice.Tensor %2350, %int1_2629, %int0_2630, %int9223372036854775807_2631, %int1_2632 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %2351, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_2633 = torch.constant.int 2
    %int0_2634 = torch.constant.int 0
    %int9223372036854775807_2635 = torch.constant.int 9223372036854775807
    %int1_2636 = torch.constant.int 1
    %2352 = torch.aten.slice.Tensor %2351, %int2_2633, %int0_2634, %int9223372036854775807_2635, %int1_2636 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %2352, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_2637 = torch.constant.int 4
    %int1_2638 = torch.constant.int 1
    %int1_2639 = torch.constant.int 1
    %2353 = torch.prim.ListConstruct %int4_2637, %int1_2638, %int1_2639 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2354 = torch.aten.repeat %2352, %2353 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %2354, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_2640 = torch.constant.int 6
    %2355 = torch.prims.convert_element_type %2302, %int6_2640 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %2355, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %2356 = torch_c.to_builtin_tensor %2355 : !torch.vtensor<[4,?,8,128],f32> -> tensor<4x?x8x128xf32>
    %2357 = torch_c.to_builtin_tensor %2354 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %2358 = util.call @sharktank_rotary_embedding_4_D_8_128_f32(%2356, %2357) : (tensor<4x?x8x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x8x128xf32>
    %2359 = torch_c.from_builtin_tensor %2358 : tensor<4x?x8x128xf32> -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %2359, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %int5_2641 = torch.constant.int 5
    %2360 = torch.prims.convert_element_type %2359, %int5_2641 : !torch.vtensor<[4,?,8,128],f32>, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %2360, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int64_2642 = torch.constant.int 64
    %2361 = torch.aten.mul.Scalar %arg2, %int64_2642 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2361, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int20 = torch.constant.int 20
    %int1_2643 = torch.constant.int 1
    %2362 = torch.aten.add.Scalar %2361, %int20, %int1_2643 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2362, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_2644 = torch.constant.int 4
    %int32_2645 = torch.constant.int 32
    %int8_2646 = torch.constant.int 8
    %int128_2647 = torch.constant.int 128
    %2363 = torch.prim.ListConstruct %int4_2644, %398, %int32_2645, %int8_2646, %int128_2647 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2364 = torch.aten.view %2360, %2363 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2364, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_2648 = torch.constant.int 4
    %2365 = torch.aten.mul.int %int4_2648, %398 : !torch.int, !torch.int -> !torch.int
    %int32_2649 = torch.constant.int 32
    %int8_2650 = torch.constant.int 8
    %int128_2651 = torch.constant.int 128
    %2366 = torch.prim.ListConstruct %2365, %int32_2649, %int8_2650, %int128_2651 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2367 = torch.aten.view %2364, %2366 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %2367, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_2652 = torch.constant.int 4
    %2368 = torch.aten.mul.int %int4_2652, %398 : !torch.int, !torch.int -> !torch.int
    %2369 = torch.prim.ListConstruct %2368 : (!torch.int) -> !torch.list<int>
    %2370 = torch.aten.view %2362, %2369 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2370, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_2653 = torch.constant.int 32
    %int2_2654 = torch.constant.int 2
    %int32_2655 = torch.constant.int 32
    %int8_2656 = torch.constant.int 8
    %int128_2657 = torch.constant.int 128
    %2371 = torch.prim.ListConstruct %389, %int32_2653, %int2_2654, %int32_2655, %int8_2656, %int128_2657 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2372 = torch.aten.view %2204, %2371 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2372, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_2658 = torch.constant.int 32
    %2373 = torch.aten.mul.int %389, %int32_2658 : !torch.int, !torch.int -> !torch.int
    %int2_2659 = torch.constant.int 2
    %2374 = torch.aten.mul.int %2373, %int2_2659 : !torch.int, !torch.int -> !torch.int
    %int32_2660 = torch.constant.int 32
    %int8_2661 = torch.constant.int 8
    %int128_2662 = torch.constant.int 128
    %2375 = torch.prim.ListConstruct %2374, %int32_2660, %int8_2661, %int128_2662 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2376 = torch.aten.view %2372, %2375 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %2376, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %2377 = torch.prim.ListConstruct %2370 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_2663 = torch.constant.bool false
    %2378 = torch.aten.index_put %2376, %2377, %2367, %false_2663 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %2378, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_2664 = torch.constant.int 32
    %int2_2665 = torch.constant.int 2
    %int32_2666 = torch.constant.int 32
    %int8_2667 = torch.constant.int 8
    %int128_2668 = torch.constant.int 128
    %2379 = torch.prim.ListConstruct %389, %int32_2664, %int2_2665, %int32_2666, %int8_2667, %int128_2668 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2380 = torch.aten.view %2378, %2379 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2380, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_2669 = torch.constant.int 2097152
    %2381 = torch.prim.ListConstruct %389, %int2097152_2669 : (!torch.int, !torch.int) -> !torch.list<int>
    %2382 = torch.aten.view %2380, %2381 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %2382, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_2670 = torch.constant.int 32
    %int2_2671 = torch.constant.int 2
    %int32_2672 = torch.constant.int 32
    %int8_2673 = torch.constant.int 8
    %int128_2674 = torch.constant.int 128
    %2383 = torch.prim.ListConstruct %389, %int32_2670, %int2_2671, %int32_2672, %int8_2673, %int128_2674 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2384 = torch.aten.view %2382, %2383 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2384, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_2675 = torch.constant.int 32
    %int8_2676 = torch.constant.int 8
    %int128_2677 = torch.constant.int 128
    %2385 = torch.prim.ListConstruct %2374, %int32_2675, %int8_2676, %int128_2677 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2386 = torch.aten.view %2384, %2385 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %2386, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_2678 = torch.constant.int 4
    %int32_2679 = torch.constant.int 32
    %int8_2680 = torch.constant.int 8
    %int128_2681 = torch.constant.int 128
    %2387 = torch.prim.ListConstruct %int4_2678, %398, %int32_2679, %int8_2680, %int128_2681 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2388 = torch.aten.view %2304, %2387 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2388, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_2682 = torch.constant.int 4
    %2389 = torch.aten.mul.int %int4_2682, %398 : !torch.int, !torch.int -> !torch.int
    %int32_2683 = torch.constant.int 32
    %int8_2684 = torch.constant.int 8
    %int128_2685 = torch.constant.int 128
    %2390 = torch.prim.ListConstruct %2389, %int32_2683, %int8_2684, %int128_2685 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2391 = torch.aten.view %2388, %2390 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %2391, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int1_2686 = torch.constant.int 1
    %int1_2687 = torch.constant.int 1
    %2392 = torch.aten.add.Scalar %2362, %int1_2686, %int1_2687 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2392, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_2688 = torch.constant.int 4
    %2393 = torch.aten.mul.int %int4_2688, %398 : !torch.int, !torch.int -> !torch.int
    %2394 = torch.prim.ListConstruct %2393 : (!torch.int) -> !torch.list<int>
    %2395 = torch.aten.view %2392, %2394 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2395, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %2396 = torch.prim.ListConstruct %2395 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_2689 = torch.constant.bool false
    %2397 = torch.aten.index_put %2386, %2396, %2391, %false_2689 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %2397, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_2690 = torch.constant.int 32
    %int2_2691 = torch.constant.int 2
    %int32_2692 = torch.constant.int 32
    %int8_2693 = torch.constant.int 8
    %int128_2694 = torch.constant.int 128
    %2398 = torch.prim.ListConstruct %389, %int32_2690, %int2_2691, %int32_2692, %int8_2693, %int128_2694 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2399 = torch.aten.view %2397, %2398 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2399, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_2695 = torch.constant.int 2097152
    %2400 = torch.prim.ListConstruct %389, %int2097152_2695 : (!torch.int, !torch.int) -> !torch.list<int>
    %2401 = torch.aten.view %2399, %2400 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %2401, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int-2_2696 = torch.constant.int -2
    %2402 = torch.aten.unsqueeze %2360, %int-2_2696 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %2402, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int4_2697 = torch.constant.int 4
    %int8_2698 = torch.constant.int 8
    %int4_2699 = torch.constant.int 4
    %int128_2700 = torch.constant.int 128
    %2403 = torch.prim.ListConstruct %int4_2697, %2345, %int8_2698, %int4_2699, %int128_2700 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_2701 = torch.constant.bool false
    %2404 = torch.aten.expand %2402, %2403, %false_2701 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2404, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_2702 = torch.constant.int 0
    %2405 = torch.aten.clone %2404, %int0_2702 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2405, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_2703 = torch.constant.int 4
    %int32_2704 = torch.constant.int 32
    %int128_2705 = torch.constant.int 128
    %2406 = torch.prim.ListConstruct %int4_2703, %2345, %int32_2704, %int128_2705 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2407 = torch.aten._unsafe_view %2405, %2406 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2407, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_2706 = torch.constant.int -2
    %2408 = torch.aten.unsqueeze %2304, %int-2_2706 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %2408, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_2707 = torch.constant.int 1
    %2409 = torch.aten.size.int %2298, %int1_2707 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int4_2708 = torch.constant.int 4
    %int8_2709 = torch.constant.int 8
    %int4_2710 = torch.constant.int 4
    %int128_2711 = torch.constant.int 128
    %2410 = torch.prim.ListConstruct %int4_2708, %2409, %int8_2709, %int4_2710, %int128_2711 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_2712 = torch.constant.bool false
    %2411 = torch.aten.expand %2408, %2410, %false_2712 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2411, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_2713 = torch.constant.int 0
    %2412 = torch.aten.clone %2411, %int0_2713 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2412, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_2714 = torch.constant.int 4
    %int32_2715 = torch.constant.int 32
    %int128_2716 = torch.constant.int 128
    %2413 = torch.prim.ListConstruct %int4_2714, %2409, %int32_2715, %int128_2716 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2414 = torch.aten._unsafe_view %2412, %2413 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2414, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_2717 = torch.constant.int 1
    %int2_2718 = torch.constant.int 2
    %2415 = torch.aten.transpose.int %2332, %int1_2717, %int2_2718 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %2415, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_2719 = torch.constant.int 1
    %int2_2720 = torch.constant.int 2
    %2416 = torch.aten.transpose.int %2407, %int1_2719, %int2_2720 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %2416, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_2721 = torch.constant.int 1
    %int2_2722 = torch.constant.int 2
    %2417 = torch.aten.transpose.int %2414, %int1_2721, %int2_2722 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %2417, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_2723 = torch.constant.float 0.000000e+00
    %true_2724 = torch.constant.bool true
    %none_2725 = torch.constant.none
    %none_2726 = torch.constant.none
    %2418:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2415, %2416, %2417, %float0.000000e00_2723, %true_2724, %none_2725, %none_2726) : (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?],f32>) 
    torch.bind_symbolic_shape %2418#0, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_2727 = torch.constant.int 1
    %int2_2728 = torch.constant.int 2
    %2419 = torch.aten.transpose.int %2418#0, %int1_2727, %int2_2728 : !torch.vtensor<[4,32,?,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2419, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_2729 = torch.constant.int 4
    %int4096_2730 = torch.constant.int 4096
    %2420 = torch.prim.ListConstruct %int4_2729, %2317, %int4096_2730 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2421 = torch.aten.view %2419, %2420 : !torch.vtensor<[4,?,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2421, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_2731 = torch.constant.int -2
    %int-1_2732 = torch.constant.int -1
    %2422 = torch.aten.transpose.int %95, %int-2_2731, %int-1_2732 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_2733 = torch.constant.int 4
    %2423 = torch.aten.mul.int %int4_2733, %2317 : !torch.int, !torch.int -> !torch.int
    %int4096_2734 = torch.constant.int 4096
    %2424 = torch.prim.ListConstruct %2423, %int4096_2734 : (!torch.int, !torch.int) -> !torch.list<int>
    %2425 = torch.aten.view %2421, %2424 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2425, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %2426 = torch.aten.mm %2425, %2422 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2426, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_2735 = torch.constant.int 4
    %int4096_2736 = torch.constant.int 4096
    %2427 = torch.prim.ListConstruct %int4_2735, %2317, %int4096_2736 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2428 = torch.aten.view %2426, %2427 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2428, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_2737 = torch.constant.int 1
    %2429 = torch.aten.add.Tensor %2267, %2428, %int1_2737 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2429, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_2738 = torch.constant.int 6
    %2430 = torch.prims.convert_element_type %2429, %int6_2738 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2430, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_2739 = torch.constant.int 2
    %2431 = torch.aten.pow.Tensor_Scalar %2430, %int2_2739 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2431, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_2740 = torch.constant.int -1
    %2432 = torch.prim.ListConstruct %int-1_2740 : (!torch.int) -> !torch.list<int>
    %true_2741 = torch.constant.bool true
    %none_2742 = torch.constant.none
    %2433 = torch.aten.mean.dim %2431, %2432, %true_2741, %none_2742 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2433, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_2743 = torch.constant.float 9.9999997473787516E-6
    %int1_2744 = torch.constant.int 1
    %2434 = torch.aten.add.Scalar %2433, %float9.999990e-06_2743, %int1_2744 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2434, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %2435 = torch.aten.rsqrt %2434 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2435, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %2436 = torch.aten.mul.Tensor %2430, %2435 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2436, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_2745 = torch.constant.int 5
    %2437 = torch.prims.convert_element_type %2436, %int5_2745 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2437, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %2438 = torch.aten.mul.Tensor %96, %2437 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2438, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_2746 = torch.constant.int 5
    %2439 = torch.prims.convert_element_type %2438, %int5_2746 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2439, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_2747 = torch.constant.int -2
    %int-1_2748 = torch.constant.int -1
    %2440 = torch.aten.transpose.int %97, %int-2_2747, %int-1_2748 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_2749 = torch.constant.int 4
    %2441 = torch.aten.mul.int %int4_2749, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_2750 = torch.constant.int 4096
    %2442 = torch.prim.ListConstruct %2441, %int4096_2750 : (!torch.int, !torch.int) -> !torch.list<int>
    %2443 = torch.aten.view %2439, %2442 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2443, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %2444 = torch.aten.mm %2443, %2440 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %2444, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_2751 = torch.constant.int 4
    %int14336_2752 = torch.constant.int 14336
    %2445 = torch.prim.ListConstruct %int4_2751, %306, %int14336_2752 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2446 = torch.aten.view %2444, %2445 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %2446, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %2447 = torch.aten.silu %2446 : !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %2447, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_2753 = torch.constant.int -2
    %int-1_2754 = torch.constant.int -1
    %2448 = torch.aten.transpose.int %98, %int-2_2753, %int-1_2754 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_2755 = torch.constant.int 4
    %2449 = torch.aten.mul.int %int4_2755, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_2756 = torch.constant.int 4096
    %2450 = torch.prim.ListConstruct %2449, %int4096_2756 : (!torch.int, !torch.int) -> !torch.list<int>
    %2451 = torch.aten.view %2439, %2450 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2451, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %2452 = torch.aten.mm %2451, %2448 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %2452, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_2757 = torch.constant.int 4
    %int14336_2758 = torch.constant.int 14336
    %2453 = torch.prim.ListConstruct %int4_2757, %306, %int14336_2758 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2454 = torch.aten.view %2452, %2453 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %2454, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %2455 = torch.aten.mul.Tensor %2447, %2454 : !torch.vtensor<[4,?,14336],f16>, !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %2455, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_2759 = torch.constant.int -2
    %int-1_2760 = torch.constant.int -1
    %2456 = torch.aten.transpose.int %99, %int-2_2759, %int-1_2760 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int1_2761 = torch.constant.int 1
    %2457 = torch.aten.size.int %2446, %int1_2761 : !torch.vtensor<[4,?,14336],f16>, !torch.int -> !torch.int
    %int4_2762 = torch.constant.int 4
    %2458 = torch.aten.mul.int %int4_2762, %2457 : !torch.int, !torch.int -> !torch.int
    %int14336_2763 = torch.constant.int 14336
    %2459 = torch.prim.ListConstruct %2458, %int14336_2763 : (!torch.int, !torch.int) -> !torch.list<int>
    %2460 = torch.aten.view %2455, %2459 : !torch.vtensor<[4,?,14336],f16>, !torch.list<int> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %2460, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %2461 = torch.aten.mm %2460, %2456 : !torch.vtensor<[?,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2461, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_2764 = torch.constant.int 4
    %int4096_2765 = torch.constant.int 4096
    %2462 = torch.prim.ListConstruct %int4_2764, %2457, %int4096_2765 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2463 = torch.aten.view %2461, %2462 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2463, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_2766 = torch.constant.int 1
    %2464 = torch.aten.add.Tensor %2429, %2463, %int1_2766 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2464, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_2767 = torch.constant.int 6
    %2465 = torch.prims.convert_element_type %2464, %int6_2767 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2465, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_2768 = torch.constant.int 2
    %2466 = torch.aten.pow.Tensor_Scalar %2465, %int2_2768 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2466, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_2769 = torch.constant.int -1
    %2467 = torch.prim.ListConstruct %int-1_2769 : (!torch.int) -> !torch.list<int>
    %true_2770 = torch.constant.bool true
    %none_2771 = torch.constant.none
    %2468 = torch.aten.mean.dim %2466, %2467, %true_2770, %none_2771 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2468, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_2772 = torch.constant.float 9.9999997473787516E-6
    %int1_2773 = torch.constant.int 1
    %2469 = torch.aten.add.Scalar %2468, %float9.999990e-06_2772, %int1_2773 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2469, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %2470 = torch.aten.rsqrt %2469 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2470, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %2471 = torch.aten.mul.Tensor %2465, %2470 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2471, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_2774 = torch.constant.int 5
    %2472 = torch.prims.convert_element_type %2471, %int5_2774 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2472, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %2473 = torch.aten.mul.Tensor %100, %2472 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2473, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_2775 = torch.constant.int 5
    %2474 = torch.prims.convert_element_type %2473, %int5_2775 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2474, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_2776 = torch.constant.int -2
    %int-1_2777 = torch.constant.int -1
    %2475 = torch.aten.transpose.int %101, %int-2_2776, %int-1_2777 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_2778 = torch.constant.int 4
    %2476 = torch.aten.mul.int %int4_2778, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_2779 = torch.constant.int 4096
    %2477 = torch.prim.ListConstruct %2476, %int4096_2779 : (!torch.int, !torch.int) -> !torch.list<int>
    %2478 = torch.aten.view %2474, %2477 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2478, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %2479 = torch.aten.mm %2478, %2475 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2479, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_2780 = torch.constant.int 4
    %int4096_2781 = torch.constant.int 4096
    %2480 = torch.prim.ListConstruct %int4_2780, %306, %int4096_2781 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2481 = torch.aten.view %2479, %2480 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2481, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_2782 = torch.constant.int -2
    %int-1_2783 = torch.constant.int -1
    %2482 = torch.aten.transpose.int %102, %int-2_2782, %int-1_2783 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_2784 = torch.constant.int 4
    %2483 = torch.aten.mul.int %int4_2784, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_2785 = torch.constant.int 4096
    %2484 = torch.prim.ListConstruct %2483, %int4096_2785 : (!torch.int, !torch.int) -> !torch.list<int>
    %2485 = torch.aten.view %2474, %2484 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2485, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %2486 = torch.aten.mm %2485, %2482 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %2486, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_2786 = torch.constant.int 4
    %int1024_2787 = torch.constant.int 1024
    %2487 = torch.prim.ListConstruct %int4_2786, %306, %int1024_2787 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2488 = torch.aten.view %2486, %2487 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %2488, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int-2_2788 = torch.constant.int -2
    %int-1_2789 = torch.constant.int -1
    %2489 = torch.aten.transpose.int %103, %int-2_2788, %int-1_2789 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_2790 = torch.constant.int 4
    %2490 = torch.aten.mul.int %int4_2790, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_2791 = torch.constant.int 4096
    %2491 = torch.prim.ListConstruct %2490, %int4096_2791 : (!torch.int, !torch.int) -> !torch.list<int>
    %2492 = torch.aten.view %2474, %2491 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2492, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %2493 = torch.aten.mm %2492, %2489 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %2493, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_2792 = torch.constant.int 4
    %int1024_2793 = torch.constant.int 1024
    %2494 = torch.prim.ListConstruct %int4_2792, %306, %int1024_2793 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2495 = torch.aten.view %2493, %2494 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %2495, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int4_2794 = torch.constant.int 4
    %int32_2795 = torch.constant.int 32
    %int128_2796 = torch.constant.int 128
    %2496 = torch.prim.ListConstruct %int4_2794, %306, %int32_2795, %int128_2796 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2497 = torch.aten.view %2481, %2496 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2497, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_2797 = torch.constant.int 4
    %int8_2798 = torch.constant.int 8
    %int128_2799 = torch.constant.int 128
    %2498 = torch.prim.ListConstruct %int4_2797, %306, %int8_2798, %int128_2799 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2499 = torch.aten.view %2488, %2498 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %2499, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int4_2800 = torch.constant.int 4
    %int8_2801 = torch.constant.int 8
    %int128_2802 = torch.constant.int 128
    %2500 = torch.prim.ListConstruct %int4_2800, %306, %int8_2801, %int128_2802 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2501 = torch.aten.view %2495, %2500 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %2501, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int131072_2803 = torch.constant.int 131072
    %none_2804 = torch.constant.none
    %none_2805 = torch.constant.none
    %cpu_2806 = torch.constant.device "cpu"
    %false_2807 = torch.constant.bool false
    %2502 = torch.aten.arange %int131072_2803, %none_2804, %none_2805, %cpu_2806, %false_2807 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_2808 = torch.constant.int 0
    %int128_2809 = torch.constant.int 128
    %none_2810 = torch.constant.none
    %none_2811 = torch.constant.none
    %cpu_2812 = torch.constant.device "cpu"
    %false_2813 = torch.constant.bool false
    %2503 = torch.aten.arange.start %int0_2808, %int128_2809, %none_2810, %none_2811, %cpu_2812, %false_2813 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_2814 = torch.constant.int 2
    %2504 = torch.aten.floor_divide.Scalar %2503, %int2_2814 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_2815 = torch.constant.int 6
    %2505 = torch.prims.convert_element_type %2504, %int6_2815 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_2816 = torch.constant.int 128
    %2506 = torch.aten.div.Scalar %2505, %int128_2816 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_2817 = torch.constant.float 2.000000e+00
    %2507 = torch.aten.mul.Scalar %2506, %float2.000000e00_2817 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_2818 = torch.constant.float 5.000000e+05
    %2508 = torch.aten.pow.Scalar %float5.000000e05_2818, %2507 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %2509 = torch.aten.reciprocal %2508 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_2819 = torch.constant.float 1.000000e+00
    %2510 = torch.aten.mul.Scalar %2509, %float1.000000e00_2819 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_2820 = torch.constant.int 1
    %2511 = torch.aten.unsqueeze %2502, %int1_2820 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_2821 = torch.constant.int 0
    %2512 = torch.aten.unsqueeze %2510, %int0_2821 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %2513 = torch.aten.mul.Tensor %2511, %2512 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_2822 = torch.constant.int 1
    %2514 = torch.aten.size.int %2481, %int1_2822 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.int
    %int0_2823 = torch.constant.int 0
    %2515 = torch.aten.add.int %int0_2823, %2514 : !torch.int, !torch.int -> !torch.int
    %int0_2824 = torch.constant.int 0
    %int0_2825 = torch.constant.int 0
    %int1_2826 = torch.constant.int 1
    %2516 = torch.aten.slice.Tensor %2513, %int0_2824, %int0_2825, %2515, %int1_2826 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %2516, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_2827 = torch.constant.int 1
    %int0_2828 = torch.constant.int 0
    %int9223372036854775807_2829 = torch.constant.int 9223372036854775807
    %int1_2830 = torch.constant.int 1
    %2517 = torch.aten.slice.Tensor %2516, %int1_2827, %int0_2828, %int9223372036854775807_2829, %int1_2830 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %2517, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_2831 = torch.constant.int 1
    %int0_2832 = torch.constant.int 0
    %int9223372036854775807_2833 = torch.constant.int 9223372036854775807
    %int1_2834 = torch.constant.int 1
    %2518 = torch.aten.slice.Tensor %2517, %int1_2831, %int0_2832, %int9223372036854775807_2833, %int1_2834 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %2518, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_2835 = torch.constant.int 0
    %2519 = torch.aten.unsqueeze %2518, %int0_2835 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %2519, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_2836 = torch.constant.int 1
    %int0_2837 = torch.constant.int 0
    %int9223372036854775807_2838 = torch.constant.int 9223372036854775807
    %int1_2839 = torch.constant.int 1
    %2520 = torch.aten.slice.Tensor %2519, %int1_2836, %int0_2837, %int9223372036854775807_2838, %int1_2839 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %2520, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_2840 = torch.constant.int 2
    %int0_2841 = torch.constant.int 0
    %int9223372036854775807_2842 = torch.constant.int 9223372036854775807
    %int1_2843 = torch.constant.int 1
    %2521 = torch.aten.slice.Tensor %2520, %int2_2840, %int0_2841, %int9223372036854775807_2842, %int1_2843 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %2521, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_2844 = torch.constant.int 4
    %int1_2845 = torch.constant.int 1
    %int1_2846 = torch.constant.int 1
    %2522 = torch.prim.ListConstruct %int4_2844, %int1_2845, %int1_2846 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2523 = torch.aten.repeat %2521, %2522 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %2523, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_2847 = torch.constant.int 6
    %2524 = torch.prims.convert_element_type %2497, %int6_2847 : !torch.vtensor<[4,?,32,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %2524, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %2525 = torch_c.to_builtin_tensor %2524 : !torch.vtensor<[4,?,32,128],f32> -> tensor<4x?x32x128xf32>
    %2526 = torch_c.to_builtin_tensor %2523 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %2527 = util.call @sharktank_rotary_embedding_4_D_32_128_f32(%2525, %2526) : (tensor<4x?x32x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x32x128xf32>
    %2528 = torch_c.from_builtin_tensor %2527 : tensor<4x?x32x128xf32> -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %2528, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %int5_2848 = torch.constant.int 5
    %2529 = torch.prims.convert_element_type %2528, %int5_2848 : !torch.vtensor<[4,?,32,128],f32>, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2529, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int131072_2849 = torch.constant.int 131072
    %none_2850 = torch.constant.none
    %none_2851 = torch.constant.none
    %cpu_2852 = torch.constant.device "cpu"
    %false_2853 = torch.constant.bool false
    %2530 = torch.aten.arange %int131072_2849, %none_2850, %none_2851, %cpu_2852, %false_2853 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_2854 = torch.constant.int 0
    %int128_2855 = torch.constant.int 128
    %none_2856 = torch.constant.none
    %none_2857 = torch.constant.none
    %cpu_2858 = torch.constant.device "cpu"
    %false_2859 = torch.constant.bool false
    %2531 = torch.aten.arange.start %int0_2854, %int128_2855, %none_2856, %none_2857, %cpu_2858, %false_2859 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_2860 = torch.constant.int 2
    %2532 = torch.aten.floor_divide.Scalar %2531, %int2_2860 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_2861 = torch.constant.int 6
    %2533 = torch.prims.convert_element_type %2532, %int6_2861 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_2862 = torch.constant.int 128
    %2534 = torch.aten.div.Scalar %2533, %int128_2862 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_2863 = torch.constant.float 2.000000e+00
    %2535 = torch.aten.mul.Scalar %2534, %float2.000000e00_2863 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_2864 = torch.constant.float 5.000000e+05
    %2536 = torch.aten.pow.Scalar %float5.000000e05_2864, %2535 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %2537 = torch.aten.reciprocal %2536 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_2865 = torch.constant.float 1.000000e+00
    %2538 = torch.aten.mul.Scalar %2537, %float1.000000e00_2865 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_2866 = torch.constant.int 1
    %2539 = torch.aten.unsqueeze %2530, %int1_2866 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_2867 = torch.constant.int 0
    %2540 = torch.aten.unsqueeze %2538, %int0_2867 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %2541 = torch.aten.mul.Tensor %2539, %2540 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_2868 = torch.constant.int 1
    %2542 = torch.aten.size.int %2488, %int1_2868 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int0_2869 = torch.constant.int 0
    %2543 = torch.aten.add.int %int0_2869, %2542 : !torch.int, !torch.int -> !torch.int
    %int0_2870 = torch.constant.int 0
    %int0_2871 = torch.constant.int 0
    %int1_2872 = torch.constant.int 1
    %2544 = torch.aten.slice.Tensor %2541, %int0_2870, %int0_2871, %2543, %int1_2872 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %2544, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_2873 = torch.constant.int 1
    %int0_2874 = torch.constant.int 0
    %int9223372036854775807_2875 = torch.constant.int 9223372036854775807
    %int1_2876 = torch.constant.int 1
    %2545 = torch.aten.slice.Tensor %2544, %int1_2873, %int0_2874, %int9223372036854775807_2875, %int1_2876 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %2545, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_2877 = torch.constant.int 1
    %int0_2878 = torch.constant.int 0
    %int9223372036854775807_2879 = torch.constant.int 9223372036854775807
    %int1_2880 = torch.constant.int 1
    %2546 = torch.aten.slice.Tensor %2545, %int1_2877, %int0_2878, %int9223372036854775807_2879, %int1_2880 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %2546, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_2881 = torch.constant.int 0
    %2547 = torch.aten.unsqueeze %2546, %int0_2881 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %2547, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_2882 = torch.constant.int 1
    %int0_2883 = torch.constant.int 0
    %int9223372036854775807_2884 = torch.constant.int 9223372036854775807
    %int1_2885 = torch.constant.int 1
    %2548 = torch.aten.slice.Tensor %2547, %int1_2882, %int0_2883, %int9223372036854775807_2884, %int1_2885 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %2548, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_2886 = torch.constant.int 2
    %int0_2887 = torch.constant.int 0
    %int9223372036854775807_2888 = torch.constant.int 9223372036854775807
    %int1_2889 = torch.constant.int 1
    %2549 = torch.aten.slice.Tensor %2548, %int2_2886, %int0_2887, %int9223372036854775807_2888, %int1_2889 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %2549, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_2890 = torch.constant.int 4
    %int1_2891 = torch.constant.int 1
    %int1_2892 = torch.constant.int 1
    %2550 = torch.prim.ListConstruct %int4_2890, %int1_2891, %int1_2892 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2551 = torch.aten.repeat %2549, %2550 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %2551, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_2893 = torch.constant.int 6
    %2552 = torch.prims.convert_element_type %2499, %int6_2893 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %2552, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %2553 = torch_c.to_builtin_tensor %2552 : !torch.vtensor<[4,?,8,128],f32> -> tensor<4x?x8x128xf32>
    %2554 = torch_c.to_builtin_tensor %2551 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %2555 = util.call @sharktank_rotary_embedding_4_D_8_128_f32(%2553, %2554) : (tensor<4x?x8x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x8x128xf32>
    %2556 = torch_c.from_builtin_tensor %2555 : tensor<4x?x8x128xf32> -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %2556, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %int5_2894 = torch.constant.int 5
    %2557 = torch.prims.convert_element_type %2556, %int5_2894 : !torch.vtensor<[4,?,8,128],f32>, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %2557, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int64_2895 = torch.constant.int 64
    %2558 = torch.aten.mul.Scalar %arg2, %int64_2895 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2558, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int22 = torch.constant.int 22
    %int1_2896 = torch.constant.int 1
    %2559 = torch.aten.add.Scalar %2558, %int22, %int1_2896 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2559, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_2897 = torch.constant.int 4
    %int32_2898 = torch.constant.int 32
    %int8_2899 = torch.constant.int 8
    %int128_2900 = torch.constant.int 128
    %2560 = torch.prim.ListConstruct %int4_2897, %398, %int32_2898, %int8_2899, %int128_2900 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2561 = torch.aten.view %2557, %2560 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2561, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_2901 = torch.constant.int 4
    %2562 = torch.aten.mul.int %int4_2901, %398 : !torch.int, !torch.int -> !torch.int
    %int32_2902 = torch.constant.int 32
    %int8_2903 = torch.constant.int 8
    %int128_2904 = torch.constant.int 128
    %2563 = torch.prim.ListConstruct %2562, %int32_2902, %int8_2903, %int128_2904 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2564 = torch.aten.view %2561, %2563 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %2564, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_2905 = torch.constant.int 4
    %2565 = torch.aten.mul.int %int4_2905, %398 : !torch.int, !torch.int -> !torch.int
    %2566 = torch.prim.ListConstruct %2565 : (!torch.int) -> !torch.list<int>
    %2567 = torch.aten.view %2559, %2566 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2567, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_2906 = torch.constant.int 32
    %int2_2907 = torch.constant.int 2
    %int32_2908 = torch.constant.int 32
    %int8_2909 = torch.constant.int 8
    %int128_2910 = torch.constant.int 128
    %2568 = torch.prim.ListConstruct %389, %int32_2906, %int2_2907, %int32_2908, %int8_2909, %int128_2910 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2569 = torch.aten.view %2401, %2568 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2569, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_2911 = torch.constant.int 32
    %2570 = torch.aten.mul.int %389, %int32_2911 : !torch.int, !torch.int -> !torch.int
    %int2_2912 = torch.constant.int 2
    %2571 = torch.aten.mul.int %2570, %int2_2912 : !torch.int, !torch.int -> !torch.int
    %int32_2913 = torch.constant.int 32
    %int8_2914 = torch.constant.int 8
    %int128_2915 = torch.constant.int 128
    %2572 = torch.prim.ListConstruct %2571, %int32_2913, %int8_2914, %int128_2915 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2573 = torch.aten.view %2569, %2572 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %2573, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %2574 = torch.prim.ListConstruct %2567 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_2916 = torch.constant.bool false
    %2575 = torch.aten.index_put %2573, %2574, %2564, %false_2916 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %2575, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_2917 = torch.constant.int 32
    %int2_2918 = torch.constant.int 2
    %int32_2919 = torch.constant.int 32
    %int8_2920 = torch.constant.int 8
    %int128_2921 = torch.constant.int 128
    %2576 = torch.prim.ListConstruct %389, %int32_2917, %int2_2918, %int32_2919, %int8_2920, %int128_2921 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2577 = torch.aten.view %2575, %2576 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2577, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_2922 = torch.constant.int 2097152
    %2578 = torch.prim.ListConstruct %389, %int2097152_2922 : (!torch.int, !torch.int) -> !torch.list<int>
    %2579 = torch.aten.view %2577, %2578 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %2579, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_2923 = torch.constant.int 32
    %int2_2924 = torch.constant.int 2
    %int32_2925 = torch.constant.int 32
    %int8_2926 = torch.constant.int 8
    %int128_2927 = torch.constant.int 128
    %2580 = torch.prim.ListConstruct %389, %int32_2923, %int2_2924, %int32_2925, %int8_2926, %int128_2927 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2581 = torch.aten.view %2579, %2580 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2581, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_2928 = torch.constant.int 32
    %int8_2929 = torch.constant.int 8
    %int128_2930 = torch.constant.int 128
    %2582 = torch.prim.ListConstruct %2571, %int32_2928, %int8_2929, %int128_2930 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2583 = torch.aten.view %2581, %2582 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %2583, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_2931 = torch.constant.int 4
    %int32_2932 = torch.constant.int 32
    %int8_2933 = torch.constant.int 8
    %int128_2934 = torch.constant.int 128
    %2584 = torch.prim.ListConstruct %int4_2931, %398, %int32_2932, %int8_2933, %int128_2934 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2585 = torch.aten.view %2501, %2584 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2585, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_2935 = torch.constant.int 4
    %2586 = torch.aten.mul.int %int4_2935, %398 : !torch.int, !torch.int -> !torch.int
    %int32_2936 = torch.constant.int 32
    %int8_2937 = torch.constant.int 8
    %int128_2938 = torch.constant.int 128
    %2587 = torch.prim.ListConstruct %2586, %int32_2936, %int8_2937, %int128_2938 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2588 = torch.aten.view %2585, %2587 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %2588, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int1_2939 = torch.constant.int 1
    %int1_2940 = torch.constant.int 1
    %2589 = torch.aten.add.Scalar %2559, %int1_2939, %int1_2940 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2589, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_2941 = torch.constant.int 4
    %2590 = torch.aten.mul.int %int4_2941, %398 : !torch.int, !torch.int -> !torch.int
    %2591 = torch.prim.ListConstruct %2590 : (!torch.int) -> !torch.list<int>
    %2592 = torch.aten.view %2589, %2591 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2592, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %2593 = torch.prim.ListConstruct %2592 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_2942 = torch.constant.bool false
    %2594 = torch.aten.index_put %2583, %2593, %2588, %false_2942 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %2594, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_2943 = torch.constant.int 32
    %int2_2944 = torch.constant.int 2
    %int32_2945 = torch.constant.int 32
    %int8_2946 = torch.constant.int 8
    %int128_2947 = torch.constant.int 128
    %2595 = torch.prim.ListConstruct %389, %int32_2943, %int2_2944, %int32_2945, %int8_2946, %int128_2947 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2596 = torch.aten.view %2594, %2595 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2596, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_2948 = torch.constant.int 2097152
    %2597 = torch.prim.ListConstruct %389, %int2097152_2948 : (!torch.int, !torch.int) -> !torch.list<int>
    %2598 = torch.aten.view %2596, %2597 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %2598, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int-2_2949 = torch.constant.int -2
    %2599 = torch.aten.unsqueeze %2557, %int-2_2949 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %2599, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int4_2950 = torch.constant.int 4
    %int8_2951 = torch.constant.int 8
    %int4_2952 = torch.constant.int 4
    %int128_2953 = torch.constant.int 128
    %2600 = torch.prim.ListConstruct %int4_2950, %2542, %int8_2951, %int4_2952, %int128_2953 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_2954 = torch.constant.bool false
    %2601 = torch.aten.expand %2599, %2600, %false_2954 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2601, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_2955 = torch.constant.int 0
    %2602 = torch.aten.clone %2601, %int0_2955 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2602, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_2956 = torch.constant.int 4
    %int32_2957 = torch.constant.int 32
    %int128_2958 = torch.constant.int 128
    %2603 = torch.prim.ListConstruct %int4_2956, %2542, %int32_2957, %int128_2958 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2604 = torch.aten._unsafe_view %2602, %2603 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2604, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_2959 = torch.constant.int -2
    %2605 = torch.aten.unsqueeze %2501, %int-2_2959 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %2605, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_2960 = torch.constant.int 1
    %2606 = torch.aten.size.int %2495, %int1_2960 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int4_2961 = torch.constant.int 4
    %int8_2962 = torch.constant.int 8
    %int4_2963 = torch.constant.int 4
    %int128_2964 = torch.constant.int 128
    %2607 = torch.prim.ListConstruct %int4_2961, %2606, %int8_2962, %int4_2963, %int128_2964 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_2965 = torch.constant.bool false
    %2608 = torch.aten.expand %2605, %2607, %false_2965 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2608, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_2966 = torch.constant.int 0
    %2609 = torch.aten.clone %2608, %int0_2966 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2609, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_2967 = torch.constant.int 4
    %int32_2968 = torch.constant.int 32
    %int128_2969 = torch.constant.int 128
    %2610 = torch.prim.ListConstruct %int4_2967, %2606, %int32_2968, %int128_2969 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2611 = torch.aten._unsafe_view %2609, %2610 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2611, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_2970 = torch.constant.int 1
    %int2_2971 = torch.constant.int 2
    %2612 = torch.aten.transpose.int %2529, %int1_2970, %int2_2971 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %2612, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_2972 = torch.constant.int 1
    %int2_2973 = torch.constant.int 2
    %2613 = torch.aten.transpose.int %2604, %int1_2972, %int2_2973 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %2613, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_2974 = torch.constant.int 1
    %int2_2975 = torch.constant.int 2
    %2614 = torch.aten.transpose.int %2611, %int1_2974, %int2_2975 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %2614, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_2976 = torch.constant.float 0.000000e+00
    %true_2977 = torch.constant.bool true
    %none_2978 = torch.constant.none
    %none_2979 = torch.constant.none
    %2615:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2612, %2613, %2614, %float0.000000e00_2976, %true_2977, %none_2978, %none_2979) : (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?],f32>) 
    torch.bind_symbolic_shape %2615#0, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_2980 = torch.constant.int 1
    %int2_2981 = torch.constant.int 2
    %2616 = torch.aten.transpose.int %2615#0, %int1_2980, %int2_2981 : !torch.vtensor<[4,32,?,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2616, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_2982 = torch.constant.int 4
    %int4096_2983 = torch.constant.int 4096
    %2617 = torch.prim.ListConstruct %int4_2982, %2514, %int4096_2983 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2618 = torch.aten.view %2616, %2617 : !torch.vtensor<[4,?,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2618, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_2984 = torch.constant.int -2
    %int-1_2985 = torch.constant.int -1
    %2619 = torch.aten.transpose.int %104, %int-2_2984, %int-1_2985 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_2986 = torch.constant.int 4
    %2620 = torch.aten.mul.int %int4_2986, %2514 : !torch.int, !torch.int -> !torch.int
    %int4096_2987 = torch.constant.int 4096
    %2621 = torch.prim.ListConstruct %2620, %int4096_2987 : (!torch.int, !torch.int) -> !torch.list<int>
    %2622 = torch.aten.view %2618, %2621 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2622, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %2623 = torch.aten.mm %2622, %2619 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2623, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_2988 = torch.constant.int 4
    %int4096_2989 = torch.constant.int 4096
    %2624 = torch.prim.ListConstruct %int4_2988, %2514, %int4096_2989 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2625 = torch.aten.view %2623, %2624 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2625, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_2990 = torch.constant.int 1
    %2626 = torch.aten.add.Tensor %2464, %2625, %int1_2990 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2626, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_2991 = torch.constant.int 6
    %2627 = torch.prims.convert_element_type %2626, %int6_2991 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2627, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_2992 = torch.constant.int 2
    %2628 = torch.aten.pow.Tensor_Scalar %2627, %int2_2992 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2628, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_2993 = torch.constant.int -1
    %2629 = torch.prim.ListConstruct %int-1_2993 : (!torch.int) -> !torch.list<int>
    %true_2994 = torch.constant.bool true
    %none_2995 = torch.constant.none
    %2630 = torch.aten.mean.dim %2628, %2629, %true_2994, %none_2995 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2630, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_2996 = torch.constant.float 9.9999997473787516E-6
    %int1_2997 = torch.constant.int 1
    %2631 = torch.aten.add.Scalar %2630, %float9.999990e-06_2996, %int1_2997 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2631, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %2632 = torch.aten.rsqrt %2631 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2632, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %2633 = torch.aten.mul.Tensor %2627, %2632 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2633, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_2998 = torch.constant.int 5
    %2634 = torch.prims.convert_element_type %2633, %int5_2998 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2634, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %2635 = torch.aten.mul.Tensor %105, %2634 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2635, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_2999 = torch.constant.int 5
    %2636 = torch.prims.convert_element_type %2635, %int5_2999 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2636, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_3000 = torch.constant.int -2
    %int-1_3001 = torch.constant.int -1
    %2637 = torch.aten.transpose.int %106, %int-2_3000, %int-1_3001 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_3002 = torch.constant.int 4
    %2638 = torch.aten.mul.int %int4_3002, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_3003 = torch.constant.int 4096
    %2639 = torch.prim.ListConstruct %2638, %int4096_3003 : (!torch.int, !torch.int) -> !torch.list<int>
    %2640 = torch.aten.view %2636, %2639 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2640, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %2641 = torch.aten.mm %2640, %2637 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %2641, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_3004 = torch.constant.int 4
    %int14336_3005 = torch.constant.int 14336
    %2642 = torch.prim.ListConstruct %int4_3004, %306, %int14336_3005 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2643 = torch.aten.view %2641, %2642 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %2643, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %2644 = torch.aten.silu %2643 : !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %2644, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_3006 = torch.constant.int -2
    %int-1_3007 = torch.constant.int -1
    %2645 = torch.aten.transpose.int %107, %int-2_3006, %int-1_3007 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_3008 = torch.constant.int 4
    %2646 = torch.aten.mul.int %int4_3008, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_3009 = torch.constant.int 4096
    %2647 = torch.prim.ListConstruct %2646, %int4096_3009 : (!torch.int, !torch.int) -> !torch.list<int>
    %2648 = torch.aten.view %2636, %2647 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2648, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %2649 = torch.aten.mm %2648, %2645 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %2649, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_3010 = torch.constant.int 4
    %int14336_3011 = torch.constant.int 14336
    %2650 = torch.prim.ListConstruct %int4_3010, %306, %int14336_3011 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2651 = torch.aten.view %2649, %2650 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %2651, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %2652 = torch.aten.mul.Tensor %2644, %2651 : !torch.vtensor<[4,?,14336],f16>, !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %2652, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_3012 = torch.constant.int -2
    %int-1_3013 = torch.constant.int -1
    %2653 = torch.aten.transpose.int %108, %int-2_3012, %int-1_3013 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int1_3014 = torch.constant.int 1
    %2654 = torch.aten.size.int %2643, %int1_3014 : !torch.vtensor<[4,?,14336],f16>, !torch.int -> !torch.int
    %int4_3015 = torch.constant.int 4
    %2655 = torch.aten.mul.int %int4_3015, %2654 : !torch.int, !torch.int -> !torch.int
    %int14336_3016 = torch.constant.int 14336
    %2656 = torch.prim.ListConstruct %2655, %int14336_3016 : (!torch.int, !torch.int) -> !torch.list<int>
    %2657 = torch.aten.view %2652, %2656 : !torch.vtensor<[4,?,14336],f16>, !torch.list<int> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %2657, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %2658 = torch.aten.mm %2657, %2653 : !torch.vtensor<[?,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2658, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_3017 = torch.constant.int 4
    %int4096_3018 = torch.constant.int 4096
    %2659 = torch.prim.ListConstruct %int4_3017, %2654, %int4096_3018 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2660 = torch.aten.view %2658, %2659 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2660, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_3019 = torch.constant.int 1
    %2661 = torch.aten.add.Tensor %2626, %2660, %int1_3019 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2661, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_3020 = torch.constant.int 6
    %2662 = torch.prims.convert_element_type %2661, %int6_3020 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2662, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_3021 = torch.constant.int 2
    %2663 = torch.aten.pow.Tensor_Scalar %2662, %int2_3021 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2663, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_3022 = torch.constant.int -1
    %2664 = torch.prim.ListConstruct %int-1_3022 : (!torch.int) -> !torch.list<int>
    %true_3023 = torch.constant.bool true
    %none_3024 = torch.constant.none
    %2665 = torch.aten.mean.dim %2663, %2664, %true_3023, %none_3024 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2665, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_3025 = torch.constant.float 9.9999997473787516E-6
    %int1_3026 = torch.constant.int 1
    %2666 = torch.aten.add.Scalar %2665, %float9.999990e-06_3025, %int1_3026 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2666, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %2667 = torch.aten.rsqrt %2666 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2667, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %2668 = torch.aten.mul.Tensor %2662, %2667 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2668, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_3027 = torch.constant.int 5
    %2669 = torch.prims.convert_element_type %2668, %int5_3027 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2669, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %2670 = torch.aten.mul.Tensor %109, %2669 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2670, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_3028 = torch.constant.int 5
    %2671 = torch.prims.convert_element_type %2670, %int5_3028 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2671, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_3029 = torch.constant.int -2
    %int-1_3030 = torch.constant.int -1
    %2672 = torch.aten.transpose.int %110, %int-2_3029, %int-1_3030 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_3031 = torch.constant.int 4
    %2673 = torch.aten.mul.int %int4_3031, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_3032 = torch.constant.int 4096
    %2674 = torch.prim.ListConstruct %2673, %int4096_3032 : (!torch.int, !torch.int) -> !torch.list<int>
    %2675 = torch.aten.view %2671, %2674 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2675, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %2676 = torch.aten.mm %2675, %2672 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2676, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_3033 = torch.constant.int 4
    %int4096_3034 = torch.constant.int 4096
    %2677 = torch.prim.ListConstruct %int4_3033, %306, %int4096_3034 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2678 = torch.aten.view %2676, %2677 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2678, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_3035 = torch.constant.int -2
    %int-1_3036 = torch.constant.int -1
    %2679 = torch.aten.transpose.int %111, %int-2_3035, %int-1_3036 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_3037 = torch.constant.int 4
    %2680 = torch.aten.mul.int %int4_3037, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_3038 = torch.constant.int 4096
    %2681 = torch.prim.ListConstruct %2680, %int4096_3038 : (!torch.int, !torch.int) -> !torch.list<int>
    %2682 = torch.aten.view %2671, %2681 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2682, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %2683 = torch.aten.mm %2682, %2679 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %2683, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_3039 = torch.constant.int 4
    %int1024_3040 = torch.constant.int 1024
    %2684 = torch.prim.ListConstruct %int4_3039, %306, %int1024_3040 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2685 = torch.aten.view %2683, %2684 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %2685, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int-2_3041 = torch.constant.int -2
    %int-1_3042 = torch.constant.int -1
    %2686 = torch.aten.transpose.int %112, %int-2_3041, %int-1_3042 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_3043 = torch.constant.int 4
    %2687 = torch.aten.mul.int %int4_3043, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_3044 = torch.constant.int 4096
    %2688 = torch.prim.ListConstruct %2687, %int4096_3044 : (!torch.int, !torch.int) -> !torch.list<int>
    %2689 = torch.aten.view %2671, %2688 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2689, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %2690 = torch.aten.mm %2689, %2686 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %2690, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_3045 = torch.constant.int 4
    %int1024_3046 = torch.constant.int 1024
    %2691 = torch.prim.ListConstruct %int4_3045, %306, %int1024_3046 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2692 = torch.aten.view %2690, %2691 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %2692, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int4_3047 = torch.constant.int 4
    %int32_3048 = torch.constant.int 32
    %int128_3049 = torch.constant.int 128
    %2693 = torch.prim.ListConstruct %int4_3047, %306, %int32_3048, %int128_3049 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2694 = torch.aten.view %2678, %2693 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2694, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_3050 = torch.constant.int 4
    %int8_3051 = torch.constant.int 8
    %int128_3052 = torch.constant.int 128
    %2695 = torch.prim.ListConstruct %int4_3050, %306, %int8_3051, %int128_3052 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2696 = torch.aten.view %2685, %2695 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %2696, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int4_3053 = torch.constant.int 4
    %int8_3054 = torch.constant.int 8
    %int128_3055 = torch.constant.int 128
    %2697 = torch.prim.ListConstruct %int4_3053, %306, %int8_3054, %int128_3055 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2698 = torch.aten.view %2692, %2697 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %2698, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int131072_3056 = torch.constant.int 131072
    %none_3057 = torch.constant.none
    %none_3058 = torch.constant.none
    %cpu_3059 = torch.constant.device "cpu"
    %false_3060 = torch.constant.bool false
    %2699 = torch.aten.arange %int131072_3056, %none_3057, %none_3058, %cpu_3059, %false_3060 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_3061 = torch.constant.int 0
    %int128_3062 = torch.constant.int 128
    %none_3063 = torch.constant.none
    %none_3064 = torch.constant.none
    %cpu_3065 = torch.constant.device "cpu"
    %false_3066 = torch.constant.bool false
    %2700 = torch.aten.arange.start %int0_3061, %int128_3062, %none_3063, %none_3064, %cpu_3065, %false_3066 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_3067 = torch.constant.int 2
    %2701 = torch.aten.floor_divide.Scalar %2700, %int2_3067 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_3068 = torch.constant.int 6
    %2702 = torch.prims.convert_element_type %2701, %int6_3068 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_3069 = torch.constant.int 128
    %2703 = torch.aten.div.Scalar %2702, %int128_3069 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_3070 = torch.constant.float 2.000000e+00
    %2704 = torch.aten.mul.Scalar %2703, %float2.000000e00_3070 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_3071 = torch.constant.float 5.000000e+05
    %2705 = torch.aten.pow.Scalar %float5.000000e05_3071, %2704 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %2706 = torch.aten.reciprocal %2705 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_3072 = torch.constant.float 1.000000e+00
    %2707 = torch.aten.mul.Scalar %2706, %float1.000000e00_3072 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_3073 = torch.constant.int 1
    %2708 = torch.aten.unsqueeze %2699, %int1_3073 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_3074 = torch.constant.int 0
    %2709 = torch.aten.unsqueeze %2707, %int0_3074 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %2710 = torch.aten.mul.Tensor %2708, %2709 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_3075 = torch.constant.int 1
    %2711 = torch.aten.size.int %2678, %int1_3075 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.int
    %int0_3076 = torch.constant.int 0
    %2712 = torch.aten.add.int %int0_3076, %2711 : !torch.int, !torch.int -> !torch.int
    %int0_3077 = torch.constant.int 0
    %int0_3078 = torch.constant.int 0
    %int1_3079 = torch.constant.int 1
    %2713 = torch.aten.slice.Tensor %2710, %int0_3077, %int0_3078, %2712, %int1_3079 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %2713, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_3080 = torch.constant.int 1
    %int0_3081 = torch.constant.int 0
    %int9223372036854775807_3082 = torch.constant.int 9223372036854775807
    %int1_3083 = torch.constant.int 1
    %2714 = torch.aten.slice.Tensor %2713, %int1_3080, %int0_3081, %int9223372036854775807_3082, %int1_3083 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %2714, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_3084 = torch.constant.int 1
    %int0_3085 = torch.constant.int 0
    %int9223372036854775807_3086 = torch.constant.int 9223372036854775807
    %int1_3087 = torch.constant.int 1
    %2715 = torch.aten.slice.Tensor %2714, %int1_3084, %int0_3085, %int9223372036854775807_3086, %int1_3087 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %2715, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_3088 = torch.constant.int 0
    %2716 = torch.aten.unsqueeze %2715, %int0_3088 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %2716, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_3089 = torch.constant.int 1
    %int0_3090 = torch.constant.int 0
    %int9223372036854775807_3091 = torch.constant.int 9223372036854775807
    %int1_3092 = torch.constant.int 1
    %2717 = torch.aten.slice.Tensor %2716, %int1_3089, %int0_3090, %int9223372036854775807_3091, %int1_3092 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %2717, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_3093 = torch.constant.int 2
    %int0_3094 = torch.constant.int 0
    %int9223372036854775807_3095 = torch.constant.int 9223372036854775807
    %int1_3096 = torch.constant.int 1
    %2718 = torch.aten.slice.Tensor %2717, %int2_3093, %int0_3094, %int9223372036854775807_3095, %int1_3096 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %2718, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_3097 = torch.constant.int 4
    %int1_3098 = torch.constant.int 1
    %int1_3099 = torch.constant.int 1
    %2719 = torch.prim.ListConstruct %int4_3097, %int1_3098, %int1_3099 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2720 = torch.aten.repeat %2718, %2719 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %2720, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_3100 = torch.constant.int 6
    %2721 = torch.prims.convert_element_type %2694, %int6_3100 : !torch.vtensor<[4,?,32,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %2721, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %2722 = torch_c.to_builtin_tensor %2721 : !torch.vtensor<[4,?,32,128],f32> -> tensor<4x?x32x128xf32>
    %2723 = torch_c.to_builtin_tensor %2720 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %2724 = util.call @sharktank_rotary_embedding_4_D_32_128_f32(%2722, %2723) : (tensor<4x?x32x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x32x128xf32>
    %2725 = torch_c.from_builtin_tensor %2724 : tensor<4x?x32x128xf32> -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %2725, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %int5_3101 = torch.constant.int 5
    %2726 = torch.prims.convert_element_type %2725, %int5_3101 : !torch.vtensor<[4,?,32,128],f32>, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2726, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int131072_3102 = torch.constant.int 131072
    %none_3103 = torch.constant.none
    %none_3104 = torch.constant.none
    %cpu_3105 = torch.constant.device "cpu"
    %false_3106 = torch.constant.bool false
    %2727 = torch.aten.arange %int131072_3102, %none_3103, %none_3104, %cpu_3105, %false_3106 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_3107 = torch.constant.int 0
    %int128_3108 = torch.constant.int 128
    %none_3109 = torch.constant.none
    %none_3110 = torch.constant.none
    %cpu_3111 = torch.constant.device "cpu"
    %false_3112 = torch.constant.bool false
    %2728 = torch.aten.arange.start %int0_3107, %int128_3108, %none_3109, %none_3110, %cpu_3111, %false_3112 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_3113 = torch.constant.int 2
    %2729 = torch.aten.floor_divide.Scalar %2728, %int2_3113 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_3114 = torch.constant.int 6
    %2730 = torch.prims.convert_element_type %2729, %int6_3114 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_3115 = torch.constant.int 128
    %2731 = torch.aten.div.Scalar %2730, %int128_3115 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_3116 = torch.constant.float 2.000000e+00
    %2732 = torch.aten.mul.Scalar %2731, %float2.000000e00_3116 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_3117 = torch.constant.float 5.000000e+05
    %2733 = torch.aten.pow.Scalar %float5.000000e05_3117, %2732 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %2734 = torch.aten.reciprocal %2733 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_3118 = torch.constant.float 1.000000e+00
    %2735 = torch.aten.mul.Scalar %2734, %float1.000000e00_3118 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_3119 = torch.constant.int 1
    %2736 = torch.aten.unsqueeze %2727, %int1_3119 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_3120 = torch.constant.int 0
    %2737 = torch.aten.unsqueeze %2735, %int0_3120 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %2738 = torch.aten.mul.Tensor %2736, %2737 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_3121 = torch.constant.int 1
    %2739 = torch.aten.size.int %2685, %int1_3121 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int0_3122 = torch.constant.int 0
    %2740 = torch.aten.add.int %int0_3122, %2739 : !torch.int, !torch.int -> !torch.int
    %int0_3123 = torch.constant.int 0
    %int0_3124 = torch.constant.int 0
    %int1_3125 = torch.constant.int 1
    %2741 = torch.aten.slice.Tensor %2738, %int0_3123, %int0_3124, %2740, %int1_3125 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %2741, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_3126 = torch.constant.int 1
    %int0_3127 = torch.constant.int 0
    %int9223372036854775807_3128 = torch.constant.int 9223372036854775807
    %int1_3129 = torch.constant.int 1
    %2742 = torch.aten.slice.Tensor %2741, %int1_3126, %int0_3127, %int9223372036854775807_3128, %int1_3129 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %2742, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_3130 = torch.constant.int 1
    %int0_3131 = torch.constant.int 0
    %int9223372036854775807_3132 = torch.constant.int 9223372036854775807
    %int1_3133 = torch.constant.int 1
    %2743 = torch.aten.slice.Tensor %2742, %int1_3130, %int0_3131, %int9223372036854775807_3132, %int1_3133 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %2743, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_3134 = torch.constant.int 0
    %2744 = torch.aten.unsqueeze %2743, %int0_3134 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %2744, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_3135 = torch.constant.int 1
    %int0_3136 = torch.constant.int 0
    %int9223372036854775807_3137 = torch.constant.int 9223372036854775807
    %int1_3138 = torch.constant.int 1
    %2745 = torch.aten.slice.Tensor %2744, %int1_3135, %int0_3136, %int9223372036854775807_3137, %int1_3138 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %2745, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_3139 = torch.constant.int 2
    %int0_3140 = torch.constant.int 0
    %int9223372036854775807_3141 = torch.constant.int 9223372036854775807
    %int1_3142 = torch.constant.int 1
    %2746 = torch.aten.slice.Tensor %2745, %int2_3139, %int0_3140, %int9223372036854775807_3141, %int1_3142 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %2746, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_3143 = torch.constant.int 4
    %int1_3144 = torch.constant.int 1
    %int1_3145 = torch.constant.int 1
    %2747 = torch.prim.ListConstruct %int4_3143, %int1_3144, %int1_3145 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2748 = torch.aten.repeat %2746, %2747 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %2748, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_3146 = torch.constant.int 6
    %2749 = torch.prims.convert_element_type %2696, %int6_3146 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %2749, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %2750 = torch_c.to_builtin_tensor %2749 : !torch.vtensor<[4,?,8,128],f32> -> tensor<4x?x8x128xf32>
    %2751 = torch_c.to_builtin_tensor %2748 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %2752 = util.call @sharktank_rotary_embedding_4_D_8_128_f32(%2750, %2751) : (tensor<4x?x8x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x8x128xf32>
    %2753 = torch_c.from_builtin_tensor %2752 : tensor<4x?x8x128xf32> -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %2753, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %int5_3147 = torch.constant.int 5
    %2754 = torch.prims.convert_element_type %2753, %int5_3147 : !torch.vtensor<[4,?,8,128],f32>, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %2754, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int64_3148 = torch.constant.int 64
    %2755 = torch.aten.mul.Scalar %arg2, %int64_3148 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2755, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int24 = torch.constant.int 24
    %int1_3149 = torch.constant.int 1
    %2756 = torch.aten.add.Scalar %2755, %int24, %int1_3149 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2756, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_3150 = torch.constant.int 4
    %int32_3151 = torch.constant.int 32
    %int8_3152 = torch.constant.int 8
    %int128_3153 = torch.constant.int 128
    %2757 = torch.prim.ListConstruct %int4_3150, %398, %int32_3151, %int8_3152, %int128_3153 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2758 = torch.aten.view %2754, %2757 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2758, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_3154 = torch.constant.int 4
    %2759 = torch.aten.mul.int %int4_3154, %398 : !torch.int, !torch.int -> !torch.int
    %int32_3155 = torch.constant.int 32
    %int8_3156 = torch.constant.int 8
    %int128_3157 = torch.constant.int 128
    %2760 = torch.prim.ListConstruct %2759, %int32_3155, %int8_3156, %int128_3157 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2761 = torch.aten.view %2758, %2760 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %2761, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_3158 = torch.constant.int 4
    %2762 = torch.aten.mul.int %int4_3158, %398 : !torch.int, !torch.int -> !torch.int
    %2763 = torch.prim.ListConstruct %2762 : (!torch.int) -> !torch.list<int>
    %2764 = torch.aten.view %2756, %2763 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2764, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_3159 = torch.constant.int 32
    %int2_3160 = torch.constant.int 2
    %int32_3161 = torch.constant.int 32
    %int8_3162 = torch.constant.int 8
    %int128_3163 = torch.constant.int 128
    %2765 = torch.prim.ListConstruct %389, %int32_3159, %int2_3160, %int32_3161, %int8_3162, %int128_3163 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2766 = torch.aten.view %2598, %2765 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2766, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_3164 = torch.constant.int 32
    %2767 = torch.aten.mul.int %389, %int32_3164 : !torch.int, !torch.int -> !torch.int
    %int2_3165 = torch.constant.int 2
    %2768 = torch.aten.mul.int %2767, %int2_3165 : !torch.int, !torch.int -> !torch.int
    %int32_3166 = torch.constant.int 32
    %int8_3167 = torch.constant.int 8
    %int128_3168 = torch.constant.int 128
    %2769 = torch.prim.ListConstruct %2768, %int32_3166, %int8_3167, %int128_3168 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2770 = torch.aten.view %2766, %2769 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %2770, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %2771 = torch.prim.ListConstruct %2764 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_3169 = torch.constant.bool false
    %2772 = torch.aten.index_put %2770, %2771, %2761, %false_3169 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %2772, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_3170 = torch.constant.int 32
    %int2_3171 = torch.constant.int 2
    %int32_3172 = torch.constant.int 32
    %int8_3173 = torch.constant.int 8
    %int128_3174 = torch.constant.int 128
    %2773 = torch.prim.ListConstruct %389, %int32_3170, %int2_3171, %int32_3172, %int8_3173, %int128_3174 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2774 = torch.aten.view %2772, %2773 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2774, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_3175 = torch.constant.int 2097152
    %2775 = torch.prim.ListConstruct %389, %int2097152_3175 : (!torch.int, !torch.int) -> !torch.list<int>
    %2776 = torch.aten.view %2774, %2775 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %2776, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_3176 = torch.constant.int 32
    %int2_3177 = torch.constant.int 2
    %int32_3178 = torch.constant.int 32
    %int8_3179 = torch.constant.int 8
    %int128_3180 = torch.constant.int 128
    %2777 = torch.prim.ListConstruct %389, %int32_3176, %int2_3177, %int32_3178, %int8_3179, %int128_3180 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2778 = torch.aten.view %2776, %2777 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2778, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_3181 = torch.constant.int 32
    %int8_3182 = torch.constant.int 8
    %int128_3183 = torch.constant.int 128
    %2779 = torch.prim.ListConstruct %2768, %int32_3181, %int8_3182, %int128_3183 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2780 = torch.aten.view %2778, %2779 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %2780, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_3184 = torch.constant.int 4
    %int32_3185 = torch.constant.int 32
    %int8_3186 = torch.constant.int 8
    %int128_3187 = torch.constant.int 128
    %2781 = torch.prim.ListConstruct %int4_3184, %398, %int32_3185, %int8_3186, %int128_3187 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2782 = torch.aten.view %2698, %2781 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2782, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_3188 = torch.constant.int 4
    %2783 = torch.aten.mul.int %int4_3188, %398 : !torch.int, !torch.int -> !torch.int
    %int32_3189 = torch.constant.int 32
    %int8_3190 = torch.constant.int 8
    %int128_3191 = torch.constant.int 128
    %2784 = torch.prim.ListConstruct %2783, %int32_3189, %int8_3190, %int128_3191 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2785 = torch.aten.view %2782, %2784 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %2785, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int1_3192 = torch.constant.int 1
    %int1_3193 = torch.constant.int 1
    %2786 = torch.aten.add.Scalar %2756, %int1_3192, %int1_3193 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2786, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_3194 = torch.constant.int 4
    %2787 = torch.aten.mul.int %int4_3194, %398 : !torch.int, !torch.int -> !torch.int
    %2788 = torch.prim.ListConstruct %2787 : (!torch.int) -> !torch.list<int>
    %2789 = torch.aten.view %2786, %2788 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2789, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %2790 = torch.prim.ListConstruct %2789 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_3195 = torch.constant.bool false
    %2791 = torch.aten.index_put %2780, %2790, %2785, %false_3195 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %2791, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_3196 = torch.constant.int 32
    %int2_3197 = torch.constant.int 2
    %int32_3198 = torch.constant.int 32
    %int8_3199 = torch.constant.int 8
    %int128_3200 = torch.constant.int 128
    %2792 = torch.prim.ListConstruct %389, %int32_3196, %int2_3197, %int32_3198, %int8_3199, %int128_3200 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2793 = torch.aten.view %2791, %2792 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2793, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_3201 = torch.constant.int 2097152
    %2794 = torch.prim.ListConstruct %389, %int2097152_3201 : (!torch.int, !torch.int) -> !torch.list<int>
    %2795 = torch.aten.view %2793, %2794 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %2795, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int-2_3202 = torch.constant.int -2
    %2796 = torch.aten.unsqueeze %2754, %int-2_3202 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %2796, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int4_3203 = torch.constant.int 4
    %int8_3204 = torch.constant.int 8
    %int4_3205 = torch.constant.int 4
    %int128_3206 = torch.constant.int 128
    %2797 = torch.prim.ListConstruct %int4_3203, %2739, %int8_3204, %int4_3205, %int128_3206 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_3207 = torch.constant.bool false
    %2798 = torch.aten.expand %2796, %2797, %false_3207 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2798, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_3208 = torch.constant.int 0
    %2799 = torch.aten.clone %2798, %int0_3208 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2799, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_3209 = torch.constant.int 4
    %int32_3210 = torch.constant.int 32
    %int128_3211 = torch.constant.int 128
    %2800 = torch.prim.ListConstruct %int4_3209, %2739, %int32_3210, %int128_3211 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2801 = torch.aten._unsafe_view %2799, %2800 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2801, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_3212 = torch.constant.int -2
    %2802 = torch.aten.unsqueeze %2698, %int-2_3212 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %2802, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_3213 = torch.constant.int 1
    %2803 = torch.aten.size.int %2692, %int1_3213 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int4_3214 = torch.constant.int 4
    %int8_3215 = torch.constant.int 8
    %int4_3216 = torch.constant.int 4
    %int128_3217 = torch.constant.int 128
    %2804 = torch.prim.ListConstruct %int4_3214, %2803, %int8_3215, %int4_3216, %int128_3217 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_3218 = torch.constant.bool false
    %2805 = torch.aten.expand %2802, %2804, %false_3218 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2805, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_3219 = torch.constant.int 0
    %2806 = torch.aten.clone %2805, %int0_3219 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2806, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_3220 = torch.constant.int 4
    %int32_3221 = torch.constant.int 32
    %int128_3222 = torch.constant.int 128
    %2807 = torch.prim.ListConstruct %int4_3220, %2803, %int32_3221, %int128_3222 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2808 = torch.aten._unsafe_view %2806, %2807 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2808, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_3223 = torch.constant.int 1
    %int2_3224 = torch.constant.int 2
    %2809 = torch.aten.transpose.int %2726, %int1_3223, %int2_3224 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %2809, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_3225 = torch.constant.int 1
    %int2_3226 = torch.constant.int 2
    %2810 = torch.aten.transpose.int %2801, %int1_3225, %int2_3226 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %2810, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_3227 = torch.constant.int 1
    %int2_3228 = torch.constant.int 2
    %2811 = torch.aten.transpose.int %2808, %int1_3227, %int2_3228 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %2811, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_3229 = torch.constant.float 0.000000e+00
    %true_3230 = torch.constant.bool true
    %none_3231 = torch.constant.none
    %none_3232 = torch.constant.none
    %2812:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2809, %2810, %2811, %float0.000000e00_3229, %true_3230, %none_3231, %none_3232) : (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?],f32>) 
    torch.bind_symbolic_shape %2812#0, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_3233 = torch.constant.int 1
    %int2_3234 = torch.constant.int 2
    %2813 = torch.aten.transpose.int %2812#0, %int1_3233, %int2_3234 : !torch.vtensor<[4,32,?,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2813, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_3235 = torch.constant.int 4
    %int4096_3236 = torch.constant.int 4096
    %2814 = torch.prim.ListConstruct %int4_3235, %2711, %int4096_3236 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2815 = torch.aten.view %2813, %2814 : !torch.vtensor<[4,?,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2815, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_3237 = torch.constant.int -2
    %int-1_3238 = torch.constant.int -1
    %2816 = torch.aten.transpose.int %113, %int-2_3237, %int-1_3238 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_3239 = torch.constant.int 4
    %2817 = torch.aten.mul.int %int4_3239, %2711 : !torch.int, !torch.int -> !torch.int
    %int4096_3240 = torch.constant.int 4096
    %2818 = torch.prim.ListConstruct %2817, %int4096_3240 : (!torch.int, !torch.int) -> !torch.list<int>
    %2819 = torch.aten.view %2815, %2818 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2819, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %2820 = torch.aten.mm %2819, %2816 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2820, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_3241 = torch.constant.int 4
    %int4096_3242 = torch.constant.int 4096
    %2821 = torch.prim.ListConstruct %int4_3241, %2711, %int4096_3242 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2822 = torch.aten.view %2820, %2821 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2822, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_3243 = torch.constant.int 1
    %2823 = torch.aten.add.Tensor %2661, %2822, %int1_3243 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2823, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_3244 = torch.constant.int 6
    %2824 = torch.prims.convert_element_type %2823, %int6_3244 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2824, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_3245 = torch.constant.int 2
    %2825 = torch.aten.pow.Tensor_Scalar %2824, %int2_3245 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2825, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_3246 = torch.constant.int -1
    %2826 = torch.prim.ListConstruct %int-1_3246 : (!torch.int) -> !torch.list<int>
    %true_3247 = torch.constant.bool true
    %none_3248 = torch.constant.none
    %2827 = torch.aten.mean.dim %2825, %2826, %true_3247, %none_3248 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2827, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_3249 = torch.constant.float 9.9999997473787516E-6
    %int1_3250 = torch.constant.int 1
    %2828 = torch.aten.add.Scalar %2827, %float9.999990e-06_3249, %int1_3250 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2828, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %2829 = torch.aten.rsqrt %2828 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2829, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %2830 = torch.aten.mul.Tensor %2824, %2829 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2830, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_3251 = torch.constant.int 5
    %2831 = torch.prims.convert_element_type %2830, %int5_3251 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2831, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %2832 = torch.aten.mul.Tensor %114, %2831 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2832, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_3252 = torch.constant.int 5
    %2833 = torch.prims.convert_element_type %2832, %int5_3252 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2833, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_3253 = torch.constant.int -2
    %int-1_3254 = torch.constant.int -1
    %2834 = torch.aten.transpose.int %115, %int-2_3253, %int-1_3254 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_3255 = torch.constant.int 4
    %2835 = torch.aten.mul.int %int4_3255, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_3256 = torch.constant.int 4096
    %2836 = torch.prim.ListConstruct %2835, %int4096_3256 : (!torch.int, !torch.int) -> !torch.list<int>
    %2837 = torch.aten.view %2833, %2836 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2837, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %2838 = torch.aten.mm %2837, %2834 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %2838, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_3257 = torch.constant.int 4
    %int14336_3258 = torch.constant.int 14336
    %2839 = torch.prim.ListConstruct %int4_3257, %306, %int14336_3258 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2840 = torch.aten.view %2838, %2839 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %2840, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %2841 = torch.aten.silu %2840 : !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %2841, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_3259 = torch.constant.int -2
    %int-1_3260 = torch.constant.int -1
    %2842 = torch.aten.transpose.int %116, %int-2_3259, %int-1_3260 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_3261 = torch.constant.int 4
    %2843 = torch.aten.mul.int %int4_3261, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_3262 = torch.constant.int 4096
    %2844 = torch.prim.ListConstruct %2843, %int4096_3262 : (!torch.int, !torch.int) -> !torch.list<int>
    %2845 = torch.aten.view %2833, %2844 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2845, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %2846 = torch.aten.mm %2845, %2842 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %2846, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_3263 = torch.constant.int 4
    %int14336_3264 = torch.constant.int 14336
    %2847 = torch.prim.ListConstruct %int4_3263, %306, %int14336_3264 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2848 = torch.aten.view %2846, %2847 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %2848, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %2849 = torch.aten.mul.Tensor %2841, %2848 : !torch.vtensor<[4,?,14336],f16>, !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %2849, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_3265 = torch.constant.int -2
    %int-1_3266 = torch.constant.int -1
    %2850 = torch.aten.transpose.int %117, %int-2_3265, %int-1_3266 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int1_3267 = torch.constant.int 1
    %2851 = torch.aten.size.int %2840, %int1_3267 : !torch.vtensor<[4,?,14336],f16>, !torch.int -> !torch.int
    %int4_3268 = torch.constant.int 4
    %2852 = torch.aten.mul.int %int4_3268, %2851 : !torch.int, !torch.int -> !torch.int
    %int14336_3269 = torch.constant.int 14336
    %2853 = torch.prim.ListConstruct %2852, %int14336_3269 : (!torch.int, !torch.int) -> !torch.list<int>
    %2854 = torch.aten.view %2849, %2853 : !torch.vtensor<[4,?,14336],f16>, !torch.list<int> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %2854, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %2855 = torch.aten.mm %2854, %2850 : !torch.vtensor<[?,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2855, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_3270 = torch.constant.int 4
    %int4096_3271 = torch.constant.int 4096
    %2856 = torch.prim.ListConstruct %int4_3270, %2851, %int4096_3271 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2857 = torch.aten.view %2855, %2856 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2857, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_3272 = torch.constant.int 1
    %2858 = torch.aten.add.Tensor %2823, %2857, %int1_3272 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2858, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_3273 = torch.constant.int 6
    %2859 = torch.prims.convert_element_type %2858, %int6_3273 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2859, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_3274 = torch.constant.int 2
    %2860 = torch.aten.pow.Tensor_Scalar %2859, %int2_3274 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2860, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_3275 = torch.constant.int -1
    %2861 = torch.prim.ListConstruct %int-1_3275 : (!torch.int) -> !torch.list<int>
    %true_3276 = torch.constant.bool true
    %none_3277 = torch.constant.none
    %2862 = torch.aten.mean.dim %2860, %2861, %true_3276, %none_3277 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2862, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_3278 = torch.constant.float 9.9999997473787516E-6
    %int1_3279 = torch.constant.int 1
    %2863 = torch.aten.add.Scalar %2862, %float9.999990e-06_3278, %int1_3279 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2863, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %2864 = torch.aten.rsqrt %2863 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %2864, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %2865 = torch.aten.mul.Tensor %2859, %2864 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2865, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_3280 = torch.constant.int 5
    %2866 = torch.prims.convert_element_type %2865, %int5_3280 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2866, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %2867 = torch.aten.mul.Tensor %118, %2866 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %2867, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_3281 = torch.constant.int 5
    %2868 = torch.prims.convert_element_type %2867, %int5_3281 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2868, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_3282 = torch.constant.int -2
    %int-1_3283 = torch.constant.int -1
    %2869 = torch.aten.transpose.int %119, %int-2_3282, %int-1_3283 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_3284 = torch.constant.int 4
    %2870 = torch.aten.mul.int %int4_3284, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_3285 = torch.constant.int 4096
    %2871 = torch.prim.ListConstruct %2870, %int4096_3285 : (!torch.int, !torch.int) -> !torch.list<int>
    %2872 = torch.aten.view %2868, %2871 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2872, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %2873 = torch.aten.mm %2872, %2869 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2873, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_3286 = torch.constant.int 4
    %int4096_3287 = torch.constant.int 4096
    %2874 = torch.prim.ListConstruct %int4_3286, %306, %int4096_3287 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2875 = torch.aten.view %2873, %2874 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %2875, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_3288 = torch.constant.int -2
    %int-1_3289 = torch.constant.int -1
    %2876 = torch.aten.transpose.int %120, %int-2_3288, %int-1_3289 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_3290 = torch.constant.int 4
    %2877 = torch.aten.mul.int %int4_3290, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_3291 = torch.constant.int 4096
    %2878 = torch.prim.ListConstruct %2877, %int4096_3291 : (!torch.int, !torch.int) -> !torch.list<int>
    %2879 = torch.aten.view %2868, %2878 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2879, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %2880 = torch.aten.mm %2879, %2876 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %2880, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_3292 = torch.constant.int 4
    %int1024_3293 = torch.constant.int 1024
    %2881 = torch.prim.ListConstruct %int4_3292, %306, %int1024_3293 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2882 = torch.aten.view %2880, %2881 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %2882, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int-2_3294 = torch.constant.int -2
    %int-1_3295 = torch.constant.int -1
    %2883 = torch.aten.transpose.int %121, %int-2_3294, %int-1_3295 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_3296 = torch.constant.int 4
    %2884 = torch.aten.mul.int %int4_3296, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_3297 = torch.constant.int 4096
    %2885 = torch.prim.ListConstruct %2884, %int4096_3297 : (!torch.int, !torch.int) -> !torch.list<int>
    %2886 = torch.aten.view %2868, %2885 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %2886, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %2887 = torch.aten.mm %2886, %2883 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %2887, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_3298 = torch.constant.int 4
    %int1024_3299 = torch.constant.int 1024
    %2888 = torch.prim.ListConstruct %int4_3298, %306, %int1024_3299 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2889 = torch.aten.view %2887, %2888 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %2889, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int4_3300 = torch.constant.int 4
    %int32_3301 = torch.constant.int 32
    %int128_3302 = torch.constant.int 128
    %2890 = torch.prim.ListConstruct %int4_3300, %306, %int32_3301, %int128_3302 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2891 = torch.aten.view %2875, %2890 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2891, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_3303 = torch.constant.int 4
    %int8_3304 = torch.constant.int 8
    %int128_3305 = torch.constant.int 128
    %2892 = torch.prim.ListConstruct %int4_3303, %306, %int8_3304, %int128_3305 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2893 = torch.aten.view %2882, %2892 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %2893, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int4_3306 = torch.constant.int 4
    %int8_3307 = torch.constant.int 8
    %int128_3308 = torch.constant.int 128
    %2894 = torch.prim.ListConstruct %int4_3306, %306, %int8_3307, %int128_3308 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2895 = torch.aten.view %2889, %2894 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %2895, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int131072_3309 = torch.constant.int 131072
    %none_3310 = torch.constant.none
    %none_3311 = torch.constant.none
    %cpu_3312 = torch.constant.device "cpu"
    %false_3313 = torch.constant.bool false
    %2896 = torch.aten.arange %int131072_3309, %none_3310, %none_3311, %cpu_3312, %false_3313 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_3314 = torch.constant.int 0
    %int128_3315 = torch.constant.int 128
    %none_3316 = torch.constant.none
    %none_3317 = torch.constant.none
    %cpu_3318 = torch.constant.device "cpu"
    %false_3319 = torch.constant.bool false
    %2897 = torch.aten.arange.start %int0_3314, %int128_3315, %none_3316, %none_3317, %cpu_3318, %false_3319 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_3320 = torch.constant.int 2
    %2898 = torch.aten.floor_divide.Scalar %2897, %int2_3320 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_3321 = torch.constant.int 6
    %2899 = torch.prims.convert_element_type %2898, %int6_3321 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_3322 = torch.constant.int 128
    %2900 = torch.aten.div.Scalar %2899, %int128_3322 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_3323 = torch.constant.float 2.000000e+00
    %2901 = torch.aten.mul.Scalar %2900, %float2.000000e00_3323 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_3324 = torch.constant.float 5.000000e+05
    %2902 = torch.aten.pow.Scalar %float5.000000e05_3324, %2901 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %2903 = torch.aten.reciprocal %2902 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_3325 = torch.constant.float 1.000000e+00
    %2904 = torch.aten.mul.Scalar %2903, %float1.000000e00_3325 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_3326 = torch.constant.int 1
    %2905 = torch.aten.unsqueeze %2896, %int1_3326 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_3327 = torch.constant.int 0
    %2906 = torch.aten.unsqueeze %2904, %int0_3327 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %2907 = torch.aten.mul.Tensor %2905, %2906 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_3328 = torch.constant.int 1
    %2908 = torch.aten.size.int %2875, %int1_3328 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.int
    %int0_3329 = torch.constant.int 0
    %2909 = torch.aten.add.int %int0_3329, %2908 : !torch.int, !torch.int -> !torch.int
    %int0_3330 = torch.constant.int 0
    %int0_3331 = torch.constant.int 0
    %int1_3332 = torch.constant.int 1
    %2910 = torch.aten.slice.Tensor %2907, %int0_3330, %int0_3331, %2909, %int1_3332 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %2910, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_3333 = torch.constant.int 1
    %int0_3334 = torch.constant.int 0
    %int9223372036854775807_3335 = torch.constant.int 9223372036854775807
    %int1_3336 = torch.constant.int 1
    %2911 = torch.aten.slice.Tensor %2910, %int1_3333, %int0_3334, %int9223372036854775807_3335, %int1_3336 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %2911, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_3337 = torch.constant.int 1
    %int0_3338 = torch.constant.int 0
    %int9223372036854775807_3339 = torch.constant.int 9223372036854775807
    %int1_3340 = torch.constant.int 1
    %2912 = torch.aten.slice.Tensor %2911, %int1_3337, %int0_3338, %int9223372036854775807_3339, %int1_3340 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %2912, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_3341 = torch.constant.int 0
    %2913 = torch.aten.unsqueeze %2912, %int0_3341 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %2913, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_3342 = torch.constant.int 1
    %int0_3343 = torch.constant.int 0
    %int9223372036854775807_3344 = torch.constant.int 9223372036854775807
    %int1_3345 = torch.constant.int 1
    %2914 = torch.aten.slice.Tensor %2913, %int1_3342, %int0_3343, %int9223372036854775807_3344, %int1_3345 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %2914, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_3346 = torch.constant.int 2
    %int0_3347 = torch.constant.int 0
    %int9223372036854775807_3348 = torch.constant.int 9223372036854775807
    %int1_3349 = torch.constant.int 1
    %2915 = torch.aten.slice.Tensor %2914, %int2_3346, %int0_3347, %int9223372036854775807_3348, %int1_3349 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %2915, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_3350 = torch.constant.int 4
    %int1_3351 = torch.constant.int 1
    %int1_3352 = torch.constant.int 1
    %2916 = torch.prim.ListConstruct %int4_3350, %int1_3351, %int1_3352 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2917 = torch.aten.repeat %2915, %2916 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %2917, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_3353 = torch.constant.int 6
    %2918 = torch.prims.convert_element_type %2891, %int6_3353 : !torch.vtensor<[4,?,32,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %2918, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %2919 = torch_c.to_builtin_tensor %2918 : !torch.vtensor<[4,?,32,128],f32> -> tensor<4x?x32x128xf32>
    %2920 = torch_c.to_builtin_tensor %2917 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %2921 = util.call @sharktank_rotary_embedding_4_D_32_128_f32(%2919, %2920) : (tensor<4x?x32x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x32x128xf32>
    %2922 = torch_c.from_builtin_tensor %2921 : tensor<4x?x32x128xf32> -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %2922, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %int5_3354 = torch.constant.int 5
    %2923 = torch.prims.convert_element_type %2922, %int5_3354 : !torch.vtensor<[4,?,32,128],f32>, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2923, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int131072_3355 = torch.constant.int 131072
    %none_3356 = torch.constant.none
    %none_3357 = torch.constant.none
    %cpu_3358 = torch.constant.device "cpu"
    %false_3359 = torch.constant.bool false
    %2924 = torch.aten.arange %int131072_3355, %none_3356, %none_3357, %cpu_3358, %false_3359 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_3360 = torch.constant.int 0
    %int128_3361 = torch.constant.int 128
    %none_3362 = torch.constant.none
    %none_3363 = torch.constant.none
    %cpu_3364 = torch.constant.device "cpu"
    %false_3365 = torch.constant.bool false
    %2925 = torch.aten.arange.start %int0_3360, %int128_3361, %none_3362, %none_3363, %cpu_3364, %false_3365 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_3366 = torch.constant.int 2
    %2926 = torch.aten.floor_divide.Scalar %2925, %int2_3366 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_3367 = torch.constant.int 6
    %2927 = torch.prims.convert_element_type %2926, %int6_3367 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_3368 = torch.constant.int 128
    %2928 = torch.aten.div.Scalar %2927, %int128_3368 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_3369 = torch.constant.float 2.000000e+00
    %2929 = torch.aten.mul.Scalar %2928, %float2.000000e00_3369 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_3370 = torch.constant.float 5.000000e+05
    %2930 = torch.aten.pow.Scalar %float5.000000e05_3370, %2929 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %2931 = torch.aten.reciprocal %2930 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_3371 = torch.constant.float 1.000000e+00
    %2932 = torch.aten.mul.Scalar %2931, %float1.000000e00_3371 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_3372 = torch.constant.int 1
    %2933 = torch.aten.unsqueeze %2924, %int1_3372 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_3373 = torch.constant.int 0
    %2934 = torch.aten.unsqueeze %2932, %int0_3373 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %2935 = torch.aten.mul.Tensor %2933, %2934 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_3374 = torch.constant.int 1
    %2936 = torch.aten.size.int %2882, %int1_3374 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int0_3375 = torch.constant.int 0
    %2937 = torch.aten.add.int %int0_3375, %2936 : !torch.int, !torch.int -> !torch.int
    %int0_3376 = torch.constant.int 0
    %int0_3377 = torch.constant.int 0
    %int1_3378 = torch.constant.int 1
    %2938 = torch.aten.slice.Tensor %2935, %int0_3376, %int0_3377, %2937, %int1_3378 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %2938, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_3379 = torch.constant.int 1
    %int0_3380 = torch.constant.int 0
    %int9223372036854775807_3381 = torch.constant.int 9223372036854775807
    %int1_3382 = torch.constant.int 1
    %2939 = torch.aten.slice.Tensor %2938, %int1_3379, %int0_3380, %int9223372036854775807_3381, %int1_3382 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %2939, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_3383 = torch.constant.int 1
    %int0_3384 = torch.constant.int 0
    %int9223372036854775807_3385 = torch.constant.int 9223372036854775807
    %int1_3386 = torch.constant.int 1
    %2940 = torch.aten.slice.Tensor %2939, %int1_3383, %int0_3384, %int9223372036854775807_3385, %int1_3386 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %2940, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_3387 = torch.constant.int 0
    %2941 = torch.aten.unsqueeze %2940, %int0_3387 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %2941, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_3388 = torch.constant.int 1
    %int0_3389 = torch.constant.int 0
    %int9223372036854775807_3390 = torch.constant.int 9223372036854775807
    %int1_3391 = torch.constant.int 1
    %2942 = torch.aten.slice.Tensor %2941, %int1_3388, %int0_3389, %int9223372036854775807_3390, %int1_3391 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %2942, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_3392 = torch.constant.int 2
    %int0_3393 = torch.constant.int 0
    %int9223372036854775807_3394 = torch.constant.int 9223372036854775807
    %int1_3395 = torch.constant.int 1
    %2943 = torch.aten.slice.Tensor %2942, %int2_3392, %int0_3393, %int9223372036854775807_3394, %int1_3395 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %2943, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_3396 = torch.constant.int 4
    %int1_3397 = torch.constant.int 1
    %int1_3398 = torch.constant.int 1
    %2944 = torch.prim.ListConstruct %int4_3396, %int1_3397, %int1_3398 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2945 = torch.aten.repeat %2943, %2944 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %2945, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_3399 = torch.constant.int 6
    %2946 = torch.prims.convert_element_type %2893, %int6_3399 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %2946, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %2947 = torch_c.to_builtin_tensor %2946 : !torch.vtensor<[4,?,8,128],f32> -> tensor<4x?x8x128xf32>
    %2948 = torch_c.to_builtin_tensor %2945 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %2949 = util.call @sharktank_rotary_embedding_4_D_8_128_f32(%2947, %2948) : (tensor<4x?x8x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x8x128xf32>
    %2950 = torch_c.from_builtin_tensor %2949 : tensor<4x?x8x128xf32> -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %2950, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %int5_3400 = torch.constant.int 5
    %2951 = torch.prims.convert_element_type %2950, %int5_3400 : !torch.vtensor<[4,?,8,128],f32>, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %2951, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int64_3401 = torch.constant.int 64
    %2952 = torch.aten.mul.Scalar %arg2, %int64_3401 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2952, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int26 = torch.constant.int 26
    %int1_3402 = torch.constant.int 1
    %2953 = torch.aten.add.Scalar %2952, %int26, %int1_3402 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2953, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_3403 = torch.constant.int 4
    %int32_3404 = torch.constant.int 32
    %int8_3405 = torch.constant.int 8
    %int128_3406 = torch.constant.int 128
    %2954 = torch.prim.ListConstruct %int4_3403, %398, %int32_3404, %int8_3405, %int128_3406 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2955 = torch.aten.view %2951, %2954 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2955, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_3407 = torch.constant.int 4
    %2956 = torch.aten.mul.int %int4_3407, %398 : !torch.int, !torch.int -> !torch.int
    %int32_3408 = torch.constant.int 32
    %int8_3409 = torch.constant.int 8
    %int128_3410 = torch.constant.int 128
    %2957 = torch.prim.ListConstruct %2956, %int32_3408, %int8_3409, %int128_3410 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2958 = torch.aten.view %2955, %2957 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %2958, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_3411 = torch.constant.int 4
    %2959 = torch.aten.mul.int %int4_3411, %398 : !torch.int, !torch.int -> !torch.int
    %2960 = torch.prim.ListConstruct %2959 : (!torch.int) -> !torch.list<int>
    %2961 = torch.aten.view %2953, %2960 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2961, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_3412 = torch.constant.int 32
    %int2_3413 = torch.constant.int 2
    %int32_3414 = torch.constant.int 32
    %int8_3415 = torch.constant.int 8
    %int128_3416 = torch.constant.int 128
    %2962 = torch.prim.ListConstruct %389, %int32_3412, %int2_3413, %int32_3414, %int8_3415, %int128_3416 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2963 = torch.aten.view %2795, %2962 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2963, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_3417 = torch.constant.int 32
    %2964 = torch.aten.mul.int %389, %int32_3417 : !torch.int, !torch.int -> !torch.int
    %int2_3418 = torch.constant.int 2
    %2965 = torch.aten.mul.int %2964, %int2_3418 : !torch.int, !torch.int -> !torch.int
    %int32_3419 = torch.constant.int 32
    %int8_3420 = torch.constant.int 8
    %int128_3421 = torch.constant.int 128
    %2966 = torch.prim.ListConstruct %2965, %int32_3419, %int8_3420, %int128_3421 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2967 = torch.aten.view %2963, %2966 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %2967, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %2968 = torch.prim.ListConstruct %2961 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_3422 = torch.constant.bool false
    %2969 = torch.aten.index_put %2967, %2968, %2958, %false_3422 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %2969, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_3423 = torch.constant.int 32
    %int2_3424 = torch.constant.int 2
    %int32_3425 = torch.constant.int 32
    %int8_3426 = torch.constant.int 8
    %int128_3427 = torch.constant.int 128
    %2970 = torch.prim.ListConstruct %389, %int32_3423, %int2_3424, %int32_3425, %int8_3426, %int128_3427 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2971 = torch.aten.view %2969, %2970 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2971, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_3428 = torch.constant.int 2097152
    %2972 = torch.prim.ListConstruct %389, %int2097152_3428 : (!torch.int, !torch.int) -> !torch.list<int>
    %2973 = torch.aten.view %2971, %2972 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %2973, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_3429 = torch.constant.int 32
    %int2_3430 = torch.constant.int 2
    %int32_3431 = torch.constant.int 32
    %int8_3432 = torch.constant.int 8
    %int128_3433 = torch.constant.int 128
    %2974 = torch.prim.ListConstruct %389, %int32_3429, %int2_3430, %int32_3431, %int8_3432, %int128_3433 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2975 = torch.aten.view %2973, %2974 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2975, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_3434 = torch.constant.int 32
    %int8_3435 = torch.constant.int 8
    %int128_3436 = torch.constant.int 128
    %2976 = torch.prim.ListConstruct %2965, %int32_3434, %int8_3435, %int128_3436 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2977 = torch.aten.view %2975, %2976 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %2977, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_3437 = torch.constant.int 4
    %int32_3438 = torch.constant.int 32
    %int8_3439 = torch.constant.int 8
    %int128_3440 = torch.constant.int 128
    %2978 = torch.prim.ListConstruct %int4_3437, %398, %int32_3438, %int8_3439, %int128_3440 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2979 = torch.aten.view %2895, %2978 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2979, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_3441 = torch.constant.int 4
    %2980 = torch.aten.mul.int %int4_3441, %398 : !torch.int, !torch.int -> !torch.int
    %int32_3442 = torch.constant.int 32
    %int8_3443 = torch.constant.int 8
    %int128_3444 = torch.constant.int 128
    %2981 = torch.prim.ListConstruct %2980, %int32_3442, %int8_3443, %int128_3444 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2982 = torch.aten.view %2979, %2981 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %2982, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int1_3445 = torch.constant.int 1
    %int1_3446 = torch.constant.int 1
    %2983 = torch.aten.add.Scalar %2953, %int1_3445, %int1_3446 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2983, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_3447 = torch.constant.int 4
    %2984 = torch.aten.mul.int %int4_3447, %398 : !torch.int, !torch.int -> !torch.int
    %2985 = torch.prim.ListConstruct %2984 : (!torch.int) -> !torch.list<int>
    %2986 = torch.aten.view %2983, %2985 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2986, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %2987 = torch.prim.ListConstruct %2986 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_3448 = torch.constant.bool false
    %2988 = torch.aten.index_put %2977, %2987, %2982, %false_3448 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %2988, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_3449 = torch.constant.int 32
    %int2_3450 = torch.constant.int 2
    %int32_3451 = torch.constant.int 32
    %int8_3452 = torch.constant.int 8
    %int128_3453 = torch.constant.int 128
    %2989 = torch.prim.ListConstruct %389, %int32_3449, %int2_3450, %int32_3451, %int8_3452, %int128_3453 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2990 = torch.aten.view %2988, %2989 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2990, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_3454 = torch.constant.int 2097152
    %2991 = torch.prim.ListConstruct %389, %int2097152_3454 : (!torch.int, !torch.int) -> !torch.list<int>
    %2992 = torch.aten.view %2990, %2991 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %2992, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int-2_3455 = torch.constant.int -2
    %2993 = torch.aten.unsqueeze %2951, %int-2_3455 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %2993, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int4_3456 = torch.constant.int 4
    %int8_3457 = torch.constant.int 8
    %int4_3458 = torch.constant.int 4
    %int128_3459 = torch.constant.int 128
    %2994 = torch.prim.ListConstruct %int4_3456, %2936, %int8_3457, %int4_3458, %int128_3459 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_3460 = torch.constant.bool false
    %2995 = torch.aten.expand %2993, %2994, %false_3460 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2995, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_3461 = torch.constant.int 0
    %2996 = torch.aten.clone %2995, %int0_3461 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2996, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_3462 = torch.constant.int 4
    %int32_3463 = torch.constant.int 32
    %int128_3464 = torch.constant.int 128
    %2997 = torch.prim.ListConstruct %int4_3462, %2936, %int32_3463, %int128_3464 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2998 = torch.aten._unsafe_view %2996, %2997 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2998, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_3465 = torch.constant.int -2
    %2999 = torch.aten.unsqueeze %2895, %int-2_3465 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %2999, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_3466 = torch.constant.int 1
    %3000 = torch.aten.size.int %2889, %int1_3466 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int4_3467 = torch.constant.int 4
    %int8_3468 = torch.constant.int 8
    %int4_3469 = torch.constant.int 4
    %int128_3470 = torch.constant.int 128
    %3001 = torch.prim.ListConstruct %int4_3467, %3000, %int8_3468, %int4_3469, %int128_3470 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_3471 = torch.constant.bool false
    %3002 = torch.aten.expand %2999, %3001, %false_3471 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3002, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_3472 = torch.constant.int 0
    %3003 = torch.aten.clone %3002, %int0_3472 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3003, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_3473 = torch.constant.int 4
    %int32_3474 = torch.constant.int 32
    %int128_3475 = torch.constant.int 128
    %3004 = torch.prim.ListConstruct %int4_3473, %3000, %int32_3474, %int128_3475 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3005 = torch.aten._unsafe_view %3003, %3004 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %3005, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_3476 = torch.constant.int 1
    %int2_3477 = torch.constant.int 2
    %3006 = torch.aten.transpose.int %2923, %int1_3476, %int2_3477 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %3006, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_3478 = torch.constant.int 1
    %int2_3479 = torch.constant.int 2
    %3007 = torch.aten.transpose.int %2998, %int1_3478, %int2_3479 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %3007, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_3480 = torch.constant.int 1
    %int2_3481 = torch.constant.int 2
    %3008 = torch.aten.transpose.int %3005, %int1_3480, %int2_3481 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %3008, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_3482 = torch.constant.float 0.000000e+00
    %true_3483 = torch.constant.bool true
    %none_3484 = torch.constant.none
    %none_3485 = torch.constant.none
    %3009:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3006, %3007, %3008, %float0.000000e00_3482, %true_3483, %none_3484, %none_3485) : (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?],f32>) 
    torch.bind_symbolic_shape %3009#0, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_3486 = torch.constant.int 1
    %int2_3487 = torch.constant.int 2
    %3010 = torch.aten.transpose.int %3009#0, %int1_3486, %int2_3487 : !torch.vtensor<[4,32,?,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %3010, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_3488 = torch.constant.int 4
    %int4096_3489 = torch.constant.int 4096
    %3011 = torch.prim.ListConstruct %int4_3488, %2908, %int4096_3489 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3012 = torch.aten.view %3010, %3011 : !torch.vtensor<[4,?,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3012, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_3490 = torch.constant.int -2
    %int-1_3491 = torch.constant.int -1
    %3013 = torch.aten.transpose.int %122, %int-2_3490, %int-1_3491 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_3492 = torch.constant.int 4
    %3014 = torch.aten.mul.int %int4_3492, %2908 : !torch.int, !torch.int -> !torch.int
    %int4096_3493 = torch.constant.int 4096
    %3015 = torch.prim.ListConstruct %3014, %int4096_3493 : (!torch.int, !torch.int) -> !torch.list<int>
    %3016 = torch.aten.view %3012, %3015 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3016, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %3017 = torch.aten.mm %3016, %3013 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3017, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_3494 = torch.constant.int 4
    %int4096_3495 = torch.constant.int 4096
    %3018 = torch.prim.ListConstruct %int4_3494, %2908, %int4096_3495 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3019 = torch.aten.view %3017, %3018 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3019, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_3496 = torch.constant.int 1
    %3020 = torch.aten.add.Tensor %2858, %3019, %int1_3496 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3020, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_3497 = torch.constant.int 6
    %3021 = torch.prims.convert_element_type %3020, %int6_3497 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3021, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_3498 = torch.constant.int 2
    %3022 = torch.aten.pow.Tensor_Scalar %3021, %int2_3498 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3022, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_3499 = torch.constant.int -1
    %3023 = torch.prim.ListConstruct %int-1_3499 : (!torch.int) -> !torch.list<int>
    %true_3500 = torch.constant.bool true
    %none_3501 = torch.constant.none
    %3024 = torch.aten.mean.dim %3022, %3023, %true_3500, %none_3501 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3024, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_3502 = torch.constant.float 9.9999997473787516E-6
    %int1_3503 = torch.constant.int 1
    %3025 = torch.aten.add.Scalar %3024, %float9.999990e-06_3502, %int1_3503 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3025, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %3026 = torch.aten.rsqrt %3025 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3026, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %3027 = torch.aten.mul.Tensor %3021, %3026 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3027, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_3504 = torch.constant.int 5
    %3028 = torch.prims.convert_element_type %3027, %int5_3504 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3028, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %3029 = torch.aten.mul.Tensor %123, %3028 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3029, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_3505 = torch.constant.int 5
    %3030 = torch.prims.convert_element_type %3029, %int5_3505 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3030, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_3506 = torch.constant.int -2
    %int-1_3507 = torch.constant.int -1
    %3031 = torch.aten.transpose.int %124, %int-2_3506, %int-1_3507 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_3508 = torch.constant.int 4
    %3032 = torch.aten.mul.int %int4_3508, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_3509 = torch.constant.int 4096
    %3033 = torch.prim.ListConstruct %3032, %int4096_3509 : (!torch.int, !torch.int) -> !torch.list<int>
    %3034 = torch.aten.view %3030, %3033 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3034, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %3035 = torch.aten.mm %3034, %3031 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %3035, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_3510 = torch.constant.int 4
    %int14336_3511 = torch.constant.int 14336
    %3036 = torch.prim.ListConstruct %int4_3510, %306, %int14336_3511 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3037 = torch.aten.view %3035, %3036 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %3037, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %3038 = torch.aten.silu %3037 : !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %3038, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_3512 = torch.constant.int -2
    %int-1_3513 = torch.constant.int -1
    %3039 = torch.aten.transpose.int %125, %int-2_3512, %int-1_3513 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_3514 = torch.constant.int 4
    %3040 = torch.aten.mul.int %int4_3514, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_3515 = torch.constant.int 4096
    %3041 = torch.prim.ListConstruct %3040, %int4096_3515 : (!torch.int, !torch.int) -> !torch.list<int>
    %3042 = torch.aten.view %3030, %3041 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3042, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %3043 = torch.aten.mm %3042, %3039 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %3043, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_3516 = torch.constant.int 4
    %int14336_3517 = torch.constant.int 14336
    %3044 = torch.prim.ListConstruct %int4_3516, %306, %int14336_3517 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3045 = torch.aten.view %3043, %3044 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %3045, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %3046 = torch.aten.mul.Tensor %3038, %3045 : !torch.vtensor<[4,?,14336],f16>, !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %3046, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_3518 = torch.constant.int -2
    %int-1_3519 = torch.constant.int -1
    %3047 = torch.aten.transpose.int %126, %int-2_3518, %int-1_3519 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int1_3520 = torch.constant.int 1
    %3048 = torch.aten.size.int %3037, %int1_3520 : !torch.vtensor<[4,?,14336],f16>, !torch.int -> !torch.int
    %int4_3521 = torch.constant.int 4
    %3049 = torch.aten.mul.int %int4_3521, %3048 : !torch.int, !torch.int -> !torch.int
    %int14336_3522 = torch.constant.int 14336
    %3050 = torch.prim.ListConstruct %3049, %int14336_3522 : (!torch.int, !torch.int) -> !torch.list<int>
    %3051 = torch.aten.view %3046, %3050 : !torch.vtensor<[4,?,14336],f16>, !torch.list<int> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %3051, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %3052 = torch.aten.mm %3051, %3047 : !torch.vtensor<[?,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3052, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_3523 = torch.constant.int 4
    %int4096_3524 = torch.constant.int 4096
    %3053 = torch.prim.ListConstruct %int4_3523, %3048, %int4096_3524 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3054 = torch.aten.view %3052, %3053 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3054, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_3525 = torch.constant.int 1
    %3055 = torch.aten.add.Tensor %3020, %3054, %int1_3525 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3055, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_3526 = torch.constant.int 6
    %3056 = torch.prims.convert_element_type %3055, %int6_3526 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3056, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_3527 = torch.constant.int 2
    %3057 = torch.aten.pow.Tensor_Scalar %3056, %int2_3527 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3057, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_3528 = torch.constant.int -1
    %3058 = torch.prim.ListConstruct %int-1_3528 : (!torch.int) -> !torch.list<int>
    %true_3529 = torch.constant.bool true
    %none_3530 = torch.constant.none
    %3059 = torch.aten.mean.dim %3057, %3058, %true_3529, %none_3530 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3059, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_3531 = torch.constant.float 9.9999997473787516E-6
    %int1_3532 = torch.constant.int 1
    %3060 = torch.aten.add.Scalar %3059, %float9.999990e-06_3531, %int1_3532 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3060, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %3061 = torch.aten.rsqrt %3060 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3061, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %3062 = torch.aten.mul.Tensor %3056, %3061 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3062, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_3533 = torch.constant.int 5
    %3063 = torch.prims.convert_element_type %3062, %int5_3533 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3063, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %3064 = torch.aten.mul.Tensor %127, %3063 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3064, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_3534 = torch.constant.int 5
    %3065 = torch.prims.convert_element_type %3064, %int5_3534 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3065, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_3535 = torch.constant.int -2
    %int-1_3536 = torch.constant.int -1
    %3066 = torch.aten.transpose.int %128, %int-2_3535, %int-1_3536 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_3537 = torch.constant.int 4
    %3067 = torch.aten.mul.int %int4_3537, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_3538 = torch.constant.int 4096
    %3068 = torch.prim.ListConstruct %3067, %int4096_3538 : (!torch.int, !torch.int) -> !torch.list<int>
    %3069 = torch.aten.view %3065, %3068 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3069, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %3070 = torch.aten.mm %3069, %3066 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3070, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_3539 = torch.constant.int 4
    %int4096_3540 = torch.constant.int 4096
    %3071 = torch.prim.ListConstruct %int4_3539, %306, %int4096_3540 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3072 = torch.aten.view %3070, %3071 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3072, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_3541 = torch.constant.int -2
    %int-1_3542 = torch.constant.int -1
    %3073 = torch.aten.transpose.int %129, %int-2_3541, %int-1_3542 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_3543 = torch.constant.int 4
    %3074 = torch.aten.mul.int %int4_3543, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_3544 = torch.constant.int 4096
    %3075 = torch.prim.ListConstruct %3074, %int4096_3544 : (!torch.int, !torch.int) -> !torch.list<int>
    %3076 = torch.aten.view %3065, %3075 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3076, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %3077 = torch.aten.mm %3076, %3073 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %3077, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_3545 = torch.constant.int 4
    %int1024_3546 = torch.constant.int 1024
    %3078 = torch.prim.ListConstruct %int4_3545, %306, %int1024_3546 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3079 = torch.aten.view %3077, %3078 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %3079, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int-2_3547 = torch.constant.int -2
    %int-1_3548 = torch.constant.int -1
    %3080 = torch.aten.transpose.int %130, %int-2_3547, %int-1_3548 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_3549 = torch.constant.int 4
    %3081 = torch.aten.mul.int %int4_3549, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_3550 = torch.constant.int 4096
    %3082 = torch.prim.ListConstruct %3081, %int4096_3550 : (!torch.int, !torch.int) -> !torch.list<int>
    %3083 = torch.aten.view %3065, %3082 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3083, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %3084 = torch.aten.mm %3083, %3080 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %3084, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_3551 = torch.constant.int 4
    %int1024_3552 = torch.constant.int 1024
    %3085 = torch.prim.ListConstruct %int4_3551, %306, %int1024_3552 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3086 = torch.aten.view %3084, %3085 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %3086, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int4_3553 = torch.constant.int 4
    %int32_3554 = torch.constant.int 32
    %int128_3555 = torch.constant.int 128
    %3087 = torch.prim.ListConstruct %int4_3553, %306, %int32_3554, %int128_3555 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3088 = torch.aten.view %3072, %3087 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %3088, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_3556 = torch.constant.int 4
    %int8_3557 = torch.constant.int 8
    %int128_3558 = torch.constant.int 128
    %3089 = torch.prim.ListConstruct %int4_3556, %306, %int8_3557, %int128_3558 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3090 = torch.aten.view %3079, %3089 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %3090, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int4_3559 = torch.constant.int 4
    %int8_3560 = torch.constant.int 8
    %int128_3561 = torch.constant.int 128
    %3091 = torch.prim.ListConstruct %int4_3559, %306, %int8_3560, %int128_3561 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3092 = torch.aten.view %3086, %3091 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %3092, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int131072_3562 = torch.constant.int 131072
    %none_3563 = torch.constant.none
    %none_3564 = torch.constant.none
    %cpu_3565 = torch.constant.device "cpu"
    %false_3566 = torch.constant.bool false
    %3093 = torch.aten.arange %int131072_3562, %none_3563, %none_3564, %cpu_3565, %false_3566 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_3567 = torch.constant.int 0
    %int128_3568 = torch.constant.int 128
    %none_3569 = torch.constant.none
    %none_3570 = torch.constant.none
    %cpu_3571 = torch.constant.device "cpu"
    %false_3572 = torch.constant.bool false
    %3094 = torch.aten.arange.start %int0_3567, %int128_3568, %none_3569, %none_3570, %cpu_3571, %false_3572 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_3573 = torch.constant.int 2
    %3095 = torch.aten.floor_divide.Scalar %3094, %int2_3573 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_3574 = torch.constant.int 6
    %3096 = torch.prims.convert_element_type %3095, %int6_3574 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_3575 = torch.constant.int 128
    %3097 = torch.aten.div.Scalar %3096, %int128_3575 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_3576 = torch.constant.float 2.000000e+00
    %3098 = torch.aten.mul.Scalar %3097, %float2.000000e00_3576 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_3577 = torch.constant.float 5.000000e+05
    %3099 = torch.aten.pow.Scalar %float5.000000e05_3577, %3098 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %3100 = torch.aten.reciprocal %3099 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_3578 = torch.constant.float 1.000000e+00
    %3101 = torch.aten.mul.Scalar %3100, %float1.000000e00_3578 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_3579 = torch.constant.int 1
    %3102 = torch.aten.unsqueeze %3093, %int1_3579 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_3580 = torch.constant.int 0
    %3103 = torch.aten.unsqueeze %3101, %int0_3580 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %3104 = torch.aten.mul.Tensor %3102, %3103 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_3581 = torch.constant.int 1
    %3105 = torch.aten.size.int %3072, %int1_3581 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.int
    %int0_3582 = torch.constant.int 0
    %3106 = torch.aten.add.int %int0_3582, %3105 : !torch.int, !torch.int -> !torch.int
    %int0_3583 = torch.constant.int 0
    %int0_3584 = torch.constant.int 0
    %int1_3585 = torch.constant.int 1
    %3107 = torch.aten.slice.Tensor %3104, %int0_3583, %int0_3584, %3106, %int1_3585 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %3107, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_3586 = torch.constant.int 1
    %int0_3587 = torch.constant.int 0
    %int9223372036854775807_3588 = torch.constant.int 9223372036854775807
    %int1_3589 = torch.constant.int 1
    %3108 = torch.aten.slice.Tensor %3107, %int1_3586, %int0_3587, %int9223372036854775807_3588, %int1_3589 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %3108, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_3590 = torch.constant.int 1
    %int0_3591 = torch.constant.int 0
    %int9223372036854775807_3592 = torch.constant.int 9223372036854775807
    %int1_3593 = torch.constant.int 1
    %3109 = torch.aten.slice.Tensor %3108, %int1_3590, %int0_3591, %int9223372036854775807_3592, %int1_3593 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %3109, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_3594 = torch.constant.int 0
    %3110 = torch.aten.unsqueeze %3109, %int0_3594 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %3110, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_3595 = torch.constant.int 1
    %int0_3596 = torch.constant.int 0
    %int9223372036854775807_3597 = torch.constant.int 9223372036854775807
    %int1_3598 = torch.constant.int 1
    %3111 = torch.aten.slice.Tensor %3110, %int1_3595, %int0_3596, %int9223372036854775807_3597, %int1_3598 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %3111, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_3599 = torch.constant.int 2
    %int0_3600 = torch.constant.int 0
    %int9223372036854775807_3601 = torch.constant.int 9223372036854775807
    %int1_3602 = torch.constant.int 1
    %3112 = torch.aten.slice.Tensor %3111, %int2_3599, %int0_3600, %int9223372036854775807_3601, %int1_3602 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %3112, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_3603 = torch.constant.int 4
    %int1_3604 = torch.constant.int 1
    %int1_3605 = torch.constant.int 1
    %3113 = torch.prim.ListConstruct %int4_3603, %int1_3604, %int1_3605 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3114 = torch.aten.repeat %3112, %3113 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %3114, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_3606 = torch.constant.int 6
    %3115 = torch.prims.convert_element_type %3088, %int6_3606 : !torch.vtensor<[4,?,32,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %3115, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %3116 = torch_c.to_builtin_tensor %3115 : !torch.vtensor<[4,?,32,128],f32> -> tensor<4x?x32x128xf32>
    %3117 = torch_c.to_builtin_tensor %3114 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %3118 = util.call @sharktank_rotary_embedding_4_D_32_128_f32(%3116, %3117) : (tensor<4x?x32x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x32x128xf32>
    %3119 = torch_c.from_builtin_tensor %3118 : tensor<4x?x32x128xf32> -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %3119, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %int5_3607 = torch.constant.int 5
    %3120 = torch.prims.convert_element_type %3119, %int5_3607 : !torch.vtensor<[4,?,32,128],f32>, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %3120, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int131072_3608 = torch.constant.int 131072
    %none_3609 = torch.constant.none
    %none_3610 = torch.constant.none
    %cpu_3611 = torch.constant.device "cpu"
    %false_3612 = torch.constant.bool false
    %3121 = torch.aten.arange %int131072_3608, %none_3609, %none_3610, %cpu_3611, %false_3612 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_3613 = torch.constant.int 0
    %int128_3614 = torch.constant.int 128
    %none_3615 = torch.constant.none
    %none_3616 = torch.constant.none
    %cpu_3617 = torch.constant.device "cpu"
    %false_3618 = torch.constant.bool false
    %3122 = torch.aten.arange.start %int0_3613, %int128_3614, %none_3615, %none_3616, %cpu_3617, %false_3618 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_3619 = torch.constant.int 2
    %3123 = torch.aten.floor_divide.Scalar %3122, %int2_3619 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_3620 = torch.constant.int 6
    %3124 = torch.prims.convert_element_type %3123, %int6_3620 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_3621 = torch.constant.int 128
    %3125 = torch.aten.div.Scalar %3124, %int128_3621 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_3622 = torch.constant.float 2.000000e+00
    %3126 = torch.aten.mul.Scalar %3125, %float2.000000e00_3622 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_3623 = torch.constant.float 5.000000e+05
    %3127 = torch.aten.pow.Scalar %float5.000000e05_3623, %3126 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %3128 = torch.aten.reciprocal %3127 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_3624 = torch.constant.float 1.000000e+00
    %3129 = torch.aten.mul.Scalar %3128, %float1.000000e00_3624 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_3625 = torch.constant.int 1
    %3130 = torch.aten.unsqueeze %3121, %int1_3625 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_3626 = torch.constant.int 0
    %3131 = torch.aten.unsqueeze %3129, %int0_3626 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %3132 = torch.aten.mul.Tensor %3130, %3131 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_3627 = torch.constant.int 1
    %3133 = torch.aten.size.int %3079, %int1_3627 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int0_3628 = torch.constant.int 0
    %3134 = torch.aten.add.int %int0_3628, %3133 : !torch.int, !torch.int -> !torch.int
    %int0_3629 = torch.constant.int 0
    %int0_3630 = torch.constant.int 0
    %int1_3631 = torch.constant.int 1
    %3135 = torch.aten.slice.Tensor %3132, %int0_3629, %int0_3630, %3134, %int1_3631 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %3135, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_3632 = torch.constant.int 1
    %int0_3633 = torch.constant.int 0
    %int9223372036854775807_3634 = torch.constant.int 9223372036854775807
    %int1_3635 = torch.constant.int 1
    %3136 = torch.aten.slice.Tensor %3135, %int1_3632, %int0_3633, %int9223372036854775807_3634, %int1_3635 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %3136, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_3636 = torch.constant.int 1
    %int0_3637 = torch.constant.int 0
    %int9223372036854775807_3638 = torch.constant.int 9223372036854775807
    %int1_3639 = torch.constant.int 1
    %3137 = torch.aten.slice.Tensor %3136, %int1_3636, %int0_3637, %int9223372036854775807_3638, %int1_3639 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %3137, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_3640 = torch.constant.int 0
    %3138 = torch.aten.unsqueeze %3137, %int0_3640 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %3138, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_3641 = torch.constant.int 1
    %int0_3642 = torch.constant.int 0
    %int9223372036854775807_3643 = torch.constant.int 9223372036854775807
    %int1_3644 = torch.constant.int 1
    %3139 = torch.aten.slice.Tensor %3138, %int1_3641, %int0_3642, %int9223372036854775807_3643, %int1_3644 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %3139, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_3645 = torch.constant.int 2
    %int0_3646 = torch.constant.int 0
    %int9223372036854775807_3647 = torch.constant.int 9223372036854775807
    %int1_3648 = torch.constant.int 1
    %3140 = torch.aten.slice.Tensor %3139, %int2_3645, %int0_3646, %int9223372036854775807_3647, %int1_3648 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %3140, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_3649 = torch.constant.int 4
    %int1_3650 = torch.constant.int 1
    %int1_3651 = torch.constant.int 1
    %3141 = torch.prim.ListConstruct %int4_3649, %int1_3650, %int1_3651 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3142 = torch.aten.repeat %3140, %3141 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %3142, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_3652 = torch.constant.int 6
    %3143 = torch.prims.convert_element_type %3090, %int6_3652 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %3143, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %3144 = torch_c.to_builtin_tensor %3143 : !torch.vtensor<[4,?,8,128],f32> -> tensor<4x?x8x128xf32>
    %3145 = torch_c.to_builtin_tensor %3142 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %3146 = util.call @sharktank_rotary_embedding_4_D_8_128_f32(%3144, %3145) : (tensor<4x?x8x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x8x128xf32>
    %3147 = torch_c.from_builtin_tensor %3146 : tensor<4x?x8x128xf32> -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %3147, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %int5_3653 = torch.constant.int 5
    %3148 = torch.prims.convert_element_type %3147, %int5_3653 : !torch.vtensor<[4,?,8,128],f32>, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %3148, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int64_3654 = torch.constant.int 64
    %3149 = torch.aten.mul.Scalar %arg2, %int64_3654 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3149, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int28 = torch.constant.int 28
    %int1_3655 = torch.constant.int 1
    %3150 = torch.aten.add.Scalar %3149, %int28, %int1_3655 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3150, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_3656 = torch.constant.int 4
    %int32_3657 = torch.constant.int 32
    %int8_3658 = torch.constant.int 8
    %int128_3659 = torch.constant.int 128
    %3151 = torch.prim.ListConstruct %int4_3656, %398, %int32_3657, %int8_3658, %int128_3659 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3152 = torch.aten.view %3148, %3151 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3152, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_3660 = torch.constant.int 4
    %3153 = torch.aten.mul.int %int4_3660, %398 : !torch.int, !torch.int -> !torch.int
    %int32_3661 = torch.constant.int 32
    %int8_3662 = torch.constant.int 8
    %int128_3663 = torch.constant.int 128
    %3154 = torch.prim.ListConstruct %3153, %int32_3661, %int8_3662, %int128_3663 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3155 = torch.aten.view %3152, %3154 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %3155, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_3664 = torch.constant.int 4
    %3156 = torch.aten.mul.int %int4_3664, %398 : !torch.int, !torch.int -> !torch.int
    %3157 = torch.prim.ListConstruct %3156 : (!torch.int) -> !torch.list<int>
    %3158 = torch.aten.view %3150, %3157 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3158, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_3665 = torch.constant.int 32
    %int2_3666 = torch.constant.int 2
    %int32_3667 = torch.constant.int 32
    %int8_3668 = torch.constant.int 8
    %int128_3669 = torch.constant.int 128
    %3159 = torch.prim.ListConstruct %389, %int32_3665, %int2_3666, %int32_3667, %int8_3668, %int128_3669 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3160 = torch.aten.view %2992, %3159 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3160, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_3670 = torch.constant.int 32
    %3161 = torch.aten.mul.int %389, %int32_3670 : !torch.int, !torch.int -> !torch.int
    %int2_3671 = torch.constant.int 2
    %3162 = torch.aten.mul.int %3161, %int2_3671 : !torch.int, !torch.int -> !torch.int
    %int32_3672 = torch.constant.int 32
    %int8_3673 = torch.constant.int 8
    %int128_3674 = torch.constant.int 128
    %3163 = torch.prim.ListConstruct %3162, %int32_3672, %int8_3673, %int128_3674 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3164 = torch.aten.view %3160, %3163 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %3164, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %3165 = torch.prim.ListConstruct %3158 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_3675 = torch.constant.bool false
    %3166 = torch.aten.index_put %3164, %3165, %3155, %false_3675 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %3166, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_3676 = torch.constant.int 32
    %int2_3677 = torch.constant.int 2
    %int32_3678 = torch.constant.int 32
    %int8_3679 = torch.constant.int 8
    %int128_3680 = torch.constant.int 128
    %3167 = torch.prim.ListConstruct %389, %int32_3676, %int2_3677, %int32_3678, %int8_3679, %int128_3680 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3168 = torch.aten.view %3166, %3167 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3168, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_3681 = torch.constant.int 2097152
    %3169 = torch.prim.ListConstruct %389, %int2097152_3681 : (!torch.int, !torch.int) -> !torch.list<int>
    %3170 = torch.aten.view %3168, %3169 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %3170, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_3682 = torch.constant.int 32
    %int2_3683 = torch.constant.int 2
    %int32_3684 = torch.constant.int 32
    %int8_3685 = torch.constant.int 8
    %int128_3686 = torch.constant.int 128
    %3171 = torch.prim.ListConstruct %389, %int32_3682, %int2_3683, %int32_3684, %int8_3685, %int128_3686 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3172 = torch.aten.view %3170, %3171 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3172, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_3687 = torch.constant.int 32
    %int8_3688 = torch.constant.int 8
    %int128_3689 = torch.constant.int 128
    %3173 = torch.prim.ListConstruct %3162, %int32_3687, %int8_3688, %int128_3689 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3174 = torch.aten.view %3172, %3173 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %3174, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_3690 = torch.constant.int 4
    %int32_3691 = torch.constant.int 32
    %int8_3692 = torch.constant.int 8
    %int128_3693 = torch.constant.int 128
    %3175 = torch.prim.ListConstruct %int4_3690, %398, %int32_3691, %int8_3692, %int128_3693 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3176 = torch.aten.view %3092, %3175 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3176, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_3694 = torch.constant.int 4
    %3177 = torch.aten.mul.int %int4_3694, %398 : !torch.int, !torch.int -> !torch.int
    %int32_3695 = torch.constant.int 32
    %int8_3696 = torch.constant.int 8
    %int128_3697 = torch.constant.int 128
    %3178 = torch.prim.ListConstruct %3177, %int32_3695, %int8_3696, %int128_3697 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3179 = torch.aten.view %3176, %3178 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %3179, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int1_3698 = torch.constant.int 1
    %int1_3699 = torch.constant.int 1
    %3180 = torch.aten.add.Scalar %3150, %int1_3698, %int1_3699 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3180, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_3700 = torch.constant.int 4
    %3181 = torch.aten.mul.int %int4_3700, %398 : !torch.int, !torch.int -> !torch.int
    %3182 = torch.prim.ListConstruct %3181 : (!torch.int) -> !torch.list<int>
    %3183 = torch.aten.view %3180, %3182 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3183, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %3184 = torch.prim.ListConstruct %3183 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_3701 = torch.constant.bool false
    %3185 = torch.aten.index_put %3174, %3184, %3179, %false_3701 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %3185, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_3702 = torch.constant.int 32
    %int2_3703 = torch.constant.int 2
    %int32_3704 = torch.constant.int 32
    %int8_3705 = torch.constant.int 8
    %int128_3706 = torch.constant.int 128
    %3186 = torch.prim.ListConstruct %389, %int32_3702, %int2_3703, %int32_3704, %int8_3705, %int128_3706 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3187 = torch.aten.view %3185, %3186 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3187, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_3707 = torch.constant.int 2097152
    %3188 = torch.prim.ListConstruct %389, %int2097152_3707 : (!torch.int, !torch.int) -> !torch.list<int>
    %3189 = torch.aten.view %3187, %3188 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %3189, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int-2_3708 = torch.constant.int -2
    %3190 = torch.aten.unsqueeze %3148, %int-2_3708 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %3190, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int4_3709 = torch.constant.int 4
    %int8_3710 = torch.constant.int 8
    %int4_3711 = torch.constant.int 4
    %int128_3712 = torch.constant.int 128
    %3191 = torch.prim.ListConstruct %int4_3709, %3133, %int8_3710, %int4_3711, %int128_3712 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_3713 = torch.constant.bool false
    %3192 = torch.aten.expand %3190, %3191, %false_3713 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3192, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_3714 = torch.constant.int 0
    %3193 = torch.aten.clone %3192, %int0_3714 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3193, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_3715 = torch.constant.int 4
    %int32_3716 = torch.constant.int 32
    %int128_3717 = torch.constant.int 128
    %3194 = torch.prim.ListConstruct %int4_3715, %3133, %int32_3716, %int128_3717 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3195 = torch.aten._unsafe_view %3193, %3194 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %3195, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_3718 = torch.constant.int -2
    %3196 = torch.aten.unsqueeze %3092, %int-2_3718 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %3196, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_3719 = torch.constant.int 1
    %3197 = torch.aten.size.int %3086, %int1_3719 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int4_3720 = torch.constant.int 4
    %int8_3721 = torch.constant.int 8
    %int4_3722 = torch.constant.int 4
    %int128_3723 = torch.constant.int 128
    %3198 = torch.prim.ListConstruct %int4_3720, %3197, %int8_3721, %int4_3722, %int128_3723 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_3724 = torch.constant.bool false
    %3199 = torch.aten.expand %3196, %3198, %false_3724 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3199, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_3725 = torch.constant.int 0
    %3200 = torch.aten.clone %3199, %int0_3725 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3200, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_3726 = torch.constant.int 4
    %int32_3727 = torch.constant.int 32
    %int128_3728 = torch.constant.int 128
    %3201 = torch.prim.ListConstruct %int4_3726, %3197, %int32_3727, %int128_3728 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3202 = torch.aten._unsafe_view %3200, %3201 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %3202, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_3729 = torch.constant.int 1
    %int2_3730 = torch.constant.int 2
    %3203 = torch.aten.transpose.int %3120, %int1_3729, %int2_3730 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %3203, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_3731 = torch.constant.int 1
    %int2_3732 = torch.constant.int 2
    %3204 = torch.aten.transpose.int %3195, %int1_3731, %int2_3732 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %3204, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_3733 = torch.constant.int 1
    %int2_3734 = torch.constant.int 2
    %3205 = torch.aten.transpose.int %3202, %int1_3733, %int2_3734 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %3205, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_3735 = torch.constant.float 0.000000e+00
    %true_3736 = torch.constant.bool true
    %none_3737 = torch.constant.none
    %none_3738 = torch.constant.none
    %3206:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3203, %3204, %3205, %float0.000000e00_3735, %true_3736, %none_3737, %none_3738) : (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?],f32>) 
    torch.bind_symbolic_shape %3206#0, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_3739 = torch.constant.int 1
    %int2_3740 = torch.constant.int 2
    %3207 = torch.aten.transpose.int %3206#0, %int1_3739, %int2_3740 : !torch.vtensor<[4,32,?,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %3207, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_3741 = torch.constant.int 4
    %int4096_3742 = torch.constant.int 4096
    %3208 = torch.prim.ListConstruct %int4_3741, %3105, %int4096_3742 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3209 = torch.aten.view %3207, %3208 : !torch.vtensor<[4,?,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3209, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_3743 = torch.constant.int -2
    %int-1_3744 = torch.constant.int -1
    %3210 = torch.aten.transpose.int %131, %int-2_3743, %int-1_3744 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_3745 = torch.constant.int 4
    %3211 = torch.aten.mul.int %int4_3745, %3105 : !torch.int, !torch.int -> !torch.int
    %int4096_3746 = torch.constant.int 4096
    %3212 = torch.prim.ListConstruct %3211, %int4096_3746 : (!torch.int, !torch.int) -> !torch.list<int>
    %3213 = torch.aten.view %3209, %3212 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3213, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %3214 = torch.aten.mm %3213, %3210 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3214, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_3747 = torch.constant.int 4
    %int4096_3748 = torch.constant.int 4096
    %3215 = torch.prim.ListConstruct %int4_3747, %3105, %int4096_3748 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3216 = torch.aten.view %3214, %3215 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3216, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_3749 = torch.constant.int 1
    %3217 = torch.aten.add.Tensor %3055, %3216, %int1_3749 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3217, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_3750 = torch.constant.int 6
    %3218 = torch.prims.convert_element_type %3217, %int6_3750 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3218, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_3751 = torch.constant.int 2
    %3219 = torch.aten.pow.Tensor_Scalar %3218, %int2_3751 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3219, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_3752 = torch.constant.int -1
    %3220 = torch.prim.ListConstruct %int-1_3752 : (!torch.int) -> !torch.list<int>
    %true_3753 = torch.constant.bool true
    %none_3754 = torch.constant.none
    %3221 = torch.aten.mean.dim %3219, %3220, %true_3753, %none_3754 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3221, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_3755 = torch.constant.float 9.9999997473787516E-6
    %int1_3756 = torch.constant.int 1
    %3222 = torch.aten.add.Scalar %3221, %float9.999990e-06_3755, %int1_3756 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3222, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %3223 = torch.aten.rsqrt %3222 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3223, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %3224 = torch.aten.mul.Tensor %3218, %3223 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3224, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_3757 = torch.constant.int 5
    %3225 = torch.prims.convert_element_type %3224, %int5_3757 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3225, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %3226 = torch.aten.mul.Tensor %132, %3225 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3226, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_3758 = torch.constant.int 5
    %3227 = torch.prims.convert_element_type %3226, %int5_3758 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3227, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_3759 = torch.constant.int -2
    %int-1_3760 = torch.constant.int -1
    %3228 = torch.aten.transpose.int %133, %int-2_3759, %int-1_3760 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_3761 = torch.constant.int 4
    %3229 = torch.aten.mul.int %int4_3761, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_3762 = torch.constant.int 4096
    %3230 = torch.prim.ListConstruct %3229, %int4096_3762 : (!torch.int, !torch.int) -> !torch.list<int>
    %3231 = torch.aten.view %3227, %3230 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3231, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %3232 = torch.aten.mm %3231, %3228 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %3232, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_3763 = torch.constant.int 4
    %int14336_3764 = torch.constant.int 14336
    %3233 = torch.prim.ListConstruct %int4_3763, %306, %int14336_3764 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3234 = torch.aten.view %3232, %3233 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %3234, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %3235 = torch.aten.silu %3234 : !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %3235, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_3765 = torch.constant.int -2
    %int-1_3766 = torch.constant.int -1
    %3236 = torch.aten.transpose.int %134, %int-2_3765, %int-1_3766 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_3767 = torch.constant.int 4
    %3237 = torch.aten.mul.int %int4_3767, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_3768 = torch.constant.int 4096
    %3238 = torch.prim.ListConstruct %3237, %int4096_3768 : (!torch.int, !torch.int) -> !torch.list<int>
    %3239 = torch.aten.view %3227, %3238 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3239, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %3240 = torch.aten.mm %3239, %3236 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %3240, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_3769 = torch.constant.int 4
    %int14336_3770 = torch.constant.int 14336
    %3241 = torch.prim.ListConstruct %int4_3769, %306, %int14336_3770 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3242 = torch.aten.view %3240, %3241 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %3242, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %3243 = torch.aten.mul.Tensor %3235, %3242 : !torch.vtensor<[4,?,14336],f16>, !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %3243, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_3771 = torch.constant.int -2
    %int-1_3772 = torch.constant.int -1
    %3244 = torch.aten.transpose.int %135, %int-2_3771, %int-1_3772 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int1_3773 = torch.constant.int 1
    %3245 = torch.aten.size.int %3234, %int1_3773 : !torch.vtensor<[4,?,14336],f16>, !torch.int -> !torch.int
    %int4_3774 = torch.constant.int 4
    %3246 = torch.aten.mul.int %int4_3774, %3245 : !torch.int, !torch.int -> !torch.int
    %int14336_3775 = torch.constant.int 14336
    %3247 = torch.prim.ListConstruct %3246, %int14336_3775 : (!torch.int, !torch.int) -> !torch.list<int>
    %3248 = torch.aten.view %3243, %3247 : !torch.vtensor<[4,?,14336],f16>, !torch.list<int> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %3248, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %3249 = torch.aten.mm %3248, %3244 : !torch.vtensor<[?,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3249, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_3776 = torch.constant.int 4
    %int4096_3777 = torch.constant.int 4096
    %3250 = torch.prim.ListConstruct %int4_3776, %3245, %int4096_3777 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3251 = torch.aten.view %3249, %3250 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3251, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_3778 = torch.constant.int 1
    %3252 = torch.aten.add.Tensor %3217, %3251, %int1_3778 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3252, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_3779 = torch.constant.int 6
    %3253 = torch.prims.convert_element_type %3252, %int6_3779 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3253, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_3780 = torch.constant.int 2
    %3254 = torch.aten.pow.Tensor_Scalar %3253, %int2_3780 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3254, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_3781 = torch.constant.int -1
    %3255 = torch.prim.ListConstruct %int-1_3781 : (!torch.int) -> !torch.list<int>
    %true_3782 = torch.constant.bool true
    %none_3783 = torch.constant.none
    %3256 = torch.aten.mean.dim %3254, %3255, %true_3782, %none_3783 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3256, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_3784 = torch.constant.float 9.9999997473787516E-6
    %int1_3785 = torch.constant.int 1
    %3257 = torch.aten.add.Scalar %3256, %float9.999990e-06_3784, %int1_3785 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3257, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %3258 = torch.aten.rsqrt %3257 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3258, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %3259 = torch.aten.mul.Tensor %3253, %3258 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3259, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_3786 = torch.constant.int 5
    %3260 = torch.prims.convert_element_type %3259, %int5_3786 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3260, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %3261 = torch.aten.mul.Tensor %136, %3260 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3261, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_3787 = torch.constant.int 5
    %3262 = torch.prims.convert_element_type %3261, %int5_3787 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3262, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_3788 = torch.constant.int -2
    %int-1_3789 = torch.constant.int -1
    %3263 = torch.aten.transpose.int %137, %int-2_3788, %int-1_3789 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_3790 = torch.constant.int 4
    %3264 = torch.aten.mul.int %int4_3790, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_3791 = torch.constant.int 4096
    %3265 = torch.prim.ListConstruct %3264, %int4096_3791 : (!torch.int, !torch.int) -> !torch.list<int>
    %3266 = torch.aten.view %3262, %3265 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3266, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %3267 = torch.aten.mm %3266, %3263 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3267, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_3792 = torch.constant.int 4
    %int4096_3793 = torch.constant.int 4096
    %3268 = torch.prim.ListConstruct %int4_3792, %306, %int4096_3793 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3269 = torch.aten.view %3267, %3268 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3269, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_3794 = torch.constant.int -2
    %int-1_3795 = torch.constant.int -1
    %3270 = torch.aten.transpose.int %138, %int-2_3794, %int-1_3795 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_3796 = torch.constant.int 4
    %3271 = torch.aten.mul.int %int4_3796, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_3797 = torch.constant.int 4096
    %3272 = torch.prim.ListConstruct %3271, %int4096_3797 : (!torch.int, !torch.int) -> !torch.list<int>
    %3273 = torch.aten.view %3262, %3272 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3273, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %3274 = torch.aten.mm %3273, %3270 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %3274, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_3798 = torch.constant.int 4
    %int1024_3799 = torch.constant.int 1024
    %3275 = torch.prim.ListConstruct %int4_3798, %306, %int1024_3799 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3276 = torch.aten.view %3274, %3275 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %3276, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int-2_3800 = torch.constant.int -2
    %int-1_3801 = torch.constant.int -1
    %3277 = torch.aten.transpose.int %139, %int-2_3800, %int-1_3801 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_3802 = torch.constant.int 4
    %3278 = torch.aten.mul.int %int4_3802, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_3803 = torch.constant.int 4096
    %3279 = torch.prim.ListConstruct %3278, %int4096_3803 : (!torch.int, !torch.int) -> !torch.list<int>
    %3280 = torch.aten.view %3262, %3279 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3280, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %3281 = torch.aten.mm %3280, %3277 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %3281, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_3804 = torch.constant.int 4
    %int1024_3805 = torch.constant.int 1024
    %3282 = torch.prim.ListConstruct %int4_3804, %306, %int1024_3805 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3283 = torch.aten.view %3281, %3282 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %3283, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int4_3806 = torch.constant.int 4
    %int32_3807 = torch.constant.int 32
    %int128_3808 = torch.constant.int 128
    %3284 = torch.prim.ListConstruct %int4_3806, %306, %int32_3807, %int128_3808 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3285 = torch.aten.view %3269, %3284 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %3285, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_3809 = torch.constant.int 4
    %int8_3810 = torch.constant.int 8
    %int128_3811 = torch.constant.int 128
    %3286 = torch.prim.ListConstruct %int4_3809, %306, %int8_3810, %int128_3811 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3287 = torch.aten.view %3276, %3286 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %3287, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int4_3812 = torch.constant.int 4
    %int8_3813 = torch.constant.int 8
    %int128_3814 = torch.constant.int 128
    %3288 = torch.prim.ListConstruct %int4_3812, %306, %int8_3813, %int128_3814 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3289 = torch.aten.view %3283, %3288 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %3289, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int131072_3815 = torch.constant.int 131072
    %none_3816 = torch.constant.none
    %none_3817 = torch.constant.none
    %cpu_3818 = torch.constant.device "cpu"
    %false_3819 = torch.constant.bool false
    %3290 = torch.aten.arange %int131072_3815, %none_3816, %none_3817, %cpu_3818, %false_3819 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_3820 = torch.constant.int 0
    %int128_3821 = torch.constant.int 128
    %none_3822 = torch.constant.none
    %none_3823 = torch.constant.none
    %cpu_3824 = torch.constant.device "cpu"
    %false_3825 = torch.constant.bool false
    %3291 = torch.aten.arange.start %int0_3820, %int128_3821, %none_3822, %none_3823, %cpu_3824, %false_3825 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_3826 = torch.constant.int 2
    %3292 = torch.aten.floor_divide.Scalar %3291, %int2_3826 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_3827 = torch.constant.int 6
    %3293 = torch.prims.convert_element_type %3292, %int6_3827 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_3828 = torch.constant.int 128
    %3294 = torch.aten.div.Scalar %3293, %int128_3828 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_3829 = torch.constant.float 2.000000e+00
    %3295 = torch.aten.mul.Scalar %3294, %float2.000000e00_3829 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_3830 = torch.constant.float 5.000000e+05
    %3296 = torch.aten.pow.Scalar %float5.000000e05_3830, %3295 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %3297 = torch.aten.reciprocal %3296 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_3831 = torch.constant.float 1.000000e+00
    %3298 = torch.aten.mul.Scalar %3297, %float1.000000e00_3831 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_3832 = torch.constant.int 1
    %3299 = torch.aten.unsqueeze %3290, %int1_3832 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_3833 = torch.constant.int 0
    %3300 = torch.aten.unsqueeze %3298, %int0_3833 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %3301 = torch.aten.mul.Tensor %3299, %3300 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_3834 = torch.constant.int 1
    %3302 = torch.aten.size.int %3269, %int1_3834 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.int
    %int0_3835 = torch.constant.int 0
    %3303 = torch.aten.add.int %int0_3835, %3302 : !torch.int, !torch.int -> !torch.int
    %int0_3836 = torch.constant.int 0
    %int0_3837 = torch.constant.int 0
    %int1_3838 = torch.constant.int 1
    %3304 = torch.aten.slice.Tensor %3301, %int0_3836, %int0_3837, %3303, %int1_3838 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %3304, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_3839 = torch.constant.int 1
    %int0_3840 = torch.constant.int 0
    %int9223372036854775807_3841 = torch.constant.int 9223372036854775807
    %int1_3842 = torch.constant.int 1
    %3305 = torch.aten.slice.Tensor %3304, %int1_3839, %int0_3840, %int9223372036854775807_3841, %int1_3842 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %3305, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_3843 = torch.constant.int 1
    %int0_3844 = torch.constant.int 0
    %int9223372036854775807_3845 = torch.constant.int 9223372036854775807
    %int1_3846 = torch.constant.int 1
    %3306 = torch.aten.slice.Tensor %3305, %int1_3843, %int0_3844, %int9223372036854775807_3845, %int1_3846 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %3306, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_3847 = torch.constant.int 0
    %3307 = torch.aten.unsqueeze %3306, %int0_3847 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %3307, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_3848 = torch.constant.int 1
    %int0_3849 = torch.constant.int 0
    %int9223372036854775807_3850 = torch.constant.int 9223372036854775807
    %int1_3851 = torch.constant.int 1
    %3308 = torch.aten.slice.Tensor %3307, %int1_3848, %int0_3849, %int9223372036854775807_3850, %int1_3851 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %3308, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_3852 = torch.constant.int 2
    %int0_3853 = torch.constant.int 0
    %int9223372036854775807_3854 = torch.constant.int 9223372036854775807
    %int1_3855 = torch.constant.int 1
    %3309 = torch.aten.slice.Tensor %3308, %int2_3852, %int0_3853, %int9223372036854775807_3854, %int1_3855 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %3309, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_3856 = torch.constant.int 4
    %int1_3857 = torch.constant.int 1
    %int1_3858 = torch.constant.int 1
    %3310 = torch.prim.ListConstruct %int4_3856, %int1_3857, %int1_3858 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3311 = torch.aten.repeat %3309, %3310 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %3311, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_3859 = torch.constant.int 6
    %3312 = torch.prims.convert_element_type %3285, %int6_3859 : !torch.vtensor<[4,?,32,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %3312, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %3313 = torch_c.to_builtin_tensor %3312 : !torch.vtensor<[4,?,32,128],f32> -> tensor<4x?x32x128xf32>
    %3314 = torch_c.to_builtin_tensor %3311 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %3315 = util.call @sharktank_rotary_embedding_4_D_32_128_f32(%3313, %3314) : (tensor<4x?x32x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x32x128xf32>
    %3316 = torch_c.from_builtin_tensor %3315 : tensor<4x?x32x128xf32> -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %3316, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %int5_3860 = torch.constant.int 5
    %3317 = torch.prims.convert_element_type %3316, %int5_3860 : !torch.vtensor<[4,?,32,128],f32>, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %3317, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int131072_3861 = torch.constant.int 131072
    %none_3862 = torch.constant.none
    %none_3863 = torch.constant.none
    %cpu_3864 = torch.constant.device "cpu"
    %false_3865 = torch.constant.bool false
    %3318 = torch.aten.arange %int131072_3861, %none_3862, %none_3863, %cpu_3864, %false_3865 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_3866 = torch.constant.int 0
    %int128_3867 = torch.constant.int 128
    %none_3868 = torch.constant.none
    %none_3869 = torch.constant.none
    %cpu_3870 = torch.constant.device "cpu"
    %false_3871 = torch.constant.bool false
    %3319 = torch.aten.arange.start %int0_3866, %int128_3867, %none_3868, %none_3869, %cpu_3870, %false_3871 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_3872 = torch.constant.int 2
    %3320 = torch.aten.floor_divide.Scalar %3319, %int2_3872 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_3873 = torch.constant.int 6
    %3321 = torch.prims.convert_element_type %3320, %int6_3873 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_3874 = torch.constant.int 128
    %3322 = torch.aten.div.Scalar %3321, %int128_3874 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_3875 = torch.constant.float 2.000000e+00
    %3323 = torch.aten.mul.Scalar %3322, %float2.000000e00_3875 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_3876 = torch.constant.float 5.000000e+05
    %3324 = torch.aten.pow.Scalar %float5.000000e05_3876, %3323 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %3325 = torch.aten.reciprocal %3324 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_3877 = torch.constant.float 1.000000e+00
    %3326 = torch.aten.mul.Scalar %3325, %float1.000000e00_3877 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_3878 = torch.constant.int 1
    %3327 = torch.aten.unsqueeze %3318, %int1_3878 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_3879 = torch.constant.int 0
    %3328 = torch.aten.unsqueeze %3326, %int0_3879 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %3329 = torch.aten.mul.Tensor %3327, %3328 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_3880 = torch.constant.int 1
    %3330 = torch.aten.size.int %3276, %int1_3880 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int0_3881 = torch.constant.int 0
    %3331 = torch.aten.add.int %int0_3881, %3330 : !torch.int, !torch.int -> !torch.int
    %int0_3882 = torch.constant.int 0
    %int0_3883 = torch.constant.int 0
    %int1_3884 = torch.constant.int 1
    %3332 = torch.aten.slice.Tensor %3329, %int0_3882, %int0_3883, %3331, %int1_3884 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %3332, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_3885 = torch.constant.int 1
    %int0_3886 = torch.constant.int 0
    %int9223372036854775807_3887 = torch.constant.int 9223372036854775807
    %int1_3888 = torch.constant.int 1
    %3333 = torch.aten.slice.Tensor %3332, %int1_3885, %int0_3886, %int9223372036854775807_3887, %int1_3888 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %3333, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_3889 = torch.constant.int 1
    %int0_3890 = torch.constant.int 0
    %int9223372036854775807_3891 = torch.constant.int 9223372036854775807
    %int1_3892 = torch.constant.int 1
    %3334 = torch.aten.slice.Tensor %3333, %int1_3889, %int0_3890, %int9223372036854775807_3891, %int1_3892 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %3334, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_3893 = torch.constant.int 0
    %3335 = torch.aten.unsqueeze %3334, %int0_3893 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %3335, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_3894 = torch.constant.int 1
    %int0_3895 = torch.constant.int 0
    %int9223372036854775807_3896 = torch.constant.int 9223372036854775807
    %int1_3897 = torch.constant.int 1
    %3336 = torch.aten.slice.Tensor %3335, %int1_3894, %int0_3895, %int9223372036854775807_3896, %int1_3897 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %3336, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_3898 = torch.constant.int 2
    %int0_3899 = torch.constant.int 0
    %int9223372036854775807_3900 = torch.constant.int 9223372036854775807
    %int1_3901 = torch.constant.int 1
    %3337 = torch.aten.slice.Tensor %3336, %int2_3898, %int0_3899, %int9223372036854775807_3900, %int1_3901 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %3337, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_3902 = torch.constant.int 4
    %int1_3903 = torch.constant.int 1
    %int1_3904 = torch.constant.int 1
    %3338 = torch.prim.ListConstruct %int4_3902, %int1_3903, %int1_3904 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3339 = torch.aten.repeat %3337, %3338 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %3339, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_3905 = torch.constant.int 6
    %3340 = torch.prims.convert_element_type %3287, %int6_3905 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %3340, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %3341 = torch_c.to_builtin_tensor %3340 : !torch.vtensor<[4,?,8,128],f32> -> tensor<4x?x8x128xf32>
    %3342 = torch_c.to_builtin_tensor %3339 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %3343 = util.call @sharktank_rotary_embedding_4_D_8_128_f32(%3341, %3342) : (tensor<4x?x8x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x8x128xf32>
    %3344 = torch_c.from_builtin_tensor %3343 : tensor<4x?x8x128xf32> -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %3344, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %int5_3906 = torch.constant.int 5
    %3345 = torch.prims.convert_element_type %3344, %int5_3906 : !torch.vtensor<[4,?,8,128],f32>, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %3345, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int64_3907 = torch.constant.int 64
    %3346 = torch.aten.mul.Scalar %arg2, %int64_3907 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3346, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int30 = torch.constant.int 30
    %int1_3908 = torch.constant.int 1
    %3347 = torch.aten.add.Scalar %3346, %int30, %int1_3908 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3347, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_3909 = torch.constant.int 4
    %int32_3910 = torch.constant.int 32
    %int8_3911 = torch.constant.int 8
    %int128_3912 = torch.constant.int 128
    %3348 = torch.prim.ListConstruct %int4_3909, %398, %int32_3910, %int8_3911, %int128_3912 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3349 = torch.aten.view %3345, %3348 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3349, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_3913 = torch.constant.int 4
    %3350 = torch.aten.mul.int %int4_3913, %398 : !torch.int, !torch.int -> !torch.int
    %int32_3914 = torch.constant.int 32
    %int8_3915 = torch.constant.int 8
    %int128_3916 = torch.constant.int 128
    %3351 = torch.prim.ListConstruct %3350, %int32_3914, %int8_3915, %int128_3916 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3352 = torch.aten.view %3349, %3351 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %3352, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_3917 = torch.constant.int 4
    %3353 = torch.aten.mul.int %int4_3917, %398 : !torch.int, !torch.int -> !torch.int
    %3354 = torch.prim.ListConstruct %3353 : (!torch.int) -> !torch.list<int>
    %3355 = torch.aten.view %3347, %3354 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3355, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_3918 = torch.constant.int 32
    %int2_3919 = torch.constant.int 2
    %int32_3920 = torch.constant.int 32
    %int8_3921 = torch.constant.int 8
    %int128_3922 = torch.constant.int 128
    %3356 = torch.prim.ListConstruct %389, %int32_3918, %int2_3919, %int32_3920, %int8_3921, %int128_3922 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3357 = torch.aten.view %3189, %3356 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3357, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_3923 = torch.constant.int 32
    %3358 = torch.aten.mul.int %389, %int32_3923 : !torch.int, !torch.int -> !torch.int
    %int2_3924 = torch.constant.int 2
    %3359 = torch.aten.mul.int %3358, %int2_3924 : !torch.int, !torch.int -> !torch.int
    %int32_3925 = torch.constant.int 32
    %int8_3926 = torch.constant.int 8
    %int128_3927 = torch.constant.int 128
    %3360 = torch.prim.ListConstruct %3359, %int32_3925, %int8_3926, %int128_3927 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3361 = torch.aten.view %3357, %3360 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %3361, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %3362 = torch.prim.ListConstruct %3355 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_3928 = torch.constant.bool false
    %3363 = torch.aten.index_put %3361, %3362, %3352, %false_3928 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %3363, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_3929 = torch.constant.int 32
    %int2_3930 = torch.constant.int 2
    %int32_3931 = torch.constant.int 32
    %int8_3932 = torch.constant.int 8
    %int128_3933 = torch.constant.int 128
    %3364 = torch.prim.ListConstruct %389, %int32_3929, %int2_3930, %int32_3931, %int8_3932, %int128_3933 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3365 = torch.aten.view %3363, %3364 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3365, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_3934 = torch.constant.int 2097152
    %3366 = torch.prim.ListConstruct %389, %int2097152_3934 : (!torch.int, !torch.int) -> !torch.list<int>
    %3367 = torch.aten.view %3365, %3366 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %3367, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_3935 = torch.constant.int 32
    %int2_3936 = torch.constant.int 2
    %int32_3937 = torch.constant.int 32
    %int8_3938 = torch.constant.int 8
    %int128_3939 = torch.constant.int 128
    %3368 = torch.prim.ListConstruct %389, %int32_3935, %int2_3936, %int32_3937, %int8_3938, %int128_3939 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3369 = torch.aten.view %3367, %3368 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3369, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_3940 = torch.constant.int 32
    %int8_3941 = torch.constant.int 8
    %int128_3942 = torch.constant.int 128
    %3370 = torch.prim.ListConstruct %3359, %int32_3940, %int8_3941, %int128_3942 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3371 = torch.aten.view %3369, %3370 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %3371, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_3943 = torch.constant.int 4
    %int32_3944 = torch.constant.int 32
    %int8_3945 = torch.constant.int 8
    %int128_3946 = torch.constant.int 128
    %3372 = torch.prim.ListConstruct %int4_3943, %398, %int32_3944, %int8_3945, %int128_3946 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3373 = torch.aten.view %3289, %3372 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3373, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_3947 = torch.constant.int 4
    %3374 = torch.aten.mul.int %int4_3947, %398 : !torch.int, !torch.int -> !torch.int
    %int32_3948 = torch.constant.int 32
    %int8_3949 = torch.constant.int 8
    %int128_3950 = torch.constant.int 128
    %3375 = torch.prim.ListConstruct %3374, %int32_3948, %int8_3949, %int128_3950 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3376 = torch.aten.view %3373, %3375 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %3376, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int1_3951 = torch.constant.int 1
    %int1_3952 = torch.constant.int 1
    %3377 = torch.aten.add.Scalar %3347, %int1_3951, %int1_3952 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3377, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_3953 = torch.constant.int 4
    %3378 = torch.aten.mul.int %int4_3953, %398 : !torch.int, !torch.int -> !torch.int
    %3379 = torch.prim.ListConstruct %3378 : (!torch.int) -> !torch.list<int>
    %3380 = torch.aten.view %3377, %3379 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3380, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %3381 = torch.prim.ListConstruct %3380 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_3954 = torch.constant.bool false
    %3382 = torch.aten.index_put %3371, %3381, %3376, %false_3954 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %3382, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_3955 = torch.constant.int 32
    %int2_3956 = torch.constant.int 2
    %int32_3957 = torch.constant.int 32
    %int8_3958 = torch.constant.int 8
    %int128_3959 = torch.constant.int 128
    %3383 = torch.prim.ListConstruct %389, %int32_3955, %int2_3956, %int32_3957, %int8_3958, %int128_3959 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3384 = torch.aten.view %3382, %3383 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3384, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_3960 = torch.constant.int 2097152
    %3385 = torch.prim.ListConstruct %389, %int2097152_3960 : (!torch.int, !torch.int) -> !torch.list<int>
    %3386 = torch.aten.view %3384, %3385 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %3386, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int-2_3961 = torch.constant.int -2
    %3387 = torch.aten.unsqueeze %3345, %int-2_3961 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %3387, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int4_3962 = torch.constant.int 4
    %int8_3963 = torch.constant.int 8
    %int4_3964 = torch.constant.int 4
    %int128_3965 = torch.constant.int 128
    %3388 = torch.prim.ListConstruct %int4_3962, %3330, %int8_3963, %int4_3964, %int128_3965 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_3966 = torch.constant.bool false
    %3389 = torch.aten.expand %3387, %3388, %false_3966 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3389, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_3967 = torch.constant.int 0
    %3390 = torch.aten.clone %3389, %int0_3967 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3390, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_3968 = torch.constant.int 4
    %int32_3969 = torch.constant.int 32
    %int128_3970 = torch.constant.int 128
    %3391 = torch.prim.ListConstruct %int4_3968, %3330, %int32_3969, %int128_3970 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3392 = torch.aten._unsafe_view %3390, %3391 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %3392, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_3971 = torch.constant.int -2
    %3393 = torch.aten.unsqueeze %3289, %int-2_3971 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %3393, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_3972 = torch.constant.int 1
    %3394 = torch.aten.size.int %3283, %int1_3972 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int4_3973 = torch.constant.int 4
    %int8_3974 = torch.constant.int 8
    %int4_3975 = torch.constant.int 4
    %int128_3976 = torch.constant.int 128
    %3395 = torch.prim.ListConstruct %int4_3973, %3394, %int8_3974, %int4_3975, %int128_3976 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_3977 = torch.constant.bool false
    %3396 = torch.aten.expand %3393, %3395, %false_3977 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3396, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_3978 = torch.constant.int 0
    %3397 = torch.aten.clone %3396, %int0_3978 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3397, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_3979 = torch.constant.int 4
    %int32_3980 = torch.constant.int 32
    %int128_3981 = torch.constant.int 128
    %3398 = torch.prim.ListConstruct %int4_3979, %3394, %int32_3980, %int128_3981 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3399 = torch.aten._unsafe_view %3397, %3398 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %3399, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_3982 = torch.constant.int 1
    %int2_3983 = torch.constant.int 2
    %3400 = torch.aten.transpose.int %3317, %int1_3982, %int2_3983 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %3400, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_3984 = torch.constant.int 1
    %int2_3985 = torch.constant.int 2
    %3401 = torch.aten.transpose.int %3392, %int1_3984, %int2_3985 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %3401, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_3986 = torch.constant.int 1
    %int2_3987 = torch.constant.int 2
    %3402 = torch.aten.transpose.int %3399, %int1_3986, %int2_3987 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %3402, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_3988 = torch.constant.float 0.000000e+00
    %true_3989 = torch.constant.bool true
    %none_3990 = torch.constant.none
    %none_3991 = torch.constant.none
    %3403:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3400, %3401, %3402, %float0.000000e00_3988, %true_3989, %none_3990, %none_3991) : (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?],f32>) 
    torch.bind_symbolic_shape %3403#0, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_3992 = torch.constant.int 1
    %int2_3993 = torch.constant.int 2
    %3404 = torch.aten.transpose.int %3403#0, %int1_3992, %int2_3993 : !torch.vtensor<[4,32,?,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %3404, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_3994 = torch.constant.int 4
    %int4096_3995 = torch.constant.int 4096
    %3405 = torch.prim.ListConstruct %int4_3994, %3302, %int4096_3995 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3406 = torch.aten.view %3404, %3405 : !torch.vtensor<[4,?,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3406, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_3996 = torch.constant.int -2
    %int-1_3997 = torch.constant.int -1
    %3407 = torch.aten.transpose.int %140, %int-2_3996, %int-1_3997 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_3998 = torch.constant.int 4
    %3408 = torch.aten.mul.int %int4_3998, %3302 : !torch.int, !torch.int -> !torch.int
    %int4096_3999 = torch.constant.int 4096
    %3409 = torch.prim.ListConstruct %3408, %int4096_3999 : (!torch.int, !torch.int) -> !torch.list<int>
    %3410 = torch.aten.view %3406, %3409 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3410, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %3411 = torch.aten.mm %3410, %3407 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3411, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_4000 = torch.constant.int 4
    %int4096_4001 = torch.constant.int 4096
    %3412 = torch.prim.ListConstruct %int4_4000, %3302, %int4096_4001 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3413 = torch.aten.view %3411, %3412 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3413, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_4002 = torch.constant.int 1
    %3414 = torch.aten.add.Tensor %3252, %3413, %int1_4002 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3414, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_4003 = torch.constant.int 6
    %3415 = torch.prims.convert_element_type %3414, %int6_4003 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3415, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_4004 = torch.constant.int 2
    %3416 = torch.aten.pow.Tensor_Scalar %3415, %int2_4004 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3416, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_4005 = torch.constant.int -1
    %3417 = torch.prim.ListConstruct %int-1_4005 : (!torch.int) -> !torch.list<int>
    %true_4006 = torch.constant.bool true
    %none_4007 = torch.constant.none
    %3418 = torch.aten.mean.dim %3416, %3417, %true_4006, %none_4007 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3418, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_4008 = torch.constant.float 9.9999997473787516E-6
    %int1_4009 = torch.constant.int 1
    %3419 = torch.aten.add.Scalar %3418, %float9.999990e-06_4008, %int1_4009 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3419, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %3420 = torch.aten.rsqrt %3419 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3420, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %3421 = torch.aten.mul.Tensor %3415, %3420 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3421, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_4010 = torch.constant.int 5
    %3422 = torch.prims.convert_element_type %3421, %int5_4010 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3422, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %3423 = torch.aten.mul.Tensor %141, %3422 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3423, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_4011 = torch.constant.int 5
    %3424 = torch.prims.convert_element_type %3423, %int5_4011 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3424, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_4012 = torch.constant.int -2
    %int-1_4013 = torch.constant.int -1
    %3425 = torch.aten.transpose.int %142, %int-2_4012, %int-1_4013 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_4014 = torch.constant.int 4
    %3426 = torch.aten.mul.int %int4_4014, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_4015 = torch.constant.int 4096
    %3427 = torch.prim.ListConstruct %3426, %int4096_4015 : (!torch.int, !torch.int) -> !torch.list<int>
    %3428 = torch.aten.view %3424, %3427 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3428, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %3429 = torch.aten.mm %3428, %3425 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %3429, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_4016 = torch.constant.int 4
    %int14336_4017 = torch.constant.int 14336
    %3430 = torch.prim.ListConstruct %int4_4016, %306, %int14336_4017 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3431 = torch.aten.view %3429, %3430 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %3431, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %3432 = torch.aten.silu %3431 : !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %3432, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_4018 = torch.constant.int -2
    %int-1_4019 = torch.constant.int -1
    %3433 = torch.aten.transpose.int %143, %int-2_4018, %int-1_4019 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_4020 = torch.constant.int 4
    %3434 = torch.aten.mul.int %int4_4020, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_4021 = torch.constant.int 4096
    %3435 = torch.prim.ListConstruct %3434, %int4096_4021 : (!torch.int, !torch.int) -> !torch.list<int>
    %3436 = torch.aten.view %3424, %3435 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3436, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %3437 = torch.aten.mm %3436, %3433 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %3437, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_4022 = torch.constant.int 4
    %int14336_4023 = torch.constant.int 14336
    %3438 = torch.prim.ListConstruct %int4_4022, %306, %int14336_4023 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3439 = torch.aten.view %3437, %3438 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %3439, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %3440 = torch.aten.mul.Tensor %3432, %3439 : !torch.vtensor<[4,?,14336],f16>, !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %3440, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_4024 = torch.constant.int -2
    %int-1_4025 = torch.constant.int -1
    %3441 = torch.aten.transpose.int %144, %int-2_4024, %int-1_4025 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int1_4026 = torch.constant.int 1
    %3442 = torch.aten.size.int %3431, %int1_4026 : !torch.vtensor<[4,?,14336],f16>, !torch.int -> !torch.int
    %int4_4027 = torch.constant.int 4
    %3443 = torch.aten.mul.int %int4_4027, %3442 : !torch.int, !torch.int -> !torch.int
    %int14336_4028 = torch.constant.int 14336
    %3444 = torch.prim.ListConstruct %3443, %int14336_4028 : (!torch.int, !torch.int) -> !torch.list<int>
    %3445 = torch.aten.view %3440, %3444 : !torch.vtensor<[4,?,14336],f16>, !torch.list<int> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %3445, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %3446 = torch.aten.mm %3445, %3441 : !torch.vtensor<[?,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3446, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_4029 = torch.constant.int 4
    %int4096_4030 = torch.constant.int 4096
    %3447 = torch.prim.ListConstruct %int4_4029, %3442, %int4096_4030 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3448 = torch.aten.view %3446, %3447 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3448, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_4031 = torch.constant.int 1
    %3449 = torch.aten.add.Tensor %3414, %3448, %int1_4031 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3449, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_4032 = torch.constant.int 6
    %3450 = torch.prims.convert_element_type %3449, %int6_4032 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3450, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_4033 = torch.constant.int 2
    %3451 = torch.aten.pow.Tensor_Scalar %3450, %int2_4033 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3451, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_4034 = torch.constant.int -1
    %3452 = torch.prim.ListConstruct %int-1_4034 : (!torch.int) -> !torch.list<int>
    %true_4035 = torch.constant.bool true
    %none_4036 = torch.constant.none
    %3453 = torch.aten.mean.dim %3451, %3452, %true_4035, %none_4036 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3453, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_4037 = torch.constant.float 9.9999997473787516E-6
    %int1_4038 = torch.constant.int 1
    %3454 = torch.aten.add.Scalar %3453, %float9.999990e-06_4037, %int1_4038 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3454, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %3455 = torch.aten.rsqrt %3454 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3455, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %3456 = torch.aten.mul.Tensor %3450, %3455 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3456, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_4039 = torch.constant.int 5
    %3457 = torch.prims.convert_element_type %3456, %int5_4039 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3457, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %3458 = torch.aten.mul.Tensor %145, %3457 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3458, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_4040 = torch.constant.int 5
    %3459 = torch.prims.convert_element_type %3458, %int5_4040 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3459, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_4041 = torch.constant.int -2
    %int-1_4042 = torch.constant.int -1
    %3460 = torch.aten.transpose.int %146, %int-2_4041, %int-1_4042 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_4043 = torch.constant.int 4
    %3461 = torch.aten.mul.int %int4_4043, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_4044 = torch.constant.int 4096
    %3462 = torch.prim.ListConstruct %3461, %int4096_4044 : (!torch.int, !torch.int) -> !torch.list<int>
    %3463 = torch.aten.view %3459, %3462 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3463, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %3464 = torch.aten.mm %3463, %3460 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3464, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_4045 = torch.constant.int 4
    %int4096_4046 = torch.constant.int 4096
    %3465 = torch.prim.ListConstruct %int4_4045, %306, %int4096_4046 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3466 = torch.aten.view %3464, %3465 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3466, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_4047 = torch.constant.int -2
    %int-1_4048 = torch.constant.int -1
    %3467 = torch.aten.transpose.int %147, %int-2_4047, %int-1_4048 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_4049 = torch.constant.int 4
    %3468 = torch.aten.mul.int %int4_4049, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_4050 = torch.constant.int 4096
    %3469 = torch.prim.ListConstruct %3468, %int4096_4050 : (!torch.int, !torch.int) -> !torch.list<int>
    %3470 = torch.aten.view %3459, %3469 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3470, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %3471 = torch.aten.mm %3470, %3467 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %3471, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_4051 = torch.constant.int 4
    %int1024_4052 = torch.constant.int 1024
    %3472 = torch.prim.ListConstruct %int4_4051, %306, %int1024_4052 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3473 = torch.aten.view %3471, %3472 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %3473, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int-2_4053 = torch.constant.int -2
    %int-1_4054 = torch.constant.int -1
    %3474 = torch.aten.transpose.int %148, %int-2_4053, %int-1_4054 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_4055 = torch.constant.int 4
    %3475 = torch.aten.mul.int %int4_4055, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_4056 = torch.constant.int 4096
    %3476 = torch.prim.ListConstruct %3475, %int4096_4056 : (!torch.int, !torch.int) -> !torch.list<int>
    %3477 = torch.aten.view %3459, %3476 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3477, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %3478 = torch.aten.mm %3477, %3474 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %3478, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_4057 = torch.constant.int 4
    %int1024_4058 = torch.constant.int 1024
    %3479 = torch.prim.ListConstruct %int4_4057, %306, %int1024_4058 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3480 = torch.aten.view %3478, %3479 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %3480, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int4_4059 = torch.constant.int 4
    %int32_4060 = torch.constant.int 32
    %int128_4061 = torch.constant.int 128
    %3481 = torch.prim.ListConstruct %int4_4059, %306, %int32_4060, %int128_4061 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3482 = torch.aten.view %3466, %3481 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %3482, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_4062 = torch.constant.int 4
    %int8_4063 = torch.constant.int 8
    %int128_4064 = torch.constant.int 128
    %3483 = torch.prim.ListConstruct %int4_4062, %306, %int8_4063, %int128_4064 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3484 = torch.aten.view %3473, %3483 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %3484, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int4_4065 = torch.constant.int 4
    %int8_4066 = torch.constant.int 8
    %int128_4067 = torch.constant.int 128
    %3485 = torch.prim.ListConstruct %int4_4065, %306, %int8_4066, %int128_4067 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3486 = torch.aten.view %3480, %3485 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %3486, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int131072_4068 = torch.constant.int 131072
    %none_4069 = torch.constant.none
    %none_4070 = torch.constant.none
    %cpu_4071 = torch.constant.device "cpu"
    %false_4072 = torch.constant.bool false
    %3487 = torch.aten.arange %int131072_4068, %none_4069, %none_4070, %cpu_4071, %false_4072 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_4073 = torch.constant.int 0
    %int128_4074 = torch.constant.int 128
    %none_4075 = torch.constant.none
    %none_4076 = torch.constant.none
    %cpu_4077 = torch.constant.device "cpu"
    %false_4078 = torch.constant.bool false
    %3488 = torch.aten.arange.start %int0_4073, %int128_4074, %none_4075, %none_4076, %cpu_4077, %false_4078 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_4079 = torch.constant.int 2
    %3489 = torch.aten.floor_divide.Scalar %3488, %int2_4079 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_4080 = torch.constant.int 6
    %3490 = torch.prims.convert_element_type %3489, %int6_4080 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_4081 = torch.constant.int 128
    %3491 = torch.aten.div.Scalar %3490, %int128_4081 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_4082 = torch.constant.float 2.000000e+00
    %3492 = torch.aten.mul.Scalar %3491, %float2.000000e00_4082 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_4083 = torch.constant.float 5.000000e+05
    %3493 = torch.aten.pow.Scalar %float5.000000e05_4083, %3492 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %3494 = torch.aten.reciprocal %3493 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_4084 = torch.constant.float 1.000000e+00
    %3495 = torch.aten.mul.Scalar %3494, %float1.000000e00_4084 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_4085 = torch.constant.int 1
    %3496 = torch.aten.unsqueeze %3487, %int1_4085 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_4086 = torch.constant.int 0
    %3497 = torch.aten.unsqueeze %3495, %int0_4086 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %3498 = torch.aten.mul.Tensor %3496, %3497 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_4087 = torch.constant.int 1
    %3499 = torch.aten.size.int %3466, %int1_4087 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.int
    %int0_4088 = torch.constant.int 0
    %3500 = torch.aten.add.int %int0_4088, %3499 : !torch.int, !torch.int -> !torch.int
    %int0_4089 = torch.constant.int 0
    %int0_4090 = torch.constant.int 0
    %int1_4091 = torch.constant.int 1
    %3501 = torch.aten.slice.Tensor %3498, %int0_4089, %int0_4090, %3500, %int1_4091 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %3501, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_4092 = torch.constant.int 1
    %int0_4093 = torch.constant.int 0
    %int9223372036854775807_4094 = torch.constant.int 9223372036854775807
    %int1_4095 = torch.constant.int 1
    %3502 = torch.aten.slice.Tensor %3501, %int1_4092, %int0_4093, %int9223372036854775807_4094, %int1_4095 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %3502, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_4096 = torch.constant.int 1
    %int0_4097 = torch.constant.int 0
    %int9223372036854775807_4098 = torch.constant.int 9223372036854775807
    %int1_4099 = torch.constant.int 1
    %3503 = torch.aten.slice.Tensor %3502, %int1_4096, %int0_4097, %int9223372036854775807_4098, %int1_4099 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %3503, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_4100 = torch.constant.int 0
    %3504 = torch.aten.unsqueeze %3503, %int0_4100 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %3504, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_4101 = torch.constant.int 1
    %int0_4102 = torch.constant.int 0
    %int9223372036854775807_4103 = torch.constant.int 9223372036854775807
    %int1_4104 = torch.constant.int 1
    %3505 = torch.aten.slice.Tensor %3504, %int1_4101, %int0_4102, %int9223372036854775807_4103, %int1_4104 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %3505, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_4105 = torch.constant.int 2
    %int0_4106 = torch.constant.int 0
    %int9223372036854775807_4107 = torch.constant.int 9223372036854775807
    %int1_4108 = torch.constant.int 1
    %3506 = torch.aten.slice.Tensor %3505, %int2_4105, %int0_4106, %int9223372036854775807_4107, %int1_4108 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %3506, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_4109 = torch.constant.int 4
    %int1_4110 = torch.constant.int 1
    %int1_4111 = torch.constant.int 1
    %3507 = torch.prim.ListConstruct %int4_4109, %int1_4110, %int1_4111 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3508 = torch.aten.repeat %3506, %3507 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %3508, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_4112 = torch.constant.int 6
    %3509 = torch.prims.convert_element_type %3482, %int6_4112 : !torch.vtensor<[4,?,32,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %3509, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %3510 = torch_c.to_builtin_tensor %3509 : !torch.vtensor<[4,?,32,128],f32> -> tensor<4x?x32x128xf32>
    %3511 = torch_c.to_builtin_tensor %3508 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %3512 = util.call @sharktank_rotary_embedding_4_D_32_128_f32(%3510, %3511) : (tensor<4x?x32x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x32x128xf32>
    %3513 = torch_c.from_builtin_tensor %3512 : tensor<4x?x32x128xf32> -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %3513, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %int5_4113 = torch.constant.int 5
    %3514 = torch.prims.convert_element_type %3513, %int5_4113 : !torch.vtensor<[4,?,32,128],f32>, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %3514, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int131072_4114 = torch.constant.int 131072
    %none_4115 = torch.constant.none
    %none_4116 = torch.constant.none
    %cpu_4117 = torch.constant.device "cpu"
    %false_4118 = torch.constant.bool false
    %3515 = torch.aten.arange %int131072_4114, %none_4115, %none_4116, %cpu_4117, %false_4118 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_4119 = torch.constant.int 0
    %int128_4120 = torch.constant.int 128
    %none_4121 = torch.constant.none
    %none_4122 = torch.constant.none
    %cpu_4123 = torch.constant.device "cpu"
    %false_4124 = torch.constant.bool false
    %3516 = torch.aten.arange.start %int0_4119, %int128_4120, %none_4121, %none_4122, %cpu_4123, %false_4124 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_4125 = torch.constant.int 2
    %3517 = torch.aten.floor_divide.Scalar %3516, %int2_4125 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_4126 = torch.constant.int 6
    %3518 = torch.prims.convert_element_type %3517, %int6_4126 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_4127 = torch.constant.int 128
    %3519 = torch.aten.div.Scalar %3518, %int128_4127 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_4128 = torch.constant.float 2.000000e+00
    %3520 = torch.aten.mul.Scalar %3519, %float2.000000e00_4128 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_4129 = torch.constant.float 5.000000e+05
    %3521 = torch.aten.pow.Scalar %float5.000000e05_4129, %3520 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %3522 = torch.aten.reciprocal %3521 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_4130 = torch.constant.float 1.000000e+00
    %3523 = torch.aten.mul.Scalar %3522, %float1.000000e00_4130 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_4131 = torch.constant.int 1
    %3524 = torch.aten.unsqueeze %3515, %int1_4131 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_4132 = torch.constant.int 0
    %3525 = torch.aten.unsqueeze %3523, %int0_4132 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %3526 = torch.aten.mul.Tensor %3524, %3525 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_4133 = torch.constant.int 1
    %3527 = torch.aten.size.int %3473, %int1_4133 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int0_4134 = torch.constant.int 0
    %3528 = torch.aten.add.int %int0_4134, %3527 : !torch.int, !torch.int -> !torch.int
    %int0_4135 = torch.constant.int 0
    %int0_4136 = torch.constant.int 0
    %int1_4137 = torch.constant.int 1
    %3529 = torch.aten.slice.Tensor %3526, %int0_4135, %int0_4136, %3528, %int1_4137 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %3529, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_4138 = torch.constant.int 1
    %int0_4139 = torch.constant.int 0
    %int9223372036854775807_4140 = torch.constant.int 9223372036854775807
    %int1_4141 = torch.constant.int 1
    %3530 = torch.aten.slice.Tensor %3529, %int1_4138, %int0_4139, %int9223372036854775807_4140, %int1_4141 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %3530, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_4142 = torch.constant.int 1
    %int0_4143 = torch.constant.int 0
    %int9223372036854775807_4144 = torch.constant.int 9223372036854775807
    %int1_4145 = torch.constant.int 1
    %3531 = torch.aten.slice.Tensor %3530, %int1_4142, %int0_4143, %int9223372036854775807_4144, %int1_4145 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %3531, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_4146 = torch.constant.int 0
    %3532 = torch.aten.unsqueeze %3531, %int0_4146 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %3532, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_4147 = torch.constant.int 1
    %int0_4148 = torch.constant.int 0
    %int9223372036854775807_4149 = torch.constant.int 9223372036854775807
    %int1_4150 = torch.constant.int 1
    %3533 = torch.aten.slice.Tensor %3532, %int1_4147, %int0_4148, %int9223372036854775807_4149, %int1_4150 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %3533, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_4151 = torch.constant.int 2
    %int0_4152 = torch.constant.int 0
    %int9223372036854775807_4153 = torch.constant.int 9223372036854775807
    %int1_4154 = torch.constant.int 1
    %3534 = torch.aten.slice.Tensor %3533, %int2_4151, %int0_4152, %int9223372036854775807_4153, %int1_4154 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %3534, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_4155 = torch.constant.int 4
    %int1_4156 = torch.constant.int 1
    %int1_4157 = torch.constant.int 1
    %3535 = torch.prim.ListConstruct %int4_4155, %int1_4156, %int1_4157 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3536 = torch.aten.repeat %3534, %3535 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %3536, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_4158 = torch.constant.int 6
    %3537 = torch.prims.convert_element_type %3484, %int6_4158 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %3537, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %3538 = torch_c.to_builtin_tensor %3537 : !torch.vtensor<[4,?,8,128],f32> -> tensor<4x?x8x128xf32>
    %3539 = torch_c.to_builtin_tensor %3536 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %3540 = util.call @sharktank_rotary_embedding_4_D_8_128_f32(%3538, %3539) : (tensor<4x?x8x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x8x128xf32>
    %3541 = torch_c.from_builtin_tensor %3540 : tensor<4x?x8x128xf32> -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %3541, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %int5_4159 = torch.constant.int 5
    %3542 = torch.prims.convert_element_type %3541, %int5_4159 : !torch.vtensor<[4,?,8,128],f32>, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %3542, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int64_4160 = torch.constant.int 64
    %3543 = torch.aten.mul.Scalar %arg2, %int64_4160 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3543, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int32_4161 = torch.constant.int 32
    %int1_4162 = torch.constant.int 1
    %3544 = torch.aten.add.Scalar %3543, %int32_4161, %int1_4162 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3544, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_4163 = torch.constant.int 4
    %int32_4164 = torch.constant.int 32
    %int8_4165 = torch.constant.int 8
    %int128_4166 = torch.constant.int 128
    %3545 = torch.prim.ListConstruct %int4_4163, %398, %int32_4164, %int8_4165, %int128_4166 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3546 = torch.aten.view %3542, %3545 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3546, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_4167 = torch.constant.int 4
    %3547 = torch.aten.mul.int %int4_4167, %398 : !torch.int, !torch.int -> !torch.int
    %int32_4168 = torch.constant.int 32
    %int8_4169 = torch.constant.int 8
    %int128_4170 = torch.constant.int 128
    %3548 = torch.prim.ListConstruct %3547, %int32_4168, %int8_4169, %int128_4170 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3549 = torch.aten.view %3546, %3548 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %3549, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_4171 = torch.constant.int 4
    %3550 = torch.aten.mul.int %int4_4171, %398 : !torch.int, !torch.int -> !torch.int
    %3551 = torch.prim.ListConstruct %3550 : (!torch.int) -> !torch.list<int>
    %3552 = torch.aten.view %3544, %3551 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3552, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_4172 = torch.constant.int 32
    %int2_4173 = torch.constant.int 2
    %int32_4174 = torch.constant.int 32
    %int8_4175 = torch.constant.int 8
    %int128_4176 = torch.constant.int 128
    %3553 = torch.prim.ListConstruct %389, %int32_4172, %int2_4173, %int32_4174, %int8_4175, %int128_4176 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3554 = torch.aten.view %3386, %3553 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3554, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_4177 = torch.constant.int 32
    %3555 = torch.aten.mul.int %389, %int32_4177 : !torch.int, !torch.int -> !torch.int
    %int2_4178 = torch.constant.int 2
    %3556 = torch.aten.mul.int %3555, %int2_4178 : !torch.int, !torch.int -> !torch.int
    %int32_4179 = torch.constant.int 32
    %int8_4180 = torch.constant.int 8
    %int128_4181 = torch.constant.int 128
    %3557 = torch.prim.ListConstruct %3556, %int32_4179, %int8_4180, %int128_4181 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3558 = torch.aten.view %3554, %3557 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %3558, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %3559 = torch.prim.ListConstruct %3552 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_4182 = torch.constant.bool false
    %3560 = torch.aten.index_put %3558, %3559, %3549, %false_4182 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %3560, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_4183 = torch.constant.int 32
    %int2_4184 = torch.constant.int 2
    %int32_4185 = torch.constant.int 32
    %int8_4186 = torch.constant.int 8
    %int128_4187 = torch.constant.int 128
    %3561 = torch.prim.ListConstruct %389, %int32_4183, %int2_4184, %int32_4185, %int8_4186, %int128_4187 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3562 = torch.aten.view %3560, %3561 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3562, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_4188 = torch.constant.int 2097152
    %3563 = torch.prim.ListConstruct %389, %int2097152_4188 : (!torch.int, !torch.int) -> !torch.list<int>
    %3564 = torch.aten.view %3562, %3563 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %3564, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_4189 = torch.constant.int 32
    %int2_4190 = torch.constant.int 2
    %int32_4191 = torch.constant.int 32
    %int8_4192 = torch.constant.int 8
    %int128_4193 = torch.constant.int 128
    %3565 = torch.prim.ListConstruct %389, %int32_4189, %int2_4190, %int32_4191, %int8_4192, %int128_4193 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3566 = torch.aten.view %3564, %3565 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3566, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_4194 = torch.constant.int 32
    %int8_4195 = torch.constant.int 8
    %int128_4196 = torch.constant.int 128
    %3567 = torch.prim.ListConstruct %3556, %int32_4194, %int8_4195, %int128_4196 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3568 = torch.aten.view %3566, %3567 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %3568, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_4197 = torch.constant.int 4
    %int32_4198 = torch.constant.int 32
    %int8_4199 = torch.constant.int 8
    %int128_4200 = torch.constant.int 128
    %3569 = torch.prim.ListConstruct %int4_4197, %398, %int32_4198, %int8_4199, %int128_4200 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3570 = torch.aten.view %3486, %3569 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3570, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_4201 = torch.constant.int 4
    %3571 = torch.aten.mul.int %int4_4201, %398 : !torch.int, !torch.int -> !torch.int
    %int32_4202 = torch.constant.int 32
    %int8_4203 = torch.constant.int 8
    %int128_4204 = torch.constant.int 128
    %3572 = torch.prim.ListConstruct %3571, %int32_4202, %int8_4203, %int128_4204 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3573 = torch.aten.view %3570, %3572 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %3573, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int1_4205 = torch.constant.int 1
    %int1_4206 = torch.constant.int 1
    %3574 = torch.aten.add.Scalar %3544, %int1_4205, %int1_4206 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3574, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_4207 = torch.constant.int 4
    %3575 = torch.aten.mul.int %int4_4207, %398 : !torch.int, !torch.int -> !torch.int
    %3576 = torch.prim.ListConstruct %3575 : (!torch.int) -> !torch.list<int>
    %3577 = torch.aten.view %3574, %3576 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3577, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %3578 = torch.prim.ListConstruct %3577 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_4208 = torch.constant.bool false
    %3579 = torch.aten.index_put %3568, %3578, %3573, %false_4208 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %3579, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_4209 = torch.constant.int 32
    %int2_4210 = torch.constant.int 2
    %int32_4211 = torch.constant.int 32
    %int8_4212 = torch.constant.int 8
    %int128_4213 = torch.constant.int 128
    %3580 = torch.prim.ListConstruct %389, %int32_4209, %int2_4210, %int32_4211, %int8_4212, %int128_4213 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3581 = torch.aten.view %3579, %3580 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3581, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_4214 = torch.constant.int 2097152
    %3582 = torch.prim.ListConstruct %389, %int2097152_4214 : (!torch.int, !torch.int) -> !torch.list<int>
    %3583 = torch.aten.view %3581, %3582 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %3583, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int-2_4215 = torch.constant.int -2
    %3584 = torch.aten.unsqueeze %3542, %int-2_4215 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %3584, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int4_4216 = torch.constant.int 4
    %int8_4217 = torch.constant.int 8
    %int4_4218 = torch.constant.int 4
    %int128_4219 = torch.constant.int 128
    %3585 = torch.prim.ListConstruct %int4_4216, %3527, %int8_4217, %int4_4218, %int128_4219 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_4220 = torch.constant.bool false
    %3586 = torch.aten.expand %3584, %3585, %false_4220 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3586, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_4221 = torch.constant.int 0
    %3587 = torch.aten.clone %3586, %int0_4221 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3587, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_4222 = torch.constant.int 4
    %int32_4223 = torch.constant.int 32
    %int128_4224 = torch.constant.int 128
    %3588 = torch.prim.ListConstruct %int4_4222, %3527, %int32_4223, %int128_4224 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3589 = torch.aten._unsafe_view %3587, %3588 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %3589, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_4225 = torch.constant.int -2
    %3590 = torch.aten.unsqueeze %3486, %int-2_4225 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %3590, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_4226 = torch.constant.int 1
    %3591 = torch.aten.size.int %3480, %int1_4226 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int4_4227 = torch.constant.int 4
    %int8_4228 = torch.constant.int 8
    %int4_4229 = torch.constant.int 4
    %int128_4230 = torch.constant.int 128
    %3592 = torch.prim.ListConstruct %int4_4227, %3591, %int8_4228, %int4_4229, %int128_4230 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_4231 = torch.constant.bool false
    %3593 = torch.aten.expand %3590, %3592, %false_4231 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3593, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_4232 = torch.constant.int 0
    %3594 = torch.aten.clone %3593, %int0_4232 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3594, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_4233 = torch.constant.int 4
    %int32_4234 = torch.constant.int 32
    %int128_4235 = torch.constant.int 128
    %3595 = torch.prim.ListConstruct %int4_4233, %3591, %int32_4234, %int128_4235 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3596 = torch.aten._unsafe_view %3594, %3595 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %3596, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_4236 = torch.constant.int 1
    %int2_4237 = torch.constant.int 2
    %3597 = torch.aten.transpose.int %3514, %int1_4236, %int2_4237 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %3597, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_4238 = torch.constant.int 1
    %int2_4239 = torch.constant.int 2
    %3598 = torch.aten.transpose.int %3589, %int1_4238, %int2_4239 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %3598, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_4240 = torch.constant.int 1
    %int2_4241 = torch.constant.int 2
    %3599 = torch.aten.transpose.int %3596, %int1_4240, %int2_4241 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %3599, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_4242 = torch.constant.float 0.000000e+00
    %true_4243 = torch.constant.bool true
    %none_4244 = torch.constant.none
    %none_4245 = torch.constant.none
    %3600:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3597, %3598, %3599, %float0.000000e00_4242, %true_4243, %none_4244, %none_4245) : (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?],f32>) 
    torch.bind_symbolic_shape %3600#0, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_4246 = torch.constant.int 1
    %int2_4247 = torch.constant.int 2
    %3601 = torch.aten.transpose.int %3600#0, %int1_4246, %int2_4247 : !torch.vtensor<[4,32,?,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %3601, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_4248 = torch.constant.int 4
    %int4096_4249 = torch.constant.int 4096
    %3602 = torch.prim.ListConstruct %int4_4248, %3499, %int4096_4249 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3603 = torch.aten.view %3601, %3602 : !torch.vtensor<[4,?,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3603, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_4250 = torch.constant.int -2
    %int-1_4251 = torch.constant.int -1
    %3604 = torch.aten.transpose.int %149, %int-2_4250, %int-1_4251 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_4252 = torch.constant.int 4
    %3605 = torch.aten.mul.int %int4_4252, %3499 : !torch.int, !torch.int -> !torch.int
    %int4096_4253 = torch.constant.int 4096
    %3606 = torch.prim.ListConstruct %3605, %int4096_4253 : (!torch.int, !torch.int) -> !torch.list<int>
    %3607 = torch.aten.view %3603, %3606 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3607, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %3608 = torch.aten.mm %3607, %3604 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3608, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_4254 = torch.constant.int 4
    %int4096_4255 = torch.constant.int 4096
    %3609 = torch.prim.ListConstruct %int4_4254, %3499, %int4096_4255 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3610 = torch.aten.view %3608, %3609 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3610, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_4256 = torch.constant.int 1
    %3611 = torch.aten.add.Tensor %3449, %3610, %int1_4256 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3611, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_4257 = torch.constant.int 6
    %3612 = torch.prims.convert_element_type %3611, %int6_4257 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3612, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_4258 = torch.constant.int 2
    %3613 = torch.aten.pow.Tensor_Scalar %3612, %int2_4258 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3613, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_4259 = torch.constant.int -1
    %3614 = torch.prim.ListConstruct %int-1_4259 : (!torch.int) -> !torch.list<int>
    %true_4260 = torch.constant.bool true
    %none_4261 = torch.constant.none
    %3615 = torch.aten.mean.dim %3613, %3614, %true_4260, %none_4261 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3615, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_4262 = torch.constant.float 9.9999997473787516E-6
    %int1_4263 = torch.constant.int 1
    %3616 = torch.aten.add.Scalar %3615, %float9.999990e-06_4262, %int1_4263 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3616, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %3617 = torch.aten.rsqrt %3616 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3617, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %3618 = torch.aten.mul.Tensor %3612, %3617 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3618, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_4264 = torch.constant.int 5
    %3619 = torch.prims.convert_element_type %3618, %int5_4264 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3619, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %3620 = torch.aten.mul.Tensor %150, %3619 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3620, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_4265 = torch.constant.int 5
    %3621 = torch.prims.convert_element_type %3620, %int5_4265 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3621, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_4266 = torch.constant.int -2
    %int-1_4267 = torch.constant.int -1
    %3622 = torch.aten.transpose.int %151, %int-2_4266, %int-1_4267 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_4268 = torch.constant.int 4
    %3623 = torch.aten.mul.int %int4_4268, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_4269 = torch.constant.int 4096
    %3624 = torch.prim.ListConstruct %3623, %int4096_4269 : (!torch.int, !torch.int) -> !torch.list<int>
    %3625 = torch.aten.view %3621, %3624 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3625, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %3626 = torch.aten.mm %3625, %3622 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %3626, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_4270 = torch.constant.int 4
    %int14336_4271 = torch.constant.int 14336
    %3627 = torch.prim.ListConstruct %int4_4270, %306, %int14336_4271 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3628 = torch.aten.view %3626, %3627 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %3628, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %3629 = torch.aten.silu %3628 : !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %3629, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_4272 = torch.constant.int -2
    %int-1_4273 = torch.constant.int -1
    %3630 = torch.aten.transpose.int %152, %int-2_4272, %int-1_4273 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_4274 = torch.constant.int 4
    %3631 = torch.aten.mul.int %int4_4274, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_4275 = torch.constant.int 4096
    %3632 = torch.prim.ListConstruct %3631, %int4096_4275 : (!torch.int, !torch.int) -> !torch.list<int>
    %3633 = torch.aten.view %3621, %3632 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3633, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %3634 = torch.aten.mm %3633, %3630 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %3634, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_4276 = torch.constant.int 4
    %int14336_4277 = torch.constant.int 14336
    %3635 = torch.prim.ListConstruct %int4_4276, %306, %int14336_4277 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3636 = torch.aten.view %3634, %3635 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %3636, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %3637 = torch.aten.mul.Tensor %3629, %3636 : !torch.vtensor<[4,?,14336],f16>, !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %3637, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_4278 = torch.constant.int -2
    %int-1_4279 = torch.constant.int -1
    %3638 = torch.aten.transpose.int %153, %int-2_4278, %int-1_4279 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int1_4280 = torch.constant.int 1
    %3639 = torch.aten.size.int %3628, %int1_4280 : !torch.vtensor<[4,?,14336],f16>, !torch.int -> !torch.int
    %int4_4281 = torch.constant.int 4
    %3640 = torch.aten.mul.int %int4_4281, %3639 : !torch.int, !torch.int -> !torch.int
    %int14336_4282 = torch.constant.int 14336
    %3641 = torch.prim.ListConstruct %3640, %int14336_4282 : (!torch.int, !torch.int) -> !torch.list<int>
    %3642 = torch.aten.view %3637, %3641 : !torch.vtensor<[4,?,14336],f16>, !torch.list<int> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %3642, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %3643 = torch.aten.mm %3642, %3638 : !torch.vtensor<[?,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3643, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_4283 = torch.constant.int 4
    %int4096_4284 = torch.constant.int 4096
    %3644 = torch.prim.ListConstruct %int4_4283, %3639, %int4096_4284 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3645 = torch.aten.view %3643, %3644 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3645, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_4285 = torch.constant.int 1
    %3646 = torch.aten.add.Tensor %3611, %3645, %int1_4285 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3646, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_4286 = torch.constant.int 6
    %3647 = torch.prims.convert_element_type %3646, %int6_4286 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3647, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_4287 = torch.constant.int 2
    %3648 = torch.aten.pow.Tensor_Scalar %3647, %int2_4287 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3648, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_4288 = torch.constant.int -1
    %3649 = torch.prim.ListConstruct %int-1_4288 : (!torch.int) -> !torch.list<int>
    %true_4289 = torch.constant.bool true
    %none_4290 = torch.constant.none
    %3650 = torch.aten.mean.dim %3648, %3649, %true_4289, %none_4290 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3650, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_4291 = torch.constant.float 9.9999997473787516E-6
    %int1_4292 = torch.constant.int 1
    %3651 = torch.aten.add.Scalar %3650, %float9.999990e-06_4291, %int1_4292 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3651, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %3652 = torch.aten.rsqrt %3651 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3652, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %3653 = torch.aten.mul.Tensor %3647, %3652 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3653, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_4293 = torch.constant.int 5
    %3654 = torch.prims.convert_element_type %3653, %int5_4293 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3654, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %3655 = torch.aten.mul.Tensor %154, %3654 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3655, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_4294 = torch.constant.int 5
    %3656 = torch.prims.convert_element_type %3655, %int5_4294 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3656, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_4295 = torch.constant.int -2
    %int-1_4296 = torch.constant.int -1
    %3657 = torch.aten.transpose.int %155, %int-2_4295, %int-1_4296 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_4297 = torch.constant.int 4
    %3658 = torch.aten.mul.int %int4_4297, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_4298 = torch.constant.int 4096
    %3659 = torch.prim.ListConstruct %3658, %int4096_4298 : (!torch.int, !torch.int) -> !torch.list<int>
    %3660 = torch.aten.view %3656, %3659 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3660, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %3661 = torch.aten.mm %3660, %3657 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3661, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_4299 = torch.constant.int 4
    %int4096_4300 = torch.constant.int 4096
    %3662 = torch.prim.ListConstruct %int4_4299, %306, %int4096_4300 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3663 = torch.aten.view %3661, %3662 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3663, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_4301 = torch.constant.int -2
    %int-1_4302 = torch.constant.int -1
    %3664 = torch.aten.transpose.int %156, %int-2_4301, %int-1_4302 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_4303 = torch.constant.int 4
    %3665 = torch.aten.mul.int %int4_4303, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_4304 = torch.constant.int 4096
    %3666 = torch.prim.ListConstruct %3665, %int4096_4304 : (!torch.int, !torch.int) -> !torch.list<int>
    %3667 = torch.aten.view %3656, %3666 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3667, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %3668 = torch.aten.mm %3667, %3664 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %3668, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_4305 = torch.constant.int 4
    %int1024_4306 = torch.constant.int 1024
    %3669 = torch.prim.ListConstruct %int4_4305, %306, %int1024_4306 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3670 = torch.aten.view %3668, %3669 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %3670, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int-2_4307 = torch.constant.int -2
    %int-1_4308 = torch.constant.int -1
    %3671 = torch.aten.transpose.int %157, %int-2_4307, %int-1_4308 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_4309 = torch.constant.int 4
    %3672 = torch.aten.mul.int %int4_4309, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_4310 = torch.constant.int 4096
    %3673 = torch.prim.ListConstruct %3672, %int4096_4310 : (!torch.int, !torch.int) -> !torch.list<int>
    %3674 = torch.aten.view %3656, %3673 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3674, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %3675 = torch.aten.mm %3674, %3671 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %3675, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_4311 = torch.constant.int 4
    %int1024_4312 = torch.constant.int 1024
    %3676 = torch.prim.ListConstruct %int4_4311, %306, %int1024_4312 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3677 = torch.aten.view %3675, %3676 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %3677, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int4_4313 = torch.constant.int 4
    %int32_4314 = torch.constant.int 32
    %int128_4315 = torch.constant.int 128
    %3678 = torch.prim.ListConstruct %int4_4313, %306, %int32_4314, %int128_4315 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3679 = torch.aten.view %3663, %3678 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %3679, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_4316 = torch.constant.int 4
    %int8_4317 = torch.constant.int 8
    %int128_4318 = torch.constant.int 128
    %3680 = torch.prim.ListConstruct %int4_4316, %306, %int8_4317, %int128_4318 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3681 = torch.aten.view %3670, %3680 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %3681, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int4_4319 = torch.constant.int 4
    %int8_4320 = torch.constant.int 8
    %int128_4321 = torch.constant.int 128
    %3682 = torch.prim.ListConstruct %int4_4319, %306, %int8_4320, %int128_4321 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3683 = torch.aten.view %3677, %3682 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %3683, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int131072_4322 = torch.constant.int 131072
    %none_4323 = torch.constant.none
    %none_4324 = torch.constant.none
    %cpu_4325 = torch.constant.device "cpu"
    %false_4326 = torch.constant.bool false
    %3684 = torch.aten.arange %int131072_4322, %none_4323, %none_4324, %cpu_4325, %false_4326 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_4327 = torch.constant.int 0
    %int128_4328 = torch.constant.int 128
    %none_4329 = torch.constant.none
    %none_4330 = torch.constant.none
    %cpu_4331 = torch.constant.device "cpu"
    %false_4332 = torch.constant.bool false
    %3685 = torch.aten.arange.start %int0_4327, %int128_4328, %none_4329, %none_4330, %cpu_4331, %false_4332 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_4333 = torch.constant.int 2
    %3686 = torch.aten.floor_divide.Scalar %3685, %int2_4333 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_4334 = torch.constant.int 6
    %3687 = torch.prims.convert_element_type %3686, %int6_4334 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_4335 = torch.constant.int 128
    %3688 = torch.aten.div.Scalar %3687, %int128_4335 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_4336 = torch.constant.float 2.000000e+00
    %3689 = torch.aten.mul.Scalar %3688, %float2.000000e00_4336 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_4337 = torch.constant.float 5.000000e+05
    %3690 = torch.aten.pow.Scalar %float5.000000e05_4337, %3689 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %3691 = torch.aten.reciprocal %3690 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_4338 = torch.constant.float 1.000000e+00
    %3692 = torch.aten.mul.Scalar %3691, %float1.000000e00_4338 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_4339 = torch.constant.int 1
    %3693 = torch.aten.unsqueeze %3684, %int1_4339 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_4340 = torch.constant.int 0
    %3694 = torch.aten.unsqueeze %3692, %int0_4340 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %3695 = torch.aten.mul.Tensor %3693, %3694 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_4341 = torch.constant.int 1
    %3696 = torch.aten.size.int %3663, %int1_4341 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.int
    %int0_4342 = torch.constant.int 0
    %3697 = torch.aten.add.int %int0_4342, %3696 : !torch.int, !torch.int -> !torch.int
    %int0_4343 = torch.constant.int 0
    %int0_4344 = torch.constant.int 0
    %int1_4345 = torch.constant.int 1
    %3698 = torch.aten.slice.Tensor %3695, %int0_4343, %int0_4344, %3697, %int1_4345 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %3698, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_4346 = torch.constant.int 1
    %int0_4347 = torch.constant.int 0
    %int9223372036854775807_4348 = torch.constant.int 9223372036854775807
    %int1_4349 = torch.constant.int 1
    %3699 = torch.aten.slice.Tensor %3698, %int1_4346, %int0_4347, %int9223372036854775807_4348, %int1_4349 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %3699, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_4350 = torch.constant.int 1
    %int0_4351 = torch.constant.int 0
    %int9223372036854775807_4352 = torch.constant.int 9223372036854775807
    %int1_4353 = torch.constant.int 1
    %3700 = torch.aten.slice.Tensor %3699, %int1_4350, %int0_4351, %int9223372036854775807_4352, %int1_4353 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %3700, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_4354 = torch.constant.int 0
    %3701 = torch.aten.unsqueeze %3700, %int0_4354 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %3701, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_4355 = torch.constant.int 1
    %int0_4356 = torch.constant.int 0
    %int9223372036854775807_4357 = torch.constant.int 9223372036854775807
    %int1_4358 = torch.constant.int 1
    %3702 = torch.aten.slice.Tensor %3701, %int1_4355, %int0_4356, %int9223372036854775807_4357, %int1_4358 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %3702, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_4359 = torch.constant.int 2
    %int0_4360 = torch.constant.int 0
    %int9223372036854775807_4361 = torch.constant.int 9223372036854775807
    %int1_4362 = torch.constant.int 1
    %3703 = torch.aten.slice.Tensor %3702, %int2_4359, %int0_4360, %int9223372036854775807_4361, %int1_4362 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %3703, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_4363 = torch.constant.int 4
    %int1_4364 = torch.constant.int 1
    %int1_4365 = torch.constant.int 1
    %3704 = torch.prim.ListConstruct %int4_4363, %int1_4364, %int1_4365 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3705 = torch.aten.repeat %3703, %3704 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %3705, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_4366 = torch.constant.int 6
    %3706 = torch.prims.convert_element_type %3679, %int6_4366 : !torch.vtensor<[4,?,32,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %3706, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %3707 = torch_c.to_builtin_tensor %3706 : !torch.vtensor<[4,?,32,128],f32> -> tensor<4x?x32x128xf32>
    %3708 = torch_c.to_builtin_tensor %3705 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %3709 = util.call @sharktank_rotary_embedding_4_D_32_128_f32(%3707, %3708) : (tensor<4x?x32x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x32x128xf32>
    %3710 = torch_c.from_builtin_tensor %3709 : tensor<4x?x32x128xf32> -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %3710, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %int5_4367 = torch.constant.int 5
    %3711 = torch.prims.convert_element_type %3710, %int5_4367 : !torch.vtensor<[4,?,32,128],f32>, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %3711, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int131072_4368 = torch.constant.int 131072
    %none_4369 = torch.constant.none
    %none_4370 = torch.constant.none
    %cpu_4371 = torch.constant.device "cpu"
    %false_4372 = torch.constant.bool false
    %3712 = torch.aten.arange %int131072_4368, %none_4369, %none_4370, %cpu_4371, %false_4372 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_4373 = torch.constant.int 0
    %int128_4374 = torch.constant.int 128
    %none_4375 = torch.constant.none
    %none_4376 = torch.constant.none
    %cpu_4377 = torch.constant.device "cpu"
    %false_4378 = torch.constant.bool false
    %3713 = torch.aten.arange.start %int0_4373, %int128_4374, %none_4375, %none_4376, %cpu_4377, %false_4378 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_4379 = torch.constant.int 2
    %3714 = torch.aten.floor_divide.Scalar %3713, %int2_4379 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_4380 = torch.constant.int 6
    %3715 = torch.prims.convert_element_type %3714, %int6_4380 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_4381 = torch.constant.int 128
    %3716 = torch.aten.div.Scalar %3715, %int128_4381 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_4382 = torch.constant.float 2.000000e+00
    %3717 = torch.aten.mul.Scalar %3716, %float2.000000e00_4382 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_4383 = torch.constant.float 5.000000e+05
    %3718 = torch.aten.pow.Scalar %float5.000000e05_4383, %3717 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %3719 = torch.aten.reciprocal %3718 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_4384 = torch.constant.float 1.000000e+00
    %3720 = torch.aten.mul.Scalar %3719, %float1.000000e00_4384 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_4385 = torch.constant.int 1
    %3721 = torch.aten.unsqueeze %3712, %int1_4385 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_4386 = torch.constant.int 0
    %3722 = torch.aten.unsqueeze %3720, %int0_4386 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %3723 = torch.aten.mul.Tensor %3721, %3722 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_4387 = torch.constant.int 1
    %3724 = torch.aten.size.int %3670, %int1_4387 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int0_4388 = torch.constant.int 0
    %3725 = torch.aten.add.int %int0_4388, %3724 : !torch.int, !torch.int -> !torch.int
    %int0_4389 = torch.constant.int 0
    %int0_4390 = torch.constant.int 0
    %int1_4391 = torch.constant.int 1
    %3726 = torch.aten.slice.Tensor %3723, %int0_4389, %int0_4390, %3725, %int1_4391 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %3726, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_4392 = torch.constant.int 1
    %int0_4393 = torch.constant.int 0
    %int9223372036854775807_4394 = torch.constant.int 9223372036854775807
    %int1_4395 = torch.constant.int 1
    %3727 = torch.aten.slice.Tensor %3726, %int1_4392, %int0_4393, %int9223372036854775807_4394, %int1_4395 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %3727, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_4396 = torch.constant.int 1
    %int0_4397 = torch.constant.int 0
    %int9223372036854775807_4398 = torch.constant.int 9223372036854775807
    %int1_4399 = torch.constant.int 1
    %3728 = torch.aten.slice.Tensor %3727, %int1_4396, %int0_4397, %int9223372036854775807_4398, %int1_4399 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %3728, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_4400 = torch.constant.int 0
    %3729 = torch.aten.unsqueeze %3728, %int0_4400 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %3729, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_4401 = torch.constant.int 1
    %int0_4402 = torch.constant.int 0
    %int9223372036854775807_4403 = torch.constant.int 9223372036854775807
    %int1_4404 = torch.constant.int 1
    %3730 = torch.aten.slice.Tensor %3729, %int1_4401, %int0_4402, %int9223372036854775807_4403, %int1_4404 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %3730, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_4405 = torch.constant.int 2
    %int0_4406 = torch.constant.int 0
    %int9223372036854775807_4407 = torch.constant.int 9223372036854775807
    %int1_4408 = torch.constant.int 1
    %3731 = torch.aten.slice.Tensor %3730, %int2_4405, %int0_4406, %int9223372036854775807_4407, %int1_4408 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %3731, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_4409 = torch.constant.int 4
    %int1_4410 = torch.constant.int 1
    %int1_4411 = torch.constant.int 1
    %3732 = torch.prim.ListConstruct %int4_4409, %int1_4410, %int1_4411 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3733 = torch.aten.repeat %3731, %3732 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %3733, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_4412 = torch.constant.int 6
    %3734 = torch.prims.convert_element_type %3681, %int6_4412 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %3734, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %3735 = torch_c.to_builtin_tensor %3734 : !torch.vtensor<[4,?,8,128],f32> -> tensor<4x?x8x128xf32>
    %3736 = torch_c.to_builtin_tensor %3733 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %3737 = util.call @sharktank_rotary_embedding_4_D_8_128_f32(%3735, %3736) : (tensor<4x?x8x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x8x128xf32>
    %3738 = torch_c.from_builtin_tensor %3737 : tensor<4x?x8x128xf32> -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %3738, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %int5_4413 = torch.constant.int 5
    %3739 = torch.prims.convert_element_type %3738, %int5_4413 : !torch.vtensor<[4,?,8,128],f32>, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %3739, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int64_4414 = torch.constant.int 64
    %3740 = torch.aten.mul.Scalar %arg2, %int64_4414 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3740, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int34 = torch.constant.int 34
    %int1_4415 = torch.constant.int 1
    %3741 = torch.aten.add.Scalar %3740, %int34, %int1_4415 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3741, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_4416 = torch.constant.int 4
    %int32_4417 = torch.constant.int 32
    %int8_4418 = torch.constant.int 8
    %int128_4419 = torch.constant.int 128
    %3742 = torch.prim.ListConstruct %int4_4416, %398, %int32_4417, %int8_4418, %int128_4419 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3743 = torch.aten.view %3739, %3742 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3743, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_4420 = torch.constant.int 4
    %3744 = torch.aten.mul.int %int4_4420, %398 : !torch.int, !torch.int -> !torch.int
    %int32_4421 = torch.constant.int 32
    %int8_4422 = torch.constant.int 8
    %int128_4423 = torch.constant.int 128
    %3745 = torch.prim.ListConstruct %3744, %int32_4421, %int8_4422, %int128_4423 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3746 = torch.aten.view %3743, %3745 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %3746, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_4424 = torch.constant.int 4
    %3747 = torch.aten.mul.int %int4_4424, %398 : !torch.int, !torch.int -> !torch.int
    %3748 = torch.prim.ListConstruct %3747 : (!torch.int) -> !torch.list<int>
    %3749 = torch.aten.view %3741, %3748 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3749, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_4425 = torch.constant.int 32
    %int2_4426 = torch.constant.int 2
    %int32_4427 = torch.constant.int 32
    %int8_4428 = torch.constant.int 8
    %int128_4429 = torch.constant.int 128
    %3750 = torch.prim.ListConstruct %389, %int32_4425, %int2_4426, %int32_4427, %int8_4428, %int128_4429 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3751 = torch.aten.view %3583, %3750 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3751, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_4430 = torch.constant.int 32
    %3752 = torch.aten.mul.int %389, %int32_4430 : !torch.int, !torch.int -> !torch.int
    %int2_4431 = torch.constant.int 2
    %3753 = torch.aten.mul.int %3752, %int2_4431 : !torch.int, !torch.int -> !torch.int
    %int32_4432 = torch.constant.int 32
    %int8_4433 = torch.constant.int 8
    %int128_4434 = torch.constant.int 128
    %3754 = torch.prim.ListConstruct %3753, %int32_4432, %int8_4433, %int128_4434 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3755 = torch.aten.view %3751, %3754 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %3755, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %3756 = torch.prim.ListConstruct %3749 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_4435 = torch.constant.bool false
    %3757 = torch.aten.index_put %3755, %3756, %3746, %false_4435 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %3757, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_4436 = torch.constant.int 32
    %int2_4437 = torch.constant.int 2
    %int32_4438 = torch.constant.int 32
    %int8_4439 = torch.constant.int 8
    %int128_4440 = torch.constant.int 128
    %3758 = torch.prim.ListConstruct %389, %int32_4436, %int2_4437, %int32_4438, %int8_4439, %int128_4440 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3759 = torch.aten.view %3757, %3758 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3759, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_4441 = torch.constant.int 2097152
    %3760 = torch.prim.ListConstruct %389, %int2097152_4441 : (!torch.int, !torch.int) -> !torch.list<int>
    %3761 = torch.aten.view %3759, %3760 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %3761, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_4442 = torch.constant.int 32
    %int2_4443 = torch.constant.int 2
    %int32_4444 = torch.constant.int 32
    %int8_4445 = torch.constant.int 8
    %int128_4446 = torch.constant.int 128
    %3762 = torch.prim.ListConstruct %389, %int32_4442, %int2_4443, %int32_4444, %int8_4445, %int128_4446 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3763 = torch.aten.view %3761, %3762 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3763, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_4447 = torch.constant.int 32
    %int8_4448 = torch.constant.int 8
    %int128_4449 = torch.constant.int 128
    %3764 = torch.prim.ListConstruct %3753, %int32_4447, %int8_4448, %int128_4449 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3765 = torch.aten.view %3763, %3764 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %3765, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_4450 = torch.constant.int 4
    %int32_4451 = torch.constant.int 32
    %int8_4452 = torch.constant.int 8
    %int128_4453 = torch.constant.int 128
    %3766 = torch.prim.ListConstruct %int4_4450, %398, %int32_4451, %int8_4452, %int128_4453 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3767 = torch.aten.view %3683, %3766 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3767, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_4454 = torch.constant.int 4
    %3768 = torch.aten.mul.int %int4_4454, %398 : !torch.int, !torch.int -> !torch.int
    %int32_4455 = torch.constant.int 32
    %int8_4456 = torch.constant.int 8
    %int128_4457 = torch.constant.int 128
    %3769 = torch.prim.ListConstruct %3768, %int32_4455, %int8_4456, %int128_4457 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3770 = torch.aten.view %3767, %3769 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %3770, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int1_4458 = torch.constant.int 1
    %int1_4459 = torch.constant.int 1
    %3771 = torch.aten.add.Scalar %3741, %int1_4458, %int1_4459 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3771, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_4460 = torch.constant.int 4
    %3772 = torch.aten.mul.int %int4_4460, %398 : !torch.int, !torch.int -> !torch.int
    %3773 = torch.prim.ListConstruct %3772 : (!torch.int) -> !torch.list<int>
    %3774 = torch.aten.view %3771, %3773 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3774, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %3775 = torch.prim.ListConstruct %3774 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_4461 = torch.constant.bool false
    %3776 = torch.aten.index_put %3765, %3775, %3770, %false_4461 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %3776, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_4462 = torch.constant.int 32
    %int2_4463 = torch.constant.int 2
    %int32_4464 = torch.constant.int 32
    %int8_4465 = torch.constant.int 8
    %int128_4466 = torch.constant.int 128
    %3777 = torch.prim.ListConstruct %389, %int32_4462, %int2_4463, %int32_4464, %int8_4465, %int128_4466 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3778 = torch.aten.view %3776, %3777 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3778, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_4467 = torch.constant.int 2097152
    %3779 = torch.prim.ListConstruct %389, %int2097152_4467 : (!torch.int, !torch.int) -> !torch.list<int>
    %3780 = torch.aten.view %3778, %3779 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %3780, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int-2_4468 = torch.constant.int -2
    %3781 = torch.aten.unsqueeze %3739, %int-2_4468 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %3781, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int4_4469 = torch.constant.int 4
    %int8_4470 = torch.constant.int 8
    %int4_4471 = torch.constant.int 4
    %int128_4472 = torch.constant.int 128
    %3782 = torch.prim.ListConstruct %int4_4469, %3724, %int8_4470, %int4_4471, %int128_4472 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_4473 = torch.constant.bool false
    %3783 = torch.aten.expand %3781, %3782, %false_4473 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3783, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_4474 = torch.constant.int 0
    %3784 = torch.aten.clone %3783, %int0_4474 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3784, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_4475 = torch.constant.int 4
    %int32_4476 = torch.constant.int 32
    %int128_4477 = torch.constant.int 128
    %3785 = torch.prim.ListConstruct %int4_4475, %3724, %int32_4476, %int128_4477 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3786 = torch.aten._unsafe_view %3784, %3785 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %3786, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_4478 = torch.constant.int -2
    %3787 = torch.aten.unsqueeze %3683, %int-2_4478 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %3787, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_4479 = torch.constant.int 1
    %3788 = torch.aten.size.int %3677, %int1_4479 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int4_4480 = torch.constant.int 4
    %int8_4481 = torch.constant.int 8
    %int4_4482 = torch.constant.int 4
    %int128_4483 = torch.constant.int 128
    %3789 = torch.prim.ListConstruct %int4_4480, %3788, %int8_4481, %int4_4482, %int128_4483 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_4484 = torch.constant.bool false
    %3790 = torch.aten.expand %3787, %3789, %false_4484 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3790, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_4485 = torch.constant.int 0
    %3791 = torch.aten.clone %3790, %int0_4485 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3791, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_4486 = torch.constant.int 4
    %int32_4487 = torch.constant.int 32
    %int128_4488 = torch.constant.int 128
    %3792 = torch.prim.ListConstruct %int4_4486, %3788, %int32_4487, %int128_4488 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3793 = torch.aten._unsafe_view %3791, %3792 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %3793, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_4489 = torch.constant.int 1
    %int2_4490 = torch.constant.int 2
    %3794 = torch.aten.transpose.int %3711, %int1_4489, %int2_4490 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %3794, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_4491 = torch.constant.int 1
    %int2_4492 = torch.constant.int 2
    %3795 = torch.aten.transpose.int %3786, %int1_4491, %int2_4492 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %3795, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_4493 = torch.constant.int 1
    %int2_4494 = torch.constant.int 2
    %3796 = torch.aten.transpose.int %3793, %int1_4493, %int2_4494 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %3796, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_4495 = torch.constant.float 0.000000e+00
    %true_4496 = torch.constant.bool true
    %none_4497 = torch.constant.none
    %none_4498 = torch.constant.none
    %3797:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3794, %3795, %3796, %float0.000000e00_4495, %true_4496, %none_4497, %none_4498) : (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?],f32>) 
    torch.bind_symbolic_shape %3797#0, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_4499 = torch.constant.int 1
    %int2_4500 = torch.constant.int 2
    %3798 = torch.aten.transpose.int %3797#0, %int1_4499, %int2_4500 : !torch.vtensor<[4,32,?,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %3798, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_4501 = torch.constant.int 4
    %int4096_4502 = torch.constant.int 4096
    %3799 = torch.prim.ListConstruct %int4_4501, %3696, %int4096_4502 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3800 = torch.aten.view %3798, %3799 : !torch.vtensor<[4,?,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3800, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_4503 = torch.constant.int -2
    %int-1_4504 = torch.constant.int -1
    %3801 = torch.aten.transpose.int %158, %int-2_4503, %int-1_4504 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_4505 = torch.constant.int 4
    %3802 = torch.aten.mul.int %int4_4505, %3696 : !torch.int, !torch.int -> !torch.int
    %int4096_4506 = torch.constant.int 4096
    %3803 = torch.prim.ListConstruct %3802, %int4096_4506 : (!torch.int, !torch.int) -> !torch.list<int>
    %3804 = torch.aten.view %3800, %3803 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3804, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %3805 = torch.aten.mm %3804, %3801 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3805, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_4507 = torch.constant.int 4
    %int4096_4508 = torch.constant.int 4096
    %3806 = torch.prim.ListConstruct %int4_4507, %3696, %int4096_4508 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3807 = torch.aten.view %3805, %3806 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3807, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_4509 = torch.constant.int 1
    %3808 = torch.aten.add.Tensor %3646, %3807, %int1_4509 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3808, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_4510 = torch.constant.int 6
    %3809 = torch.prims.convert_element_type %3808, %int6_4510 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3809, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_4511 = torch.constant.int 2
    %3810 = torch.aten.pow.Tensor_Scalar %3809, %int2_4511 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3810, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_4512 = torch.constant.int -1
    %3811 = torch.prim.ListConstruct %int-1_4512 : (!torch.int) -> !torch.list<int>
    %true_4513 = torch.constant.bool true
    %none_4514 = torch.constant.none
    %3812 = torch.aten.mean.dim %3810, %3811, %true_4513, %none_4514 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3812, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_4515 = torch.constant.float 9.9999997473787516E-6
    %int1_4516 = torch.constant.int 1
    %3813 = torch.aten.add.Scalar %3812, %float9.999990e-06_4515, %int1_4516 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3813, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %3814 = torch.aten.rsqrt %3813 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3814, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %3815 = torch.aten.mul.Tensor %3809, %3814 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3815, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_4517 = torch.constant.int 5
    %3816 = torch.prims.convert_element_type %3815, %int5_4517 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3816, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %3817 = torch.aten.mul.Tensor %159, %3816 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3817, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_4518 = torch.constant.int 5
    %3818 = torch.prims.convert_element_type %3817, %int5_4518 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3818, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_4519 = torch.constant.int -2
    %int-1_4520 = torch.constant.int -1
    %3819 = torch.aten.transpose.int %160, %int-2_4519, %int-1_4520 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_4521 = torch.constant.int 4
    %3820 = torch.aten.mul.int %int4_4521, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_4522 = torch.constant.int 4096
    %3821 = torch.prim.ListConstruct %3820, %int4096_4522 : (!torch.int, !torch.int) -> !torch.list<int>
    %3822 = torch.aten.view %3818, %3821 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3822, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %3823 = torch.aten.mm %3822, %3819 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %3823, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_4523 = torch.constant.int 4
    %int14336_4524 = torch.constant.int 14336
    %3824 = torch.prim.ListConstruct %int4_4523, %306, %int14336_4524 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3825 = torch.aten.view %3823, %3824 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %3825, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %3826 = torch.aten.silu %3825 : !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %3826, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_4525 = torch.constant.int -2
    %int-1_4526 = torch.constant.int -1
    %3827 = torch.aten.transpose.int %161, %int-2_4525, %int-1_4526 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_4527 = torch.constant.int 4
    %3828 = torch.aten.mul.int %int4_4527, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_4528 = torch.constant.int 4096
    %3829 = torch.prim.ListConstruct %3828, %int4096_4528 : (!torch.int, !torch.int) -> !torch.list<int>
    %3830 = torch.aten.view %3818, %3829 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3830, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %3831 = torch.aten.mm %3830, %3827 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %3831, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_4529 = torch.constant.int 4
    %int14336_4530 = torch.constant.int 14336
    %3832 = torch.prim.ListConstruct %int4_4529, %306, %int14336_4530 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3833 = torch.aten.view %3831, %3832 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %3833, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %3834 = torch.aten.mul.Tensor %3826, %3833 : !torch.vtensor<[4,?,14336],f16>, !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %3834, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_4531 = torch.constant.int -2
    %int-1_4532 = torch.constant.int -1
    %3835 = torch.aten.transpose.int %162, %int-2_4531, %int-1_4532 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int1_4533 = torch.constant.int 1
    %3836 = torch.aten.size.int %3825, %int1_4533 : !torch.vtensor<[4,?,14336],f16>, !torch.int -> !torch.int
    %int4_4534 = torch.constant.int 4
    %3837 = torch.aten.mul.int %int4_4534, %3836 : !torch.int, !torch.int -> !torch.int
    %int14336_4535 = torch.constant.int 14336
    %3838 = torch.prim.ListConstruct %3837, %int14336_4535 : (!torch.int, !torch.int) -> !torch.list<int>
    %3839 = torch.aten.view %3834, %3838 : !torch.vtensor<[4,?,14336],f16>, !torch.list<int> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %3839, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %3840 = torch.aten.mm %3839, %3835 : !torch.vtensor<[?,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3840, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_4536 = torch.constant.int 4
    %int4096_4537 = torch.constant.int 4096
    %3841 = torch.prim.ListConstruct %int4_4536, %3836, %int4096_4537 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3842 = torch.aten.view %3840, %3841 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3842, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_4538 = torch.constant.int 1
    %3843 = torch.aten.add.Tensor %3808, %3842, %int1_4538 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3843, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_4539 = torch.constant.int 6
    %3844 = torch.prims.convert_element_type %3843, %int6_4539 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3844, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_4540 = torch.constant.int 2
    %3845 = torch.aten.pow.Tensor_Scalar %3844, %int2_4540 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3845, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_4541 = torch.constant.int -1
    %3846 = torch.prim.ListConstruct %int-1_4541 : (!torch.int) -> !torch.list<int>
    %true_4542 = torch.constant.bool true
    %none_4543 = torch.constant.none
    %3847 = torch.aten.mean.dim %3845, %3846, %true_4542, %none_4543 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3847, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_4544 = torch.constant.float 9.9999997473787516E-6
    %int1_4545 = torch.constant.int 1
    %3848 = torch.aten.add.Scalar %3847, %float9.999990e-06_4544, %int1_4545 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3848, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %3849 = torch.aten.rsqrt %3848 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %3849, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %3850 = torch.aten.mul.Tensor %3844, %3849 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3850, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_4546 = torch.constant.int 5
    %3851 = torch.prims.convert_element_type %3850, %int5_4546 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3851, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %3852 = torch.aten.mul.Tensor %163, %3851 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %3852, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_4547 = torch.constant.int 5
    %3853 = torch.prims.convert_element_type %3852, %int5_4547 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3853, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_4548 = torch.constant.int -2
    %int-1_4549 = torch.constant.int -1
    %3854 = torch.aten.transpose.int %164, %int-2_4548, %int-1_4549 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_4550 = torch.constant.int 4
    %3855 = torch.aten.mul.int %int4_4550, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_4551 = torch.constant.int 4096
    %3856 = torch.prim.ListConstruct %3855, %int4096_4551 : (!torch.int, !torch.int) -> !torch.list<int>
    %3857 = torch.aten.view %3853, %3856 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3857, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %3858 = torch.aten.mm %3857, %3854 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3858, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_4552 = torch.constant.int 4
    %int4096_4553 = torch.constant.int 4096
    %3859 = torch.prim.ListConstruct %int4_4552, %306, %int4096_4553 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3860 = torch.aten.view %3858, %3859 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3860, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_4554 = torch.constant.int -2
    %int-1_4555 = torch.constant.int -1
    %3861 = torch.aten.transpose.int %165, %int-2_4554, %int-1_4555 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_4556 = torch.constant.int 4
    %3862 = torch.aten.mul.int %int4_4556, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_4557 = torch.constant.int 4096
    %3863 = torch.prim.ListConstruct %3862, %int4096_4557 : (!torch.int, !torch.int) -> !torch.list<int>
    %3864 = torch.aten.view %3853, %3863 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3864, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %3865 = torch.aten.mm %3864, %3861 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %3865, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_4558 = torch.constant.int 4
    %int1024_4559 = torch.constant.int 1024
    %3866 = torch.prim.ListConstruct %int4_4558, %306, %int1024_4559 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3867 = torch.aten.view %3865, %3866 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %3867, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int-2_4560 = torch.constant.int -2
    %int-1_4561 = torch.constant.int -1
    %3868 = torch.aten.transpose.int %166, %int-2_4560, %int-1_4561 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_4562 = torch.constant.int 4
    %3869 = torch.aten.mul.int %int4_4562, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_4563 = torch.constant.int 4096
    %3870 = torch.prim.ListConstruct %3869, %int4096_4563 : (!torch.int, !torch.int) -> !torch.list<int>
    %3871 = torch.aten.view %3853, %3870 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %3871, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %3872 = torch.aten.mm %3871, %3868 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %3872, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_4564 = torch.constant.int 4
    %int1024_4565 = torch.constant.int 1024
    %3873 = torch.prim.ListConstruct %int4_4564, %306, %int1024_4565 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3874 = torch.aten.view %3872, %3873 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %3874, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int4_4566 = torch.constant.int 4
    %int32_4567 = torch.constant.int 32
    %int128_4568 = torch.constant.int 128
    %3875 = torch.prim.ListConstruct %int4_4566, %306, %int32_4567, %int128_4568 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3876 = torch.aten.view %3860, %3875 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %3876, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_4569 = torch.constant.int 4
    %int8_4570 = torch.constant.int 8
    %int128_4571 = torch.constant.int 128
    %3877 = torch.prim.ListConstruct %int4_4569, %306, %int8_4570, %int128_4571 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3878 = torch.aten.view %3867, %3877 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %3878, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int4_4572 = torch.constant.int 4
    %int8_4573 = torch.constant.int 8
    %int128_4574 = torch.constant.int 128
    %3879 = torch.prim.ListConstruct %int4_4572, %306, %int8_4573, %int128_4574 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3880 = torch.aten.view %3874, %3879 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %3880, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int131072_4575 = torch.constant.int 131072
    %none_4576 = torch.constant.none
    %none_4577 = torch.constant.none
    %cpu_4578 = torch.constant.device "cpu"
    %false_4579 = torch.constant.bool false
    %3881 = torch.aten.arange %int131072_4575, %none_4576, %none_4577, %cpu_4578, %false_4579 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_4580 = torch.constant.int 0
    %int128_4581 = torch.constant.int 128
    %none_4582 = torch.constant.none
    %none_4583 = torch.constant.none
    %cpu_4584 = torch.constant.device "cpu"
    %false_4585 = torch.constant.bool false
    %3882 = torch.aten.arange.start %int0_4580, %int128_4581, %none_4582, %none_4583, %cpu_4584, %false_4585 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_4586 = torch.constant.int 2
    %3883 = torch.aten.floor_divide.Scalar %3882, %int2_4586 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_4587 = torch.constant.int 6
    %3884 = torch.prims.convert_element_type %3883, %int6_4587 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_4588 = torch.constant.int 128
    %3885 = torch.aten.div.Scalar %3884, %int128_4588 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_4589 = torch.constant.float 2.000000e+00
    %3886 = torch.aten.mul.Scalar %3885, %float2.000000e00_4589 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_4590 = torch.constant.float 5.000000e+05
    %3887 = torch.aten.pow.Scalar %float5.000000e05_4590, %3886 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %3888 = torch.aten.reciprocal %3887 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_4591 = torch.constant.float 1.000000e+00
    %3889 = torch.aten.mul.Scalar %3888, %float1.000000e00_4591 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_4592 = torch.constant.int 1
    %3890 = torch.aten.unsqueeze %3881, %int1_4592 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_4593 = torch.constant.int 0
    %3891 = torch.aten.unsqueeze %3889, %int0_4593 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %3892 = torch.aten.mul.Tensor %3890, %3891 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_4594 = torch.constant.int 1
    %3893 = torch.aten.size.int %3860, %int1_4594 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.int
    %int0_4595 = torch.constant.int 0
    %3894 = torch.aten.add.int %int0_4595, %3893 : !torch.int, !torch.int -> !torch.int
    %int0_4596 = torch.constant.int 0
    %int0_4597 = torch.constant.int 0
    %int1_4598 = torch.constant.int 1
    %3895 = torch.aten.slice.Tensor %3892, %int0_4596, %int0_4597, %3894, %int1_4598 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %3895, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_4599 = torch.constant.int 1
    %int0_4600 = torch.constant.int 0
    %int9223372036854775807_4601 = torch.constant.int 9223372036854775807
    %int1_4602 = torch.constant.int 1
    %3896 = torch.aten.slice.Tensor %3895, %int1_4599, %int0_4600, %int9223372036854775807_4601, %int1_4602 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %3896, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_4603 = torch.constant.int 1
    %int0_4604 = torch.constant.int 0
    %int9223372036854775807_4605 = torch.constant.int 9223372036854775807
    %int1_4606 = torch.constant.int 1
    %3897 = torch.aten.slice.Tensor %3896, %int1_4603, %int0_4604, %int9223372036854775807_4605, %int1_4606 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %3897, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_4607 = torch.constant.int 0
    %3898 = torch.aten.unsqueeze %3897, %int0_4607 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %3898, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_4608 = torch.constant.int 1
    %int0_4609 = torch.constant.int 0
    %int9223372036854775807_4610 = torch.constant.int 9223372036854775807
    %int1_4611 = torch.constant.int 1
    %3899 = torch.aten.slice.Tensor %3898, %int1_4608, %int0_4609, %int9223372036854775807_4610, %int1_4611 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %3899, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_4612 = torch.constant.int 2
    %int0_4613 = torch.constant.int 0
    %int9223372036854775807_4614 = torch.constant.int 9223372036854775807
    %int1_4615 = torch.constant.int 1
    %3900 = torch.aten.slice.Tensor %3899, %int2_4612, %int0_4613, %int9223372036854775807_4614, %int1_4615 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %3900, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_4616 = torch.constant.int 4
    %int1_4617 = torch.constant.int 1
    %int1_4618 = torch.constant.int 1
    %3901 = torch.prim.ListConstruct %int4_4616, %int1_4617, %int1_4618 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3902 = torch.aten.repeat %3900, %3901 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %3902, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_4619 = torch.constant.int 6
    %3903 = torch.prims.convert_element_type %3876, %int6_4619 : !torch.vtensor<[4,?,32,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %3903, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %3904 = torch_c.to_builtin_tensor %3903 : !torch.vtensor<[4,?,32,128],f32> -> tensor<4x?x32x128xf32>
    %3905 = torch_c.to_builtin_tensor %3902 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %3906 = util.call @sharktank_rotary_embedding_4_D_32_128_f32(%3904, %3905) : (tensor<4x?x32x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x32x128xf32>
    %3907 = torch_c.from_builtin_tensor %3906 : tensor<4x?x32x128xf32> -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %3907, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %int5_4620 = torch.constant.int 5
    %3908 = torch.prims.convert_element_type %3907, %int5_4620 : !torch.vtensor<[4,?,32,128],f32>, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %3908, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int131072_4621 = torch.constant.int 131072
    %none_4622 = torch.constant.none
    %none_4623 = torch.constant.none
    %cpu_4624 = torch.constant.device "cpu"
    %false_4625 = torch.constant.bool false
    %3909 = torch.aten.arange %int131072_4621, %none_4622, %none_4623, %cpu_4624, %false_4625 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_4626 = torch.constant.int 0
    %int128_4627 = torch.constant.int 128
    %none_4628 = torch.constant.none
    %none_4629 = torch.constant.none
    %cpu_4630 = torch.constant.device "cpu"
    %false_4631 = torch.constant.bool false
    %3910 = torch.aten.arange.start %int0_4626, %int128_4627, %none_4628, %none_4629, %cpu_4630, %false_4631 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_4632 = torch.constant.int 2
    %3911 = torch.aten.floor_divide.Scalar %3910, %int2_4632 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_4633 = torch.constant.int 6
    %3912 = torch.prims.convert_element_type %3911, %int6_4633 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_4634 = torch.constant.int 128
    %3913 = torch.aten.div.Scalar %3912, %int128_4634 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_4635 = torch.constant.float 2.000000e+00
    %3914 = torch.aten.mul.Scalar %3913, %float2.000000e00_4635 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_4636 = torch.constant.float 5.000000e+05
    %3915 = torch.aten.pow.Scalar %float5.000000e05_4636, %3914 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %3916 = torch.aten.reciprocal %3915 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_4637 = torch.constant.float 1.000000e+00
    %3917 = torch.aten.mul.Scalar %3916, %float1.000000e00_4637 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_4638 = torch.constant.int 1
    %3918 = torch.aten.unsqueeze %3909, %int1_4638 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_4639 = torch.constant.int 0
    %3919 = torch.aten.unsqueeze %3917, %int0_4639 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %3920 = torch.aten.mul.Tensor %3918, %3919 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_4640 = torch.constant.int 1
    %3921 = torch.aten.size.int %3867, %int1_4640 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int0_4641 = torch.constant.int 0
    %3922 = torch.aten.add.int %int0_4641, %3921 : !torch.int, !torch.int -> !torch.int
    %int0_4642 = torch.constant.int 0
    %int0_4643 = torch.constant.int 0
    %int1_4644 = torch.constant.int 1
    %3923 = torch.aten.slice.Tensor %3920, %int0_4642, %int0_4643, %3922, %int1_4644 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %3923, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_4645 = torch.constant.int 1
    %int0_4646 = torch.constant.int 0
    %int9223372036854775807_4647 = torch.constant.int 9223372036854775807
    %int1_4648 = torch.constant.int 1
    %3924 = torch.aten.slice.Tensor %3923, %int1_4645, %int0_4646, %int9223372036854775807_4647, %int1_4648 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %3924, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_4649 = torch.constant.int 1
    %int0_4650 = torch.constant.int 0
    %int9223372036854775807_4651 = torch.constant.int 9223372036854775807
    %int1_4652 = torch.constant.int 1
    %3925 = torch.aten.slice.Tensor %3924, %int1_4649, %int0_4650, %int9223372036854775807_4651, %int1_4652 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %3925, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_4653 = torch.constant.int 0
    %3926 = torch.aten.unsqueeze %3925, %int0_4653 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %3926, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_4654 = torch.constant.int 1
    %int0_4655 = torch.constant.int 0
    %int9223372036854775807_4656 = torch.constant.int 9223372036854775807
    %int1_4657 = torch.constant.int 1
    %3927 = torch.aten.slice.Tensor %3926, %int1_4654, %int0_4655, %int9223372036854775807_4656, %int1_4657 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %3927, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_4658 = torch.constant.int 2
    %int0_4659 = torch.constant.int 0
    %int9223372036854775807_4660 = torch.constant.int 9223372036854775807
    %int1_4661 = torch.constant.int 1
    %3928 = torch.aten.slice.Tensor %3927, %int2_4658, %int0_4659, %int9223372036854775807_4660, %int1_4661 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %3928, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_4662 = torch.constant.int 4
    %int1_4663 = torch.constant.int 1
    %int1_4664 = torch.constant.int 1
    %3929 = torch.prim.ListConstruct %int4_4662, %int1_4663, %int1_4664 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3930 = torch.aten.repeat %3928, %3929 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %3930, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_4665 = torch.constant.int 6
    %3931 = torch.prims.convert_element_type %3878, %int6_4665 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %3931, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %3932 = torch_c.to_builtin_tensor %3931 : !torch.vtensor<[4,?,8,128],f32> -> tensor<4x?x8x128xf32>
    %3933 = torch_c.to_builtin_tensor %3930 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %3934 = util.call @sharktank_rotary_embedding_4_D_8_128_f32(%3932, %3933) : (tensor<4x?x8x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x8x128xf32>
    %3935 = torch_c.from_builtin_tensor %3934 : tensor<4x?x8x128xf32> -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %3935, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %int5_4666 = torch.constant.int 5
    %3936 = torch.prims.convert_element_type %3935, %int5_4666 : !torch.vtensor<[4,?,8,128],f32>, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %3936, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int64_4667 = torch.constant.int 64
    %3937 = torch.aten.mul.Scalar %arg2, %int64_4667 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3937, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int36 = torch.constant.int 36
    %int1_4668 = torch.constant.int 1
    %3938 = torch.aten.add.Scalar %3937, %int36, %int1_4668 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3938, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_4669 = torch.constant.int 4
    %int32_4670 = torch.constant.int 32
    %int8_4671 = torch.constant.int 8
    %int128_4672 = torch.constant.int 128
    %3939 = torch.prim.ListConstruct %int4_4669, %398, %int32_4670, %int8_4671, %int128_4672 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3940 = torch.aten.view %3936, %3939 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3940, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_4673 = torch.constant.int 4
    %3941 = torch.aten.mul.int %int4_4673, %398 : !torch.int, !torch.int -> !torch.int
    %int32_4674 = torch.constant.int 32
    %int8_4675 = torch.constant.int 8
    %int128_4676 = torch.constant.int 128
    %3942 = torch.prim.ListConstruct %3941, %int32_4674, %int8_4675, %int128_4676 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3943 = torch.aten.view %3940, %3942 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %3943, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_4677 = torch.constant.int 4
    %3944 = torch.aten.mul.int %int4_4677, %398 : !torch.int, !torch.int -> !torch.int
    %3945 = torch.prim.ListConstruct %3944 : (!torch.int) -> !torch.list<int>
    %3946 = torch.aten.view %3938, %3945 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3946, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_4678 = torch.constant.int 32
    %int2_4679 = torch.constant.int 2
    %int32_4680 = torch.constant.int 32
    %int8_4681 = torch.constant.int 8
    %int128_4682 = torch.constant.int 128
    %3947 = torch.prim.ListConstruct %389, %int32_4678, %int2_4679, %int32_4680, %int8_4681, %int128_4682 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3948 = torch.aten.view %3780, %3947 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3948, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_4683 = torch.constant.int 32
    %3949 = torch.aten.mul.int %389, %int32_4683 : !torch.int, !torch.int -> !torch.int
    %int2_4684 = torch.constant.int 2
    %3950 = torch.aten.mul.int %3949, %int2_4684 : !torch.int, !torch.int -> !torch.int
    %int32_4685 = torch.constant.int 32
    %int8_4686 = torch.constant.int 8
    %int128_4687 = torch.constant.int 128
    %3951 = torch.prim.ListConstruct %3950, %int32_4685, %int8_4686, %int128_4687 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3952 = torch.aten.view %3948, %3951 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %3952, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %3953 = torch.prim.ListConstruct %3946 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_4688 = torch.constant.bool false
    %3954 = torch.aten.index_put %3952, %3953, %3943, %false_4688 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %3954, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_4689 = torch.constant.int 32
    %int2_4690 = torch.constant.int 2
    %int32_4691 = torch.constant.int 32
    %int8_4692 = torch.constant.int 8
    %int128_4693 = torch.constant.int 128
    %3955 = torch.prim.ListConstruct %389, %int32_4689, %int2_4690, %int32_4691, %int8_4692, %int128_4693 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3956 = torch.aten.view %3954, %3955 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3956, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_4694 = torch.constant.int 2097152
    %3957 = torch.prim.ListConstruct %389, %int2097152_4694 : (!torch.int, !torch.int) -> !torch.list<int>
    %3958 = torch.aten.view %3956, %3957 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %3958, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_4695 = torch.constant.int 32
    %int2_4696 = torch.constant.int 2
    %int32_4697 = torch.constant.int 32
    %int8_4698 = torch.constant.int 8
    %int128_4699 = torch.constant.int 128
    %3959 = torch.prim.ListConstruct %389, %int32_4695, %int2_4696, %int32_4697, %int8_4698, %int128_4699 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3960 = torch.aten.view %3958, %3959 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3960, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_4700 = torch.constant.int 32
    %int8_4701 = torch.constant.int 8
    %int128_4702 = torch.constant.int 128
    %3961 = torch.prim.ListConstruct %3950, %int32_4700, %int8_4701, %int128_4702 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3962 = torch.aten.view %3960, %3961 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %3962, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_4703 = torch.constant.int 4
    %int32_4704 = torch.constant.int 32
    %int8_4705 = torch.constant.int 8
    %int128_4706 = torch.constant.int 128
    %3963 = torch.prim.ListConstruct %int4_4703, %398, %int32_4704, %int8_4705, %int128_4706 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3964 = torch.aten.view %3880, %3963 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3964, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_4707 = torch.constant.int 4
    %3965 = torch.aten.mul.int %int4_4707, %398 : !torch.int, !torch.int -> !torch.int
    %int32_4708 = torch.constant.int 32
    %int8_4709 = torch.constant.int 8
    %int128_4710 = torch.constant.int 128
    %3966 = torch.prim.ListConstruct %3965, %int32_4708, %int8_4709, %int128_4710 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3967 = torch.aten.view %3964, %3966 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %3967, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int1_4711 = torch.constant.int 1
    %int1_4712 = torch.constant.int 1
    %3968 = torch.aten.add.Scalar %3938, %int1_4711, %int1_4712 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3968, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_4713 = torch.constant.int 4
    %3969 = torch.aten.mul.int %int4_4713, %398 : !torch.int, !torch.int -> !torch.int
    %3970 = torch.prim.ListConstruct %3969 : (!torch.int) -> !torch.list<int>
    %3971 = torch.aten.view %3968, %3970 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3971, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %3972 = torch.prim.ListConstruct %3971 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_4714 = torch.constant.bool false
    %3973 = torch.aten.index_put %3962, %3972, %3967, %false_4714 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %3973, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_4715 = torch.constant.int 32
    %int2_4716 = torch.constant.int 2
    %int32_4717 = torch.constant.int 32
    %int8_4718 = torch.constant.int 8
    %int128_4719 = torch.constant.int 128
    %3974 = torch.prim.ListConstruct %389, %int32_4715, %int2_4716, %int32_4717, %int8_4718, %int128_4719 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3975 = torch.aten.view %3973, %3974 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3975, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_4720 = torch.constant.int 2097152
    %3976 = torch.prim.ListConstruct %389, %int2097152_4720 : (!torch.int, !torch.int) -> !torch.list<int>
    %3977 = torch.aten.view %3975, %3976 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %3977, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int-2_4721 = torch.constant.int -2
    %3978 = torch.aten.unsqueeze %3936, %int-2_4721 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %3978, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int4_4722 = torch.constant.int 4
    %int8_4723 = torch.constant.int 8
    %int4_4724 = torch.constant.int 4
    %int128_4725 = torch.constant.int 128
    %3979 = torch.prim.ListConstruct %int4_4722, %3921, %int8_4723, %int4_4724, %int128_4725 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_4726 = torch.constant.bool false
    %3980 = torch.aten.expand %3978, %3979, %false_4726 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3980, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_4727 = torch.constant.int 0
    %3981 = torch.aten.clone %3980, %int0_4727 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3981, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_4728 = torch.constant.int 4
    %int32_4729 = torch.constant.int 32
    %int128_4730 = torch.constant.int 128
    %3982 = torch.prim.ListConstruct %int4_4728, %3921, %int32_4729, %int128_4730 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3983 = torch.aten._unsafe_view %3981, %3982 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %3983, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_4731 = torch.constant.int -2
    %3984 = torch.aten.unsqueeze %3880, %int-2_4731 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %3984, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_4732 = torch.constant.int 1
    %3985 = torch.aten.size.int %3874, %int1_4732 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int4_4733 = torch.constant.int 4
    %int8_4734 = torch.constant.int 8
    %int4_4735 = torch.constant.int 4
    %int128_4736 = torch.constant.int 128
    %3986 = torch.prim.ListConstruct %int4_4733, %3985, %int8_4734, %int4_4735, %int128_4736 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_4737 = torch.constant.bool false
    %3987 = torch.aten.expand %3984, %3986, %false_4737 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3987, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_4738 = torch.constant.int 0
    %3988 = torch.aten.clone %3987, %int0_4738 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3988, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_4739 = torch.constant.int 4
    %int32_4740 = torch.constant.int 32
    %int128_4741 = torch.constant.int 128
    %3989 = torch.prim.ListConstruct %int4_4739, %3985, %int32_4740, %int128_4741 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3990 = torch.aten._unsafe_view %3988, %3989 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %3990, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_4742 = torch.constant.int 1
    %int2_4743 = torch.constant.int 2
    %3991 = torch.aten.transpose.int %3908, %int1_4742, %int2_4743 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %3991, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_4744 = torch.constant.int 1
    %int2_4745 = torch.constant.int 2
    %3992 = torch.aten.transpose.int %3983, %int1_4744, %int2_4745 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %3992, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_4746 = torch.constant.int 1
    %int2_4747 = torch.constant.int 2
    %3993 = torch.aten.transpose.int %3990, %int1_4746, %int2_4747 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %3993, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_4748 = torch.constant.float 0.000000e+00
    %true_4749 = torch.constant.bool true
    %none_4750 = torch.constant.none
    %none_4751 = torch.constant.none
    %3994:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3991, %3992, %3993, %float0.000000e00_4748, %true_4749, %none_4750, %none_4751) : (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?],f32>) 
    torch.bind_symbolic_shape %3994#0, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_4752 = torch.constant.int 1
    %int2_4753 = torch.constant.int 2
    %3995 = torch.aten.transpose.int %3994#0, %int1_4752, %int2_4753 : !torch.vtensor<[4,32,?,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %3995, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_4754 = torch.constant.int 4
    %int4096_4755 = torch.constant.int 4096
    %3996 = torch.prim.ListConstruct %int4_4754, %3893, %int4096_4755 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3997 = torch.aten.view %3995, %3996 : !torch.vtensor<[4,?,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %3997, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_4756 = torch.constant.int -2
    %int-1_4757 = torch.constant.int -1
    %3998 = torch.aten.transpose.int %167, %int-2_4756, %int-1_4757 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_4758 = torch.constant.int 4
    %3999 = torch.aten.mul.int %int4_4758, %3893 : !torch.int, !torch.int -> !torch.int
    %int4096_4759 = torch.constant.int 4096
    %4000 = torch.prim.ListConstruct %3999, %int4096_4759 : (!torch.int, !torch.int) -> !torch.list<int>
    %4001 = torch.aten.view %3997, %4000 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4001, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %4002 = torch.aten.mm %4001, %3998 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4002, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_4760 = torch.constant.int 4
    %int4096_4761 = torch.constant.int 4096
    %4003 = torch.prim.ListConstruct %int4_4760, %3893, %int4096_4761 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4004 = torch.aten.view %4002, %4003 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4004, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_4762 = torch.constant.int 1
    %4005 = torch.aten.add.Tensor %3843, %4004, %int1_4762 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4005, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_4763 = torch.constant.int 6
    %4006 = torch.prims.convert_element_type %4005, %int6_4763 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4006, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_4764 = torch.constant.int 2
    %4007 = torch.aten.pow.Tensor_Scalar %4006, %int2_4764 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4007, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_4765 = torch.constant.int -1
    %4008 = torch.prim.ListConstruct %int-1_4765 : (!torch.int) -> !torch.list<int>
    %true_4766 = torch.constant.bool true
    %none_4767 = torch.constant.none
    %4009 = torch.aten.mean.dim %4007, %4008, %true_4766, %none_4767 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4009, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_4768 = torch.constant.float 9.9999997473787516E-6
    %int1_4769 = torch.constant.int 1
    %4010 = torch.aten.add.Scalar %4009, %float9.999990e-06_4768, %int1_4769 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4010, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %4011 = torch.aten.rsqrt %4010 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4011, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %4012 = torch.aten.mul.Tensor %4006, %4011 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4012, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_4770 = torch.constant.int 5
    %4013 = torch.prims.convert_element_type %4012, %int5_4770 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4013, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %4014 = torch.aten.mul.Tensor %168, %4013 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4014, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_4771 = torch.constant.int 5
    %4015 = torch.prims.convert_element_type %4014, %int5_4771 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4015, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_4772 = torch.constant.int -2
    %int-1_4773 = torch.constant.int -1
    %4016 = torch.aten.transpose.int %169, %int-2_4772, %int-1_4773 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_4774 = torch.constant.int 4
    %4017 = torch.aten.mul.int %int4_4774, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_4775 = torch.constant.int 4096
    %4018 = torch.prim.ListConstruct %4017, %int4096_4775 : (!torch.int, !torch.int) -> !torch.list<int>
    %4019 = torch.aten.view %4015, %4018 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4019, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %4020 = torch.aten.mm %4019, %4016 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %4020, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_4776 = torch.constant.int 4
    %int14336_4777 = torch.constant.int 14336
    %4021 = torch.prim.ListConstruct %int4_4776, %306, %int14336_4777 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4022 = torch.aten.view %4020, %4021 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %4022, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %4023 = torch.aten.silu %4022 : !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %4023, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_4778 = torch.constant.int -2
    %int-1_4779 = torch.constant.int -1
    %4024 = torch.aten.transpose.int %170, %int-2_4778, %int-1_4779 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_4780 = torch.constant.int 4
    %4025 = torch.aten.mul.int %int4_4780, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_4781 = torch.constant.int 4096
    %4026 = torch.prim.ListConstruct %4025, %int4096_4781 : (!torch.int, !torch.int) -> !torch.list<int>
    %4027 = torch.aten.view %4015, %4026 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4027, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %4028 = torch.aten.mm %4027, %4024 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %4028, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_4782 = torch.constant.int 4
    %int14336_4783 = torch.constant.int 14336
    %4029 = torch.prim.ListConstruct %int4_4782, %306, %int14336_4783 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4030 = torch.aten.view %4028, %4029 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %4030, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %4031 = torch.aten.mul.Tensor %4023, %4030 : !torch.vtensor<[4,?,14336],f16>, !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %4031, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_4784 = torch.constant.int -2
    %int-1_4785 = torch.constant.int -1
    %4032 = torch.aten.transpose.int %171, %int-2_4784, %int-1_4785 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int1_4786 = torch.constant.int 1
    %4033 = torch.aten.size.int %4022, %int1_4786 : !torch.vtensor<[4,?,14336],f16>, !torch.int -> !torch.int
    %int4_4787 = torch.constant.int 4
    %4034 = torch.aten.mul.int %int4_4787, %4033 : !torch.int, !torch.int -> !torch.int
    %int14336_4788 = torch.constant.int 14336
    %4035 = torch.prim.ListConstruct %4034, %int14336_4788 : (!torch.int, !torch.int) -> !torch.list<int>
    %4036 = torch.aten.view %4031, %4035 : !torch.vtensor<[4,?,14336],f16>, !torch.list<int> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %4036, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %4037 = torch.aten.mm %4036, %4032 : !torch.vtensor<[?,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4037, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_4789 = torch.constant.int 4
    %int4096_4790 = torch.constant.int 4096
    %4038 = torch.prim.ListConstruct %int4_4789, %4033, %int4096_4790 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4039 = torch.aten.view %4037, %4038 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4039, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_4791 = torch.constant.int 1
    %4040 = torch.aten.add.Tensor %4005, %4039, %int1_4791 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4040, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_4792 = torch.constant.int 6
    %4041 = torch.prims.convert_element_type %4040, %int6_4792 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4041, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_4793 = torch.constant.int 2
    %4042 = torch.aten.pow.Tensor_Scalar %4041, %int2_4793 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4042, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_4794 = torch.constant.int -1
    %4043 = torch.prim.ListConstruct %int-1_4794 : (!torch.int) -> !torch.list<int>
    %true_4795 = torch.constant.bool true
    %none_4796 = torch.constant.none
    %4044 = torch.aten.mean.dim %4042, %4043, %true_4795, %none_4796 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4044, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_4797 = torch.constant.float 9.9999997473787516E-6
    %int1_4798 = torch.constant.int 1
    %4045 = torch.aten.add.Scalar %4044, %float9.999990e-06_4797, %int1_4798 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4045, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %4046 = torch.aten.rsqrt %4045 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4046, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %4047 = torch.aten.mul.Tensor %4041, %4046 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4047, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_4799 = torch.constant.int 5
    %4048 = torch.prims.convert_element_type %4047, %int5_4799 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4048, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %4049 = torch.aten.mul.Tensor %172, %4048 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4049, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_4800 = torch.constant.int 5
    %4050 = torch.prims.convert_element_type %4049, %int5_4800 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4050, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_4801 = torch.constant.int -2
    %int-1_4802 = torch.constant.int -1
    %4051 = torch.aten.transpose.int %173, %int-2_4801, %int-1_4802 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_4803 = torch.constant.int 4
    %4052 = torch.aten.mul.int %int4_4803, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_4804 = torch.constant.int 4096
    %4053 = torch.prim.ListConstruct %4052, %int4096_4804 : (!torch.int, !torch.int) -> !torch.list<int>
    %4054 = torch.aten.view %4050, %4053 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4054, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %4055 = torch.aten.mm %4054, %4051 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4055, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_4805 = torch.constant.int 4
    %int4096_4806 = torch.constant.int 4096
    %4056 = torch.prim.ListConstruct %int4_4805, %306, %int4096_4806 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4057 = torch.aten.view %4055, %4056 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4057, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_4807 = torch.constant.int -2
    %int-1_4808 = torch.constant.int -1
    %4058 = torch.aten.transpose.int %174, %int-2_4807, %int-1_4808 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_4809 = torch.constant.int 4
    %4059 = torch.aten.mul.int %int4_4809, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_4810 = torch.constant.int 4096
    %4060 = torch.prim.ListConstruct %4059, %int4096_4810 : (!torch.int, !torch.int) -> !torch.list<int>
    %4061 = torch.aten.view %4050, %4060 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4061, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %4062 = torch.aten.mm %4061, %4058 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %4062, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_4811 = torch.constant.int 4
    %int1024_4812 = torch.constant.int 1024
    %4063 = torch.prim.ListConstruct %int4_4811, %306, %int1024_4812 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4064 = torch.aten.view %4062, %4063 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %4064, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int-2_4813 = torch.constant.int -2
    %int-1_4814 = torch.constant.int -1
    %4065 = torch.aten.transpose.int %175, %int-2_4813, %int-1_4814 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_4815 = torch.constant.int 4
    %4066 = torch.aten.mul.int %int4_4815, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_4816 = torch.constant.int 4096
    %4067 = torch.prim.ListConstruct %4066, %int4096_4816 : (!torch.int, !torch.int) -> !torch.list<int>
    %4068 = torch.aten.view %4050, %4067 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4068, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %4069 = torch.aten.mm %4068, %4065 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %4069, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_4817 = torch.constant.int 4
    %int1024_4818 = torch.constant.int 1024
    %4070 = torch.prim.ListConstruct %int4_4817, %306, %int1024_4818 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4071 = torch.aten.view %4069, %4070 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %4071, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int4_4819 = torch.constant.int 4
    %int32_4820 = torch.constant.int 32
    %int128_4821 = torch.constant.int 128
    %4072 = torch.prim.ListConstruct %int4_4819, %306, %int32_4820, %int128_4821 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4073 = torch.aten.view %4057, %4072 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %4073, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_4822 = torch.constant.int 4
    %int8_4823 = torch.constant.int 8
    %int128_4824 = torch.constant.int 128
    %4074 = torch.prim.ListConstruct %int4_4822, %306, %int8_4823, %int128_4824 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4075 = torch.aten.view %4064, %4074 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %4075, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int4_4825 = torch.constant.int 4
    %int8_4826 = torch.constant.int 8
    %int128_4827 = torch.constant.int 128
    %4076 = torch.prim.ListConstruct %int4_4825, %306, %int8_4826, %int128_4827 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4077 = torch.aten.view %4071, %4076 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %4077, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int131072_4828 = torch.constant.int 131072
    %none_4829 = torch.constant.none
    %none_4830 = torch.constant.none
    %cpu_4831 = torch.constant.device "cpu"
    %false_4832 = torch.constant.bool false
    %4078 = torch.aten.arange %int131072_4828, %none_4829, %none_4830, %cpu_4831, %false_4832 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_4833 = torch.constant.int 0
    %int128_4834 = torch.constant.int 128
    %none_4835 = torch.constant.none
    %none_4836 = torch.constant.none
    %cpu_4837 = torch.constant.device "cpu"
    %false_4838 = torch.constant.bool false
    %4079 = torch.aten.arange.start %int0_4833, %int128_4834, %none_4835, %none_4836, %cpu_4837, %false_4838 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_4839 = torch.constant.int 2
    %4080 = torch.aten.floor_divide.Scalar %4079, %int2_4839 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_4840 = torch.constant.int 6
    %4081 = torch.prims.convert_element_type %4080, %int6_4840 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_4841 = torch.constant.int 128
    %4082 = torch.aten.div.Scalar %4081, %int128_4841 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_4842 = torch.constant.float 2.000000e+00
    %4083 = torch.aten.mul.Scalar %4082, %float2.000000e00_4842 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_4843 = torch.constant.float 5.000000e+05
    %4084 = torch.aten.pow.Scalar %float5.000000e05_4843, %4083 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %4085 = torch.aten.reciprocal %4084 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_4844 = torch.constant.float 1.000000e+00
    %4086 = torch.aten.mul.Scalar %4085, %float1.000000e00_4844 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_4845 = torch.constant.int 1
    %4087 = torch.aten.unsqueeze %4078, %int1_4845 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_4846 = torch.constant.int 0
    %4088 = torch.aten.unsqueeze %4086, %int0_4846 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %4089 = torch.aten.mul.Tensor %4087, %4088 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_4847 = torch.constant.int 1
    %4090 = torch.aten.size.int %4057, %int1_4847 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.int
    %int0_4848 = torch.constant.int 0
    %4091 = torch.aten.add.int %int0_4848, %4090 : !torch.int, !torch.int -> !torch.int
    %int0_4849 = torch.constant.int 0
    %int0_4850 = torch.constant.int 0
    %int1_4851 = torch.constant.int 1
    %4092 = torch.aten.slice.Tensor %4089, %int0_4849, %int0_4850, %4091, %int1_4851 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %4092, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_4852 = torch.constant.int 1
    %int0_4853 = torch.constant.int 0
    %int9223372036854775807_4854 = torch.constant.int 9223372036854775807
    %int1_4855 = torch.constant.int 1
    %4093 = torch.aten.slice.Tensor %4092, %int1_4852, %int0_4853, %int9223372036854775807_4854, %int1_4855 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %4093, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_4856 = torch.constant.int 1
    %int0_4857 = torch.constant.int 0
    %int9223372036854775807_4858 = torch.constant.int 9223372036854775807
    %int1_4859 = torch.constant.int 1
    %4094 = torch.aten.slice.Tensor %4093, %int1_4856, %int0_4857, %int9223372036854775807_4858, %int1_4859 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %4094, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_4860 = torch.constant.int 0
    %4095 = torch.aten.unsqueeze %4094, %int0_4860 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %4095, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_4861 = torch.constant.int 1
    %int0_4862 = torch.constant.int 0
    %int9223372036854775807_4863 = torch.constant.int 9223372036854775807
    %int1_4864 = torch.constant.int 1
    %4096 = torch.aten.slice.Tensor %4095, %int1_4861, %int0_4862, %int9223372036854775807_4863, %int1_4864 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %4096, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_4865 = torch.constant.int 2
    %int0_4866 = torch.constant.int 0
    %int9223372036854775807_4867 = torch.constant.int 9223372036854775807
    %int1_4868 = torch.constant.int 1
    %4097 = torch.aten.slice.Tensor %4096, %int2_4865, %int0_4866, %int9223372036854775807_4867, %int1_4868 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %4097, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_4869 = torch.constant.int 4
    %int1_4870 = torch.constant.int 1
    %int1_4871 = torch.constant.int 1
    %4098 = torch.prim.ListConstruct %int4_4869, %int1_4870, %int1_4871 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4099 = torch.aten.repeat %4097, %4098 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %4099, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_4872 = torch.constant.int 6
    %4100 = torch.prims.convert_element_type %4073, %int6_4872 : !torch.vtensor<[4,?,32,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %4100, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %4101 = torch_c.to_builtin_tensor %4100 : !torch.vtensor<[4,?,32,128],f32> -> tensor<4x?x32x128xf32>
    %4102 = torch_c.to_builtin_tensor %4099 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %4103 = util.call @sharktank_rotary_embedding_4_D_32_128_f32(%4101, %4102) : (tensor<4x?x32x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x32x128xf32>
    %4104 = torch_c.from_builtin_tensor %4103 : tensor<4x?x32x128xf32> -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %4104, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %int5_4873 = torch.constant.int 5
    %4105 = torch.prims.convert_element_type %4104, %int5_4873 : !torch.vtensor<[4,?,32,128],f32>, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %4105, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int131072_4874 = torch.constant.int 131072
    %none_4875 = torch.constant.none
    %none_4876 = torch.constant.none
    %cpu_4877 = torch.constant.device "cpu"
    %false_4878 = torch.constant.bool false
    %4106 = torch.aten.arange %int131072_4874, %none_4875, %none_4876, %cpu_4877, %false_4878 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_4879 = torch.constant.int 0
    %int128_4880 = torch.constant.int 128
    %none_4881 = torch.constant.none
    %none_4882 = torch.constant.none
    %cpu_4883 = torch.constant.device "cpu"
    %false_4884 = torch.constant.bool false
    %4107 = torch.aten.arange.start %int0_4879, %int128_4880, %none_4881, %none_4882, %cpu_4883, %false_4884 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_4885 = torch.constant.int 2
    %4108 = torch.aten.floor_divide.Scalar %4107, %int2_4885 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_4886 = torch.constant.int 6
    %4109 = torch.prims.convert_element_type %4108, %int6_4886 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_4887 = torch.constant.int 128
    %4110 = torch.aten.div.Scalar %4109, %int128_4887 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_4888 = torch.constant.float 2.000000e+00
    %4111 = torch.aten.mul.Scalar %4110, %float2.000000e00_4888 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_4889 = torch.constant.float 5.000000e+05
    %4112 = torch.aten.pow.Scalar %float5.000000e05_4889, %4111 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %4113 = torch.aten.reciprocal %4112 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_4890 = torch.constant.float 1.000000e+00
    %4114 = torch.aten.mul.Scalar %4113, %float1.000000e00_4890 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_4891 = torch.constant.int 1
    %4115 = torch.aten.unsqueeze %4106, %int1_4891 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_4892 = torch.constant.int 0
    %4116 = torch.aten.unsqueeze %4114, %int0_4892 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %4117 = torch.aten.mul.Tensor %4115, %4116 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_4893 = torch.constant.int 1
    %4118 = torch.aten.size.int %4064, %int1_4893 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int0_4894 = torch.constant.int 0
    %4119 = torch.aten.add.int %int0_4894, %4118 : !torch.int, !torch.int -> !torch.int
    %int0_4895 = torch.constant.int 0
    %int0_4896 = torch.constant.int 0
    %int1_4897 = torch.constant.int 1
    %4120 = torch.aten.slice.Tensor %4117, %int0_4895, %int0_4896, %4119, %int1_4897 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %4120, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_4898 = torch.constant.int 1
    %int0_4899 = torch.constant.int 0
    %int9223372036854775807_4900 = torch.constant.int 9223372036854775807
    %int1_4901 = torch.constant.int 1
    %4121 = torch.aten.slice.Tensor %4120, %int1_4898, %int0_4899, %int9223372036854775807_4900, %int1_4901 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %4121, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_4902 = torch.constant.int 1
    %int0_4903 = torch.constant.int 0
    %int9223372036854775807_4904 = torch.constant.int 9223372036854775807
    %int1_4905 = torch.constant.int 1
    %4122 = torch.aten.slice.Tensor %4121, %int1_4902, %int0_4903, %int9223372036854775807_4904, %int1_4905 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %4122, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_4906 = torch.constant.int 0
    %4123 = torch.aten.unsqueeze %4122, %int0_4906 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %4123, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_4907 = torch.constant.int 1
    %int0_4908 = torch.constant.int 0
    %int9223372036854775807_4909 = torch.constant.int 9223372036854775807
    %int1_4910 = torch.constant.int 1
    %4124 = torch.aten.slice.Tensor %4123, %int1_4907, %int0_4908, %int9223372036854775807_4909, %int1_4910 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %4124, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_4911 = torch.constant.int 2
    %int0_4912 = torch.constant.int 0
    %int9223372036854775807_4913 = torch.constant.int 9223372036854775807
    %int1_4914 = torch.constant.int 1
    %4125 = torch.aten.slice.Tensor %4124, %int2_4911, %int0_4912, %int9223372036854775807_4913, %int1_4914 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %4125, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_4915 = torch.constant.int 4
    %int1_4916 = torch.constant.int 1
    %int1_4917 = torch.constant.int 1
    %4126 = torch.prim.ListConstruct %int4_4915, %int1_4916, %int1_4917 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4127 = torch.aten.repeat %4125, %4126 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %4127, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_4918 = torch.constant.int 6
    %4128 = torch.prims.convert_element_type %4075, %int6_4918 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %4128, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %4129 = torch_c.to_builtin_tensor %4128 : !torch.vtensor<[4,?,8,128],f32> -> tensor<4x?x8x128xf32>
    %4130 = torch_c.to_builtin_tensor %4127 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %4131 = util.call @sharktank_rotary_embedding_4_D_8_128_f32(%4129, %4130) : (tensor<4x?x8x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x8x128xf32>
    %4132 = torch_c.from_builtin_tensor %4131 : tensor<4x?x8x128xf32> -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %4132, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %int5_4919 = torch.constant.int 5
    %4133 = torch.prims.convert_element_type %4132, %int5_4919 : !torch.vtensor<[4,?,8,128],f32>, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %4133, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int64_4920 = torch.constant.int 64
    %4134 = torch.aten.mul.Scalar %arg2, %int64_4920 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4134, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int38 = torch.constant.int 38
    %int1_4921 = torch.constant.int 1
    %4135 = torch.aten.add.Scalar %4134, %int38, %int1_4921 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4135, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_4922 = torch.constant.int 4
    %int32_4923 = torch.constant.int 32
    %int8_4924 = torch.constant.int 8
    %int128_4925 = torch.constant.int 128
    %4136 = torch.prim.ListConstruct %int4_4922, %398, %int32_4923, %int8_4924, %int128_4925 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4137 = torch.aten.view %4133, %4136 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %4137, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_4926 = torch.constant.int 4
    %4138 = torch.aten.mul.int %int4_4926, %398 : !torch.int, !torch.int -> !torch.int
    %int32_4927 = torch.constant.int 32
    %int8_4928 = torch.constant.int 8
    %int128_4929 = torch.constant.int 128
    %4139 = torch.prim.ListConstruct %4138, %int32_4927, %int8_4928, %int128_4929 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4140 = torch.aten.view %4137, %4139 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %4140, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_4930 = torch.constant.int 4
    %4141 = torch.aten.mul.int %int4_4930, %398 : !torch.int, !torch.int -> !torch.int
    %4142 = torch.prim.ListConstruct %4141 : (!torch.int) -> !torch.list<int>
    %4143 = torch.aten.view %4135, %4142 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4143, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_4931 = torch.constant.int 32
    %int2_4932 = torch.constant.int 2
    %int32_4933 = torch.constant.int 32
    %int8_4934 = torch.constant.int 8
    %int128_4935 = torch.constant.int 128
    %4144 = torch.prim.ListConstruct %389, %int32_4931, %int2_4932, %int32_4933, %int8_4934, %int128_4935 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4145 = torch.aten.view %3977, %4144 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4145, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_4936 = torch.constant.int 32
    %4146 = torch.aten.mul.int %389, %int32_4936 : !torch.int, !torch.int -> !torch.int
    %int2_4937 = torch.constant.int 2
    %4147 = torch.aten.mul.int %4146, %int2_4937 : !torch.int, !torch.int -> !torch.int
    %int32_4938 = torch.constant.int 32
    %int8_4939 = torch.constant.int 8
    %int128_4940 = torch.constant.int 128
    %4148 = torch.prim.ListConstruct %4147, %int32_4938, %int8_4939, %int128_4940 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4149 = torch.aten.view %4145, %4148 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %4149, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %4150 = torch.prim.ListConstruct %4143 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_4941 = torch.constant.bool false
    %4151 = torch.aten.index_put %4149, %4150, %4140, %false_4941 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %4151, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_4942 = torch.constant.int 32
    %int2_4943 = torch.constant.int 2
    %int32_4944 = torch.constant.int 32
    %int8_4945 = torch.constant.int 8
    %int128_4946 = torch.constant.int 128
    %4152 = torch.prim.ListConstruct %389, %int32_4942, %int2_4943, %int32_4944, %int8_4945, %int128_4946 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4153 = torch.aten.view %4151, %4152 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4153, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_4947 = torch.constant.int 2097152
    %4154 = torch.prim.ListConstruct %389, %int2097152_4947 : (!torch.int, !torch.int) -> !torch.list<int>
    %4155 = torch.aten.view %4153, %4154 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %4155, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_4948 = torch.constant.int 32
    %int2_4949 = torch.constant.int 2
    %int32_4950 = torch.constant.int 32
    %int8_4951 = torch.constant.int 8
    %int128_4952 = torch.constant.int 128
    %4156 = torch.prim.ListConstruct %389, %int32_4948, %int2_4949, %int32_4950, %int8_4951, %int128_4952 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4157 = torch.aten.view %4155, %4156 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4157, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_4953 = torch.constant.int 32
    %int8_4954 = torch.constant.int 8
    %int128_4955 = torch.constant.int 128
    %4158 = torch.prim.ListConstruct %4147, %int32_4953, %int8_4954, %int128_4955 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4159 = torch.aten.view %4157, %4158 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %4159, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_4956 = torch.constant.int 4
    %int32_4957 = torch.constant.int 32
    %int8_4958 = torch.constant.int 8
    %int128_4959 = torch.constant.int 128
    %4160 = torch.prim.ListConstruct %int4_4956, %398, %int32_4957, %int8_4958, %int128_4959 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4161 = torch.aten.view %4077, %4160 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %4161, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_4960 = torch.constant.int 4
    %4162 = torch.aten.mul.int %int4_4960, %398 : !torch.int, !torch.int -> !torch.int
    %int32_4961 = torch.constant.int 32
    %int8_4962 = torch.constant.int 8
    %int128_4963 = torch.constant.int 128
    %4163 = torch.prim.ListConstruct %4162, %int32_4961, %int8_4962, %int128_4963 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4164 = torch.aten.view %4161, %4163 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %4164, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int1_4964 = torch.constant.int 1
    %int1_4965 = torch.constant.int 1
    %4165 = torch.aten.add.Scalar %4135, %int1_4964, %int1_4965 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4165, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_4966 = torch.constant.int 4
    %4166 = torch.aten.mul.int %int4_4966, %398 : !torch.int, !torch.int -> !torch.int
    %4167 = torch.prim.ListConstruct %4166 : (!torch.int) -> !torch.list<int>
    %4168 = torch.aten.view %4165, %4167 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4168, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %4169 = torch.prim.ListConstruct %4168 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_4967 = torch.constant.bool false
    %4170 = torch.aten.index_put %4159, %4169, %4164, %false_4967 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %4170, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_4968 = torch.constant.int 32
    %int2_4969 = torch.constant.int 2
    %int32_4970 = torch.constant.int 32
    %int8_4971 = torch.constant.int 8
    %int128_4972 = torch.constant.int 128
    %4171 = torch.prim.ListConstruct %389, %int32_4968, %int2_4969, %int32_4970, %int8_4971, %int128_4972 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4172 = torch.aten.view %4170, %4171 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4172, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_4973 = torch.constant.int 2097152
    %4173 = torch.prim.ListConstruct %389, %int2097152_4973 : (!torch.int, !torch.int) -> !torch.list<int>
    %4174 = torch.aten.view %4172, %4173 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %4174, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int-2_4974 = torch.constant.int -2
    %4175 = torch.aten.unsqueeze %4133, %int-2_4974 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %4175, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int4_4975 = torch.constant.int 4
    %int8_4976 = torch.constant.int 8
    %int4_4977 = torch.constant.int 4
    %int128_4978 = torch.constant.int 128
    %4176 = torch.prim.ListConstruct %int4_4975, %4118, %int8_4976, %int4_4977, %int128_4978 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_4979 = torch.constant.bool false
    %4177 = torch.aten.expand %4175, %4176, %false_4979 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4177, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_4980 = torch.constant.int 0
    %4178 = torch.aten.clone %4177, %int0_4980 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4178, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_4981 = torch.constant.int 4
    %int32_4982 = torch.constant.int 32
    %int128_4983 = torch.constant.int 128
    %4179 = torch.prim.ListConstruct %int4_4981, %4118, %int32_4982, %int128_4983 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4180 = torch.aten._unsafe_view %4178, %4179 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %4180, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_4984 = torch.constant.int -2
    %4181 = torch.aten.unsqueeze %4077, %int-2_4984 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %4181, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_4985 = torch.constant.int 1
    %4182 = torch.aten.size.int %4071, %int1_4985 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int4_4986 = torch.constant.int 4
    %int8_4987 = torch.constant.int 8
    %int4_4988 = torch.constant.int 4
    %int128_4989 = torch.constant.int 128
    %4183 = torch.prim.ListConstruct %int4_4986, %4182, %int8_4987, %int4_4988, %int128_4989 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_4990 = torch.constant.bool false
    %4184 = torch.aten.expand %4181, %4183, %false_4990 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4184, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_4991 = torch.constant.int 0
    %4185 = torch.aten.clone %4184, %int0_4991 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4185, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_4992 = torch.constant.int 4
    %int32_4993 = torch.constant.int 32
    %int128_4994 = torch.constant.int 128
    %4186 = torch.prim.ListConstruct %int4_4992, %4182, %int32_4993, %int128_4994 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4187 = torch.aten._unsafe_view %4185, %4186 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %4187, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_4995 = torch.constant.int 1
    %int2_4996 = torch.constant.int 2
    %4188 = torch.aten.transpose.int %4105, %int1_4995, %int2_4996 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %4188, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_4997 = torch.constant.int 1
    %int2_4998 = torch.constant.int 2
    %4189 = torch.aten.transpose.int %4180, %int1_4997, %int2_4998 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %4189, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_4999 = torch.constant.int 1
    %int2_5000 = torch.constant.int 2
    %4190 = torch.aten.transpose.int %4187, %int1_4999, %int2_5000 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %4190, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_5001 = torch.constant.float 0.000000e+00
    %true_5002 = torch.constant.bool true
    %none_5003 = torch.constant.none
    %none_5004 = torch.constant.none
    %4191:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4188, %4189, %4190, %float0.000000e00_5001, %true_5002, %none_5003, %none_5004) : (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?],f32>) 
    torch.bind_symbolic_shape %4191#0, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_5005 = torch.constant.int 1
    %int2_5006 = torch.constant.int 2
    %4192 = torch.aten.transpose.int %4191#0, %int1_5005, %int2_5006 : !torch.vtensor<[4,32,?,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %4192, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_5007 = torch.constant.int 4
    %int4096_5008 = torch.constant.int 4096
    %4193 = torch.prim.ListConstruct %int4_5007, %4090, %int4096_5008 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4194 = torch.aten.view %4192, %4193 : !torch.vtensor<[4,?,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4194, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_5009 = torch.constant.int -2
    %int-1_5010 = torch.constant.int -1
    %4195 = torch.aten.transpose.int %176, %int-2_5009, %int-1_5010 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_5011 = torch.constant.int 4
    %4196 = torch.aten.mul.int %int4_5011, %4090 : !torch.int, !torch.int -> !torch.int
    %int4096_5012 = torch.constant.int 4096
    %4197 = torch.prim.ListConstruct %4196, %int4096_5012 : (!torch.int, !torch.int) -> !torch.list<int>
    %4198 = torch.aten.view %4194, %4197 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4198, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %4199 = torch.aten.mm %4198, %4195 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4199, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_5013 = torch.constant.int 4
    %int4096_5014 = torch.constant.int 4096
    %4200 = torch.prim.ListConstruct %int4_5013, %4090, %int4096_5014 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4201 = torch.aten.view %4199, %4200 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4201, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_5015 = torch.constant.int 1
    %4202 = torch.aten.add.Tensor %4040, %4201, %int1_5015 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4202, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_5016 = torch.constant.int 6
    %4203 = torch.prims.convert_element_type %4202, %int6_5016 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4203, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_5017 = torch.constant.int 2
    %4204 = torch.aten.pow.Tensor_Scalar %4203, %int2_5017 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4204, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_5018 = torch.constant.int -1
    %4205 = torch.prim.ListConstruct %int-1_5018 : (!torch.int) -> !torch.list<int>
    %true_5019 = torch.constant.bool true
    %none_5020 = torch.constant.none
    %4206 = torch.aten.mean.dim %4204, %4205, %true_5019, %none_5020 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4206, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_5021 = torch.constant.float 9.9999997473787516E-6
    %int1_5022 = torch.constant.int 1
    %4207 = torch.aten.add.Scalar %4206, %float9.999990e-06_5021, %int1_5022 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4207, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %4208 = torch.aten.rsqrt %4207 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4208, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %4209 = torch.aten.mul.Tensor %4203, %4208 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4209, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_5023 = torch.constant.int 5
    %4210 = torch.prims.convert_element_type %4209, %int5_5023 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4210, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %4211 = torch.aten.mul.Tensor %177, %4210 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4211, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_5024 = torch.constant.int 5
    %4212 = torch.prims.convert_element_type %4211, %int5_5024 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4212, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_5025 = torch.constant.int -2
    %int-1_5026 = torch.constant.int -1
    %4213 = torch.aten.transpose.int %178, %int-2_5025, %int-1_5026 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_5027 = torch.constant.int 4
    %4214 = torch.aten.mul.int %int4_5027, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_5028 = torch.constant.int 4096
    %4215 = torch.prim.ListConstruct %4214, %int4096_5028 : (!torch.int, !torch.int) -> !torch.list<int>
    %4216 = torch.aten.view %4212, %4215 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4216, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %4217 = torch.aten.mm %4216, %4213 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %4217, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_5029 = torch.constant.int 4
    %int14336_5030 = torch.constant.int 14336
    %4218 = torch.prim.ListConstruct %int4_5029, %306, %int14336_5030 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4219 = torch.aten.view %4217, %4218 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %4219, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %4220 = torch.aten.silu %4219 : !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %4220, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_5031 = torch.constant.int -2
    %int-1_5032 = torch.constant.int -1
    %4221 = torch.aten.transpose.int %179, %int-2_5031, %int-1_5032 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_5033 = torch.constant.int 4
    %4222 = torch.aten.mul.int %int4_5033, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_5034 = torch.constant.int 4096
    %4223 = torch.prim.ListConstruct %4222, %int4096_5034 : (!torch.int, !torch.int) -> !torch.list<int>
    %4224 = torch.aten.view %4212, %4223 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4224, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %4225 = torch.aten.mm %4224, %4221 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %4225, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_5035 = torch.constant.int 4
    %int14336_5036 = torch.constant.int 14336
    %4226 = torch.prim.ListConstruct %int4_5035, %306, %int14336_5036 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4227 = torch.aten.view %4225, %4226 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %4227, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %4228 = torch.aten.mul.Tensor %4220, %4227 : !torch.vtensor<[4,?,14336],f16>, !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %4228, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_5037 = torch.constant.int -2
    %int-1_5038 = torch.constant.int -1
    %4229 = torch.aten.transpose.int %180, %int-2_5037, %int-1_5038 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int1_5039 = torch.constant.int 1
    %4230 = torch.aten.size.int %4219, %int1_5039 : !torch.vtensor<[4,?,14336],f16>, !torch.int -> !torch.int
    %int4_5040 = torch.constant.int 4
    %4231 = torch.aten.mul.int %int4_5040, %4230 : !torch.int, !torch.int -> !torch.int
    %int14336_5041 = torch.constant.int 14336
    %4232 = torch.prim.ListConstruct %4231, %int14336_5041 : (!torch.int, !torch.int) -> !torch.list<int>
    %4233 = torch.aten.view %4228, %4232 : !torch.vtensor<[4,?,14336],f16>, !torch.list<int> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %4233, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %4234 = torch.aten.mm %4233, %4229 : !torch.vtensor<[?,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4234, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_5042 = torch.constant.int 4
    %int4096_5043 = torch.constant.int 4096
    %4235 = torch.prim.ListConstruct %int4_5042, %4230, %int4096_5043 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4236 = torch.aten.view %4234, %4235 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4236, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_5044 = torch.constant.int 1
    %4237 = torch.aten.add.Tensor %4202, %4236, %int1_5044 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4237, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_5045 = torch.constant.int 6
    %4238 = torch.prims.convert_element_type %4237, %int6_5045 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4238, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_5046 = torch.constant.int 2
    %4239 = torch.aten.pow.Tensor_Scalar %4238, %int2_5046 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4239, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_5047 = torch.constant.int -1
    %4240 = torch.prim.ListConstruct %int-1_5047 : (!torch.int) -> !torch.list<int>
    %true_5048 = torch.constant.bool true
    %none_5049 = torch.constant.none
    %4241 = torch.aten.mean.dim %4239, %4240, %true_5048, %none_5049 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4241, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_5050 = torch.constant.float 9.9999997473787516E-6
    %int1_5051 = torch.constant.int 1
    %4242 = torch.aten.add.Scalar %4241, %float9.999990e-06_5050, %int1_5051 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4242, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %4243 = torch.aten.rsqrt %4242 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4243, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %4244 = torch.aten.mul.Tensor %4238, %4243 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4244, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_5052 = torch.constant.int 5
    %4245 = torch.prims.convert_element_type %4244, %int5_5052 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4245, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %4246 = torch.aten.mul.Tensor %181, %4245 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4246, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_5053 = torch.constant.int 5
    %4247 = torch.prims.convert_element_type %4246, %int5_5053 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4247, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_5054 = torch.constant.int -2
    %int-1_5055 = torch.constant.int -1
    %4248 = torch.aten.transpose.int %182, %int-2_5054, %int-1_5055 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_5056 = torch.constant.int 4
    %4249 = torch.aten.mul.int %int4_5056, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_5057 = torch.constant.int 4096
    %4250 = torch.prim.ListConstruct %4249, %int4096_5057 : (!torch.int, !torch.int) -> !torch.list<int>
    %4251 = torch.aten.view %4247, %4250 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4251, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %4252 = torch.aten.mm %4251, %4248 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4252, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_5058 = torch.constant.int 4
    %int4096_5059 = torch.constant.int 4096
    %4253 = torch.prim.ListConstruct %int4_5058, %306, %int4096_5059 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4254 = torch.aten.view %4252, %4253 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4254, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_5060 = torch.constant.int -2
    %int-1_5061 = torch.constant.int -1
    %4255 = torch.aten.transpose.int %183, %int-2_5060, %int-1_5061 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_5062 = torch.constant.int 4
    %4256 = torch.aten.mul.int %int4_5062, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_5063 = torch.constant.int 4096
    %4257 = torch.prim.ListConstruct %4256, %int4096_5063 : (!torch.int, !torch.int) -> !torch.list<int>
    %4258 = torch.aten.view %4247, %4257 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4258, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %4259 = torch.aten.mm %4258, %4255 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %4259, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_5064 = torch.constant.int 4
    %int1024_5065 = torch.constant.int 1024
    %4260 = torch.prim.ListConstruct %int4_5064, %306, %int1024_5065 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4261 = torch.aten.view %4259, %4260 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %4261, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int-2_5066 = torch.constant.int -2
    %int-1_5067 = torch.constant.int -1
    %4262 = torch.aten.transpose.int %184, %int-2_5066, %int-1_5067 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_5068 = torch.constant.int 4
    %4263 = torch.aten.mul.int %int4_5068, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_5069 = torch.constant.int 4096
    %4264 = torch.prim.ListConstruct %4263, %int4096_5069 : (!torch.int, !torch.int) -> !torch.list<int>
    %4265 = torch.aten.view %4247, %4264 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4265, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %4266 = torch.aten.mm %4265, %4262 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %4266, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_5070 = torch.constant.int 4
    %int1024_5071 = torch.constant.int 1024
    %4267 = torch.prim.ListConstruct %int4_5070, %306, %int1024_5071 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4268 = torch.aten.view %4266, %4267 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %4268, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int4_5072 = torch.constant.int 4
    %int32_5073 = torch.constant.int 32
    %int128_5074 = torch.constant.int 128
    %4269 = torch.prim.ListConstruct %int4_5072, %306, %int32_5073, %int128_5074 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4270 = torch.aten.view %4254, %4269 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %4270, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_5075 = torch.constant.int 4
    %int8_5076 = torch.constant.int 8
    %int128_5077 = torch.constant.int 128
    %4271 = torch.prim.ListConstruct %int4_5075, %306, %int8_5076, %int128_5077 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4272 = torch.aten.view %4261, %4271 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %4272, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int4_5078 = torch.constant.int 4
    %int8_5079 = torch.constant.int 8
    %int128_5080 = torch.constant.int 128
    %4273 = torch.prim.ListConstruct %int4_5078, %306, %int8_5079, %int128_5080 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4274 = torch.aten.view %4268, %4273 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %4274, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int131072_5081 = torch.constant.int 131072
    %none_5082 = torch.constant.none
    %none_5083 = torch.constant.none
    %cpu_5084 = torch.constant.device "cpu"
    %false_5085 = torch.constant.bool false
    %4275 = torch.aten.arange %int131072_5081, %none_5082, %none_5083, %cpu_5084, %false_5085 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_5086 = torch.constant.int 0
    %int128_5087 = torch.constant.int 128
    %none_5088 = torch.constant.none
    %none_5089 = torch.constant.none
    %cpu_5090 = torch.constant.device "cpu"
    %false_5091 = torch.constant.bool false
    %4276 = torch.aten.arange.start %int0_5086, %int128_5087, %none_5088, %none_5089, %cpu_5090, %false_5091 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_5092 = torch.constant.int 2
    %4277 = torch.aten.floor_divide.Scalar %4276, %int2_5092 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_5093 = torch.constant.int 6
    %4278 = torch.prims.convert_element_type %4277, %int6_5093 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_5094 = torch.constant.int 128
    %4279 = torch.aten.div.Scalar %4278, %int128_5094 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_5095 = torch.constant.float 2.000000e+00
    %4280 = torch.aten.mul.Scalar %4279, %float2.000000e00_5095 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_5096 = torch.constant.float 5.000000e+05
    %4281 = torch.aten.pow.Scalar %float5.000000e05_5096, %4280 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %4282 = torch.aten.reciprocal %4281 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_5097 = torch.constant.float 1.000000e+00
    %4283 = torch.aten.mul.Scalar %4282, %float1.000000e00_5097 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_5098 = torch.constant.int 1
    %4284 = torch.aten.unsqueeze %4275, %int1_5098 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_5099 = torch.constant.int 0
    %4285 = torch.aten.unsqueeze %4283, %int0_5099 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %4286 = torch.aten.mul.Tensor %4284, %4285 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_5100 = torch.constant.int 1
    %4287 = torch.aten.size.int %4254, %int1_5100 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.int
    %int0_5101 = torch.constant.int 0
    %4288 = torch.aten.add.int %int0_5101, %4287 : !torch.int, !torch.int -> !torch.int
    %int0_5102 = torch.constant.int 0
    %int0_5103 = torch.constant.int 0
    %int1_5104 = torch.constant.int 1
    %4289 = torch.aten.slice.Tensor %4286, %int0_5102, %int0_5103, %4288, %int1_5104 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %4289, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_5105 = torch.constant.int 1
    %int0_5106 = torch.constant.int 0
    %int9223372036854775807_5107 = torch.constant.int 9223372036854775807
    %int1_5108 = torch.constant.int 1
    %4290 = torch.aten.slice.Tensor %4289, %int1_5105, %int0_5106, %int9223372036854775807_5107, %int1_5108 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %4290, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_5109 = torch.constant.int 1
    %int0_5110 = torch.constant.int 0
    %int9223372036854775807_5111 = torch.constant.int 9223372036854775807
    %int1_5112 = torch.constant.int 1
    %4291 = torch.aten.slice.Tensor %4290, %int1_5109, %int0_5110, %int9223372036854775807_5111, %int1_5112 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %4291, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_5113 = torch.constant.int 0
    %4292 = torch.aten.unsqueeze %4291, %int0_5113 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %4292, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_5114 = torch.constant.int 1
    %int0_5115 = torch.constant.int 0
    %int9223372036854775807_5116 = torch.constant.int 9223372036854775807
    %int1_5117 = torch.constant.int 1
    %4293 = torch.aten.slice.Tensor %4292, %int1_5114, %int0_5115, %int9223372036854775807_5116, %int1_5117 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %4293, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_5118 = torch.constant.int 2
    %int0_5119 = torch.constant.int 0
    %int9223372036854775807_5120 = torch.constant.int 9223372036854775807
    %int1_5121 = torch.constant.int 1
    %4294 = torch.aten.slice.Tensor %4293, %int2_5118, %int0_5119, %int9223372036854775807_5120, %int1_5121 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %4294, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_5122 = torch.constant.int 4
    %int1_5123 = torch.constant.int 1
    %int1_5124 = torch.constant.int 1
    %4295 = torch.prim.ListConstruct %int4_5122, %int1_5123, %int1_5124 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4296 = torch.aten.repeat %4294, %4295 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %4296, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_5125 = torch.constant.int 6
    %4297 = torch.prims.convert_element_type %4270, %int6_5125 : !torch.vtensor<[4,?,32,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %4297, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %4298 = torch_c.to_builtin_tensor %4297 : !torch.vtensor<[4,?,32,128],f32> -> tensor<4x?x32x128xf32>
    %4299 = torch_c.to_builtin_tensor %4296 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %4300 = util.call @sharktank_rotary_embedding_4_D_32_128_f32(%4298, %4299) : (tensor<4x?x32x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x32x128xf32>
    %4301 = torch_c.from_builtin_tensor %4300 : tensor<4x?x32x128xf32> -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %4301, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %int5_5126 = torch.constant.int 5
    %4302 = torch.prims.convert_element_type %4301, %int5_5126 : !torch.vtensor<[4,?,32,128],f32>, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %4302, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int131072_5127 = torch.constant.int 131072
    %none_5128 = torch.constant.none
    %none_5129 = torch.constant.none
    %cpu_5130 = torch.constant.device "cpu"
    %false_5131 = torch.constant.bool false
    %4303 = torch.aten.arange %int131072_5127, %none_5128, %none_5129, %cpu_5130, %false_5131 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_5132 = torch.constant.int 0
    %int128_5133 = torch.constant.int 128
    %none_5134 = torch.constant.none
    %none_5135 = torch.constant.none
    %cpu_5136 = torch.constant.device "cpu"
    %false_5137 = torch.constant.bool false
    %4304 = torch.aten.arange.start %int0_5132, %int128_5133, %none_5134, %none_5135, %cpu_5136, %false_5137 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_5138 = torch.constant.int 2
    %4305 = torch.aten.floor_divide.Scalar %4304, %int2_5138 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_5139 = torch.constant.int 6
    %4306 = torch.prims.convert_element_type %4305, %int6_5139 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_5140 = torch.constant.int 128
    %4307 = torch.aten.div.Scalar %4306, %int128_5140 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_5141 = torch.constant.float 2.000000e+00
    %4308 = torch.aten.mul.Scalar %4307, %float2.000000e00_5141 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_5142 = torch.constant.float 5.000000e+05
    %4309 = torch.aten.pow.Scalar %float5.000000e05_5142, %4308 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %4310 = torch.aten.reciprocal %4309 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_5143 = torch.constant.float 1.000000e+00
    %4311 = torch.aten.mul.Scalar %4310, %float1.000000e00_5143 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_5144 = torch.constant.int 1
    %4312 = torch.aten.unsqueeze %4303, %int1_5144 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_5145 = torch.constant.int 0
    %4313 = torch.aten.unsqueeze %4311, %int0_5145 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %4314 = torch.aten.mul.Tensor %4312, %4313 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_5146 = torch.constant.int 1
    %4315 = torch.aten.size.int %4261, %int1_5146 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int0_5147 = torch.constant.int 0
    %4316 = torch.aten.add.int %int0_5147, %4315 : !torch.int, !torch.int -> !torch.int
    %int0_5148 = torch.constant.int 0
    %int0_5149 = torch.constant.int 0
    %int1_5150 = torch.constant.int 1
    %4317 = torch.aten.slice.Tensor %4314, %int0_5148, %int0_5149, %4316, %int1_5150 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %4317, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_5151 = torch.constant.int 1
    %int0_5152 = torch.constant.int 0
    %int9223372036854775807_5153 = torch.constant.int 9223372036854775807
    %int1_5154 = torch.constant.int 1
    %4318 = torch.aten.slice.Tensor %4317, %int1_5151, %int0_5152, %int9223372036854775807_5153, %int1_5154 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %4318, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_5155 = torch.constant.int 1
    %int0_5156 = torch.constant.int 0
    %int9223372036854775807_5157 = torch.constant.int 9223372036854775807
    %int1_5158 = torch.constant.int 1
    %4319 = torch.aten.slice.Tensor %4318, %int1_5155, %int0_5156, %int9223372036854775807_5157, %int1_5158 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %4319, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_5159 = torch.constant.int 0
    %4320 = torch.aten.unsqueeze %4319, %int0_5159 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %4320, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_5160 = torch.constant.int 1
    %int0_5161 = torch.constant.int 0
    %int9223372036854775807_5162 = torch.constant.int 9223372036854775807
    %int1_5163 = torch.constant.int 1
    %4321 = torch.aten.slice.Tensor %4320, %int1_5160, %int0_5161, %int9223372036854775807_5162, %int1_5163 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %4321, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_5164 = torch.constant.int 2
    %int0_5165 = torch.constant.int 0
    %int9223372036854775807_5166 = torch.constant.int 9223372036854775807
    %int1_5167 = torch.constant.int 1
    %4322 = torch.aten.slice.Tensor %4321, %int2_5164, %int0_5165, %int9223372036854775807_5166, %int1_5167 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %4322, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_5168 = torch.constant.int 4
    %int1_5169 = torch.constant.int 1
    %int1_5170 = torch.constant.int 1
    %4323 = torch.prim.ListConstruct %int4_5168, %int1_5169, %int1_5170 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4324 = torch.aten.repeat %4322, %4323 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %4324, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_5171 = torch.constant.int 6
    %4325 = torch.prims.convert_element_type %4272, %int6_5171 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %4325, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %4326 = torch_c.to_builtin_tensor %4325 : !torch.vtensor<[4,?,8,128],f32> -> tensor<4x?x8x128xf32>
    %4327 = torch_c.to_builtin_tensor %4324 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %4328 = util.call @sharktank_rotary_embedding_4_D_8_128_f32(%4326, %4327) : (tensor<4x?x8x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x8x128xf32>
    %4329 = torch_c.from_builtin_tensor %4328 : tensor<4x?x8x128xf32> -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %4329, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %int5_5172 = torch.constant.int 5
    %4330 = torch.prims.convert_element_type %4329, %int5_5172 : !torch.vtensor<[4,?,8,128],f32>, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %4330, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int64_5173 = torch.constant.int 64
    %4331 = torch.aten.mul.Scalar %arg2, %int64_5173 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4331, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int40 = torch.constant.int 40
    %int1_5174 = torch.constant.int 1
    %4332 = torch.aten.add.Scalar %4331, %int40, %int1_5174 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4332, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_5175 = torch.constant.int 4
    %int32_5176 = torch.constant.int 32
    %int8_5177 = torch.constant.int 8
    %int128_5178 = torch.constant.int 128
    %4333 = torch.prim.ListConstruct %int4_5175, %398, %int32_5176, %int8_5177, %int128_5178 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4334 = torch.aten.view %4330, %4333 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %4334, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_5179 = torch.constant.int 4
    %4335 = torch.aten.mul.int %int4_5179, %398 : !torch.int, !torch.int -> !torch.int
    %int32_5180 = torch.constant.int 32
    %int8_5181 = torch.constant.int 8
    %int128_5182 = torch.constant.int 128
    %4336 = torch.prim.ListConstruct %4335, %int32_5180, %int8_5181, %int128_5182 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4337 = torch.aten.view %4334, %4336 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %4337, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_5183 = torch.constant.int 4
    %4338 = torch.aten.mul.int %int4_5183, %398 : !torch.int, !torch.int -> !torch.int
    %4339 = torch.prim.ListConstruct %4338 : (!torch.int) -> !torch.list<int>
    %4340 = torch.aten.view %4332, %4339 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4340, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_5184 = torch.constant.int 32
    %int2_5185 = torch.constant.int 2
    %int32_5186 = torch.constant.int 32
    %int8_5187 = torch.constant.int 8
    %int128_5188 = torch.constant.int 128
    %4341 = torch.prim.ListConstruct %389, %int32_5184, %int2_5185, %int32_5186, %int8_5187, %int128_5188 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4342 = torch.aten.view %4174, %4341 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4342, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_5189 = torch.constant.int 32
    %4343 = torch.aten.mul.int %389, %int32_5189 : !torch.int, !torch.int -> !torch.int
    %int2_5190 = torch.constant.int 2
    %4344 = torch.aten.mul.int %4343, %int2_5190 : !torch.int, !torch.int -> !torch.int
    %int32_5191 = torch.constant.int 32
    %int8_5192 = torch.constant.int 8
    %int128_5193 = torch.constant.int 128
    %4345 = torch.prim.ListConstruct %4344, %int32_5191, %int8_5192, %int128_5193 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4346 = torch.aten.view %4342, %4345 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %4346, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %4347 = torch.prim.ListConstruct %4340 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_5194 = torch.constant.bool false
    %4348 = torch.aten.index_put %4346, %4347, %4337, %false_5194 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %4348, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_5195 = torch.constant.int 32
    %int2_5196 = torch.constant.int 2
    %int32_5197 = torch.constant.int 32
    %int8_5198 = torch.constant.int 8
    %int128_5199 = torch.constant.int 128
    %4349 = torch.prim.ListConstruct %389, %int32_5195, %int2_5196, %int32_5197, %int8_5198, %int128_5199 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4350 = torch.aten.view %4348, %4349 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4350, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_5200 = torch.constant.int 2097152
    %4351 = torch.prim.ListConstruct %389, %int2097152_5200 : (!torch.int, !torch.int) -> !torch.list<int>
    %4352 = torch.aten.view %4350, %4351 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %4352, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_5201 = torch.constant.int 32
    %int2_5202 = torch.constant.int 2
    %int32_5203 = torch.constant.int 32
    %int8_5204 = torch.constant.int 8
    %int128_5205 = torch.constant.int 128
    %4353 = torch.prim.ListConstruct %389, %int32_5201, %int2_5202, %int32_5203, %int8_5204, %int128_5205 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4354 = torch.aten.view %4352, %4353 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4354, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_5206 = torch.constant.int 32
    %int8_5207 = torch.constant.int 8
    %int128_5208 = torch.constant.int 128
    %4355 = torch.prim.ListConstruct %4344, %int32_5206, %int8_5207, %int128_5208 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4356 = torch.aten.view %4354, %4355 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %4356, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_5209 = torch.constant.int 4
    %int32_5210 = torch.constant.int 32
    %int8_5211 = torch.constant.int 8
    %int128_5212 = torch.constant.int 128
    %4357 = torch.prim.ListConstruct %int4_5209, %398, %int32_5210, %int8_5211, %int128_5212 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4358 = torch.aten.view %4274, %4357 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %4358, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_5213 = torch.constant.int 4
    %4359 = torch.aten.mul.int %int4_5213, %398 : !torch.int, !torch.int -> !torch.int
    %int32_5214 = torch.constant.int 32
    %int8_5215 = torch.constant.int 8
    %int128_5216 = torch.constant.int 128
    %4360 = torch.prim.ListConstruct %4359, %int32_5214, %int8_5215, %int128_5216 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4361 = torch.aten.view %4358, %4360 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %4361, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int1_5217 = torch.constant.int 1
    %int1_5218 = torch.constant.int 1
    %4362 = torch.aten.add.Scalar %4332, %int1_5217, %int1_5218 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4362, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_5219 = torch.constant.int 4
    %4363 = torch.aten.mul.int %int4_5219, %398 : !torch.int, !torch.int -> !torch.int
    %4364 = torch.prim.ListConstruct %4363 : (!torch.int) -> !torch.list<int>
    %4365 = torch.aten.view %4362, %4364 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4365, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %4366 = torch.prim.ListConstruct %4365 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_5220 = torch.constant.bool false
    %4367 = torch.aten.index_put %4356, %4366, %4361, %false_5220 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %4367, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_5221 = torch.constant.int 32
    %int2_5222 = torch.constant.int 2
    %int32_5223 = torch.constant.int 32
    %int8_5224 = torch.constant.int 8
    %int128_5225 = torch.constant.int 128
    %4368 = torch.prim.ListConstruct %389, %int32_5221, %int2_5222, %int32_5223, %int8_5224, %int128_5225 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4369 = torch.aten.view %4367, %4368 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4369, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_5226 = torch.constant.int 2097152
    %4370 = torch.prim.ListConstruct %389, %int2097152_5226 : (!torch.int, !torch.int) -> !torch.list<int>
    %4371 = torch.aten.view %4369, %4370 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %4371, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int-2_5227 = torch.constant.int -2
    %4372 = torch.aten.unsqueeze %4330, %int-2_5227 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %4372, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int4_5228 = torch.constant.int 4
    %int8_5229 = torch.constant.int 8
    %int4_5230 = torch.constant.int 4
    %int128_5231 = torch.constant.int 128
    %4373 = torch.prim.ListConstruct %int4_5228, %4315, %int8_5229, %int4_5230, %int128_5231 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_5232 = torch.constant.bool false
    %4374 = torch.aten.expand %4372, %4373, %false_5232 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4374, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_5233 = torch.constant.int 0
    %4375 = torch.aten.clone %4374, %int0_5233 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4375, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_5234 = torch.constant.int 4
    %int32_5235 = torch.constant.int 32
    %int128_5236 = torch.constant.int 128
    %4376 = torch.prim.ListConstruct %int4_5234, %4315, %int32_5235, %int128_5236 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4377 = torch.aten._unsafe_view %4375, %4376 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %4377, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_5237 = torch.constant.int -2
    %4378 = torch.aten.unsqueeze %4274, %int-2_5237 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %4378, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_5238 = torch.constant.int 1
    %4379 = torch.aten.size.int %4268, %int1_5238 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int4_5239 = torch.constant.int 4
    %int8_5240 = torch.constant.int 8
    %int4_5241 = torch.constant.int 4
    %int128_5242 = torch.constant.int 128
    %4380 = torch.prim.ListConstruct %int4_5239, %4379, %int8_5240, %int4_5241, %int128_5242 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_5243 = torch.constant.bool false
    %4381 = torch.aten.expand %4378, %4380, %false_5243 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4381, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_5244 = torch.constant.int 0
    %4382 = torch.aten.clone %4381, %int0_5244 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4382, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_5245 = torch.constant.int 4
    %int32_5246 = torch.constant.int 32
    %int128_5247 = torch.constant.int 128
    %4383 = torch.prim.ListConstruct %int4_5245, %4379, %int32_5246, %int128_5247 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4384 = torch.aten._unsafe_view %4382, %4383 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %4384, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_5248 = torch.constant.int 1
    %int2_5249 = torch.constant.int 2
    %4385 = torch.aten.transpose.int %4302, %int1_5248, %int2_5249 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %4385, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_5250 = torch.constant.int 1
    %int2_5251 = torch.constant.int 2
    %4386 = torch.aten.transpose.int %4377, %int1_5250, %int2_5251 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %4386, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_5252 = torch.constant.int 1
    %int2_5253 = torch.constant.int 2
    %4387 = torch.aten.transpose.int %4384, %int1_5252, %int2_5253 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %4387, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_5254 = torch.constant.float 0.000000e+00
    %true_5255 = torch.constant.bool true
    %none_5256 = torch.constant.none
    %none_5257 = torch.constant.none
    %4388:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4385, %4386, %4387, %float0.000000e00_5254, %true_5255, %none_5256, %none_5257) : (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?],f32>) 
    torch.bind_symbolic_shape %4388#0, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_5258 = torch.constant.int 1
    %int2_5259 = torch.constant.int 2
    %4389 = torch.aten.transpose.int %4388#0, %int1_5258, %int2_5259 : !torch.vtensor<[4,32,?,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %4389, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_5260 = torch.constant.int 4
    %int4096_5261 = torch.constant.int 4096
    %4390 = torch.prim.ListConstruct %int4_5260, %4287, %int4096_5261 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4391 = torch.aten.view %4389, %4390 : !torch.vtensor<[4,?,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4391, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_5262 = torch.constant.int -2
    %int-1_5263 = torch.constant.int -1
    %4392 = torch.aten.transpose.int %185, %int-2_5262, %int-1_5263 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_5264 = torch.constant.int 4
    %4393 = torch.aten.mul.int %int4_5264, %4287 : !torch.int, !torch.int -> !torch.int
    %int4096_5265 = torch.constant.int 4096
    %4394 = torch.prim.ListConstruct %4393, %int4096_5265 : (!torch.int, !torch.int) -> !torch.list<int>
    %4395 = torch.aten.view %4391, %4394 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4395, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %4396 = torch.aten.mm %4395, %4392 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4396, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_5266 = torch.constant.int 4
    %int4096_5267 = torch.constant.int 4096
    %4397 = torch.prim.ListConstruct %int4_5266, %4287, %int4096_5267 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4398 = torch.aten.view %4396, %4397 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4398, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_5268 = torch.constant.int 1
    %4399 = torch.aten.add.Tensor %4237, %4398, %int1_5268 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4399, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_5269 = torch.constant.int 6
    %4400 = torch.prims.convert_element_type %4399, %int6_5269 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4400, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_5270 = torch.constant.int 2
    %4401 = torch.aten.pow.Tensor_Scalar %4400, %int2_5270 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4401, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_5271 = torch.constant.int -1
    %4402 = torch.prim.ListConstruct %int-1_5271 : (!torch.int) -> !torch.list<int>
    %true_5272 = torch.constant.bool true
    %none_5273 = torch.constant.none
    %4403 = torch.aten.mean.dim %4401, %4402, %true_5272, %none_5273 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4403, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_5274 = torch.constant.float 9.9999997473787516E-6
    %int1_5275 = torch.constant.int 1
    %4404 = torch.aten.add.Scalar %4403, %float9.999990e-06_5274, %int1_5275 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4404, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %4405 = torch.aten.rsqrt %4404 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4405, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %4406 = torch.aten.mul.Tensor %4400, %4405 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4406, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_5276 = torch.constant.int 5
    %4407 = torch.prims.convert_element_type %4406, %int5_5276 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4407, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %4408 = torch.aten.mul.Tensor %186, %4407 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4408, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_5277 = torch.constant.int 5
    %4409 = torch.prims.convert_element_type %4408, %int5_5277 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4409, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_5278 = torch.constant.int -2
    %int-1_5279 = torch.constant.int -1
    %4410 = torch.aten.transpose.int %187, %int-2_5278, %int-1_5279 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_5280 = torch.constant.int 4
    %4411 = torch.aten.mul.int %int4_5280, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_5281 = torch.constant.int 4096
    %4412 = torch.prim.ListConstruct %4411, %int4096_5281 : (!torch.int, !torch.int) -> !torch.list<int>
    %4413 = torch.aten.view %4409, %4412 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4413, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %4414 = torch.aten.mm %4413, %4410 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %4414, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_5282 = torch.constant.int 4
    %int14336_5283 = torch.constant.int 14336
    %4415 = torch.prim.ListConstruct %int4_5282, %306, %int14336_5283 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4416 = torch.aten.view %4414, %4415 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %4416, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %4417 = torch.aten.silu %4416 : !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %4417, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_5284 = torch.constant.int -2
    %int-1_5285 = torch.constant.int -1
    %4418 = torch.aten.transpose.int %188, %int-2_5284, %int-1_5285 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_5286 = torch.constant.int 4
    %4419 = torch.aten.mul.int %int4_5286, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_5287 = torch.constant.int 4096
    %4420 = torch.prim.ListConstruct %4419, %int4096_5287 : (!torch.int, !torch.int) -> !torch.list<int>
    %4421 = torch.aten.view %4409, %4420 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4421, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %4422 = torch.aten.mm %4421, %4418 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %4422, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_5288 = torch.constant.int 4
    %int14336_5289 = torch.constant.int 14336
    %4423 = torch.prim.ListConstruct %int4_5288, %306, %int14336_5289 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4424 = torch.aten.view %4422, %4423 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %4424, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %4425 = torch.aten.mul.Tensor %4417, %4424 : !torch.vtensor<[4,?,14336],f16>, !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %4425, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_5290 = torch.constant.int -2
    %int-1_5291 = torch.constant.int -1
    %4426 = torch.aten.transpose.int %189, %int-2_5290, %int-1_5291 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int1_5292 = torch.constant.int 1
    %4427 = torch.aten.size.int %4416, %int1_5292 : !torch.vtensor<[4,?,14336],f16>, !torch.int -> !torch.int
    %int4_5293 = torch.constant.int 4
    %4428 = torch.aten.mul.int %int4_5293, %4427 : !torch.int, !torch.int -> !torch.int
    %int14336_5294 = torch.constant.int 14336
    %4429 = torch.prim.ListConstruct %4428, %int14336_5294 : (!torch.int, !torch.int) -> !torch.list<int>
    %4430 = torch.aten.view %4425, %4429 : !torch.vtensor<[4,?,14336],f16>, !torch.list<int> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %4430, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %4431 = torch.aten.mm %4430, %4426 : !torch.vtensor<[?,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4431, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_5295 = torch.constant.int 4
    %int4096_5296 = torch.constant.int 4096
    %4432 = torch.prim.ListConstruct %int4_5295, %4427, %int4096_5296 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4433 = torch.aten.view %4431, %4432 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4433, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_5297 = torch.constant.int 1
    %4434 = torch.aten.add.Tensor %4399, %4433, %int1_5297 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4434, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_5298 = torch.constant.int 6
    %4435 = torch.prims.convert_element_type %4434, %int6_5298 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4435, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_5299 = torch.constant.int 2
    %4436 = torch.aten.pow.Tensor_Scalar %4435, %int2_5299 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4436, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_5300 = torch.constant.int -1
    %4437 = torch.prim.ListConstruct %int-1_5300 : (!torch.int) -> !torch.list<int>
    %true_5301 = torch.constant.bool true
    %none_5302 = torch.constant.none
    %4438 = torch.aten.mean.dim %4436, %4437, %true_5301, %none_5302 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4438, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_5303 = torch.constant.float 9.9999997473787516E-6
    %int1_5304 = torch.constant.int 1
    %4439 = torch.aten.add.Scalar %4438, %float9.999990e-06_5303, %int1_5304 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4439, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %4440 = torch.aten.rsqrt %4439 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4440, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %4441 = torch.aten.mul.Tensor %4435, %4440 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4441, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_5305 = torch.constant.int 5
    %4442 = torch.prims.convert_element_type %4441, %int5_5305 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4442, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %4443 = torch.aten.mul.Tensor %190, %4442 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4443, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_5306 = torch.constant.int 5
    %4444 = torch.prims.convert_element_type %4443, %int5_5306 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4444, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_5307 = torch.constant.int -2
    %int-1_5308 = torch.constant.int -1
    %4445 = torch.aten.transpose.int %191, %int-2_5307, %int-1_5308 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_5309 = torch.constant.int 4
    %4446 = torch.aten.mul.int %int4_5309, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_5310 = torch.constant.int 4096
    %4447 = torch.prim.ListConstruct %4446, %int4096_5310 : (!torch.int, !torch.int) -> !torch.list<int>
    %4448 = torch.aten.view %4444, %4447 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4448, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %4449 = torch.aten.mm %4448, %4445 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4449, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_5311 = torch.constant.int 4
    %int4096_5312 = torch.constant.int 4096
    %4450 = torch.prim.ListConstruct %int4_5311, %306, %int4096_5312 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4451 = torch.aten.view %4449, %4450 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4451, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_5313 = torch.constant.int -2
    %int-1_5314 = torch.constant.int -1
    %4452 = torch.aten.transpose.int %192, %int-2_5313, %int-1_5314 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_5315 = torch.constant.int 4
    %4453 = torch.aten.mul.int %int4_5315, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_5316 = torch.constant.int 4096
    %4454 = torch.prim.ListConstruct %4453, %int4096_5316 : (!torch.int, !torch.int) -> !torch.list<int>
    %4455 = torch.aten.view %4444, %4454 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4455, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %4456 = torch.aten.mm %4455, %4452 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %4456, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_5317 = torch.constant.int 4
    %int1024_5318 = torch.constant.int 1024
    %4457 = torch.prim.ListConstruct %int4_5317, %306, %int1024_5318 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4458 = torch.aten.view %4456, %4457 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %4458, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int-2_5319 = torch.constant.int -2
    %int-1_5320 = torch.constant.int -1
    %4459 = torch.aten.transpose.int %193, %int-2_5319, %int-1_5320 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_5321 = torch.constant.int 4
    %4460 = torch.aten.mul.int %int4_5321, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_5322 = torch.constant.int 4096
    %4461 = torch.prim.ListConstruct %4460, %int4096_5322 : (!torch.int, !torch.int) -> !torch.list<int>
    %4462 = torch.aten.view %4444, %4461 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4462, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %4463 = torch.aten.mm %4462, %4459 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %4463, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_5323 = torch.constant.int 4
    %int1024_5324 = torch.constant.int 1024
    %4464 = torch.prim.ListConstruct %int4_5323, %306, %int1024_5324 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4465 = torch.aten.view %4463, %4464 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %4465, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int4_5325 = torch.constant.int 4
    %int32_5326 = torch.constant.int 32
    %int128_5327 = torch.constant.int 128
    %4466 = torch.prim.ListConstruct %int4_5325, %306, %int32_5326, %int128_5327 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4467 = torch.aten.view %4451, %4466 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %4467, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_5328 = torch.constant.int 4
    %int8_5329 = torch.constant.int 8
    %int128_5330 = torch.constant.int 128
    %4468 = torch.prim.ListConstruct %int4_5328, %306, %int8_5329, %int128_5330 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4469 = torch.aten.view %4458, %4468 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %4469, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int4_5331 = torch.constant.int 4
    %int8_5332 = torch.constant.int 8
    %int128_5333 = torch.constant.int 128
    %4470 = torch.prim.ListConstruct %int4_5331, %306, %int8_5332, %int128_5333 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4471 = torch.aten.view %4465, %4470 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %4471, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int131072_5334 = torch.constant.int 131072
    %none_5335 = torch.constant.none
    %none_5336 = torch.constant.none
    %cpu_5337 = torch.constant.device "cpu"
    %false_5338 = torch.constant.bool false
    %4472 = torch.aten.arange %int131072_5334, %none_5335, %none_5336, %cpu_5337, %false_5338 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_5339 = torch.constant.int 0
    %int128_5340 = torch.constant.int 128
    %none_5341 = torch.constant.none
    %none_5342 = torch.constant.none
    %cpu_5343 = torch.constant.device "cpu"
    %false_5344 = torch.constant.bool false
    %4473 = torch.aten.arange.start %int0_5339, %int128_5340, %none_5341, %none_5342, %cpu_5343, %false_5344 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_5345 = torch.constant.int 2
    %4474 = torch.aten.floor_divide.Scalar %4473, %int2_5345 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_5346 = torch.constant.int 6
    %4475 = torch.prims.convert_element_type %4474, %int6_5346 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_5347 = torch.constant.int 128
    %4476 = torch.aten.div.Scalar %4475, %int128_5347 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_5348 = torch.constant.float 2.000000e+00
    %4477 = torch.aten.mul.Scalar %4476, %float2.000000e00_5348 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_5349 = torch.constant.float 5.000000e+05
    %4478 = torch.aten.pow.Scalar %float5.000000e05_5349, %4477 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %4479 = torch.aten.reciprocal %4478 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_5350 = torch.constant.float 1.000000e+00
    %4480 = torch.aten.mul.Scalar %4479, %float1.000000e00_5350 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_5351 = torch.constant.int 1
    %4481 = torch.aten.unsqueeze %4472, %int1_5351 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_5352 = torch.constant.int 0
    %4482 = torch.aten.unsqueeze %4480, %int0_5352 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %4483 = torch.aten.mul.Tensor %4481, %4482 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_5353 = torch.constant.int 1
    %4484 = torch.aten.size.int %4451, %int1_5353 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.int
    %int0_5354 = torch.constant.int 0
    %4485 = torch.aten.add.int %int0_5354, %4484 : !torch.int, !torch.int -> !torch.int
    %int0_5355 = torch.constant.int 0
    %int0_5356 = torch.constant.int 0
    %int1_5357 = torch.constant.int 1
    %4486 = torch.aten.slice.Tensor %4483, %int0_5355, %int0_5356, %4485, %int1_5357 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %4486, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_5358 = torch.constant.int 1
    %int0_5359 = torch.constant.int 0
    %int9223372036854775807_5360 = torch.constant.int 9223372036854775807
    %int1_5361 = torch.constant.int 1
    %4487 = torch.aten.slice.Tensor %4486, %int1_5358, %int0_5359, %int9223372036854775807_5360, %int1_5361 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %4487, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_5362 = torch.constant.int 1
    %int0_5363 = torch.constant.int 0
    %int9223372036854775807_5364 = torch.constant.int 9223372036854775807
    %int1_5365 = torch.constant.int 1
    %4488 = torch.aten.slice.Tensor %4487, %int1_5362, %int0_5363, %int9223372036854775807_5364, %int1_5365 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %4488, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_5366 = torch.constant.int 0
    %4489 = torch.aten.unsqueeze %4488, %int0_5366 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %4489, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_5367 = torch.constant.int 1
    %int0_5368 = torch.constant.int 0
    %int9223372036854775807_5369 = torch.constant.int 9223372036854775807
    %int1_5370 = torch.constant.int 1
    %4490 = torch.aten.slice.Tensor %4489, %int1_5367, %int0_5368, %int9223372036854775807_5369, %int1_5370 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %4490, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_5371 = torch.constant.int 2
    %int0_5372 = torch.constant.int 0
    %int9223372036854775807_5373 = torch.constant.int 9223372036854775807
    %int1_5374 = torch.constant.int 1
    %4491 = torch.aten.slice.Tensor %4490, %int2_5371, %int0_5372, %int9223372036854775807_5373, %int1_5374 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %4491, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_5375 = torch.constant.int 4
    %int1_5376 = torch.constant.int 1
    %int1_5377 = torch.constant.int 1
    %4492 = torch.prim.ListConstruct %int4_5375, %int1_5376, %int1_5377 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4493 = torch.aten.repeat %4491, %4492 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %4493, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_5378 = torch.constant.int 6
    %4494 = torch.prims.convert_element_type %4467, %int6_5378 : !torch.vtensor<[4,?,32,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %4494, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %4495 = torch_c.to_builtin_tensor %4494 : !torch.vtensor<[4,?,32,128],f32> -> tensor<4x?x32x128xf32>
    %4496 = torch_c.to_builtin_tensor %4493 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %4497 = util.call @sharktank_rotary_embedding_4_D_32_128_f32(%4495, %4496) : (tensor<4x?x32x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x32x128xf32>
    %4498 = torch_c.from_builtin_tensor %4497 : tensor<4x?x32x128xf32> -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %4498, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %int5_5379 = torch.constant.int 5
    %4499 = torch.prims.convert_element_type %4498, %int5_5379 : !torch.vtensor<[4,?,32,128],f32>, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %4499, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int131072_5380 = torch.constant.int 131072
    %none_5381 = torch.constant.none
    %none_5382 = torch.constant.none
    %cpu_5383 = torch.constant.device "cpu"
    %false_5384 = torch.constant.bool false
    %4500 = torch.aten.arange %int131072_5380, %none_5381, %none_5382, %cpu_5383, %false_5384 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_5385 = torch.constant.int 0
    %int128_5386 = torch.constant.int 128
    %none_5387 = torch.constant.none
    %none_5388 = torch.constant.none
    %cpu_5389 = torch.constant.device "cpu"
    %false_5390 = torch.constant.bool false
    %4501 = torch.aten.arange.start %int0_5385, %int128_5386, %none_5387, %none_5388, %cpu_5389, %false_5390 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_5391 = torch.constant.int 2
    %4502 = torch.aten.floor_divide.Scalar %4501, %int2_5391 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_5392 = torch.constant.int 6
    %4503 = torch.prims.convert_element_type %4502, %int6_5392 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_5393 = torch.constant.int 128
    %4504 = torch.aten.div.Scalar %4503, %int128_5393 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_5394 = torch.constant.float 2.000000e+00
    %4505 = torch.aten.mul.Scalar %4504, %float2.000000e00_5394 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_5395 = torch.constant.float 5.000000e+05
    %4506 = torch.aten.pow.Scalar %float5.000000e05_5395, %4505 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %4507 = torch.aten.reciprocal %4506 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_5396 = torch.constant.float 1.000000e+00
    %4508 = torch.aten.mul.Scalar %4507, %float1.000000e00_5396 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_5397 = torch.constant.int 1
    %4509 = torch.aten.unsqueeze %4500, %int1_5397 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_5398 = torch.constant.int 0
    %4510 = torch.aten.unsqueeze %4508, %int0_5398 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %4511 = torch.aten.mul.Tensor %4509, %4510 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_5399 = torch.constant.int 1
    %4512 = torch.aten.size.int %4458, %int1_5399 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int0_5400 = torch.constant.int 0
    %4513 = torch.aten.add.int %int0_5400, %4512 : !torch.int, !torch.int -> !torch.int
    %int0_5401 = torch.constant.int 0
    %int0_5402 = torch.constant.int 0
    %int1_5403 = torch.constant.int 1
    %4514 = torch.aten.slice.Tensor %4511, %int0_5401, %int0_5402, %4513, %int1_5403 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %4514, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_5404 = torch.constant.int 1
    %int0_5405 = torch.constant.int 0
    %int9223372036854775807_5406 = torch.constant.int 9223372036854775807
    %int1_5407 = torch.constant.int 1
    %4515 = torch.aten.slice.Tensor %4514, %int1_5404, %int0_5405, %int9223372036854775807_5406, %int1_5407 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %4515, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_5408 = torch.constant.int 1
    %int0_5409 = torch.constant.int 0
    %int9223372036854775807_5410 = torch.constant.int 9223372036854775807
    %int1_5411 = torch.constant.int 1
    %4516 = torch.aten.slice.Tensor %4515, %int1_5408, %int0_5409, %int9223372036854775807_5410, %int1_5411 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %4516, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_5412 = torch.constant.int 0
    %4517 = torch.aten.unsqueeze %4516, %int0_5412 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %4517, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_5413 = torch.constant.int 1
    %int0_5414 = torch.constant.int 0
    %int9223372036854775807_5415 = torch.constant.int 9223372036854775807
    %int1_5416 = torch.constant.int 1
    %4518 = torch.aten.slice.Tensor %4517, %int1_5413, %int0_5414, %int9223372036854775807_5415, %int1_5416 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %4518, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_5417 = torch.constant.int 2
    %int0_5418 = torch.constant.int 0
    %int9223372036854775807_5419 = torch.constant.int 9223372036854775807
    %int1_5420 = torch.constant.int 1
    %4519 = torch.aten.slice.Tensor %4518, %int2_5417, %int0_5418, %int9223372036854775807_5419, %int1_5420 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %4519, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_5421 = torch.constant.int 4
    %int1_5422 = torch.constant.int 1
    %int1_5423 = torch.constant.int 1
    %4520 = torch.prim.ListConstruct %int4_5421, %int1_5422, %int1_5423 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4521 = torch.aten.repeat %4519, %4520 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %4521, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_5424 = torch.constant.int 6
    %4522 = torch.prims.convert_element_type %4469, %int6_5424 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %4522, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %4523 = torch_c.to_builtin_tensor %4522 : !torch.vtensor<[4,?,8,128],f32> -> tensor<4x?x8x128xf32>
    %4524 = torch_c.to_builtin_tensor %4521 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %4525 = util.call @sharktank_rotary_embedding_4_D_8_128_f32(%4523, %4524) : (tensor<4x?x8x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x8x128xf32>
    %4526 = torch_c.from_builtin_tensor %4525 : tensor<4x?x8x128xf32> -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %4526, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %int5_5425 = torch.constant.int 5
    %4527 = torch.prims.convert_element_type %4526, %int5_5425 : !torch.vtensor<[4,?,8,128],f32>, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %4527, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int64_5426 = torch.constant.int 64
    %4528 = torch.aten.mul.Scalar %arg2, %int64_5426 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4528, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int42 = torch.constant.int 42
    %int1_5427 = torch.constant.int 1
    %4529 = torch.aten.add.Scalar %4528, %int42, %int1_5427 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4529, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_5428 = torch.constant.int 4
    %int32_5429 = torch.constant.int 32
    %int8_5430 = torch.constant.int 8
    %int128_5431 = torch.constant.int 128
    %4530 = torch.prim.ListConstruct %int4_5428, %398, %int32_5429, %int8_5430, %int128_5431 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4531 = torch.aten.view %4527, %4530 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %4531, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_5432 = torch.constant.int 4
    %4532 = torch.aten.mul.int %int4_5432, %398 : !torch.int, !torch.int -> !torch.int
    %int32_5433 = torch.constant.int 32
    %int8_5434 = torch.constant.int 8
    %int128_5435 = torch.constant.int 128
    %4533 = torch.prim.ListConstruct %4532, %int32_5433, %int8_5434, %int128_5435 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4534 = torch.aten.view %4531, %4533 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %4534, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_5436 = torch.constant.int 4
    %4535 = torch.aten.mul.int %int4_5436, %398 : !torch.int, !torch.int -> !torch.int
    %4536 = torch.prim.ListConstruct %4535 : (!torch.int) -> !torch.list<int>
    %4537 = torch.aten.view %4529, %4536 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4537, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_5437 = torch.constant.int 32
    %int2_5438 = torch.constant.int 2
    %int32_5439 = torch.constant.int 32
    %int8_5440 = torch.constant.int 8
    %int128_5441 = torch.constant.int 128
    %4538 = torch.prim.ListConstruct %389, %int32_5437, %int2_5438, %int32_5439, %int8_5440, %int128_5441 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4539 = torch.aten.view %4371, %4538 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4539, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_5442 = torch.constant.int 32
    %4540 = torch.aten.mul.int %389, %int32_5442 : !torch.int, !torch.int -> !torch.int
    %int2_5443 = torch.constant.int 2
    %4541 = torch.aten.mul.int %4540, %int2_5443 : !torch.int, !torch.int -> !torch.int
    %int32_5444 = torch.constant.int 32
    %int8_5445 = torch.constant.int 8
    %int128_5446 = torch.constant.int 128
    %4542 = torch.prim.ListConstruct %4541, %int32_5444, %int8_5445, %int128_5446 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4543 = torch.aten.view %4539, %4542 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %4543, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %4544 = torch.prim.ListConstruct %4537 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_5447 = torch.constant.bool false
    %4545 = torch.aten.index_put %4543, %4544, %4534, %false_5447 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %4545, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_5448 = torch.constant.int 32
    %int2_5449 = torch.constant.int 2
    %int32_5450 = torch.constant.int 32
    %int8_5451 = torch.constant.int 8
    %int128_5452 = torch.constant.int 128
    %4546 = torch.prim.ListConstruct %389, %int32_5448, %int2_5449, %int32_5450, %int8_5451, %int128_5452 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4547 = torch.aten.view %4545, %4546 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4547, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_5453 = torch.constant.int 2097152
    %4548 = torch.prim.ListConstruct %389, %int2097152_5453 : (!torch.int, !torch.int) -> !torch.list<int>
    %4549 = torch.aten.view %4547, %4548 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %4549, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_5454 = torch.constant.int 32
    %int2_5455 = torch.constant.int 2
    %int32_5456 = torch.constant.int 32
    %int8_5457 = torch.constant.int 8
    %int128_5458 = torch.constant.int 128
    %4550 = torch.prim.ListConstruct %389, %int32_5454, %int2_5455, %int32_5456, %int8_5457, %int128_5458 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4551 = torch.aten.view %4549, %4550 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4551, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_5459 = torch.constant.int 32
    %int8_5460 = torch.constant.int 8
    %int128_5461 = torch.constant.int 128
    %4552 = torch.prim.ListConstruct %4541, %int32_5459, %int8_5460, %int128_5461 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4553 = torch.aten.view %4551, %4552 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %4553, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_5462 = torch.constant.int 4
    %int32_5463 = torch.constant.int 32
    %int8_5464 = torch.constant.int 8
    %int128_5465 = torch.constant.int 128
    %4554 = torch.prim.ListConstruct %int4_5462, %398, %int32_5463, %int8_5464, %int128_5465 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4555 = torch.aten.view %4471, %4554 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %4555, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_5466 = torch.constant.int 4
    %4556 = torch.aten.mul.int %int4_5466, %398 : !torch.int, !torch.int -> !torch.int
    %int32_5467 = torch.constant.int 32
    %int8_5468 = torch.constant.int 8
    %int128_5469 = torch.constant.int 128
    %4557 = torch.prim.ListConstruct %4556, %int32_5467, %int8_5468, %int128_5469 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4558 = torch.aten.view %4555, %4557 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %4558, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int1_5470 = torch.constant.int 1
    %int1_5471 = torch.constant.int 1
    %4559 = torch.aten.add.Scalar %4529, %int1_5470, %int1_5471 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4559, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_5472 = torch.constant.int 4
    %4560 = torch.aten.mul.int %int4_5472, %398 : !torch.int, !torch.int -> !torch.int
    %4561 = torch.prim.ListConstruct %4560 : (!torch.int) -> !torch.list<int>
    %4562 = torch.aten.view %4559, %4561 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4562, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %4563 = torch.prim.ListConstruct %4562 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_5473 = torch.constant.bool false
    %4564 = torch.aten.index_put %4553, %4563, %4558, %false_5473 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %4564, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_5474 = torch.constant.int 32
    %int2_5475 = torch.constant.int 2
    %int32_5476 = torch.constant.int 32
    %int8_5477 = torch.constant.int 8
    %int128_5478 = torch.constant.int 128
    %4565 = torch.prim.ListConstruct %389, %int32_5474, %int2_5475, %int32_5476, %int8_5477, %int128_5478 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4566 = torch.aten.view %4564, %4565 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4566, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_5479 = torch.constant.int 2097152
    %4567 = torch.prim.ListConstruct %389, %int2097152_5479 : (!torch.int, !torch.int) -> !torch.list<int>
    %4568 = torch.aten.view %4566, %4567 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %4568, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int-2_5480 = torch.constant.int -2
    %4569 = torch.aten.unsqueeze %4527, %int-2_5480 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %4569, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int4_5481 = torch.constant.int 4
    %int8_5482 = torch.constant.int 8
    %int4_5483 = torch.constant.int 4
    %int128_5484 = torch.constant.int 128
    %4570 = torch.prim.ListConstruct %int4_5481, %4512, %int8_5482, %int4_5483, %int128_5484 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_5485 = torch.constant.bool false
    %4571 = torch.aten.expand %4569, %4570, %false_5485 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4571, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_5486 = torch.constant.int 0
    %4572 = torch.aten.clone %4571, %int0_5486 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4572, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_5487 = torch.constant.int 4
    %int32_5488 = torch.constant.int 32
    %int128_5489 = torch.constant.int 128
    %4573 = torch.prim.ListConstruct %int4_5487, %4512, %int32_5488, %int128_5489 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4574 = torch.aten._unsafe_view %4572, %4573 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %4574, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_5490 = torch.constant.int -2
    %4575 = torch.aten.unsqueeze %4471, %int-2_5490 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %4575, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_5491 = torch.constant.int 1
    %4576 = torch.aten.size.int %4465, %int1_5491 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int4_5492 = torch.constant.int 4
    %int8_5493 = torch.constant.int 8
    %int4_5494 = torch.constant.int 4
    %int128_5495 = torch.constant.int 128
    %4577 = torch.prim.ListConstruct %int4_5492, %4576, %int8_5493, %int4_5494, %int128_5495 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_5496 = torch.constant.bool false
    %4578 = torch.aten.expand %4575, %4577, %false_5496 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4578, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_5497 = torch.constant.int 0
    %4579 = torch.aten.clone %4578, %int0_5497 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4579, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_5498 = torch.constant.int 4
    %int32_5499 = torch.constant.int 32
    %int128_5500 = torch.constant.int 128
    %4580 = torch.prim.ListConstruct %int4_5498, %4576, %int32_5499, %int128_5500 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4581 = torch.aten._unsafe_view %4579, %4580 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %4581, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_5501 = torch.constant.int 1
    %int2_5502 = torch.constant.int 2
    %4582 = torch.aten.transpose.int %4499, %int1_5501, %int2_5502 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %4582, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_5503 = torch.constant.int 1
    %int2_5504 = torch.constant.int 2
    %4583 = torch.aten.transpose.int %4574, %int1_5503, %int2_5504 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %4583, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_5505 = torch.constant.int 1
    %int2_5506 = torch.constant.int 2
    %4584 = torch.aten.transpose.int %4581, %int1_5505, %int2_5506 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %4584, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_5507 = torch.constant.float 0.000000e+00
    %true_5508 = torch.constant.bool true
    %none_5509 = torch.constant.none
    %none_5510 = torch.constant.none
    %4585:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4582, %4583, %4584, %float0.000000e00_5507, %true_5508, %none_5509, %none_5510) : (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?],f32>) 
    torch.bind_symbolic_shape %4585#0, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_5511 = torch.constant.int 1
    %int2_5512 = torch.constant.int 2
    %4586 = torch.aten.transpose.int %4585#0, %int1_5511, %int2_5512 : !torch.vtensor<[4,32,?,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %4586, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_5513 = torch.constant.int 4
    %int4096_5514 = torch.constant.int 4096
    %4587 = torch.prim.ListConstruct %int4_5513, %4484, %int4096_5514 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4588 = torch.aten.view %4586, %4587 : !torch.vtensor<[4,?,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4588, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_5515 = torch.constant.int -2
    %int-1_5516 = torch.constant.int -1
    %4589 = torch.aten.transpose.int %194, %int-2_5515, %int-1_5516 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_5517 = torch.constant.int 4
    %4590 = torch.aten.mul.int %int4_5517, %4484 : !torch.int, !torch.int -> !torch.int
    %int4096_5518 = torch.constant.int 4096
    %4591 = torch.prim.ListConstruct %4590, %int4096_5518 : (!torch.int, !torch.int) -> !torch.list<int>
    %4592 = torch.aten.view %4588, %4591 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4592, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %4593 = torch.aten.mm %4592, %4589 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4593, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_5519 = torch.constant.int 4
    %int4096_5520 = torch.constant.int 4096
    %4594 = torch.prim.ListConstruct %int4_5519, %4484, %int4096_5520 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4595 = torch.aten.view %4593, %4594 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4595, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_5521 = torch.constant.int 1
    %4596 = torch.aten.add.Tensor %4434, %4595, %int1_5521 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4596, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_5522 = torch.constant.int 6
    %4597 = torch.prims.convert_element_type %4596, %int6_5522 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4597, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_5523 = torch.constant.int 2
    %4598 = torch.aten.pow.Tensor_Scalar %4597, %int2_5523 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4598, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_5524 = torch.constant.int -1
    %4599 = torch.prim.ListConstruct %int-1_5524 : (!torch.int) -> !torch.list<int>
    %true_5525 = torch.constant.bool true
    %none_5526 = torch.constant.none
    %4600 = torch.aten.mean.dim %4598, %4599, %true_5525, %none_5526 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4600, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_5527 = torch.constant.float 9.9999997473787516E-6
    %int1_5528 = torch.constant.int 1
    %4601 = torch.aten.add.Scalar %4600, %float9.999990e-06_5527, %int1_5528 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4601, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %4602 = torch.aten.rsqrt %4601 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4602, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %4603 = torch.aten.mul.Tensor %4597, %4602 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4603, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_5529 = torch.constant.int 5
    %4604 = torch.prims.convert_element_type %4603, %int5_5529 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4604, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %4605 = torch.aten.mul.Tensor %195, %4604 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4605, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_5530 = torch.constant.int 5
    %4606 = torch.prims.convert_element_type %4605, %int5_5530 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4606, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_5531 = torch.constant.int -2
    %int-1_5532 = torch.constant.int -1
    %4607 = torch.aten.transpose.int %196, %int-2_5531, %int-1_5532 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_5533 = torch.constant.int 4
    %4608 = torch.aten.mul.int %int4_5533, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_5534 = torch.constant.int 4096
    %4609 = torch.prim.ListConstruct %4608, %int4096_5534 : (!torch.int, !torch.int) -> !torch.list<int>
    %4610 = torch.aten.view %4606, %4609 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4610, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %4611 = torch.aten.mm %4610, %4607 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %4611, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_5535 = torch.constant.int 4
    %int14336_5536 = torch.constant.int 14336
    %4612 = torch.prim.ListConstruct %int4_5535, %306, %int14336_5536 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4613 = torch.aten.view %4611, %4612 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %4613, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %4614 = torch.aten.silu %4613 : !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %4614, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_5537 = torch.constant.int -2
    %int-1_5538 = torch.constant.int -1
    %4615 = torch.aten.transpose.int %197, %int-2_5537, %int-1_5538 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_5539 = torch.constant.int 4
    %4616 = torch.aten.mul.int %int4_5539, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_5540 = torch.constant.int 4096
    %4617 = torch.prim.ListConstruct %4616, %int4096_5540 : (!torch.int, !torch.int) -> !torch.list<int>
    %4618 = torch.aten.view %4606, %4617 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4618, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %4619 = torch.aten.mm %4618, %4615 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %4619, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_5541 = torch.constant.int 4
    %int14336_5542 = torch.constant.int 14336
    %4620 = torch.prim.ListConstruct %int4_5541, %306, %int14336_5542 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4621 = torch.aten.view %4619, %4620 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %4621, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %4622 = torch.aten.mul.Tensor %4614, %4621 : !torch.vtensor<[4,?,14336],f16>, !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %4622, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_5543 = torch.constant.int -2
    %int-1_5544 = torch.constant.int -1
    %4623 = torch.aten.transpose.int %198, %int-2_5543, %int-1_5544 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int1_5545 = torch.constant.int 1
    %4624 = torch.aten.size.int %4613, %int1_5545 : !torch.vtensor<[4,?,14336],f16>, !torch.int -> !torch.int
    %int4_5546 = torch.constant.int 4
    %4625 = torch.aten.mul.int %int4_5546, %4624 : !torch.int, !torch.int -> !torch.int
    %int14336_5547 = torch.constant.int 14336
    %4626 = torch.prim.ListConstruct %4625, %int14336_5547 : (!torch.int, !torch.int) -> !torch.list<int>
    %4627 = torch.aten.view %4622, %4626 : !torch.vtensor<[4,?,14336],f16>, !torch.list<int> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %4627, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %4628 = torch.aten.mm %4627, %4623 : !torch.vtensor<[?,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4628, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_5548 = torch.constant.int 4
    %int4096_5549 = torch.constant.int 4096
    %4629 = torch.prim.ListConstruct %int4_5548, %4624, %int4096_5549 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4630 = torch.aten.view %4628, %4629 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4630, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_5550 = torch.constant.int 1
    %4631 = torch.aten.add.Tensor %4596, %4630, %int1_5550 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4631, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_5551 = torch.constant.int 6
    %4632 = torch.prims.convert_element_type %4631, %int6_5551 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4632, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_5552 = torch.constant.int 2
    %4633 = torch.aten.pow.Tensor_Scalar %4632, %int2_5552 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4633, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_5553 = torch.constant.int -1
    %4634 = torch.prim.ListConstruct %int-1_5553 : (!torch.int) -> !torch.list<int>
    %true_5554 = torch.constant.bool true
    %none_5555 = torch.constant.none
    %4635 = torch.aten.mean.dim %4633, %4634, %true_5554, %none_5555 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4635, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_5556 = torch.constant.float 9.9999997473787516E-6
    %int1_5557 = torch.constant.int 1
    %4636 = torch.aten.add.Scalar %4635, %float9.999990e-06_5556, %int1_5557 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4636, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %4637 = torch.aten.rsqrt %4636 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4637, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %4638 = torch.aten.mul.Tensor %4632, %4637 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4638, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_5558 = torch.constant.int 5
    %4639 = torch.prims.convert_element_type %4638, %int5_5558 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4639, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %4640 = torch.aten.mul.Tensor %199, %4639 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4640, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_5559 = torch.constant.int 5
    %4641 = torch.prims.convert_element_type %4640, %int5_5559 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4641, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_5560 = torch.constant.int -2
    %int-1_5561 = torch.constant.int -1
    %4642 = torch.aten.transpose.int %200, %int-2_5560, %int-1_5561 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_5562 = torch.constant.int 4
    %4643 = torch.aten.mul.int %int4_5562, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_5563 = torch.constant.int 4096
    %4644 = torch.prim.ListConstruct %4643, %int4096_5563 : (!torch.int, !torch.int) -> !torch.list<int>
    %4645 = torch.aten.view %4641, %4644 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4645, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %4646 = torch.aten.mm %4645, %4642 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4646, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_5564 = torch.constant.int 4
    %int4096_5565 = torch.constant.int 4096
    %4647 = torch.prim.ListConstruct %int4_5564, %306, %int4096_5565 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4648 = torch.aten.view %4646, %4647 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4648, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_5566 = torch.constant.int -2
    %int-1_5567 = torch.constant.int -1
    %4649 = torch.aten.transpose.int %201, %int-2_5566, %int-1_5567 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_5568 = torch.constant.int 4
    %4650 = torch.aten.mul.int %int4_5568, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_5569 = torch.constant.int 4096
    %4651 = torch.prim.ListConstruct %4650, %int4096_5569 : (!torch.int, !torch.int) -> !torch.list<int>
    %4652 = torch.aten.view %4641, %4651 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4652, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %4653 = torch.aten.mm %4652, %4649 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %4653, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_5570 = torch.constant.int 4
    %int1024_5571 = torch.constant.int 1024
    %4654 = torch.prim.ListConstruct %int4_5570, %306, %int1024_5571 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4655 = torch.aten.view %4653, %4654 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %4655, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int-2_5572 = torch.constant.int -2
    %int-1_5573 = torch.constant.int -1
    %4656 = torch.aten.transpose.int %202, %int-2_5572, %int-1_5573 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_5574 = torch.constant.int 4
    %4657 = torch.aten.mul.int %int4_5574, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_5575 = torch.constant.int 4096
    %4658 = torch.prim.ListConstruct %4657, %int4096_5575 : (!torch.int, !torch.int) -> !torch.list<int>
    %4659 = torch.aten.view %4641, %4658 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4659, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %4660 = torch.aten.mm %4659, %4656 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %4660, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_5576 = torch.constant.int 4
    %int1024_5577 = torch.constant.int 1024
    %4661 = torch.prim.ListConstruct %int4_5576, %306, %int1024_5577 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4662 = torch.aten.view %4660, %4661 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %4662, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int4_5578 = torch.constant.int 4
    %int32_5579 = torch.constant.int 32
    %int128_5580 = torch.constant.int 128
    %4663 = torch.prim.ListConstruct %int4_5578, %306, %int32_5579, %int128_5580 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4664 = torch.aten.view %4648, %4663 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %4664, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_5581 = torch.constant.int 4
    %int8_5582 = torch.constant.int 8
    %int128_5583 = torch.constant.int 128
    %4665 = torch.prim.ListConstruct %int4_5581, %306, %int8_5582, %int128_5583 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4666 = torch.aten.view %4655, %4665 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %4666, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int4_5584 = torch.constant.int 4
    %int8_5585 = torch.constant.int 8
    %int128_5586 = torch.constant.int 128
    %4667 = torch.prim.ListConstruct %int4_5584, %306, %int8_5585, %int128_5586 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4668 = torch.aten.view %4662, %4667 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %4668, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int131072_5587 = torch.constant.int 131072
    %none_5588 = torch.constant.none
    %none_5589 = torch.constant.none
    %cpu_5590 = torch.constant.device "cpu"
    %false_5591 = torch.constant.bool false
    %4669 = torch.aten.arange %int131072_5587, %none_5588, %none_5589, %cpu_5590, %false_5591 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_5592 = torch.constant.int 0
    %int128_5593 = torch.constant.int 128
    %none_5594 = torch.constant.none
    %none_5595 = torch.constant.none
    %cpu_5596 = torch.constant.device "cpu"
    %false_5597 = torch.constant.bool false
    %4670 = torch.aten.arange.start %int0_5592, %int128_5593, %none_5594, %none_5595, %cpu_5596, %false_5597 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_5598 = torch.constant.int 2
    %4671 = torch.aten.floor_divide.Scalar %4670, %int2_5598 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_5599 = torch.constant.int 6
    %4672 = torch.prims.convert_element_type %4671, %int6_5599 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_5600 = torch.constant.int 128
    %4673 = torch.aten.div.Scalar %4672, %int128_5600 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_5601 = torch.constant.float 2.000000e+00
    %4674 = torch.aten.mul.Scalar %4673, %float2.000000e00_5601 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_5602 = torch.constant.float 5.000000e+05
    %4675 = torch.aten.pow.Scalar %float5.000000e05_5602, %4674 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %4676 = torch.aten.reciprocal %4675 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_5603 = torch.constant.float 1.000000e+00
    %4677 = torch.aten.mul.Scalar %4676, %float1.000000e00_5603 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_5604 = torch.constant.int 1
    %4678 = torch.aten.unsqueeze %4669, %int1_5604 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_5605 = torch.constant.int 0
    %4679 = torch.aten.unsqueeze %4677, %int0_5605 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %4680 = torch.aten.mul.Tensor %4678, %4679 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_5606 = torch.constant.int 1
    %4681 = torch.aten.size.int %4648, %int1_5606 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.int
    %int0_5607 = torch.constant.int 0
    %4682 = torch.aten.add.int %int0_5607, %4681 : !torch.int, !torch.int -> !torch.int
    %int0_5608 = torch.constant.int 0
    %int0_5609 = torch.constant.int 0
    %int1_5610 = torch.constant.int 1
    %4683 = torch.aten.slice.Tensor %4680, %int0_5608, %int0_5609, %4682, %int1_5610 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %4683, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_5611 = torch.constant.int 1
    %int0_5612 = torch.constant.int 0
    %int9223372036854775807_5613 = torch.constant.int 9223372036854775807
    %int1_5614 = torch.constant.int 1
    %4684 = torch.aten.slice.Tensor %4683, %int1_5611, %int0_5612, %int9223372036854775807_5613, %int1_5614 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %4684, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_5615 = torch.constant.int 1
    %int0_5616 = torch.constant.int 0
    %int9223372036854775807_5617 = torch.constant.int 9223372036854775807
    %int1_5618 = torch.constant.int 1
    %4685 = torch.aten.slice.Tensor %4684, %int1_5615, %int0_5616, %int9223372036854775807_5617, %int1_5618 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %4685, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_5619 = torch.constant.int 0
    %4686 = torch.aten.unsqueeze %4685, %int0_5619 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %4686, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_5620 = torch.constant.int 1
    %int0_5621 = torch.constant.int 0
    %int9223372036854775807_5622 = torch.constant.int 9223372036854775807
    %int1_5623 = torch.constant.int 1
    %4687 = torch.aten.slice.Tensor %4686, %int1_5620, %int0_5621, %int9223372036854775807_5622, %int1_5623 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %4687, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_5624 = torch.constant.int 2
    %int0_5625 = torch.constant.int 0
    %int9223372036854775807_5626 = torch.constant.int 9223372036854775807
    %int1_5627 = torch.constant.int 1
    %4688 = torch.aten.slice.Tensor %4687, %int2_5624, %int0_5625, %int9223372036854775807_5626, %int1_5627 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %4688, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_5628 = torch.constant.int 4
    %int1_5629 = torch.constant.int 1
    %int1_5630 = torch.constant.int 1
    %4689 = torch.prim.ListConstruct %int4_5628, %int1_5629, %int1_5630 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4690 = torch.aten.repeat %4688, %4689 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %4690, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_5631 = torch.constant.int 6
    %4691 = torch.prims.convert_element_type %4664, %int6_5631 : !torch.vtensor<[4,?,32,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %4691, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %4692 = torch_c.to_builtin_tensor %4691 : !torch.vtensor<[4,?,32,128],f32> -> tensor<4x?x32x128xf32>
    %4693 = torch_c.to_builtin_tensor %4690 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %4694 = util.call @sharktank_rotary_embedding_4_D_32_128_f32(%4692, %4693) : (tensor<4x?x32x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x32x128xf32>
    %4695 = torch_c.from_builtin_tensor %4694 : tensor<4x?x32x128xf32> -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %4695, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %int5_5632 = torch.constant.int 5
    %4696 = torch.prims.convert_element_type %4695, %int5_5632 : !torch.vtensor<[4,?,32,128],f32>, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %4696, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int131072_5633 = torch.constant.int 131072
    %none_5634 = torch.constant.none
    %none_5635 = torch.constant.none
    %cpu_5636 = torch.constant.device "cpu"
    %false_5637 = torch.constant.bool false
    %4697 = torch.aten.arange %int131072_5633, %none_5634, %none_5635, %cpu_5636, %false_5637 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_5638 = torch.constant.int 0
    %int128_5639 = torch.constant.int 128
    %none_5640 = torch.constant.none
    %none_5641 = torch.constant.none
    %cpu_5642 = torch.constant.device "cpu"
    %false_5643 = torch.constant.bool false
    %4698 = torch.aten.arange.start %int0_5638, %int128_5639, %none_5640, %none_5641, %cpu_5642, %false_5643 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_5644 = torch.constant.int 2
    %4699 = torch.aten.floor_divide.Scalar %4698, %int2_5644 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_5645 = torch.constant.int 6
    %4700 = torch.prims.convert_element_type %4699, %int6_5645 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_5646 = torch.constant.int 128
    %4701 = torch.aten.div.Scalar %4700, %int128_5646 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_5647 = torch.constant.float 2.000000e+00
    %4702 = torch.aten.mul.Scalar %4701, %float2.000000e00_5647 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_5648 = torch.constant.float 5.000000e+05
    %4703 = torch.aten.pow.Scalar %float5.000000e05_5648, %4702 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %4704 = torch.aten.reciprocal %4703 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_5649 = torch.constant.float 1.000000e+00
    %4705 = torch.aten.mul.Scalar %4704, %float1.000000e00_5649 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_5650 = torch.constant.int 1
    %4706 = torch.aten.unsqueeze %4697, %int1_5650 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_5651 = torch.constant.int 0
    %4707 = torch.aten.unsqueeze %4705, %int0_5651 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %4708 = torch.aten.mul.Tensor %4706, %4707 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_5652 = torch.constant.int 1
    %4709 = torch.aten.size.int %4655, %int1_5652 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int0_5653 = torch.constant.int 0
    %4710 = torch.aten.add.int %int0_5653, %4709 : !torch.int, !torch.int -> !torch.int
    %int0_5654 = torch.constant.int 0
    %int0_5655 = torch.constant.int 0
    %int1_5656 = torch.constant.int 1
    %4711 = torch.aten.slice.Tensor %4708, %int0_5654, %int0_5655, %4710, %int1_5656 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %4711, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_5657 = torch.constant.int 1
    %int0_5658 = torch.constant.int 0
    %int9223372036854775807_5659 = torch.constant.int 9223372036854775807
    %int1_5660 = torch.constant.int 1
    %4712 = torch.aten.slice.Tensor %4711, %int1_5657, %int0_5658, %int9223372036854775807_5659, %int1_5660 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %4712, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_5661 = torch.constant.int 1
    %int0_5662 = torch.constant.int 0
    %int9223372036854775807_5663 = torch.constant.int 9223372036854775807
    %int1_5664 = torch.constant.int 1
    %4713 = torch.aten.slice.Tensor %4712, %int1_5661, %int0_5662, %int9223372036854775807_5663, %int1_5664 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %4713, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_5665 = torch.constant.int 0
    %4714 = torch.aten.unsqueeze %4713, %int0_5665 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %4714, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_5666 = torch.constant.int 1
    %int0_5667 = torch.constant.int 0
    %int9223372036854775807_5668 = torch.constant.int 9223372036854775807
    %int1_5669 = torch.constant.int 1
    %4715 = torch.aten.slice.Tensor %4714, %int1_5666, %int0_5667, %int9223372036854775807_5668, %int1_5669 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %4715, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_5670 = torch.constant.int 2
    %int0_5671 = torch.constant.int 0
    %int9223372036854775807_5672 = torch.constant.int 9223372036854775807
    %int1_5673 = torch.constant.int 1
    %4716 = torch.aten.slice.Tensor %4715, %int2_5670, %int0_5671, %int9223372036854775807_5672, %int1_5673 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %4716, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_5674 = torch.constant.int 4
    %int1_5675 = torch.constant.int 1
    %int1_5676 = torch.constant.int 1
    %4717 = torch.prim.ListConstruct %int4_5674, %int1_5675, %int1_5676 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4718 = torch.aten.repeat %4716, %4717 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %4718, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_5677 = torch.constant.int 6
    %4719 = torch.prims.convert_element_type %4666, %int6_5677 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %4719, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %4720 = torch_c.to_builtin_tensor %4719 : !torch.vtensor<[4,?,8,128],f32> -> tensor<4x?x8x128xf32>
    %4721 = torch_c.to_builtin_tensor %4718 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %4722 = util.call @sharktank_rotary_embedding_4_D_8_128_f32(%4720, %4721) : (tensor<4x?x8x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x8x128xf32>
    %4723 = torch_c.from_builtin_tensor %4722 : tensor<4x?x8x128xf32> -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %4723, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %int5_5678 = torch.constant.int 5
    %4724 = torch.prims.convert_element_type %4723, %int5_5678 : !torch.vtensor<[4,?,8,128],f32>, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %4724, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int64_5679 = torch.constant.int 64
    %4725 = torch.aten.mul.Scalar %arg2, %int64_5679 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4725, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int44 = torch.constant.int 44
    %int1_5680 = torch.constant.int 1
    %4726 = torch.aten.add.Scalar %4725, %int44, %int1_5680 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4726, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_5681 = torch.constant.int 4
    %int32_5682 = torch.constant.int 32
    %int8_5683 = torch.constant.int 8
    %int128_5684 = torch.constant.int 128
    %4727 = torch.prim.ListConstruct %int4_5681, %398, %int32_5682, %int8_5683, %int128_5684 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4728 = torch.aten.view %4724, %4727 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %4728, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_5685 = torch.constant.int 4
    %4729 = torch.aten.mul.int %int4_5685, %398 : !torch.int, !torch.int -> !torch.int
    %int32_5686 = torch.constant.int 32
    %int8_5687 = torch.constant.int 8
    %int128_5688 = torch.constant.int 128
    %4730 = torch.prim.ListConstruct %4729, %int32_5686, %int8_5687, %int128_5688 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4731 = torch.aten.view %4728, %4730 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %4731, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_5689 = torch.constant.int 4
    %4732 = torch.aten.mul.int %int4_5689, %398 : !torch.int, !torch.int -> !torch.int
    %4733 = torch.prim.ListConstruct %4732 : (!torch.int) -> !torch.list<int>
    %4734 = torch.aten.view %4726, %4733 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4734, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_5690 = torch.constant.int 32
    %int2_5691 = torch.constant.int 2
    %int32_5692 = torch.constant.int 32
    %int8_5693 = torch.constant.int 8
    %int128_5694 = torch.constant.int 128
    %4735 = torch.prim.ListConstruct %389, %int32_5690, %int2_5691, %int32_5692, %int8_5693, %int128_5694 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4736 = torch.aten.view %4568, %4735 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4736, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_5695 = torch.constant.int 32
    %4737 = torch.aten.mul.int %389, %int32_5695 : !torch.int, !torch.int -> !torch.int
    %int2_5696 = torch.constant.int 2
    %4738 = torch.aten.mul.int %4737, %int2_5696 : !torch.int, !torch.int -> !torch.int
    %int32_5697 = torch.constant.int 32
    %int8_5698 = torch.constant.int 8
    %int128_5699 = torch.constant.int 128
    %4739 = torch.prim.ListConstruct %4738, %int32_5697, %int8_5698, %int128_5699 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4740 = torch.aten.view %4736, %4739 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %4740, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %4741 = torch.prim.ListConstruct %4734 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_5700 = torch.constant.bool false
    %4742 = torch.aten.index_put %4740, %4741, %4731, %false_5700 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %4742, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_5701 = torch.constant.int 32
    %int2_5702 = torch.constant.int 2
    %int32_5703 = torch.constant.int 32
    %int8_5704 = torch.constant.int 8
    %int128_5705 = torch.constant.int 128
    %4743 = torch.prim.ListConstruct %389, %int32_5701, %int2_5702, %int32_5703, %int8_5704, %int128_5705 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4744 = torch.aten.view %4742, %4743 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4744, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_5706 = torch.constant.int 2097152
    %4745 = torch.prim.ListConstruct %389, %int2097152_5706 : (!torch.int, !torch.int) -> !torch.list<int>
    %4746 = torch.aten.view %4744, %4745 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %4746, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_5707 = torch.constant.int 32
    %int2_5708 = torch.constant.int 2
    %int32_5709 = torch.constant.int 32
    %int8_5710 = torch.constant.int 8
    %int128_5711 = torch.constant.int 128
    %4747 = torch.prim.ListConstruct %389, %int32_5707, %int2_5708, %int32_5709, %int8_5710, %int128_5711 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4748 = torch.aten.view %4746, %4747 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4748, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_5712 = torch.constant.int 32
    %int8_5713 = torch.constant.int 8
    %int128_5714 = torch.constant.int 128
    %4749 = torch.prim.ListConstruct %4738, %int32_5712, %int8_5713, %int128_5714 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4750 = torch.aten.view %4748, %4749 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %4750, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_5715 = torch.constant.int 4
    %int32_5716 = torch.constant.int 32
    %int8_5717 = torch.constant.int 8
    %int128_5718 = torch.constant.int 128
    %4751 = torch.prim.ListConstruct %int4_5715, %398, %int32_5716, %int8_5717, %int128_5718 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4752 = torch.aten.view %4668, %4751 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %4752, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_5719 = torch.constant.int 4
    %4753 = torch.aten.mul.int %int4_5719, %398 : !torch.int, !torch.int -> !torch.int
    %int32_5720 = torch.constant.int 32
    %int8_5721 = torch.constant.int 8
    %int128_5722 = torch.constant.int 128
    %4754 = torch.prim.ListConstruct %4753, %int32_5720, %int8_5721, %int128_5722 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4755 = torch.aten.view %4752, %4754 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %4755, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int1_5723 = torch.constant.int 1
    %int1_5724 = torch.constant.int 1
    %4756 = torch.aten.add.Scalar %4726, %int1_5723, %int1_5724 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4756, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_5725 = torch.constant.int 4
    %4757 = torch.aten.mul.int %int4_5725, %398 : !torch.int, !torch.int -> !torch.int
    %4758 = torch.prim.ListConstruct %4757 : (!torch.int) -> !torch.list<int>
    %4759 = torch.aten.view %4756, %4758 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4759, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %4760 = torch.prim.ListConstruct %4759 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_5726 = torch.constant.bool false
    %4761 = torch.aten.index_put %4750, %4760, %4755, %false_5726 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %4761, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_5727 = torch.constant.int 32
    %int2_5728 = torch.constant.int 2
    %int32_5729 = torch.constant.int 32
    %int8_5730 = torch.constant.int 8
    %int128_5731 = torch.constant.int 128
    %4762 = torch.prim.ListConstruct %389, %int32_5727, %int2_5728, %int32_5729, %int8_5730, %int128_5731 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4763 = torch.aten.view %4761, %4762 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4763, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_5732 = torch.constant.int 2097152
    %4764 = torch.prim.ListConstruct %389, %int2097152_5732 : (!torch.int, !torch.int) -> !torch.list<int>
    %4765 = torch.aten.view %4763, %4764 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %4765, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int-2_5733 = torch.constant.int -2
    %4766 = torch.aten.unsqueeze %4724, %int-2_5733 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %4766, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int4_5734 = torch.constant.int 4
    %int8_5735 = torch.constant.int 8
    %int4_5736 = torch.constant.int 4
    %int128_5737 = torch.constant.int 128
    %4767 = torch.prim.ListConstruct %int4_5734, %4709, %int8_5735, %int4_5736, %int128_5737 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_5738 = torch.constant.bool false
    %4768 = torch.aten.expand %4766, %4767, %false_5738 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4768, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_5739 = torch.constant.int 0
    %4769 = torch.aten.clone %4768, %int0_5739 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4769, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_5740 = torch.constant.int 4
    %int32_5741 = torch.constant.int 32
    %int128_5742 = torch.constant.int 128
    %4770 = torch.prim.ListConstruct %int4_5740, %4709, %int32_5741, %int128_5742 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4771 = torch.aten._unsafe_view %4769, %4770 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %4771, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_5743 = torch.constant.int -2
    %4772 = torch.aten.unsqueeze %4668, %int-2_5743 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %4772, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_5744 = torch.constant.int 1
    %4773 = torch.aten.size.int %4662, %int1_5744 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int4_5745 = torch.constant.int 4
    %int8_5746 = torch.constant.int 8
    %int4_5747 = torch.constant.int 4
    %int128_5748 = torch.constant.int 128
    %4774 = torch.prim.ListConstruct %int4_5745, %4773, %int8_5746, %int4_5747, %int128_5748 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_5749 = torch.constant.bool false
    %4775 = torch.aten.expand %4772, %4774, %false_5749 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4775, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_5750 = torch.constant.int 0
    %4776 = torch.aten.clone %4775, %int0_5750 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4776, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_5751 = torch.constant.int 4
    %int32_5752 = torch.constant.int 32
    %int128_5753 = torch.constant.int 128
    %4777 = torch.prim.ListConstruct %int4_5751, %4773, %int32_5752, %int128_5753 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4778 = torch.aten._unsafe_view %4776, %4777 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %4778, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_5754 = torch.constant.int 1
    %int2_5755 = torch.constant.int 2
    %4779 = torch.aten.transpose.int %4696, %int1_5754, %int2_5755 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %4779, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_5756 = torch.constant.int 1
    %int2_5757 = torch.constant.int 2
    %4780 = torch.aten.transpose.int %4771, %int1_5756, %int2_5757 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %4780, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_5758 = torch.constant.int 1
    %int2_5759 = torch.constant.int 2
    %4781 = torch.aten.transpose.int %4778, %int1_5758, %int2_5759 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %4781, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_5760 = torch.constant.float 0.000000e+00
    %true_5761 = torch.constant.bool true
    %none_5762 = torch.constant.none
    %none_5763 = torch.constant.none
    %4782:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4779, %4780, %4781, %float0.000000e00_5760, %true_5761, %none_5762, %none_5763) : (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?],f32>) 
    torch.bind_symbolic_shape %4782#0, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_5764 = torch.constant.int 1
    %int2_5765 = torch.constant.int 2
    %4783 = torch.aten.transpose.int %4782#0, %int1_5764, %int2_5765 : !torch.vtensor<[4,32,?,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %4783, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_5766 = torch.constant.int 4
    %int4096_5767 = torch.constant.int 4096
    %4784 = torch.prim.ListConstruct %int4_5766, %4681, %int4096_5767 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4785 = torch.aten.view %4783, %4784 : !torch.vtensor<[4,?,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4785, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_5768 = torch.constant.int -2
    %int-1_5769 = torch.constant.int -1
    %4786 = torch.aten.transpose.int %203, %int-2_5768, %int-1_5769 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_5770 = torch.constant.int 4
    %4787 = torch.aten.mul.int %int4_5770, %4681 : !torch.int, !torch.int -> !torch.int
    %int4096_5771 = torch.constant.int 4096
    %4788 = torch.prim.ListConstruct %4787, %int4096_5771 : (!torch.int, !torch.int) -> !torch.list<int>
    %4789 = torch.aten.view %4785, %4788 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4789, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %4790 = torch.aten.mm %4789, %4786 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4790, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_5772 = torch.constant.int 4
    %int4096_5773 = torch.constant.int 4096
    %4791 = torch.prim.ListConstruct %int4_5772, %4681, %int4096_5773 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4792 = torch.aten.view %4790, %4791 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4792, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_5774 = torch.constant.int 1
    %4793 = torch.aten.add.Tensor %4631, %4792, %int1_5774 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4793, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_5775 = torch.constant.int 6
    %4794 = torch.prims.convert_element_type %4793, %int6_5775 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4794, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_5776 = torch.constant.int 2
    %4795 = torch.aten.pow.Tensor_Scalar %4794, %int2_5776 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4795, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_5777 = torch.constant.int -1
    %4796 = torch.prim.ListConstruct %int-1_5777 : (!torch.int) -> !torch.list<int>
    %true_5778 = torch.constant.bool true
    %none_5779 = torch.constant.none
    %4797 = torch.aten.mean.dim %4795, %4796, %true_5778, %none_5779 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4797, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_5780 = torch.constant.float 9.9999997473787516E-6
    %int1_5781 = torch.constant.int 1
    %4798 = torch.aten.add.Scalar %4797, %float9.999990e-06_5780, %int1_5781 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4798, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %4799 = torch.aten.rsqrt %4798 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4799, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %4800 = torch.aten.mul.Tensor %4794, %4799 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4800, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_5782 = torch.constant.int 5
    %4801 = torch.prims.convert_element_type %4800, %int5_5782 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4801, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %4802 = torch.aten.mul.Tensor %204, %4801 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4802, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_5783 = torch.constant.int 5
    %4803 = torch.prims.convert_element_type %4802, %int5_5783 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4803, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_5784 = torch.constant.int -2
    %int-1_5785 = torch.constant.int -1
    %4804 = torch.aten.transpose.int %205, %int-2_5784, %int-1_5785 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_5786 = torch.constant.int 4
    %4805 = torch.aten.mul.int %int4_5786, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_5787 = torch.constant.int 4096
    %4806 = torch.prim.ListConstruct %4805, %int4096_5787 : (!torch.int, !torch.int) -> !torch.list<int>
    %4807 = torch.aten.view %4803, %4806 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4807, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %4808 = torch.aten.mm %4807, %4804 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %4808, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_5788 = torch.constant.int 4
    %int14336_5789 = torch.constant.int 14336
    %4809 = torch.prim.ListConstruct %int4_5788, %306, %int14336_5789 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4810 = torch.aten.view %4808, %4809 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %4810, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %4811 = torch.aten.silu %4810 : !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %4811, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_5790 = torch.constant.int -2
    %int-1_5791 = torch.constant.int -1
    %4812 = torch.aten.transpose.int %206, %int-2_5790, %int-1_5791 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_5792 = torch.constant.int 4
    %4813 = torch.aten.mul.int %int4_5792, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_5793 = torch.constant.int 4096
    %4814 = torch.prim.ListConstruct %4813, %int4096_5793 : (!torch.int, !torch.int) -> !torch.list<int>
    %4815 = torch.aten.view %4803, %4814 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4815, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %4816 = torch.aten.mm %4815, %4812 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %4816, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_5794 = torch.constant.int 4
    %int14336_5795 = torch.constant.int 14336
    %4817 = torch.prim.ListConstruct %int4_5794, %306, %int14336_5795 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4818 = torch.aten.view %4816, %4817 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %4818, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %4819 = torch.aten.mul.Tensor %4811, %4818 : !torch.vtensor<[4,?,14336],f16>, !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %4819, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_5796 = torch.constant.int -2
    %int-1_5797 = torch.constant.int -1
    %4820 = torch.aten.transpose.int %207, %int-2_5796, %int-1_5797 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int1_5798 = torch.constant.int 1
    %4821 = torch.aten.size.int %4810, %int1_5798 : !torch.vtensor<[4,?,14336],f16>, !torch.int -> !torch.int
    %int4_5799 = torch.constant.int 4
    %4822 = torch.aten.mul.int %int4_5799, %4821 : !torch.int, !torch.int -> !torch.int
    %int14336_5800 = torch.constant.int 14336
    %4823 = torch.prim.ListConstruct %4822, %int14336_5800 : (!torch.int, !torch.int) -> !torch.list<int>
    %4824 = torch.aten.view %4819, %4823 : !torch.vtensor<[4,?,14336],f16>, !torch.list<int> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %4824, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %4825 = torch.aten.mm %4824, %4820 : !torch.vtensor<[?,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4825, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_5801 = torch.constant.int 4
    %int4096_5802 = torch.constant.int 4096
    %4826 = torch.prim.ListConstruct %int4_5801, %4821, %int4096_5802 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4827 = torch.aten.view %4825, %4826 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4827, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_5803 = torch.constant.int 1
    %4828 = torch.aten.add.Tensor %4793, %4827, %int1_5803 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4828, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_5804 = torch.constant.int 6
    %4829 = torch.prims.convert_element_type %4828, %int6_5804 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4829, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_5805 = torch.constant.int 2
    %4830 = torch.aten.pow.Tensor_Scalar %4829, %int2_5805 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4830, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_5806 = torch.constant.int -1
    %4831 = torch.prim.ListConstruct %int-1_5806 : (!torch.int) -> !torch.list<int>
    %true_5807 = torch.constant.bool true
    %none_5808 = torch.constant.none
    %4832 = torch.aten.mean.dim %4830, %4831, %true_5807, %none_5808 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4832, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_5809 = torch.constant.float 9.9999997473787516E-6
    %int1_5810 = torch.constant.int 1
    %4833 = torch.aten.add.Scalar %4832, %float9.999990e-06_5809, %int1_5810 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4833, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %4834 = torch.aten.rsqrt %4833 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4834, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %4835 = torch.aten.mul.Tensor %4829, %4834 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4835, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_5811 = torch.constant.int 5
    %4836 = torch.prims.convert_element_type %4835, %int5_5811 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4836, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %4837 = torch.aten.mul.Tensor %208, %4836 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4837, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_5812 = torch.constant.int 5
    %4838 = torch.prims.convert_element_type %4837, %int5_5812 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4838, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_5813 = torch.constant.int -2
    %int-1_5814 = torch.constant.int -1
    %4839 = torch.aten.transpose.int %209, %int-2_5813, %int-1_5814 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_5815 = torch.constant.int 4
    %4840 = torch.aten.mul.int %int4_5815, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_5816 = torch.constant.int 4096
    %4841 = torch.prim.ListConstruct %4840, %int4096_5816 : (!torch.int, !torch.int) -> !torch.list<int>
    %4842 = torch.aten.view %4838, %4841 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4842, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %4843 = torch.aten.mm %4842, %4839 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4843, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_5817 = torch.constant.int 4
    %int4096_5818 = torch.constant.int 4096
    %4844 = torch.prim.ListConstruct %int4_5817, %306, %int4096_5818 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4845 = torch.aten.view %4843, %4844 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4845, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_5819 = torch.constant.int -2
    %int-1_5820 = torch.constant.int -1
    %4846 = torch.aten.transpose.int %210, %int-2_5819, %int-1_5820 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_5821 = torch.constant.int 4
    %4847 = torch.aten.mul.int %int4_5821, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_5822 = torch.constant.int 4096
    %4848 = torch.prim.ListConstruct %4847, %int4096_5822 : (!torch.int, !torch.int) -> !torch.list<int>
    %4849 = torch.aten.view %4838, %4848 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4849, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %4850 = torch.aten.mm %4849, %4846 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %4850, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_5823 = torch.constant.int 4
    %int1024_5824 = torch.constant.int 1024
    %4851 = torch.prim.ListConstruct %int4_5823, %306, %int1024_5824 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4852 = torch.aten.view %4850, %4851 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %4852, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int-2_5825 = torch.constant.int -2
    %int-1_5826 = torch.constant.int -1
    %4853 = torch.aten.transpose.int %211, %int-2_5825, %int-1_5826 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_5827 = torch.constant.int 4
    %4854 = torch.aten.mul.int %int4_5827, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_5828 = torch.constant.int 4096
    %4855 = torch.prim.ListConstruct %4854, %int4096_5828 : (!torch.int, !torch.int) -> !torch.list<int>
    %4856 = torch.aten.view %4838, %4855 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4856, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %4857 = torch.aten.mm %4856, %4853 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %4857, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_5829 = torch.constant.int 4
    %int1024_5830 = torch.constant.int 1024
    %4858 = torch.prim.ListConstruct %int4_5829, %306, %int1024_5830 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4859 = torch.aten.view %4857, %4858 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %4859, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int4_5831 = torch.constant.int 4
    %int32_5832 = torch.constant.int 32
    %int128_5833 = torch.constant.int 128
    %4860 = torch.prim.ListConstruct %int4_5831, %306, %int32_5832, %int128_5833 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4861 = torch.aten.view %4845, %4860 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %4861, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_5834 = torch.constant.int 4
    %int8_5835 = torch.constant.int 8
    %int128_5836 = torch.constant.int 128
    %4862 = torch.prim.ListConstruct %int4_5834, %306, %int8_5835, %int128_5836 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4863 = torch.aten.view %4852, %4862 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %4863, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int4_5837 = torch.constant.int 4
    %int8_5838 = torch.constant.int 8
    %int128_5839 = torch.constant.int 128
    %4864 = torch.prim.ListConstruct %int4_5837, %306, %int8_5838, %int128_5839 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4865 = torch.aten.view %4859, %4864 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %4865, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int131072_5840 = torch.constant.int 131072
    %none_5841 = torch.constant.none
    %none_5842 = torch.constant.none
    %cpu_5843 = torch.constant.device "cpu"
    %false_5844 = torch.constant.bool false
    %4866 = torch.aten.arange %int131072_5840, %none_5841, %none_5842, %cpu_5843, %false_5844 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_5845 = torch.constant.int 0
    %int128_5846 = torch.constant.int 128
    %none_5847 = torch.constant.none
    %none_5848 = torch.constant.none
    %cpu_5849 = torch.constant.device "cpu"
    %false_5850 = torch.constant.bool false
    %4867 = torch.aten.arange.start %int0_5845, %int128_5846, %none_5847, %none_5848, %cpu_5849, %false_5850 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_5851 = torch.constant.int 2
    %4868 = torch.aten.floor_divide.Scalar %4867, %int2_5851 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_5852 = torch.constant.int 6
    %4869 = torch.prims.convert_element_type %4868, %int6_5852 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_5853 = torch.constant.int 128
    %4870 = torch.aten.div.Scalar %4869, %int128_5853 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_5854 = torch.constant.float 2.000000e+00
    %4871 = torch.aten.mul.Scalar %4870, %float2.000000e00_5854 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_5855 = torch.constant.float 5.000000e+05
    %4872 = torch.aten.pow.Scalar %float5.000000e05_5855, %4871 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %4873 = torch.aten.reciprocal %4872 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_5856 = torch.constant.float 1.000000e+00
    %4874 = torch.aten.mul.Scalar %4873, %float1.000000e00_5856 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_5857 = torch.constant.int 1
    %4875 = torch.aten.unsqueeze %4866, %int1_5857 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_5858 = torch.constant.int 0
    %4876 = torch.aten.unsqueeze %4874, %int0_5858 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %4877 = torch.aten.mul.Tensor %4875, %4876 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_5859 = torch.constant.int 1
    %4878 = torch.aten.size.int %4845, %int1_5859 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.int
    %int0_5860 = torch.constant.int 0
    %4879 = torch.aten.add.int %int0_5860, %4878 : !torch.int, !torch.int -> !torch.int
    %int0_5861 = torch.constant.int 0
    %int0_5862 = torch.constant.int 0
    %int1_5863 = torch.constant.int 1
    %4880 = torch.aten.slice.Tensor %4877, %int0_5861, %int0_5862, %4879, %int1_5863 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %4880, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_5864 = torch.constant.int 1
    %int0_5865 = torch.constant.int 0
    %int9223372036854775807_5866 = torch.constant.int 9223372036854775807
    %int1_5867 = torch.constant.int 1
    %4881 = torch.aten.slice.Tensor %4880, %int1_5864, %int0_5865, %int9223372036854775807_5866, %int1_5867 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %4881, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_5868 = torch.constant.int 1
    %int0_5869 = torch.constant.int 0
    %int9223372036854775807_5870 = torch.constant.int 9223372036854775807
    %int1_5871 = torch.constant.int 1
    %4882 = torch.aten.slice.Tensor %4881, %int1_5868, %int0_5869, %int9223372036854775807_5870, %int1_5871 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %4882, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_5872 = torch.constant.int 0
    %4883 = torch.aten.unsqueeze %4882, %int0_5872 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %4883, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_5873 = torch.constant.int 1
    %int0_5874 = torch.constant.int 0
    %int9223372036854775807_5875 = torch.constant.int 9223372036854775807
    %int1_5876 = torch.constant.int 1
    %4884 = torch.aten.slice.Tensor %4883, %int1_5873, %int0_5874, %int9223372036854775807_5875, %int1_5876 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %4884, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_5877 = torch.constant.int 2
    %int0_5878 = torch.constant.int 0
    %int9223372036854775807_5879 = torch.constant.int 9223372036854775807
    %int1_5880 = torch.constant.int 1
    %4885 = torch.aten.slice.Tensor %4884, %int2_5877, %int0_5878, %int9223372036854775807_5879, %int1_5880 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %4885, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_5881 = torch.constant.int 4
    %int1_5882 = torch.constant.int 1
    %int1_5883 = torch.constant.int 1
    %4886 = torch.prim.ListConstruct %int4_5881, %int1_5882, %int1_5883 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4887 = torch.aten.repeat %4885, %4886 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %4887, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_5884 = torch.constant.int 6
    %4888 = torch.prims.convert_element_type %4861, %int6_5884 : !torch.vtensor<[4,?,32,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %4888, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %4889 = torch_c.to_builtin_tensor %4888 : !torch.vtensor<[4,?,32,128],f32> -> tensor<4x?x32x128xf32>
    %4890 = torch_c.to_builtin_tensor %4887 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %4891 = util.call @sharktank_rotary_embedding_4_D_32_128_f32(%4889, %4890) : (tensor<4x?x32x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x32x128xf32>
    %4892 = torch_c.from_builtin_tensor %4891 : tensor<4x?x32x128xf32> -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %4892, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %int5_5885 = torch.constant.int 5
    %4893 = torch.prims.convert_element_type %4892, %int5_5885 : !torch.vtensor<[4,?,32,128],f32>, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %4893, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int131072_5886 = torch.constant.int 131072
    %none_5887 = torch.constant.none
    %none_5888 = torch.constant.none
    %cpu_5889 = torch.constant.device "cpu"
    %false_5890 = torch.constant.bool false
    %4894 = torch.aten.arange %int131072_5886, %none_5887, %none_5888, %cpu_5889, %false_5890 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_5891 = torch.constant.int 0
    %int128_5892 = torch.constant.int 128
    %none_5893 = torch.constant.none
    %none_5894 = torch.constant.none
    %cpu_5895 = torch.constant.device "cpu"
    %false_5896 = torch.constant.bool false
    %4895 = torch.aten.arange.start %int0_5891, %int128_5892, %none_5893, %none_5894, %cpu_5895, %false_5896 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_5897 = torch.constant.int 2
    %4896 = torch.aten.floor_divide.Scalar %4895, %int2_5897 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_5898 = torch.constant.int 6
    %4897 = torch.prims.convert_element_type %4896, %int6_5898 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_5899 = torch.constant.int 128
    %4898 = torch.aten.div.Scalar %4897, %int128_5899 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_5900 = torch.constant.float 2.000000e+00
    %4899 = torch.aten.mul.Scalar %4898, %float2.000000e00_5900 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_5901 = torch.constant.float 5.000000e+05
    %4900 = torch.aten.pow.Scalar %float5.000000e05_5901, %4899 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %4901 = torch.aten.reciprocal %4900 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_5902 = torch.constant.float 1.000000e+00
    %4902 = torch.aten.mul.Scalar %4901, %float1.000000e00_5902 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_5903 = torch.constant.int 1
    %4903 = torch.aten.unsqueeze %4894, %int1_5903 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_5904 = torch.constant.int 0
    %4904 = torch.aten.unsqueeze %4902, %int0_5904 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %4905 = torch.aten.mul.Tensor %4903, %4904 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_5905 = torch.constant.int 1
    %4906 = torch.aten.size.int %4852, %int1_5905 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int0_5906 = torch.constant.int 0
    %4907 = torch.aten.add.int %int0_5906, %4906 : !torch.int, !torch.int -> !torch.int
    %int0_5907 = torch.constant.int 0
    %int0_5908 = torch.constant.int 0
    %int1_5909 = torch.constant.int 1
    %4908 = torch.aten.slice.Tensor %4905, %int0_5907, %int0_5908, %4907, %int1_5909 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %4908, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_5910 = torch.constant.int 1
    %int0_5911 = torch.constant.int 0
    %int9223372036854775807_5912 = torch.constant.int 9223372036854775807
    %int1_5913 = torch.constant.int 1
    %4909 = torch.aten.slice.Tensor %4908, %int1_5910, %int0_5911, %int9223372036854775807_5912, %int1_5913 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %4909, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_5914 = torch.constant.int 1
    %int0_5915 = torch.constant.int 0
    %int9223372036854775807_5916 = torch.constant.int 9223372036854775807
    %int1_5917 = torch.constant.int 1
    %4910 = torch.aten.slice.Tensor %4909, %int1_5914, %int0_5915, %int9223372036854775807_5916, %int1_5917 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %4910, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_5918 = torch.constant.int 0
    %4911 = torch.aten.unsqueeze %4910, %int0_5918 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %4911, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_5919 = torch.constant.int 1
    %int0_5920 = torch.constant.int 0
    %int9223372036854775807_5921 = torch.constant.int 9223372036854775807
    %int1_5922 = torch.constant.int 1
    %4912 = torch.aten.slice.Tensor %4911, %int1_5919, %int0_5920, %int9223372036854775807_5921, %int1_5922 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %4912, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_5923 = torch.constant.int 2
    %int0_5924 = torch.constant.int 0
    %int9223372036854775807_5925 = torch.constant.int 9223372036854775807
    %int1_5926 = torch.constant.int 1
    %4913 = torch.aten.slice.Tensor %4912, %int2_5923, %int0_5924, %int9223372036854775807_5925, %int1_5926 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %4913, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_5927 = torch.constant.int 4
    %int1_5928 = torch.constant.int 1
    %int1_5929 = torch.constant.int 1
    %4914 = torch.prim.ListConstruct %int4_5927, %int1_5928, %int1_5929 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4915 = torch.aten.repeat %4913, %4914 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %4915, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_5930 = torch.constant.int 6
    %4916 = torch.prims.convert_element_type %4863, %int6_5930 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %4916, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %4917 = torch_c.to_builtin_tensor %4916 : !torch.vtensor<[4,?,8,128],f32> -> tensor<4x?x8x128xf32>
    %4918 = torch_c.to_builtin_tensor %4915 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %4919 = util.call @sharktank_rotary_embedding_4_D_8_128_f32(%4917, %4918) : (tensor<4x?x8x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x8x128xf32>
    %4920 = torch_c.from_builtin_tensor %4919 : tensor<4x?x8x128xf32> -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %4920, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %int5_5931 = torch.constant.int 5
    %4921 = torch.prims.convert_element_type %4920, %int5_5931 : !torch.vtensor<[4,?,8,128],f32>, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %4921, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int64_5932 = torch.constant.int 64
    %4922 = torch.aten.mul.Scalar %arg2, %int64_5932 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4922, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int46 = torch.constant.int 46
    %int1_5933 = torch.constant.int 1
    %4923 = torch.aten.add.Scalar %4922, %int46, %int1_5933 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4923, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_5934 = torch.constant.int 4
    %int32_5935 = torch.constant.int 32
    %int8_5936 = torch.constant.int 8
    %int128_5937 = torch.constant.int 128
    %4924 = torch.prim.ListConstruct %int4_5934, %398, %int32_5935, %int8_5936, %int128_5937 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4925 = torch.aten.view %4921, %4924 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %4925, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_5938 = torch.constant.int 4
    %4926 = torch.aten.mul.int %int4_5938, %398 : !torch.int, !torch.int -> !torch.int
    %int32_5939 = torch.constant.int 32
    %int8_5940 = torch.constant.int 8
    %int128_5941 = torch.constant.int 128
    %4927 = torch.prim.ListConstruct %4926, %int32_5939, %int8_5940, %int128_5941 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4928 = torch.aten.view %4925, %4927 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %4928, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_5942 = torch.constant.int 4
    %4929 = torch.aten.mul.int %int4_5942, %398 : !torch.int, !torch.int -> !torch.int
    %4930 = torch.prim.ListConstruct %4929 : (!torch.int) -> !torch.list<int>
    %4931 = torch.aten.view %4923, %4930 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4931, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_5943 = torch.constant.int 32
    %int2_5944 = torch.constant.int 2
    %int32_5945 = torch.constant.int 32
    %int8_5946 = torch.constant.int 8
    %int128_5947 = torch.constant.int 128
    %4932 = torch.prim.ListConstruct %389, %int32_5943, %int2_5944, %int32_5945, %int8_5946, %int128_5947 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4933 = torch.aten.view %4765, %4932 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4933, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_5948 = torch.constant.int 32
    %4934 = torch.aten.mul.int %389, %int32_5948 : !torch.int, !torch.int -> !torch.int
    %int2_5949 = torch.constant.int 2
    %4935 = torch.aten.mul.int %4934, %int2_5949 : !torch.int, !torch.int -> !torch.int
    %int32_5950 = torch.constant.int 32
    %int8_5951 = torch.constant.int 8
    %int128_5952 = torch.constant.int 128
    %4936 = torch.prim.ListConstruct %4935, %int32_5950, %int8_5951, %int128_5952 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4937 = torch.aten.view %4933, %4936 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %4937, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %4938 = torch.prim.ListConstruct %4931 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_5953 = torch.constant.bool false
    %4939 = torch.aten.index_put %4937, %4938, %4928, %false_5953 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %4939, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_5954 = torch.constant.int 32
    %int2_5955 = torch.constant.int 2
    %int32_5956 = torch.constant.int 32
    %int8_5957 = torch.constant.int 8
    %int128_5958 = torch.constant.int 128
    %4940 = torch.prim.ListConstruct %389, %int32_5954, %int2_5955, %int32_5956, %int8_5957, %int128_5958 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4941 = torch.aten.view %4939, %4940 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4941, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_5959 = torch.constant.int 2097152
    %4942 = torch.prim.ListConstruct %389, %int2097152_5959 : (!torch.int, !torch.int) -> !torch.list<int>
    %4943 = torch.aten.view %4941, %4942 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %4943, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_5960 = torch.constant.int 32
    %int2_5961 = torch.constant.int 2
    %int32_5962 = torch.constant.int 32
    %int8_5963 = torch.constant.int 8
    %int128_5964 = torch.constant.int 128
    %4944 = torch.prim.ListConstruct %389, %int32_5960, %int2_5961, %int32_5962, %int8_5963, %int128_5964 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4945 = torch.aten.view %4943, %4944 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4945, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_5965 = torch.constant.int 32
    %int8_5966 = torch.constant.int 8
    %int128_5967 = torch.constant.int 128
    %4946 = torch.prim.ListConstruct %4935, %int32_5965, %int8_5966, %int128_5967 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4947 = torch.aten.view %4945, %4946 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %4947, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_5968 = torch.constant.int 4
    %int32_5969 = torch.constant.int 32
    %int8_5970 = torch.constant.int 8
    %int128_5971 = torch.constant.int 128
    %4948 = torch.prim.ListConstruct %int4_5968, %398, %int32_5969, %int8_5970, %int128_5971 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4949 = torch.aten.view %4865, %4948 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %4949, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_5972 = torch.constant.int 4
    %4950 = torch.aten.mul.int %int4_5972, %398 : !torch.int, !torch.int -> !torch.int
    %int32_5973 = torch.constant.int 32
    %int8_5974 = torch.constant.int 8
    %int128_5975 = torch.constant.int 128
    %4951 = torch.prim.ListConstruct %4950, %int32_5973, %int8_5974, %int128_5975 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4952 = torch.aten.view %4949, %4951 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %4952, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int1_5976 = torch.constant.int 1
    %int1_5977 = torch.constant.int 1
    %4953 = torch.aten.add.Scalar %4923, %int1_5976, %int1_5977 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4953, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_5978 = torch.constant.int 4
    %4954 = torch.aten.mul.int %int4_5978, %398 : !torch.int, !torch.int -> !torch.int
    %4955 = torch.prim.ListConstruct %4954 : (!torch.int) -> !torch.list<int>
    %4956 = torch.aten.view %4953, %4955 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4956, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %4957 = torch.prim.ListConstruct %4956 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_5979 = torch.constant.bool false
    %4958 = torch.aten.index_put %4947, %4957, %4952, %false_5979 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %4958, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_5980 = torch.constant.int 32
    %int2_5981 = torch.constant.int 2
    %int32_5982 = torch.constant.int 32
    %int8_5983 = torch.constant.int 8
    %int128_5984 = torch.constant.int 128
    %4959 = torch.prim.ListConstruct %389, %int32_5980, %int2_5981, %int32_5982, %int8_5983, %int128_5984 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4960 = torch.aten.view %4958, %4959 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4960, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_5985 = torch.constant.int 2097152
    %4961 = torch.prim.ListConstruct %389, %int2097152_5985 : (!torch.int, !torch.int) -> !torch.list<int>
    %4962 = torch.aten.view %4960, %4961 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %4962, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int-2_5986 = torch.constant.int -2
    %4963 = torch.aten.unsqueeze %4921, %int-2_5986 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %4963, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int4_5987 = torch.constant.int 4
    %int8_5988 = torch.constant.int 8
    %int4_5989 = torch.constant.int 4
    %int128_5990 = torch.constant.int 128
    %4964 = torch.prim.ListConstruct %int4_5987, %4906, %int8_5988, %int4_5989, %int128_5990 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_5991 = torch.constant.bool false
    %4965 = torch.aten.expand %4963, %4964, %false_5991 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4965, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_5992 = torch.constant.int 0
    %4966 = torch.aten.clone %4965, %int0_5992 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4966, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_5993 = torch.constant.int 4
    %int32_5994 = torch.constant.int 32
    %int128_5995 = torch.constant.int 128
    %4967 = torch.prim.ListConstruct %int4_5993, %4906, %int32_5994, %int128_5995 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4968 = torch.aten._unsafe_view %4966, %4967 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %4968, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_5996 = torch.constant.int -2
    %4969 = torch.aten.unsqueeze %4865, %int-2_5996 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %4969, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_5997 = torch.constant.int 1
    %4970 = torch.aten.size.int %4859, %int1_5997 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int4_5998 = torch.constant.int 4
    %int8_5999 = torch.constant.int 8
    %int4_6000 = torch.constant.int 4
    %int128_6001 = torch.constant.int 128
    %4971 = torch.prim.ListConstruct %int4_5998, %4970, %int8_5999, %int4_6000, %int128_6001 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_6002 = torch.constant.bool false
    %4972 = torch.aten.expand %4969, %4971, %false_6002 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4972, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_6003 = torch.constant.int 0
    %4973 = torch.aten.clone %4972, %int0_6003 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4973, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_6004 = torch.constant.int 4
    %int32_6005 = torch.constant.int 32
    %int128_6006 = torch.constant.int 128
    %4974 = torch.prim.ListConstruct %int4_6004, %4970, %int32_6005, %int128_6006 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4975 = torch.aten._unsafe_view %4973, %4974 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %4975, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_6007 = torch.constant.int 1
    %int2_6008 = torch.constant.int 2
    %4976 = torch.aten.transpose.int %4893, %int1_6007, %int2_6008 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %4976, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_6009 = torch.constant.int 1
    %int2_6010 = torch.constant.int 2
    %4977 = torch.aten.transpose.int %4968, %int1_6009, %int2_6010 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %4977, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_6011 = torch.constant.int 1
    %int2_6012 = torch.constant.int 2
    %4978 = torch.aten.transpose.int %4975, %int1_6011, %int2_6012 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %4978, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_6013 = torch.constant.float 0.000000e+00
    %true_6014 = torch.constant.bool true
    %none_6015 = torch.constant.none
    %none_6016 = torch.constant.none
    %4979:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4976, %4977, %4978, %float0.000000e00_6013, %true_6014, %none_6015, %none_6016) : (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?],f32>) 
    torch.bind_symbolic_shape %4979#0, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_6017 = torch.constant.int 1
    %int2_6018 = torch.constant.int 2
    %4980 = torch.aten.transpose.int %4979#0, %int1_6017, %int2_6018 : !torch.vtensor<[4,32,?,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %4980, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_6019 = torch.constant.int 4
    %int4096_6020 = torch.constant.int 4096
    %4981 = torch.prim.ListConstruct %int4_6019, %4878, %int4096_6020 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4982 = torch.aten.view %4980, %4981 : !torch.vtensor<[4,?,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4982, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_6021 = torch.constant.int -2
    %int-1_6022 = torch.constant.int -1
    %4983 = torch.aten.transpose.int %212, %int-2_6021, %int-1_6022 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_6023 = torch.constant.int 4
    %4984 = torch.aten.mul.int %int4_6023, %4878 : !torch.int, !torch.int -> !torch.int
    %int4096_6024 = torch.constant.int 4096
    %4985 = torch.prim.ListConstruct %4984, %int4096_6024 : (!torch.int, !torch.int) -> !torch.list<int>
    %4986 = torch.aten.view %4982, %4985 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4986, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %4987 = torch.aten.mm %4986, %4983 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %4987, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_6025 = torch.constant.int 4
    %int4096_6026 = torch.constant.int 4096
    %4988 = torch.prim.ListConstruct %int4_6025, %4878, %int4096_6026 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4989 = torch.aten.view %4987, %4988 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4989, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_6027 = torch.constant.int 1
    %4990 = torch.aten.add.Tensor %4828, %4989, %int1_6027 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4990, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_6028 = torch.constant.int 6
    %4991 = torch.prims.convert_element_type %4990, %int6_6028 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4991, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_6029 = torch.constant.int 2
    %4992 = torch.aten.pow.Tensor_Scalar %4991, %int2_6029 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4992, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_6030 = torch.constant.int -1
    %4993 = torch.prim.ListConstruct %int-1_6030 : (!torch.int) -> !torch.list<int>
    %true_6031 = torch.constant.bool true
    %none_6032 = torch.constant.none
    %4994 = torch.aten.mean.dim %4992, %4993, %true_6031, %none_6032 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4994, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_6033 = torch.constant.float 9.9999997473787516E-6
    %int1_6034 = torch.constant.int 1
    %4995 = torch.aten.add.Scalar %4994, %float9.999990e-06_6033, %int1_6034 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4995, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %4996 = torch.aten.rsqrt %4995 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %4996, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %4997 = torch.aten.mul.Tensor %4991, %4996 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4997, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_6035 = torch.constant.int 5
    %4998 = torch.prims.convert_element_type %4997, %int5_6035 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %4998, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %4999 = torch.aten.mul.Tensor %213, %4998 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %4999, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_6036 = torch.constant.int 5
    %5000 = torch.prims.convert_element_type %4999, %int5_6036 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5000, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_6037 = torch.constant.int -2
    %int-1_6038 = torch.constant.int -1
    %5001 = torch.aten.transpose.int %214, %int-2_6037, %int-1_6038 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_6039 = torch.constant.int 4
    %5002 = torch.aten.mul.int %int4_6039, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_6040 = torch.constant.int 4096
    %5003 = torch.prim.ListConstruct %5002, %int4096_6040 : (!torch.int, !torch.int) -> !torch.list<int>
    %5004 = torch.aten.view %5000, %5003 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5004, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %5005 = torch.aten.mm %5004, %5001 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %5005, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_6041 = torch.constant.int 4
    %int14336_6042 = torch.constant.int 14336
    %5006 = torch.prim.ListConstruct %int4_6041, %306, %int14336_6042 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5007 = torch.aten.view %5005, %5006 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %5007, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %5008 = torch.aten.silu %5007 : !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %5008, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_6043 = torch.constant.int -2
    %int-1_6044 = torch.constant.int -1
    %5009 = torch.aten.transpose.int %215, %int-2_6043, %int-1_6044 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_6045 = torch.constant.int 4
    %5010 = torch.aten.mul.int %int4_6045, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_6046 = torch.constant.int 4096
    %5011 = torch.prim.ListConstruct %5010, %int4096_6046 : (!torch.int, !torch.int) -> !torch.list<int>
    %5012 = torch.aten.view %5000, %5011 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5012, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %5013 = torch.aten.mm %5012, %5009 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %5013, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_6047 = torch.constant.int 4
    %int14336_6048 = torch.constant.int 14336
    %5014 = torch.prim.ListConstruct %int4_6047, %306, %int14336_6048 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5015 = torch.aten.view %5013, %5014 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %5015, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %5016 = torch.aten.mul.Tensor %5008, %5015 : !torch.vtensor<[4,?,14336],f16>, !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %5016, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_6049 = torch.constant.int -2
    %int-1_6050 = torch.constant.int -1
    %5017 = torch.aten.transpose.int %216, %int-2_6049, %int-1_6050 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int1_6051 = torch.constant.int 1
    %5018 = torch.aten.size.int %5007, %int1_6051 : !torch.vtensor<[4,?,14336],f16>, !torch.int -> !torch.int
    %int4_6052 = torch.constant.int 4
    %5019 = torch.aten.mul.int %int4_6052, %5018 : !torch.int, !torch.int -> !torch.int
    %int14336_6053 = torch.constant.int 14336
    %5020 = torch.prim.ListConstruct %5019, %int14336_6053 : (!torch.int, !torch.int) -> !torch.list<int>
    %5021 = torch.aten.view %5016, %5020 : !torch.vtensor<[4,?,14336],f16>, !torch.list<int> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %5021, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %5022 = torch.aten.mm %5021, %5017 : !torch.vtensor<[?,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5022, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_6054 = torch.constant.int 4
    %int4096_6055 = torch.constant.int 4096
    %5023 = torch.prim.ListConstruct %int4_6054, %5018, %int4096_6055 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5024 = torch.aten.view %5022, %5023 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5024, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_6056 = torch.constant.int 1
    %5025 = torch.aten.add.Tensor %4990, %5024, %int1_6056 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5025, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_6057 = torch.constant.int 6
    %5026 = torch.prims.convert_element_type %5025, %int6_6057 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5026, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_6058 = torch.constant.int 2
    %5027 = torch.aten.pow.Tensor_Scalar %5026, %int2_6058 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5027, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_6059 = torch.constant.int -1
    %5028 = torch.prim.ListConstruct %int-1_6059 : (!torch.int) -> !torch.list<int>
    %true_6060 = torch.constant.bool true
    %none_6061 = torch.constant.none
    %5029 = torch.aten.mean.dim %5027, %5028, %true_6060, %none_6061 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5029, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_6062 = torch.constant.float 9.9999997473787516E-6
    %int1_6063 = torch.constant.int 1
    %5030 = torch.aten.add.Scalar %5029, %float9.999990e-06_6062, %int1_6063 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5030, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %5031 = torch.aten.rsqrt %5030 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5031, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %5032 = torch.aten.mul.Tensor %5026, %5031 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5032, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_6064 = torch.constant.int 5
    %5033 = torch.prims.convert_element_type %5032, %int5_6064 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5033, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %5034 = torch.aten.mul.Tensor %217, %5033 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5034, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_6065 = torch.constant.int 5
    %5035 = torch.prims.convert_element_type %5034, %int5_6065 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5035, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_6066 = torch.constant.int -2
    %int-1_6067 = torch.constant.int -1
    %5036 = torch.aten.transpose.int %218, %int-2_6066, %int-1_6067 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_6068 = torch.constant.int 4
    %5037 = torch.aten.mul.int %int4_6068, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_6069 = torch.constant.int 4096
    %5038 = torch.prim.ListConstruct %5037, %int4096_6069 : (!torch.int, !torch.int) -> !torch.list<int>
    %5039 = torch.aten.view %5035, %5038 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5039, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %5040 = torch.aten.mm %5039, %5036 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5040, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_6070 = torch.constant.int 4
    %int4096_6071 = torch.constant.int 4096
    %5041 = torch.prim.ListConstruct %int4_6070, %306, %int4096_6071 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5042 = torch.aten.view %5040, %5041 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5042, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_6072 = torch.constant.int -2
    %int-1_6073 = torch.constant.int -1
    %5043 = torch.aten.transpose.int %219, %int-2_6072, %int-1_6073 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_6074 = torch.constant.int 4
    %5044 = torch.aten.mul.int %int4_6074, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_6075 = torch.constant.int 4096
    %5045 = torch.prim.ListConstruct %5044, %int4096_6075 : (!torch.int, !torch.int) -> !torch.list<int>
    %5046 = torch.aten.view %5035, %5045 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5046, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %5047 = torch.aten.mm %5046, %5043 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %5047, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_6076 = torch.constant.int 4
    %int1024_6077 = torch.constant.int 1024
    %5048 = torch.prim.ListConstruct %int4_6076, %306, %int1024_6077 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5049 = torch.aten.view %5047, %5048 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %5049, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int-2_6078 = torch.constant.int -2
    %int-1_6079 = torch.constant.int -1
    %5050 = torch.aten.transpose.int %220, %int-2_6078, %int-1_6079 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_6080 = torch.constant.int 4
    %5051 = torch.aten.mul.int %int4_6080, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_6081 = torch.constant.int 4096
    %5052 = torch.prim.ListConstruct %5051, %int4096_6081 : (!torch.int, !torch.int) -> !torch.list<int>
    %5053 = torch.aten.view %5035, %5052 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5053, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %5054 = torch.aten.mm %5053, %5050 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %5054, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_6082 = torch.constant.int 4
    %int1024_6083 = torch.constant.int 1024
    %5055 = torch.prim.ListConstruct %int4_6082, %306, %int1024_6083 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5056 = torch.aten.view %5054, %5055 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %5056, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int4_6084 = torch.constant.int 4
    %int32_6085 = torch.constant.int 32
    %int128_6086 = torch.constant.int 128
    %5057 = torch.prim.ListConstruct %int4_6084, %306, %int32_6085, %int128_6086 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5058 = torch.aten.view %5042, %5057 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %5058, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_6087 = torch.constant.int 4
    %int8_6088 = torch.constant.int 8
    %int128_6089 = torch.constant.int 128
    %5059 = torch.prim.ListConstruct %int4_6087, %306, %int8_6088, %int128_6089 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5060 = torch.aten.view %5049, %5059 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %5060, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int4_6090 = torch.constant.int 4
    %int8_6091 = torch.constant.int 8
    %int128_6092 = torch.constant.int 128
    %5061 = torch.prim.ListConstruct %int4_6090, %306, %int8_6091, %int128_6092 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5062 = torch.aten.view %5056, %5061 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %5062, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int131072_6093 = torch.constant.int 131072
    %none_6094 = torch.constant.none
    %none_6095 = torch.constant.none
    %cpu_6096 = torch.constant.device "cpu"
    %false_6097 = torch.constant.bool false
    %5063 = torch.aten.arange %int131072_6093, %none_6094, %none_6095, %cpu_6096, %false_6097 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_6098 = torch.constant.int 0
    %int128_6099 = torch.constant.int 128
    %none_6100 = torch.constant.none
    %none_6101 = torch.constant.none
    %cpu_6102 = torch.constant.device "cpu"
    %false_6103 = torch.constant.bool false
    %5064 = torch.aten.arange.start %int0_6098, %int128_6099, %none_6100, %none_6101, %cpu_6102, %false_6103 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_6104 = torch.constant.int 2
    %5065 = torch.aten.floor_divide.Scalar %5064, %int2_6104 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_6105 = torch.constant.int 6
    %5066 = torch.prims.convert_element_type %5065, %int6_6105 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_6106 = torch.constant.int 128
    %5067 = torch.aten.div.Scalar %5066, %int128_6106 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_6107 = torch.constant.float 2.000000e+00
    %5068 = torch.aten.mul.Scalar %5067, %float2.000000e00_6107 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_6108 = torch.constant.float 5.000000e+05
    %5069 = torch.aten.pow.Scalar %float5.000000e05_6108, %5068 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %5070 = torch.aten.reciprocal %5069 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_6109 = torch.constant.float 1.000000e+00
    %5071 = torch.aten.mul.Scalar %5070, %float1.000000e00_6109 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_6110 = torch.constant.int 1
    %5072 = torch.aten.unsqueeze %5063, %int1_6110 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_6111 = torch.constant.int 0
    %5073 = torch.aten.unsqueeze %5071, %int0_6111 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %5074 = torch.aten.mul.Tensor %5072, %5073 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_6112 = torch.constant.int 1
    %5075 = torch.aten.size.int %5042, %int1_6112 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.int
    %int0_6113 = torch.constant.int 0
    %5076 = torch.aten.add.int %int0_6113, %5075 : !torch.int, !torch.int -> !torch.int
    %int0_6114 = torch.constant.int 0
    %int0_6115 = torch.constant.int 0
    %int1_6116 = torch.constant.int 1
    %5077 = torch.aten.slice.Tensor %5074, %int0_6114, %int0_6115, %5076, %int1_6116 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %5077, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_6117 = torch.constant.int 1
    %int0_6118 = torch.constant.int 0
    %int9223372036854775807_6119 = torch.constant.int 9223372036854775807
    %int1_6120 = torch.constant.int 1
    %5078 = torch.aten.slice.Tensor %5077, %int1_6117, %int0_6118, %int9223372036854775807_6119, %int1_6120 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %5078, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_6121 = torch.constant.int 1
    %int0_6122 = torch.constant.int 0
    %int9223372036854775807_6123 = torch.constant.int 9223372036854775807
    %int1_6124 = torch.constant.int 1
    %5079 = torch.aten.slice.Tensor %5078, %int1_6121, %int0_6122, %int9223372036854775807_6123, %int1_6124 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %5079, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_6125 = torch.constant.int 0
    %5080 = torch.aten.unsqueeze %5079, %int0_6125 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %5080, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_6126 = torch.constant.int 1
    %int0_6127 = torch.constant.int 0
    %int9223372036854775807_6128 = torch.constant.int 9223372036854775807
    %int1_6129 = torch.constant.int 1
    %5081 = torch.aten.slice.Tensor %5080, %int1_6126, %int0_6127, %int9223372036854775807_6128, %int1_6129 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %5081, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_6130 = torch.constant.int 2
    %int0_6131 = torch.constant.int 0
    %int9223372036854775807_6132 = torch.constant.int 9223372036854775807
    %int1_6133 = torch.constant.int 1
    %5082 = torch.aten.slice.Tensor %5081, %int2_6130, %int0_6131, %int9223372036854775807_6132, %int1_6133 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %5082, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_6134 = torch.constant.int 4
    %int1_6135 = torch.constant.int 1
    %int1_6136 = torch.constant.int 1
    %5083 = torch.prim.ListConstruct %int4_6134, %int1_6135, %int1_6136 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5084 = torch.aten.repeat %5082, %5083 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %5084, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_6137 = torch.constant.int 6
    %5085 = torch.prims.convert_element_type %5058, %int6_6137 : !torch.vtensor<[4,?,32,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %5085, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %5086 = torch_c.to_builtin_tensor %5085 : !torch.vtensor<[4,?,32,128],f32> -> tensor<4x?x32x128xf32>
    %5087 = torch_c.to_builtin_tensor %5084 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %5088 = util.call @sharktank_rotary_embedding_4_D_32_128_f32(%5086, %5087) : (tensor<4x?x32x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x32x128xf32>
    %5089 = torch_c.from_builtin_tensor %5088 : tensor<4x?x32x128xf32> -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %5089, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %int5_6138 = torch.constant.int 5
    %5090 = torch.prims.convert_element_type %5089, %int5_6138 : !torch.vtensor<[4,?,32,128],f32>, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %5090, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int131072_6139 = torch.constant.int 131072
    %none_6140 = torch.constant.none
    %none_6141 = torch.constant.none
    %cpu_6142 = torch.constant.device "cpu"
    %false_6143 = torch.constant.bool false
    %5091 = torch.aten.arange %int131072_6139, %none_6140, %none_6141, %cpu_6142, %false_6143 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_6144 = torch.constant.int 0
    %int128_6145 = torch.constant.int 128
    %none_6146 = torch.constant.none
    %none_6147 = torch.constant.none
    %cpu_6148 = torch.constant.device "cpu"
    %false_6149 = torch.constant.bool false
    %5092 = torch.aten.arange.start %int0_6144, %int128_6145, %none_6146, %none_6147, %cpu_6148, %false_6149 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_6150 = torch.constant.int 2
    %5093 = torch.aten.floor_divide.Scalar %5092, %int2_6150 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_6151 = torch.constant.int 6
    %5094 = torch.prims.convert_element_type %5093, %int6_6151 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_6152 = torch.constant.int 128
    %5095 = torch.aten.div.Scalar %5094, %int128_6152 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_6153 = torch.constant.float 2.000000e+00
    %5096 = torch.aten.mul.Scalar %5095, %float2.000000e00_6153 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_6154 = torch.constant.float 5.000000e+05
    %5097 = torch.aten.pow.Scalar %float5.000000e05_6154, %5096 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %5098 = torch.aten.reciprocal %5097 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_6155 = torch.constant.float 1.000000e+00
    %5099 = torch.aten.mul.Scalar %5098, %float1.000000e00_6155 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_6156 = torch.constant.int 1
    %5100 = torch.aten.unsqueeze %5091, %int1_6156 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_6157 = torch.constant.int 0
    %5101 = torch.aten.unsqueeze %5099, %int0_6157 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %5102 = torch.aten.mul.Tensor %5100, %5101 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_6158 = torch.constant.int 1
    %5103 = torch.aten.size.int %5049, %int1_6158 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int0_6159 = torch.constant.int 0
    %5104 = torch.aten.add.int %int0_6159, %5103 : !torch.int, !torch.int -> !torch.int
    %int0_6160 = torch.constant.int 0
    %int0_6161 = torch.constant.int 0
    %int1_6162 = torch.constant.int 1
    %5105 = torch.aten.slice.Tensor %5102, %int0_6160, %int0_6161, %5104, %int1_6162 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %5105, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_6163 = torch.constant.int 1
    %int0_6164 = torch.constant.int 0
    %int9223372036854775807_6165 = torch.constant.int 9223372036854775807
    %int1_6166 = torch.constant.int 1
    %5106 = torch.aten.slice.Tensor %5105, %int1_6163, %int0_6164, %int9223372036854775807_6165, %int1_6166 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %5106, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_6167 = torch.constant.int 1
    %int0_6168 = torch.constant.int 0
    %int9223372036854775807_6169 = torch.constant.int 9223372036854775807
    %int1_6170 = torch.constant.int 1
    %5107 = torch.aten.slice.Tensor %5106, %int1_6167, %int0_6168, %int9223372036854775807_6169, %int1_6170 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %5107, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_6171 = torch.constant.int 0
    %5108 = torch.aten.unsqueeze %5107, %int0_6171 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %5108, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_6172 = torch.constant.int 1
    %int0_6173 = torch.constant.int 0
    %int9223372036854775807_6174 = torch.constant.int 9223372036854775807
    %int1_6175 = torch.constant.int 1
    %5109 = torch.aten.slice.Tensor %5108, %int1_6172, %int0_6173, %int9223372036854775807_6174, %int1_6175 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %5109, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_6176 = torch.constant.int 2
    %int0_6177 = torch.constant.int 0
    %int9223372036854775807_6178 = torch.constant.int 9223372036854775807
    %int1_6179 = torch.constant.int 1
    %5110 = torch.aten.slice.Tensor %5109, %int2_6176, %int0_6177, %int9223372036854775807_6178, %int1_6179 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %5110, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_6180 = torch.constant.int 4
    %int1_6181 = torch.constant.int 1
    %int1_6182 = torch.constant.int 1
    %5111 = torch.prim.ListConstruct %int4_6180, %int1_6181, %int1_6182 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5112 = torch.aten.repeat %5110, %5111 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %5112, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_6183 = torch.constant.int 6
    %5113 = torch.prims.convert_element_type %5060, %int6_6183 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %5113, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %5114 = torch_c.to_builtin_tensor %5113 : !torch.vtensor<[4,?,8,128],f32> -> tensor<4x?x8x128xf32>
    %5115 = torch_c.to_builtin_tensor %5112 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %5116 = util.call @sharktank_rotary_embedding_4_D_8_128_f32(%5114, %5115) : (tensor<4x?x8x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x8x128xf32>
    %5117 = torch_c.from_builtin_tensor %5116 : tensor<4x?x8x128xf32> -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %5117, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %int5_6184 = torch.constant.int 5
    %5118 = torch.prims.convert_element_type %5117, %int5_6184 : !torch.vtensor<[4,?,8,128],f32>, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %5118, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int64_6185 = torch.constant.int 64
    %5119 = torch.aten.mul.Scalar %arg2, %int64_6185 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5119, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int48 = torch.constant.int 48
    %int1_6186 = torch.constant.int 1
    %5120 = torch.aten.add.Scalar %5119, %int48, %int1_6186 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5120, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_6187 = torch.constant.int 4
    %int32_6188 = torch.constant.int 32
    %int8_6189 = torch.constant.int 8
    %int128_6190 = torch.constant.int 128
    %5121 = torch.prim.ListConstruct %int4_6187, %398, %int32_6188, %int8_6189, %int128_6190 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5122 = torch.aten.view %5118, %5121 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5122, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_6191 = torch.constant.int 4
    %5123 = torch.aten.mul.int %int4_6191, %398 : !torch.int, !torch.int -> !torch.int
    %int32_6192 = torch.constant.int 32
    %int8_6193 = torch.constant.int 8
    %int128_6194 = torch.constant.int 128
    %5124 = torch.prim.ListConstruct %5123, %int32_6192, %int8_6193, %int128_6194 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5125 = torch.aten.view %5122, %5124 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %5125, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_6195 = torch.constant.int 4
    %5126 = torch.aten.mul.int %int4_6195, %398 : !torch.int, !torch.int -> !torch.int
    %5127 = torch.prim.ListConstruct %5126 : (!torch.int) -> !torch.list<int>
    %5128 = torch.aten.view %5120, %5127 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5128, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_6196 = torch.constant.int 32
    %int2_6197 = torch.constant.int 2
    %int32_6198 = torch.constant.int 32
    %int8_6199 = torch.constant.int 8
    %int128_6200 = torch.constant.int 128
    %5129 = torch.prim.ListConstruct %389, %int32_6196, %int2_6197, %int32_6198, %int8_6199, %int128_6200 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5130 = torch.aten.view %4962, %5129 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5130, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_6201 = torch.constant.int 32
    %5131 = torch.aten.mul.int %389, %int32_6201 : !torch.int, !torch.int -> !torch.int
    %int2_6202 = torch.constant.int 2
    %5132 = torch.aten.mul.int %5131, %int2_6202 : !torch.int, !torch.int -> !torch.int
    %int32_6203 = torch.constant.int 32
    %int8_6204 = torch.constant.int 8
    %int128_6205 = torch.constant.int 128
    %5133 = torch.prim.ListConstruct %5132, %int32_6203, %int8_6204, %int128_6205 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5134 = torch.aten.view %5130, %5133 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %5134, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %5135 = torch.prim.ListConstruct %5128 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_6206 = torch.constant.bool false
    %5136 = torch.aten.index_put %5134, %5135, %5125, %false_6206 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %5136, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_6207 = torch.constant.int 32
    %int2_6208 = torch.constant.int 2
    %int32_6209 = torch.constant.int 32
    %int8_6210 = torch.constant.int 8
    %int128_6211 = torch.constant.int 128
    %5137 = torch.prim.ListConstruct %389, %int32_6207, %int2_6208, %int32_6209, %int8_6210, %int128_6211 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5138 = torch.aten.view %5136, %5137 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5138, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_6212 = torch.constant.int 2097152
    %5139 = torch.prim.ListConstruct %389, %int2097152_6212 : (!torch.int, !torch.int) -> !torch.list<int>
    %5140 = torch.aten.view %5138, %5139 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %5140, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_6213 = torch.constant.int 32
    %int2_6214 = torch.constant.int 2
    %int32_6215 = torch.constant.int 32
    %int8_6216 = torch.constant.int 8
    %int128_6217 = torch.constant.int 128
    %5141 = torch.prim.ListConstruct %389, %int32_6213, %int2_6214, %int32_6215, %int8_6216, %int128_6217 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5142 = torch.aten.view %5140, %5141 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5142, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_6218 = torch.constant.int 32
    %int8_6219 = torch.constant.int 8
    %int128_6220 = torch.constant.int 128
    %5143 = torch.prim.ListConstruct %5132, %int32_6218, %int8_6219, %int128_6220 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5144 = torch.aten.view %5142, %5143 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %5144, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_6221 = torch.constant.int 4
    %int32_6222 = torch.constant.int 32
    %int8_6223 = torch.constant.int 8
    %int128_6224 = torch.constant.int 128
    %5145 = torch.prim.ListConstruct %int4_6221, %398, %int32_6222, %int8_6223, %int128_6224 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5146 = torch.aten.view %5062, %5145 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5146, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_6225 = torch.constant.int 4
    %5147 = torch.aten.mul.int %int4_6225, %398 : !torch.int, !torch.int -> !torch.int
    %int32_6226 = torch.constant.int 32
    %int8_6227 = torch.constant.int 8
    %int128_6228 = torch.constant.int 128
    %5148 = torch.prim.ListConstruct %5147, %int32_6226, %int8_6227, %int128_6228 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5149 = torch.aten.view %5146, %5148 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %5149, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int1_6229 = torch.constant.int 1
    %int1_6230 = torch.constant.int 1
    %5150 = torch.aten.add.Scalar %5120, %int1_6229, %int1_6230 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5150, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_6231 = torch.constant.int 4
    %5151 = torch.aten.mul.int %int4_6231, %398 : !torch.int, !torch.int -> !torch.int
    %5152 = torch.prim.ListConstruct %5151 : (!torch.int) -> !torch.list<int>
    %5153 = torch.aten.view %5150, %5152 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5153, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %5154 = torch.prim.ListConstruct %5153 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_6232 = torch.constant.bool false
    %5155 = torch.aten.index_put %5144, %5154, %5149, %false_6232 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %5155, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_6233 = torch.constant.int 32
    %int2_6234 = torch.constant.int 2
    %int32_6235 = torch.constant.int 32
    %int8_6236 = torch.constant.int 8
    %int128_6237 = torch.constant.int 128
    %5156 = torch.prim.ListConstruct %389, %int32_6233, %int2_6234, %int32_6235, %int8_6236, %int128_6237 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5157 = torch.aten.view %5155, %5156 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5157, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_6238 = torch.constant.int 2097152
    %5158 = torch.prim.ListConstruct %389, %int2097152_6238 : (!torch.int, !torch.int) -> !torch.list<int>
    %5159 = torch.aten.view %5157, %5158 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %5159, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int-2_6239 = torch.constant.int -2
    %5160 = torch.aten.unsqueeze %5118, %int-2_6239 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %5160, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int4_6240 = torch.constant.int 4
    %int8_6241 = torch.constant.int 8
    %int4_6242 = torch.constant.int 4
    %int128_6243 = torch.constant.int 128
    %5161 = torch.prim.ListConstruct %int4_6240, %5103, %int8_6241, %int4_6242, %int128_6243 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_6244 = torch.constant.bool false
    %5162 = torch.aten.expand %5160, %5161, %false_6244 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5162, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_6245 = torch.constant.int 0
    %5163 = torch.aten.clone %5162, %int0_6245 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5163, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_6246 = torch.constant.int 4
    %int32_6247 = torch.constant.int 32
    %int128_6248 = torch.constant.int 128
    %5164 = torch.prim.ListConstruct %int4_6246, %5103, %int32_6247, %int128_6248 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5165 = torch.aten._unsafe_view %5163, %5164 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %5165, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_6249 = torch.constant.int -2
    %5166 = torch.aten.unsqueeze %5062, %int-2_6249 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %5166, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_6250 = torch.constant.int 1
    %5167 = torch.aten.size.int %5056, %int1_6250 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int4_6251 = torch.constant.int 4
    %int8_6252 = torch.constant.int 8
    %int4_6253 = torch.constant.int 4
    %int128_6254 = torch.constant.int 128
    %5168 = torch.prim.ListConstruct %int4_6251, %5167, %int8_6252, %int4_6253, %int128_6254 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_6255 = torch.constant.bool false
    %5169 = torch.aten.expand %5166, %5168, %false_6255 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5169, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_6256 = torch.constant.int 0
    %5170 = torch.aten.clone %5169, %int0_6256 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5170, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_6257 = torch.constant.int 4
    %int32_6258 = torch.constant.int 32
    %int128_6259 = torch.constant.int 128
    %5171 = torch.prim.ListConstruct %int4_6257, %5167, %int32_6258, %int128_6259 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5172 = torch.aten._unsafe_view %5170, %5171 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %5172, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_6260 = torch.constant.int 1
    %int2_6261 = torch.constant.int 2
    %5173 = torch.aten.transpose.int %5090, %int1_6260, %int2_6261 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %5173, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_6262 = torch.constant.int 1
    %int2_6263 = torch.constant.int 2
    %5174 = torch.aten.transpose.int %5165, %int1_6262, %int2_6263 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %5174, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_6264 = torch.constant.int 1
    %int2_6265 = torch.constant.int 2
    %5175 = torch.aten.transpose.int %5172, %int1_6264, %int2_6265 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %5175, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_6266 = torch.constant.float 0.000000e+00
    %true_6267 = torch.constant.bool true
    %none_6268 = torch.constant.none
    %none_6269 = torch.constant.none
    %5176:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5173, %5174, %5175, %float0.000000e00_6266, %true_6267, %none_6268, %none_6269) : (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?],f32>) 
    torch.bind_symbolic_shape %5176#0, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_6270 = torch.constant.int 1
    %int2_6271 = torch.constant.int 2
    %5177 = torch.aten.transpose.int %5176#0, %int1_6270, %int2_6271 : !torch.vtensor<[4,32,?,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %5177, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_6272 = torch.constant.int 4
    %int4096_6273 = torch.constant.int 4096
    %5178 = torch.prim.ListConstruct %int4_6272, %5075, %int4096_6273 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5179 = torch.aten.view %5177, %5178 : !torch.vtensor<[4,?,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5179, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_6274 = torch.constant.int -2
    %int-1_6275 = torch.constant.int -1
    %5180 = torch.aten.transpose.int %221, %int-2_6274, %int-1_6275 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_6276 = torch.constant.int 4
    %5181 = torch.aten.mul.int %int4_6276, %5075 : !torch.int, !torch.int -> !torch.int
    %int4096_6277 = torch.constant.int 4096
    %5182 = torch.prim.ListConstruct %5181, %int4096_6277 : (!torch.int, !torch.int) -> !torch.list<int>
    %5183 = torch.aten.view %5179, %5182 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5183, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %5184 = torch.aten.mm %5183, %5180 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5184, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_6278 = torch.constant.int 4
    %int4096_6279 = torch.constant.int 4096
    %5185 = torch.prim.ListConstruct %int4_6278, %5075, %int4096_6279 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5186 = torch.aten.view %5184, %5185 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5186, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_6280 = torch.constant.int 1
    %5187 = torch.aten.add.Tensor %5025, %5186, %int1_6280 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5187, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_6281 = torch.constant.int 6
    %5188 = torch.prims.convert_element_type %5187, %int6_6281 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5188, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_6282 = torch.constant.int 2
    %5189 = torch.aten.pow.Tensor_Scalar %5188, %int2_6282 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5189, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_6283 = torch.constant.int -1
    %5190 = torch.prim.ListConstruct %int-1_6283 : (!torch.int) -> !torch.list<int>
    %true_6284 = torch.constant.bool true
    %none_6285 = torch.constant.none
    %5191 = torch.aten.mean.dim %5189, %5190, %true_6284, %none_6285 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5191, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_6286 = torch.constant.float 9.9999997473787516E-6
    %int1_6287 = torch.constant.int 1
    %5192 = torch.aten.add.Scalar %5191, %float9.999990e-06_6286, %int1_6287 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5192, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %5193 = torch.aten.rsqrt %5192 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5193, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %5194 = torch.aten.mul.Tensor %5188, %5193 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5194, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_6288 = torch.constant.int 5
    %5195 = torch.prims.convert_element_type %5194, %int5_6288 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5195, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %5196 = torch.aten.mul.Tensor %222, %5195 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5196, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_6289 = torch.constant.int 5
    %5197 = torch.prims.convert_element_type %5196, %int5_6289 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5197, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_6290 = torch.constant.int -2
    %int-1_6291 = torch.constant.int -1
    %5198 = torch.aten.transpose.int %223, %int-2_6290, %int-1_6291 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_6292 = torch.constant.int 4
    %5199 = torch.aten.mul.int %int4_6292, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_6293 = torch.constant.int 4096
    %5200 = torch.prim.ListConstruct %5199, %int4096_6293 : (!torch.int, !torch.int) -> !torch.list<int>
    %5201 = torch.aten.view %5197, %5200 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5201, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %5202 = torch.aten.mm %5201, %5198 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %5202, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_6294 = torch.constant.int 4
    %int14336_6295 = torch.constant.int 14336
    %5203 = torch.prim.ListConstruct %int4_6294, %306, %int14336_6295 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5204 = torch.aten.view %5202, %5203 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %5204, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %5205 = torch.aten.silu %5204 : !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %5205, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_6296 = torch.constant.int -2
    %int-1_6297 = torch.constant.int -1
    %5206 = torch.aten.transpose.int %224, %int-2_6296, %int-1_6297 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_6298 = torch.constant.int 4
    %5207 = torch.aten.mul.int %int4_6298, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_6299 = torch.constant.int 4096
    %5208 = torch.prim.ListConstruct %5207, %int4096_6299 : (!torch.int, !torch.int) -> !torch.list<int>
    %5209 = torch.aten.view %5197, %5208 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5209, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %5210 = torch.aten.mm %5209, %5206 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %5210, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_6300 = torch.constant.int 4
    %int14336_6301 = torch.constant.int 14336
    %5211 = torch.prim.ListConstruct %int4_6300, %306, %int14336_6301 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5212 = torch.aten.view %5210, %5211 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %5212, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %5213 = torch.aten.mul.Tensor %5205, %5212 : !torch.vtensor<[4,?,14336],f16>, !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %5213, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_6302 = torch.constant.int -2
    %int-1_6303 = torch.constant.int -1
    %5214 = torch.aten.transpose.int %225, %int-2_6302, %int-1_6303 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int1_6304 = torch.constant.int 1
    %5215 = torch.aten.size.int %5204, %int1_6304 : !torch.vtensor<[4,?,14336],f16>, !torch.int -> !torch.int
    %int4_6305 = torch.constant.int 4
    %5216 = torch.aten.mul.int %int4_6305, %5215 : !torch.int, !torch.int -> !torch.int
    %int14336_6306 = torch.constant.int 14336
    %5217 = torch.prim.ListConstruct %5216, %int14336_6306 : (!torch.int, !torch.int) -> !torch.list<int>
    %5218 = torch.aten.view %5213, %5217 : !torch.vtensor<[4,?,14336],f16>, !torch.list<int> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %5218, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %5219 = torch.aten.mm %5218, %5214 : !torch.vtensor<[?,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5219, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_6307 = torch.constant.int 4
    %int4096_6308 = torch.constant.int 4096
    %5220 = torch.prim.ListConstruct %int4_6307, %5215, %int4096_6308 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5221 = torch.aten.view %5219, %5220 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5221, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_6309 = torch.constant.int 1
    %5222 = torch.aten.add.Tensor %5187, %5221, %int1_6309 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5222, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_6310 = torch.constant.int 6
    %5223 = torch.prims.convert_element_type %5222, %int6_6310 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5223, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_6311 = torch.constant.int 2
    %5224 = torch.aten.pow.Tensor_Scalar %5223, %int2_6311 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5224, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_6312 = torch.constant.int -1
    %5225 = torch.prim.ListConstruct %int-1_6312 : (!torch.int) -> !torch.list<int>
    %true_6313 = torch.constant.bool true
    %none_6314 = torch.constant.none
    %5226 = torch.aten.mean.dim %5224, %5225, %true_6313, %none_6314 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5226, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_6315 = torch.constant.float 9.9999997473787516E-6
    %int1_6316 = torch.constant.int 1
    %5227 = torch.aten.add.Scalar %5226, %float9.999990e-06_6315, %int1_6316 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5227, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %5228 = torch.aten.rsqrt %5227 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5228, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %5229 = torch.aten.mul.Tensor %5223, %5228 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5229, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_6317 = torch.constant.int 5
    %5230 = torch.prims.convert_element_type %5229, %int5_6317 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5230, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %5231 = torch.aten.mul.Tensor %226, %5230 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5231, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_6318 = torch.constant.int 5
    %5232 = torch.prims.convert_element_type %5231, %int5_6318 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5232, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_6319 = torch.constant.int -2
    %int-1_6320 = torch.constant.int -1
    %5233 = torch.aten.transpose.int %227, %int-2_6319, %int-1_6320 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_6321 = torch.constant.int 4
    %5234 = torch.aten.mul.int %int4_6321, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_6322 = torch.constant.int 4096
    %5235 = torch.prim.ListConstruct %5234, %int4096_6322 : (!torch.int, !torch.int) -> !torch.list<int>
    %5236 = torch.aten.view %5232, %5235 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5236, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %5237 = torch.aten.mm %5236, %5233 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5237, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_6323 = torch.constant.int 4
    %int4096_6324 = torch.constant.int 4096
    %5238 = torch.prim.ListConstruct %int4_6323, %306, %int4096_6324 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5239 = torch.aten.view %5237, %5238 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5239, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_6325 = torch.constant.int -2
    %int-1_6326 = torch.constant.int -1
    %5240 = torch.aten.transpose.int %228, %int-2_6325, %int-1_6326 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_6327 = torch.constant.int 4
    %5241 = torch.aten.mul.int %int4_6327, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_6328 = torch.constant.int 4096
    %5242 = torch.prim.ListConstruct %5241, %int4096_6328 : (!torch.int, !torch.int) -> !torch.list<int>
    %5243 = torch.aten.view %5232, %5242 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5243, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %5244 = torch.aten.mm %5243, %5240 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %5244, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_6329 = torch.constant.int 4
    %int1024_6330 = torch.constant.int 1024
    %5245 = torch.prim.ListConstruct %int4_6329, %306, %int1024_6330 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5246 = torch.aten.view %5244, %5245 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %5246, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int-2_6331 = torch.constant.int -2
    %int-1_6332 = torch.constant.int -1
    %5247 = torch.aten.transpose.int %229, %int-2_6331, %int-1_6332 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_6333 = torch.constant.int 4
    %5248 = torch.aten.mul.int %int4_6333, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_6334 = torch.constant.int 4096
    %5249 = torch.prim.ListConstruct %5248, %int4096_6334 : (!torch.int, !torch.int) -> !torch.list<int>
    %5250 = torch.aten.view %5232, %5249 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5250, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %5251 = torch.aten.mm %5250, %5247 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %5251, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_6335 = torch.constant.int 4
    %int1024_6336 = torch.constant.int 1024
    %5252 = torch.prim.ListConstruct %int4_6335, %306, %int1024_6336 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5253 = torch.aten.view %5251, %5252 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %5253, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int4_6337 = torch.constant.int 4
    %int32_6338 = torch.constant.int 32
    %int128_6339 = torch.constant.int 128
    %5254 = torch.prim.ListConstruct %int4_6337, %306, %int32_6338, %int128_6339 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5255 = torch.aten.view %5239, %5254 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %5255, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_6340 = torch.constant.int 4
    %int8_6341 = torch.constant.int 8
    %int128_6342 = torch.constant.int 128
    %5256 = torch.prim.ListConstruct %int4_6340, %306, %int8_6341, %int128_6342 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5257 = torch.aten.view %5246, %5256 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %5257, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int4_6343 = torch.constant.int 4
    %int8_6344 = torch.constant.int 8
    %int128_6345 = torch.constant.int 128
    %5258 = torch.prim.ListConstruct %int4_6343, %306, %int8_6344, %int128_6345 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5259 = torch.aten.view %5253, %5258 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %5259, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int131072_6346 = torch.constant.int 131072
    %none_6347 = torch.constant.none
    %none_6348 = torch.constant.none
    %cpu_6349 = torch.constant.device "cpu"
    %false_6350 = torch.constant.bool false
    %5260 = torch.aten.arange %int131072_6346, %none_6347, %none_6348, %cpu_6349, %false_6350 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_6351 = torch.constant.int 0
    %int128_6352 = torch.constant.int 128
    %none_6353 = torch.constant.none
    %none_6354 = torch.constant.none
    %cpu_6355 = torch.constant.device "cpu"
    %false_6356 = torch.constant.bool false
    %5261 = torch.aten.arange.start %int0_6351, %int128_6352, %none_6353, %none_6354, %cpu_6355, %false_6356 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_6357 = torch.constant.int 2
    %5262 = torch.aten.floor_divide.Scalar %5261, %int2_6357 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_6358 = torch.constant.int 6
    %5263 = torch.prims.convert_element_type %5262, %int6_6358 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_6359 = torch.constant.int 128
    %5264 = torch.aten.div.Scalar %5263, %int128_6359 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_6360 = torch.constant.float 2.000000e+00
    %5265 = torch.aten.mul.Scalar %5264, %float2.000000e00_6360 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_6361 = torch.constant.float 5.000000e+05
    %5266 = torch.aten.pow.Scalar %float5.000000e05_6361, %5265 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %5267 = torch.aten.reciprocal %5266 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_6362 = torch.constant.float 1.000000e+00
    %5268 = torch.aten.mul.Scalar %5267, %float1.000000e00_6362 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_6363 = torch.constant.int 1
    %5269 = torch.aten.unsqueeze %5260, %int1_6363 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_6364 = torch.constant.int 0
    %5270 = torch.aten.unsqueeze %5268, %int0_6364 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %5271 = torch.aten.mul.Tensor %5269, %5270 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_6365 = torch.constant.int 1
    %5272 = torch.aten.size.int %5239, %int1_6365 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.int
    %int0_6366 = torch.constant.int 0
    %5273 = torch.aten.add.int %int0_6366, %5272 : !torch.int, !torch.int -> !torch.int
    %int0_6367 = torch.constant.int 0
    %int0_6368 = torch.constant.int 0
    %int1_6369 = torch.constant.int 1
    %5274 = torch.aten.slice.Tensor %5271, %int0_6367, %int0_6368, %5273, %int1_6369 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %5274, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_6370 = torch.constant.int 1
    %int0_6371 = torch.constant.int 0
    %int9223372036854775807_6372 = torch.constant.int 9223372036854775807
    %int1_6373 = torch.constant.int 1
    %5275 = torch.aten.slice.Tensor %5274, %int1_6370, %int0_6371, %int9223372036854775807_6372, %int1_6373 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %5275, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_6374 = torch.constant.int 1
    %int0_6375 = torch.constant.int 0
    %int9223372036854775807_6376 = torch.constant.int 9223372036854775807
    %int1_6377 = torch.constant.int 1
    %5276 = torch.aten.slice.Tensor %5275, %int1_6374, %int0_6375, %int9223372036854775807_6376, %int1_6377 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %5276, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_6378 = torch.constant.int 0
    %5277 = torch.aten.unsqueeze %5276, %int0_6378 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %5277, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_6379 = torch.constant.int 1
    %int0_6380 = torch.constant.int 0
    %int9223372036854775807_6381 = torch.constant.int 9223372036854775807
    %int1_6382 = torch.constant.int 1
    %5278 = torch.aten.slice.Tensor %5277, %int1_6379, %int0_6380, %int9223372036854775807_6381, %int1_6382 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %5278, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_6383 = torch.constant.int 2
    %int0_6384 = torch.constant.int 0
    %int9223372036854775807_6385 = torch.constant.int 9223372036854775807
    %int1_6386 = torch.constant.int 1
    %5279 = torch.aten.slice.Tensor %5278, %int2_6383, %int0_6384, %int9223372036854775807_6385, %int1_6386 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %5279, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_6387 = torch.constant.int 4
    %int1_6388 = torch.constant.int 1
    %int1_6389 = torch.constant.int 1
    %5280 = torch.prim.ListConstruct %int4_6387, %int1_6388, %int1_6389 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5281 = torch.aten.repeat %5279, %5280 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %5281, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_6390 = torch.constant.int 6
    %5282 = torch.prims.convert_element_type %5255, %int6_6390 : !torch.vtensor<[4,?,32,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %5282, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %5283 = torch_c.to_builtin_tensor %5282 : !torch.vtensor<[4,?,32,128],f32> -> tensor<4x?x32x128xf32>
    %5284 = torch_c.to_builtin_tensor %5281 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %5285 = util.call @sharktank_rotary_embedding_4_D_32_128_f32(%5283, %5284) : (tensor<4x?x32x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x32x128xf32>
    %5286 = torch_c.from_builtin_tensor %5285 : tensor<4x?x32x128xf32> -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %5286, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %int5_6391 = torch.constant.int 5
    %5287 = torch.prims.convert_element_type %5286, %int5_6391 : !torch.vtensor<[4,?,32,128],f32>, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %5287, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int131072_6392 = torch.constant.int 131072
    %none_6393 = torch.constant.none
    %none_6394 = torch.constant.none
    %cpu_6395 = torch.constant.device "cpu"
    %false_6396 = torch.constant.bool false
    %5288 = torch.aten.arange %int131072_6392, %none_6393, %none_6394, %cpu_6395, %false_6396 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_6397 = torch.constant.int 0
    %int128_6398 = torch.constant.int 128
    %none_6399 = torch.constant.none
    %none_6400 = torch.constant.none
    %cpu_6401 = torch.constant.device "cpu"
    %false_6402 = torch.constant.bool false
    %5289 = torch.aten.arange.start %int0_6397, %int128_6398, %none_6399, %none_6400, %cpu_6401, %false_6402 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_6403 = torch.constant.int 2
    %5290 = torch.aten.floor_divide.Scalar %5289, %int2_6403 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_6404 = torch.constant.int 6
    %5291 = torch.prims.convert_element_type %5290, %int6_6404 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_6405 = torch.constant.int 128
    %5292 = torch.aten.div.Scalar %5291, %int128_6405 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_6406 = torch.constant.float 2.000000e+00
    %5293 = torch.aten.mul.Scalar %5292, %float2.000000e00_6406 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_6407 = torch.constant.float 5.000000e+05
    %5294 = torch.aten.pow.Scalar %float5.000000e05_6407, %5293 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %5295 = torch.aten.reciprocal %5294 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_6408 = torch.constant.float 1.000000e+00
    %5296 = torch.aten.mul.Scalar %5295, %float1.000000e00_6408 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_6409 = torch.constant.int 1
    %5297 = torch.aten.unsqueeze %5288, %int1_6409 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_6410 = torch.constant.int 0
    %5298 = torch.aten.unsqueeze %5296, %int0_6410 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %5299 = torch.aten.mul.Tensor %5297, %5298 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_6411 = torch.constant.int 1
    %5300 = torch.aten.size.int %5246, %int1_6411 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int0_6412 = torch.constant.int 0
    %5301 = torch.aten.add.int %int0_6412, %5300 : !torch.int, !torch.int -> !torch.int
    %int0_6413 = torch.constant.int 0
    %int0_6414 = torch.constant.int 0
    %int1_6415 = torch.constant.int 1
    %5302 = torch.aten.slice.Tensor %5299, %int0_6413, %int0_6414, %5301, %int1_6415 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %5302, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_6416 = torch.constant.int 1
    %int0_6417 = torch.constant.int 0
    %int9223372036854775807_6418 = torch.constant.int 9223372036854775807
    %int1_6419 = torch.constant.int 1
    %5303 = torch.aten.slice.Tensor %5302, %int1_6416, %int0_6417, %int9223372036854775807_6418, %int1_6419 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %5303, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_6420 = torch.constant.int 1
    %int0_6421 = torch.constant.int 0
    %int9223372036854775807_6422 = torch.constant.int 9223372036854775807
    %int1_6423 = torch.constant.int 1
    %5304 = torch.aten.slice.Tensor %5303, %int1_6420, %int0_6421, %int9223372036854775807_6422, %int1_6423 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %5304, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_6424 = torch.constant.int 0
    %5305 = torch.aten.unsqueeze %5304, %int0_6424 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %5305, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_6425 = torch.constant.int 1
    %int0_6426 = torch.constant.int 0
    %int9223372036854775807_6427 = torch.constant.int 9223372036854775807
    %int1_6428 = torch.constant.int 1
    %5306 = torch.aten.slice.Tensor %5305, %int1_6425, %int0_6426, %int9223372036854775807_6427, %int1_6428 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %5306, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_6429 = torch.constant.int 2
    %int0_6430 = torch.constant.int 0
    %int9223372036854775807_6431 = torch.constant.int 9223372036854775807
    %int1_6432 = torch.constant.int 1
    %5307 = torch.aten.slice.Tensor %5306, %int2_6429, %int0_6430, %int9223372036854775807_6431, %int1_6432 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %5307, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_6433 = torch.constant.int 4
    %int1_6434 = torch.constant.int 1
    %int1_6435 = torch.constant.int 1
    %5308 = torch.prim.ListConstruct %int4_6433, %int1_6434, %int1_6435 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5309 = torch.aten.repeat %5307, %5308 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %5309, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_6436 = torch.constant.int 6
    %5310 = torch.prims.convert_element_type %5257, %int6_6436 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %5310, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %5311 = torch_c.to_builtin_tensor %5310 : !torch.vtensor<[4,?,8,128],f32> -> tensor<4x?x8x128xf32>
    %5312 = torch_c.to_builtin_tensor %5309 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %5313 = util.call @sharktank_rotary_embedding_4_D_8_128_f32(%5311, %5312) : (tensor<4x?x8x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x8x128xf32>
    %5314 = torch_c.from_builtin_tensor %5313 : tensor<4x?x8x128xf32> -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %5314, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %int5_6437 = torch.constant.int 5
    %5315 = torch.prims.convert_element_type %5314, %int5_6437 : !torch.vtensor<[4,?,8,128],f32>, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %5315, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int64_6438 = torch.constant.int 64
    %5316 = torch.aten.mul.Scalar %arg2, %int64_6438 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5316, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int50 = torch.constant.int 50
    %int1_6439 = torch.constant.int 1
    %5317 = torch.aten.add.Scalar %5316, %int50, %int1_6439 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5317, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_6440 = torch.constant.int 4
    %int32_6441 = torch.constant.int 32
    %int8_6442 = torch.constant.int 8
    %int128_6443 = torch.constant.int 128
    %5318 = torch.prim.ListConstruct %int4_6440, %398, %int32_6441, %int8_6442, %int128_6443 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5319 = torch.aten.view %5315, %5318 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5319, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_6444 = torch.constant.int 4
    %5320 = torch.aten.mul.int %int4_6444, %398 : !torch.int, !torch.int -> !torch.int
    %int32_6445 = torch.constant.int 32
    %int8_6446 = torch.constant.int 8
    %int128_6447 = torch.constant.int 128
    %5321 = torch.prim.ListConstruct %5320, %int32_6445, %int8_6446, %int128_6447 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5322 = torch.aten.view %5319, %5321 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %5322, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_6448 = torch.constant.int 4
    %5323 = torch.aten.mul.int %int4_6448, %398 : !torch.int, !torch.int -> !torch.int
    %5324 = torch.prim.ListConstruct %5323 : (!torch.int) -> !torch.list<int>
    %5325 = torch.aten.view %5317, %5324 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5325, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_6449 = torch.constant.int 32
    %int2_6450 = torch.constant.int 2
    %int32_6451 = torch.constant.int 32
    %int8_6452 = torch.constant.int 8
    %int128_6453 = torch.constant.int 128
    %5326 = torch.prim.ListConstruct %389, %int32_6449, %int2_6450, %int32_6451, %int8_6452, %int128_6453 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5327 = torch.aten.view %5159, %5326 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5327, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_6454 = torch.constant.int 32
    %5328 = torch.aten.mul.int %389, %int32_6454 : !torch.int, !torch.int -> !torch.int
    %int2_6455 = torch.constant.int 2
    %5329 = torch.aten.mul.int %5328, %int2_6455 : !torch.int, !torch.int -> !torch.int
    %int32_6456 = torch.constant.int 32
    %int8_6457 = torch.constant.int 8
    %int128_6458 = torch.constant.int 128
    %5330 = torch.prim.ListConstruct %5329, %int32_6456, %int8_6457, %int128_6458 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5331 = torch.aten.view %5327, %5330 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %5331, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %5332 = torch.prim.ListConstruct %5325 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_6459 = torch.constant.bool false
    %5333 = torch.aten.index_put %5331, %5332, %5322, %false_6459 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %5333, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_6460 = torch.constant.int 32
    %int2_6461 = torch.constant.int 2
    %int32_6462 = torch.constant.int 32
    %int8_6463 = torch.constant.int 8
    %int128_6464 = torch.constant.int 128
    %5334 = torch.prim.ListConstruct %389, %int32_6460, %int2_6461, %int32_6462, %int8_6463, %int128_6464 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5335 = torch.aten.view %5333, %5334 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5335, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_6465 = torch.constant.int 2097152
    %5336 = torch.prim.ListConstruct %389, %int2097152_6465 : (!torch.int, !torch.int) -> !torch.list<int>
    %5337 = torch.aten.view %5335, %5336 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %5337, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_6466 = torch.constant.int 32
    %int2_6467 = torch.constant.int 2
    %int32_6468 = torch.constant.int 32
    %int8_6469 = torch.constant.int 8
    %int128_6470 = torch.constant.int 128
    %5338 = torch.prim.ListConstruct %389, %int32_6466, %int2_6467, %int32_6468, %int8_6469, %int128_6470 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5339 = torch.aten.view %5337, %5338 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5339, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_6471 = torch.constant.int 32
    %int8_6472 = torch.constant.int 8
    %int128_6473 = torch.constant.int 128
    %5340 = torch.prim.ListConstruct %5329, %int32_6471, %int8_6472, %int128_6473 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5341 = torch.aten.view %5339, %5340 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %5341, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_6474 = torch.constant.int 4
    %int32_6475 = torch.constant.int 32
    %int8_6476 = torch.constant.int 8
    %int128_6477 = torch.constant.int 128
    %5342 = torch.prim.ListConstruct %int4_6474, %398, %int32_6475, %int8_6476, %int128_6477 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5343 = torch.aten.view %5259, %5342 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5343, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_6478 = torch.constant.int 4
    %5344 = torch.aten.mul.int %int4_6478, %398 : !torch.int, !torch.int -> !torch.int
    %int32_6479 = torch.constant.int 32
    %int8_6480 = torch.constant.int 8
    %int128_6481 = torch.constant.int 128
    %5345 = torch.prim.ListConstruct %5344, %int32_6479, %int8_6480, %int128_6481 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5346 = torch.aten.view %5343, %5345 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %5346, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int1_6482 = torch.constant.int 1
    %int1_6483 = torch.constant.int 1
    %5347 = torch.aten.add.Scalar %5317, %int1_6482, %int1_6483 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5347, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_6484 = torch.constant.int 4
    %5348 = torch.aten.mul.int %int4_6484, %398 : !torch.int, !torch.int -> !torch.int
    %5349 = torch.prim.ListConstruct %5348 : (!torch.int) -> !torch.list<int>
    %5350 = torch.aten.view %5347, %5349 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5350, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %5351 = torch.prim.ListConstruct %5350 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_6485 = torch.constant.bool false
    %5352 = torch.aten.index_put %5341, %5351, %5346, %false_6485 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %5352, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_6486 = torch.constant.int 32
    %int2_6487 = torch.constant.int 2
    %int32_6488 = torch.constant.int 32
    %int8_6489 = torch.constant.int 8
    %int128_6490 = torch.constant.int 128
    %5353 = torch.prim.ListConstruct %389, %int32_6486, %int2_6487, %int32_6488, %int8_6489, %int128_6490 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5354 = torch.aten.view %5352, %5353 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5354, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_6491 = torch.constant.int 2097152
    %5355 = torch.prim.ListConstruct %389, %int2097152_6491 : (!torch.int, !torch.int) -> !torch.list<int>
    %5356 = torch.aten.view %5354, %5355 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %5356, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int-2_6492 = torch.constant.int -2
    %5357 = torch.aten.unsqueeze %5315, %int-2_6492 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %5357, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int4_6493 = torch.constant.int 4
    %int8_6494 = torch.constant.int 8
    %int4_6495 = torch.constant.int 4
    %int128_6496 = torch.constant.int 128
    %5358 = torch.prim.ListConstruct %int4_6493, %5300, %int8_6494, %int4_6495, %int128_6496 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_6497 = torch.constant.bool false
    %5359 = torch.aten.expand %5357, %5358, %false_6497 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5359, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_6498 = torch.constant.int 0
    %5360 = torch.aten.clone %5359, %int0_6498 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5360, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_6499 = torch.constant.int 4
    %int32_6500 = torch.constant.int 32
    %int128_6501 = torch.constant.int 128
    %5361 = torch.prim.ListConstruct %int4_6499, %5300, %int32_6500, %int128_6501 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5362 = torch.aten._unsafe_view %5360, %5361 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %5362, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_6502 = torch.constant.int -2
    %5363 = torch.aten.unsqueeze %5259, %int-2_6502 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %5363, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_6503 = torch.constant.int 1
    %5364 = torch.aten.size.int %5253, %int1_6503 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int4_6504 = torch.constant.int 4
    %int8_6505 = torch.constant.int 8
    %int4_6506 = torch.constant.int 4
    %int128_6507 = torch.constant.int 128
    %5365 = torch.prim.ListConstruct %int4_6504, %5364, %int8_6505, %int4_6506, %int128_6507 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_6508 = torch.constant.bool false
    %5366 = torch.aten.expand %5363, %5365, %false_6508 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5366, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_6509 = torch.constant.int 0
    %5367 = torch.aten.clone %5366, %int0_6509 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5367, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_6510 = torch.constant.int 4
    %int32_6511 = torch.constant.int 32
    %int128_6512 = torch.constant.int 128
    %5368 = torch.prim.ListConstruct %int4_6510, %5364, %int32_6511, %int128_6512 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5369 = torch.aten._unsafe_view %5367, %5368 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %5369, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_6513 = torch.constant.int 1
    %int2_6514 = torch.constant.int 2
    %5370 = torch.aten.transpose.int %5287, %int1_6513, %int2_6514 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %5370, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_6515 = torch.constant.int 1
    %int2_6516 = torch.constant.int 2
    %5371 = torch.aten.transpose.int %5362, %int1_6515, %int2_6516 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %5371, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_6517 = torch.constant.int 1
    %int2_6518 = torch.constant.int 2
    %5372 = torch.aten.transpose.int %5369, %int1_6517, %int2_6518 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %5372, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_6519 = torch.constant.float 0.000000e+00
    %true_6520 = torch.constant.bool true
    %none_6521 = torch.constant.none
    %none_6522 = torch.constant.none
    %5373:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5370, %5371, %5372, %float0.000000e00_6519, %true_6520, %none_6521, %none_6522) : (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?],f32>) 
    torch.bind_symbolic_shape %5373#0, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_6523 = torch.constant.int 1
    %int2_6524 = torch.constant.int 2
    %5374 = torch.aten.transpose.int %5373#0, %int1_6523, %int2_6524 : !torch.vtensor<[4,32,?,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %5374, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_6525 = torch.constant.int 4
    %int4096_6526 = torch.constant.int 4096
    %5375 = torch.prim.ListConstruct %int4_6525, %5272, %int4096_6526 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5376 = torch.aten.view %5374, %5375 : !torch.vtensor<[4,?,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5376, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_6527 = torch.constant.int -2
    %int-1_6528 = torch.constant.int -1
    %5377 = torch.aten.transpose.int %230, %int-2_6527, %int-1_6528 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_6529 = torch.constant.int 4
    %5378 = torch.aten.mul.int %int4_6529, %5272 : !torch.int, !torch.int -> !torch.int
    %int4096_6530 = torch.constant.int 4096
    %5379 = torch.prim.ListConstruct %5378, %int4096_6530 : (!torch.int, !torch.int) -> !torch.list<int>
    %5380 = torch.aten.view %5376, %5379 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5380, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %5381 = torch.aten.mm %5380, %5377 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5381, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_6531 = torch.constant.int 4
    %int4096_6532 = torch.constant.int 4096
    %5382 = torch.prim.ListConstruct %int4_6531, %5272, %int4096_6532 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5383 = torch.aten.view %5381, %5382 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5383, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_6533 = torch.constant.int 1
    %5384 = torch.aten.add.Tensor %5222, %5383, %int1_6533 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5384, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_6534 = torch.constant.int 6
    %5385 = torch.prims.convert_element_type %5384, %int6_6534 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5385, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_6535 = torch.constant.int 2
    %5386 = torch.aten.pow.Tensor_Scalar %5385, %int2_6535 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5386, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_6536 = torch.constant.int -1
    %5387 = torch.prim.ListConstruct %int-1_6536 : (!torch.int) -> !torch.list<int>
    %true_6537 = torch.constant.bool true
    %none_6538 = torch.constant.none
    %5388 = torch.aten.mean.dim %5386, %5387, %true_6537, %none_6538 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5388, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_6539 = torch.constant.float 9.9999997473787516E-6
    %int1_6540 = torch.constant.int 1
    %5389 = torch.aten.add.Scalar %5388, %float9.999990e-06_6539, %int1_6540 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5389, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %5390 = torch.aten.rsqrt %5389 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5390, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %5391 = torch.aten.mul.Tensor %5385, %5390 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5391, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_6541 = torch.constant.int 5
    %5392 = torch.prims.convert_element_type %5391, %int5_6541 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5392, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %5393 = torch.aten.mul.Tensor %231, %5392 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5393, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_6542 = torch.constant.int 5
    %5394 = torch.prims.convert_element_type %5393, %int5_6542 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5394, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_6543 = torch.constant.int -2
    %int-1_6544 = torch.constant.int -1
    %5395 = torch.aten.transpose.int %232, %int-2_6543, %int-1_6544 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_6545 = torch.constant.int 4
    %5396 = torch.aten.mul.int %int4_6545, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_6546 = torch.constant.int 4096
    %5397 = torch.prim.ListConstruct %5396, %int4096_6546 : (!torch.int, !torch.int) -> !torch.list<int>
    %5398 = torch.aten.view %5394, %5397 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5398, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %5399 = torch.aten.mm %5398, %5395 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %5399, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_6547 = torch.constant.int 4
    %int14336_6548 = torch.constant.int 14336
    %5400 = torch.prim.ListConstruct %int4_6547, %306, %int14336_6548 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5401 = torch.aten.view %5399, %5400 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %5401, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %5402 = torch.aten.silu %5401 : !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %5402, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_6549 = torch.constant.int -2
    %int-1_6550 = torch.constant.int -1
    %5403 = torch.aten.transpose.int %233, %int-2_6549, %int-1_6550 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_6551 = torch.constant.int 4
    %5404 = torch.aten.mul.int %int4_6551, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_6552 = torch.constant.int 4096
    %5405 = torch.prim.ListConstruct %5404, %int4096_6552 : (!torch.int, !torch.int) -> !torch.list<int>
    %5406 = torch.aten.view %5394, %5405 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5406, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %5407 = torch.aten.mm %5406, %5403 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %5407, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_6553 = torch.constant.int 4
    %int14336_6554 = torch.constant.int 14336
    %5408 = torch.prim.ListConstruct %int4_6553, %306, %int14336_6554 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5409 = torch.aten.view %5407, %5408 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %5409, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %5410 = torch.aten.mul.Tensor %5402, %5409 : !torch.vtensor<[4,?,14336],f16>, !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %5410, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_6555 = torch.constant.int -2
    %int-1_6556 = torch.constant.int -1
    %5411 = torch.aten.transpose.int %234, %int-2_6555, %int-1_6556 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int1_6557 = torch.constant.int 1
    %5412 = torch.aten.size.int %5401, %int1_6557 : !torch.vtensor<[4,?,14336],f16>, !torch.int -> !torch.int
    %int4_6558 = torch.constant.int 4
    %5413 = torch.aten.mul.int %int4_6558, %5412 : !torch.int, !torch.int -> !torch.int
    %int14336_6559 = torch.constant.int 14336
    %5414 = torch.prim.ListConstruct %5413, %int14336_6559 : (!torch.int, !torch.int) -> !torch.list<int>
    %5415 = torch.aten.view %5410, %5414 : !torch.vtensor<[4,?,14336],f16>, !torch.list<int> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %5415, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %5416 = torch.aten.mm %5415, %5411 : !torch.vtensor<[?,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5416, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_6560 = torch.constant.int 4
    %int4096_6561 = torch.constant.int 4096
    %5417 = torch.prim.ListConstruct %int4_6560, %5412, %int4096_6561 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5418 = torch.aten.view %5416, %5417 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5418, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_6562 = torch.constant.int 1
    %5419 = torch.aten.add.Tensor %5384, %5418, %int1_6562 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5419, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_6563 = torch.constant.int 6
    %5420 = torch.prims.convert_element_type %5419, %int6_6563 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5420, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_6564 = torch.constant.int 2
    %5421 = torch.aten.pow.Tensor_Scalar %5420, %int2_6564 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5421, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_6565 = torch.constant.int -1
    %5422 = torch.prim.ListConstruct %int-1_6565 : (!torch.int) -> !torch.list<int>
    %true_6566 = torch.constant.bool true
    %none_6567 = torch.constant.none
    %5423 = torch.aten.mean.dim %5421, %5422, %true_6566, %none_6567 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5423, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_6568 = torch.constant.float 9.9999997473787516E-6
    %int1_6569 = torch.constant.int 1
    %5424 = torch.aten.add.Scalar %5423, %float9.999990e-06_6568, %int1_6569 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5424, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %5425 = torch.aten.rsqrt %5424 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5425, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %5426 = torch.aten.mul.Tensor %5420, %5425 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5426, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_6570 = torch.constant.int 5
    %5427 = torch.prims.convert_element_type %5426, %int5_6570 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5427, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %5428 = torch.aten.mul.Tensor %235, %5427 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5428, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_6571 = torch.constant.int 5
    %5429 = torch.prims.convert_element_type %5428, %int5_6571 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5429, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_6572 = torch.constant.int -2
    %int-1_6573 = torch.constant.int -1
    %5430 = torch.aten.transpose.int %236, %int-2_6572, %int-1_6573 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_6574 = torch.constant.int 4
    %5431 = torch.aten.mul.int %int4_6574, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_6575 = torch.constant.int 4096
    %5432 = torch.prim.ListConstruct %5431, %int4096_6575 : (!torch.int, !torch.int) -> !torch.list<int>
    %5433 = torch.aten.view %5429, %5432 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5433, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %5434 = torch.aten.mm %5433, %5430 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5434, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_6576 = torch.constant.int 4
    %int4096_6577 = torch.constant.int 4096
    %5435 = torch.prim.ListConstruct %int4_6576, %306, %int4096_6577 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5436 = torch.aten.view %5434, %5435 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5436, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_6578 = torch.constant.int -2
    %int-1_6579 = torch.constant.int -1
    %5437 = torch.aten.transpose.int %237, %int-2_6578, %int-1_6579 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_6580 = torch.constant.int 4
    %5438 = torch.aten.mul.int %int4_6580, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_6581 = torch.constant.int 4096
    %5439 = torch.prim.ListConstruct %5438, %int4096_6581 : (!torch.int, !torch.int) -> !torch.list<int>
    %5440 = torch.aten.view %5429, %5439 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5440, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %5441 = torch.aten.mm %5440, %5437 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %5441, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_6582 = torch.constant.int 4
    %int1024_6583 = torch.constant.int 1024
    %5442 = torch.prim.ListConstruct %int4_6582, %306, %int1024_6583 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5443 = torch.aten.view %5441, %5442 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %5443, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int-2_6584 = torch.constant.int -2
    %int-1_6585 = torch.constant.int -1
    %5444 = torch.aten.transpose.int %238, %int-2_6584, %int-1_6585 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_6586 = torch.constant.int 4
    %5445 = torch.aten.mul.int %int4_6586, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_6587 = torch.constant.int 4096
    %5446 = torch.prim.ListConstruct %5445, %int4096_6587 : (!torch.int, !torch.int) -> !torch.list<int>
    %5447 = torch.aten.view %5429, %5446 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5447, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %5448 = torch.aten.mm %5447, %5444 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %5448, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_6588 = torch.constant.int 4
    %int1024_6589 = torch.constant.int 1024
    %5449 = torch.prim.ListConstruct %int4_6588, %306, %int1024_6589 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5450 = torch.aten.view %5448, %5449 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %5450, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int4_6590 = torch.constant.int 4
    %int32_6591 = torch.constant.int 32
    %int128_6592 = torch.constant.int 128
    %5451 = torch.prim.ListConstruct %int4_6590, %306, %int32_6591, %int128_6592 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5452 = torch.aten.view %5436, %5451 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %5452, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_6593 = torch.constant.int 4
    %int8_6594 = torch.constant.int 8
    %int128_6595 = torch.constant.int 128
    %5453 = torch.prim.ListConstruct %int4_6593, %306, %int8_6594, %int128_6595 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5454 = torch.aten.view %5443, %5453 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %5454, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int4_6596 = torch.constant.int 4
    %int8_6597 = torch.constant.int 8
    %int128_6598 = torch.constant.int 128
    %5455 = torch.prim.ListConstruct %int4_6596, %306, %int8_6597, %int128_6598 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5456 = torch.aten.view %5450, %5455 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %5456, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int131072_6599 = torch.constant.int 131072
    %none_6600 = torch.constant.none
    %none_6601 = torch.constant.none
    %cpu_6602 = torch.constant.device "cpu"
    %false_6603 = torch.constant.bool false
    %5457 = torch.aten.arange %int131072_6599, %none_6600, %none_6601, %cpu_6602, %false_6603 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_6604 = torch.constant.int 0
    %int128_6605 = torch.constant.int 128
    %none_6606 = torch.constant.none
    %none_6607 = torch.constant.none
    %cpu_6608 = torch.constant.device "cpu"
    %false_6609 = torch.constant.bool false
    %5458 = torch.aten.arange.start %int0_6604, %int128_6605, %none_6606, %none_6607, %cpu_6608, %false_6609 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_6610 = torch.constant.int 2
    %5459 = torch.aten.floor_divide.Scalar %5458, %int2_6610 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_6611 = torch.constant.int 6
    %5460 = torch.prims.convert_element_type %5459, %int6_6611 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_6612 = torch.constant.int 128
    %5461 = torch.aten.div.Scalar %5460, %int128_6612 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_6613 = torch.constant.float 2.000000e+00
    %5462 = torch.aten.mul.Scalar %5461, %float2.000000e00_6613 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_6614 = torch.constant.float 5.000000e+05
    %5463 = torch.aten.pow.Scalar %float5.000000e05_6614, %5462 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %5464 = torch.aten.reciprocal %5463 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_6615 = torch.constant.float 1.000000e+00
    %5465 = torch.aten.mul.Scalar %5464, %float1.000000e00_6615 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_6616 = torch.constant.int 1
    %5466 = torch.aten.unsqueeze %5457, %int1_6616 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_6617 = torch.constant.int 0
    %5467 = torch.aten.unsqueeze %5465, %int0_6617 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %5468 = torch.aten.mul.Tensor %5466, %5467 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_6618 = torch.constant.int 1
    %5469 = torch.aten.size.int %5436, %int1_6618 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.int
    %int0_6619 = torch.constant.int 0
    %5470 = torch.aten.add.int %int0_6619, %5469 : !torch.int, !torch.int -> !torch.int
    %int0_6620 = torch.constant.int 0
    %int0_6621 = torch.constant.int 0
    %int1_6622 = torch.constant.int 1
    %5471 = torch.aten.slice.Tensor %5468, %int0_6620, %int0_6621, %5470, %int1_6622 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %5471, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_6623 = torch.constant.int 1
    %int0_6624 = torch.constant.int 0
    %int9223372036854775807_6625 = torch.constant.int 9223372036854775807
    %int1_6626 = torch.constant.int 1
    %5472 = torch.aten.slice.Tensor %5471, %int1_6623, %int0_6624, %int9223372036854775807_6625, %int1_6626 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %5472, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_6627 = torch.constant.int 1
    %int0_6628 = torch.constant.int 0
    %int9223372036854775807_6629 = torch.constant.int 9223372036854775807
    %int1_6630 = torch.constant.int 1
    %5473 = torch.aten.slice.Tensor %5472, %int1_6627, %int0_6628, %int9223372036854775807_6629, %int1_6630 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %5473, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_6631 = torch.constant.int 0
    %5474 = torch.aten.unsqueeze %5473, %int0_6631 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %5474, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_6632 = torch.constant.int 1
    %int0_6633 = torch.constant.int 0
    %int9223372036854775807_6634 = torch.constant.int 9223372036854775807
    %int1_6635 = torch.constant.int 1
    %5475 = torch.aten.slice.Tensor %5474, %int1_6632, %int0_6633, %int9223372036854775807_6634, %int1_6635 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %5475, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_6636 = torch.constant.int 2
    %int0_6637 = torch.constant.int 0
    %int9223372036854775807_6638 = torch.constant.int 9223372036854775807
    %int1_6639 = torch.constant.int 1
    %5476 = torch.aten.slice.Tensor %5475, %int2_6636, %int0_6637, %int9223372036854775807_6638, %int1_6639 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %5476, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_6640 = torch.constant.int 4
    %int1_6641 = torch.constant.int 1
    %int1_6642 = torch.constant.int 1
    %5477 = torch.prim.ListConstruct %int4_6640, %int1_6641, %int1_6642 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5478 = torch.aten.repeat %5476, %5477 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %5478, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_6643 = torch.constant.int 6
    %5479 = torch.prims.convert_element_type %5452, %int6_6643 : !torch.vtensor<[4,?,32,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %5479, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %5480 = torch_c.to_builtin_tensor %5479 : !torch.vtensor<[4,?,32,128],f32> -> tensor<4x?x32x128xf32>
    %5481 = torch_c.to_builtin_tensor %5478 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %5482 = util.call @sharktank_rotary_embedding_4_D_32_128_f32(%5480, %5481) : (tensor<4x?x32x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x32x128xf32>
    %5483 = torch_c.from_builtin_tensor %5482 : tensor<4x?x32x128xf32> -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %5483, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %int5_6644 = torch.constant.int 5
    %5484 = torch.prims.convert_element_type %5483, %int5_6644 : !torch.vtensor<[4,?,32,128],f32>, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %5484, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int131072_6645 = torch.constant.int 131072
    %none_6646 = torch.constant.none
    %none_6647 = torch.constant.none
    %cpu_6648 = torch.constant.device "cpu"
    %false_6649 = torch.constant.bool false
    %5485 = torch.aten.arange %int131072_6645, %none_6646, %none_6647, %cpu_6648, %false_6649 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_6650 = torch.constant.int 0
    %int128_6651 = torch.constant.int 128
    %none_6652 = torch.constant.none
    %none_6653 = torch.constant.none
    %cpu_6654 = torch.constant.device "cpu"
    %false_6655 = torch.constant.bool false
    %5486 = torch.aten.arange.start %int0_6650, %int128_6651, %none_6652, %none_6653, %cpu_6654, %false_6655 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_6656 = torch.constant.int 2
    %5487 = torch.aten.floor_divide.Scalar %5486, %int2_6656 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_6657 = torch.constant.int 6
    %5488 = torch.prims.convert_element_type %5487, %int6_6657 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_6658 = torch.constant.int 128
    %5489 = torch.aten.div.Scalar %5488, %int128_6658 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_6659 = torch.constant.float 2.000000e+00
    %5490 = torch.aten.mul.Scalar %5489, %float2.000000e00_6659 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_6660 = torch.constant.float 5.000000e+05
    %5491 = torch.aten.pow.Scalar %float5.000000e05_6660, %5490 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %5492 = torch.aten.reciprocal %5491 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_6661 = torch.constant.float 1.000000e+00
    %5493 = torch.aten.mul.Scalar %5492, %float1.000000e00_6661 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_6662 = torch.constant.int 1
    %5494 = torch.aten.unsqueeze %5485, %int1_6662 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_6663 = torch.constant.int 0
    %5495 = torch.aten.unsqueeze %5493, %int0_6663 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %5496 = torch.aten.mul.Tensor %5494, %5495 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_6664 = torch.constant.int 1
    %5497 = torch.aten.size.int %5443, %int1_6664 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int0_6665 = torch.constant.int 0
    %5498 = torch.aten.add.int %int0_6665, %5497 : !torch.int, !torch.int -> !torch.int
    %int0_6666 = torch.constant.int 0
    %int0_6667 = torch.constant.int 0
    %int1_6668 = torch.constant.int 1
    %5499 = torch.aten.slice.Tensor %5496, %int0_6666, %int0_6667, %5498, %int1_6668 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %5499, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_6669 = torch.constant.int 1
    %int0_6670 = torch.constant.int 0
    %int9223372036854775807_6671 = torch.constant.int 9223372036854775807
    %int1_6672 = torch.constant.int 1
    %5500 = torch.aten.slice.Tensor %5499, %int1_6669, %int0_6670, %int9223372036854775807_6671, %int1_6672 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %5500, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_6673 = torch.constant.int 1
    %int0_6674 = torch.constant.int 0
    %int9223372036854775807_6675 = torch.constant.int 9223372036854775807
    %int1_6676 = torch.constant.int 1
    %5501 = torch.aten.slice.Tensor %5500, %int1_6673, %int0_6674, %int9223372036854775807_6675, %int1_6676 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %5501, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_6677 = torch.constant.int 0
    %5502 = torch.aten.unsqueeze %5501, %int0_6677 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %5502, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_6678 = torch.constant.int 1
    %int0_6679 = torch.constant.int 0
    %int9223372036854775807_6680 = torch.constant.int 9223372036854775807
    %int1_6681 = torch.constant.int 1
    %5503 = torch.aten.slice.Tensor %5502, %int1_6678, %int0_6679, %int9223372036854775807_6680, %int1_6681 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %5503, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_6682 = torch.constant.int 2
    %int0_6683 = torch.constant.int 0
    %int9223372036854775807_6684 = torch.constant.int 9223372036854775807
    %int1_6685 = torch.constant.int 1
    %5504 = torch.aten.slice.Tensor %5503, %int2_6682, %int0_6683, %int9223372036854775807_6684, %int1_6685 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %5504, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_6686 = torch.constant.int 4
    %int1_6687 = torch.constant.int 1
    %int1_6688 = torch.constant.int 1
    %5505 = torch.prim.ListConstruct %int4_6686, %int1_6687, %int1_6688 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5506 = torch.aten.repeat %5504, %5505 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %5506, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_6689 = torch.constant.int 6
    %5507 = torch.prims.convert_element_type %5454, %int6_6689 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %5507, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %5508 = torch_c.to_builtin_tensor %5507 : !torch.vtensor<[4,?,8,128],f32> -> tensor<4x?x8x128xf32>
    %5509 = torch_c.to_builtin_tensor %5506 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %5510 = util.call @sharktank_rotary_embedding_4_D_8_128_f32(%5508, %5509) : (tensor<4x?x8x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x8x128xf32>
    %5511 = torch_c.from_builtin_tensor %5510 : tensor<4x?x8x128xf32> -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %5511, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %int5_6690 = torch.constant.int 5
    %5512 = torch.prims.convert_element_type %5511, %int5_6690 : !torch.vtensor<[4,?,8,128],f32>, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %5512, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int64_6691 = torch.constant.int 64
    %5513 = torch.aten.mul.Scalar %arg2, %int64_6691 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5513, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int52 = torch.constant.int 52
    %int1_6692 = torch.constant.int 1
    %5514 = torch.aten.add.Scalar %5513, %int52, %int1_6692 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5514, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_6693 = torch.constant.int 4
    %int32_6694 = torch.constant.int 32
    %int8_6695 = torch.constant.int 8
    %int128_6696 = torch.constant.int 128
    %5515 = torch.prim.ListConstruct %int4_6693, %398, %int32_6694, %int8_6695, %int128_6696 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5516 = torch.aten.view %5512, %5515 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5516, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_6697 = torch.constant.int 4
    %5517 = torch.aten.mul.int %int4_6697, %398 : !torch.int, !torch.int -> !torch.int
    %int32_6698 = torch.constant.int 32
    %int8_6699 = torch.constant.int 8
    %int128_6700 = torch.constant.int 128
    %5518 = torch.prim.ListConstruct %5517, %int32_6698, %int8_6699, %int128_6700 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5519 = torch.aten.view %5516, %5518 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %5519, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_6701 = torch.constant.int 4
    %5520 = torch.aten.mul.int %int4_6701, %398 : !torch.int, !torch.int -> !torch.int
    %5521 = torch.prim.ListConstruct %5520 : (!torch.int) -> !torch.list<int>
    %5522 = torch.aten.view %5514, %5521 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5522, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_6702 = torch.constant.int 32
    %int2_6703 = torch.constant.int 2
    %int32_6704 = torch.constant.int 32
    %int8_6705 = torch.constant.int 8
    %int128_6706 = torch.constant.int 128
    %5523 = torch.prim.ListConstruct %389, %int32_6702, %int2_6703, %int32_6704, %int8_6705, %int128_6706 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5524 = torch.aten.view %5356, %5523 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5524, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_6707 = torch.constant.int 32
    %5525 = torch.aten.mul.int %389, %int32_6707 : !torch.int, !torch.int -> !torch.int
    %int2_6708 = torch.constant.int 2
    %5526 = torch.aten.mul.int %5525, %int2_6708 : !torch.int, !torch.int -> !torch.int
    %int32_6709 = torch.constant.int 32
    %int8_6710 = torch.constant.int 8
    %int128_6711 = torch.constant.int 128
    %5527 = torch.prim.ListConstruct %5526, %int32_6709, %int8_6710, %int128_6711 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5528 = torch.aten.view %5524, %5527 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %5528, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %5529 = torch.prim.ListConstruct %5522 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_6712 = torch.constant.bool false
    %5530 = torch.aten.index_put %5528, %5529, %5519, %false_6712 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %5530, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_6713 = torch.constant.int 32
    %int2_6714 = torch.constant.int 2
    %int32_6715 = torch.constant.int 32
    %int8_6716 = torch.constant.int 8
    %int128_6717 = torch.constant.int 128
    %5531 = torch.prim.ListConstruct %389, %int32_6713, %int2_6714, %int32_6715, %int8_6716, %int128_6717 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5532 = torch.aten.view %5530, %5531 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5532, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_6718 = torch.constant.int 2097152
    %5533 = torch.prim.ListConstruct %389, %int2097152_6718 : (!torch.int, !torch.int) -> !torch.list<int>
    %5534 = torch.aten.view %5532, %5533 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %5534, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_6719 = torch.constant.int 32
    %int2_6720 = torch.constant.int 2
    %int32_6721 = torch.constant.int 32
    %int8_6722 = torch.constant.int 8
    %int128_6723 = torch.constant.int 128
    %5535 = torch.prim.ListConstruct %389, %int32_6719, %int2_6720, %int32_6721, %int8_6722, %int128_6723 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5536 = torch.aten.view %5534, %5535 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5536, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_6724 = torch.constant.int 32
    %int8_6725 = torch.constant.int 8
    %int128_6726 = torch.constant.int 128
    %5537 = torch.prim.ListConstruct %5526, %int32_6724, %int8_6725, %int128_6726 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5538 = torch.aten.view %5536, %5537 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %5538, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_6727 = torch.constant.int 4
    %int32_6728 = torch.constant.int 32
    %int8_6729 = torch.constant.int 8
    %int128_6730 = torch.constant.int 128
    %5539 = torch.prim.ListConstruct %int4_6727, %398, %int32_6728, %int8_6729, %int128_6730 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5540 = torch.aten.view %5456, %5539 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5540, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_6731 = torch.constant.int 4
    %5541 = torch.aten.mul.int %int4_6731, %398 : !torch.int, !torch.int -> !torch.int
    %int32_6732 = torch.constant.int 32
    %int8_6733 = torch.constant.int 8
    %int128_6734 = torch.constant.int 128
    %5542 = torch.prim.ListConstruct %5541, %int32_6732, %int8_6733, %int128_6734 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5543 = torch.aten.view %5540, %5542 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %5543, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int1_6735 = torch.constant.int 1
    %int1_6736 = torch.constant.int 1
    %5544 = torch.aten.add.Scalar %5514, %int1_6735, %int1_6736 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5544, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_6737 = torch.constant.int 4
    %5545 = torch.aten.mul.int %int4_6737, %398 : !torch.int, !torch.int -> !torch.int
    %5546 = torch.prim.ListConstruct %5545 : (!torch.int) -> !torch.list<int>
    %5547 = torch.aten.view %5544, %5546 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5547, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %5548 = torch.prim.ListConstruct %5547 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_6738 = torch.constant.bool false
    %5549 = torch.aten.index_put %5538, %5548, %5543, %false_6738 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %5549, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_6739 = torch.constant.int 32
    %int2_6740 = torch.constant.int 2
    %int32_6741 = torch.constant.int 32
    %int8_6742 = torch.constant.int 8
    %int128_6743 = torch.constant.int 128
    %5550 = torch.prim.ListConstruct %389, %int32_6739, %int2_6740, %int32_6741, %int8_6742, %int128_6743 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5551 = torch.aten.view %5549, %5550 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5551, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_6744 = torch.constant.int 2097152
    %5552 = torch.prim.ListConstruct %389, %int2097152_6744 : (!torch.int, !torch.int) -> !torch.list<int>
    %5553 = torch.aten.view %5551, %5552 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %5553, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int-2_6745 = torch.constant.int -2
    %5554 = torch.aten.unsqueeze %5512, %int-2_6745 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %5554, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int4_6746 = torch.constant.int 4
    %int8_6747 = torch.constant.int 8
    %int4_6748 = torch.constant.int 4
    %int128_6749 = torch.constant.int 128
    %5555 = torch.prim.ListConstruct %int4_6746, %5497, %int8_6747, %int4_6748, %int128_6749 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_6750 = torch.constant.bool false
    %5556 = torch.aten.expand %5554, %5555, %false_6750 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5556, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_6751 = torch.constant.int 0
    %5557 = torch.aten.clone %5556, %int0_6751 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5557, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_6752 = torch.constant.int 4
    %int32_6753 = torch.constant.int 32
    %int128_6754 = torch.constant.int 128
    %5558 = torch.prim.ListConstruct %int4_6752, %5497, %int32_6753, %int128_6754 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5559 = torch.aten._unsafe_view %5557, %5558 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %5559, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_6755 = torch.constant.int -2
    %5560 = torch.aten.unsqueeze %5456, %int-2_6755 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %5560, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_6756 = torch.constant.int 1
    %5561 = torch.aten.size.int %5450, %int1_6756 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int4_6757 = torch.constant.int 4
    %int8_6758 = torch.constant.int 8
    %int4_6759 = torch.constant.int 4
    %int128_6760 = torch.constant.int 128
    %5562 = torch.prim.ListConstruct %int4_6757, %5561, %int8_6758, %int4_6759, %int128_6760 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_6761 = torch.constant.bool false
    %5563 = torch.aten.expand %5560, %5562, %false_6761 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5563, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_6762 = torch.constant.int 0
    %5564 = torch.aten.clone %5563, %int0_6762 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5564, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_6763 = torch.constant.int 4
    %int32_6764 = torch.constant.int 32
    %int128_6765 = torch.constant.int 128
    %5565 = torch.prim.ListConstruct %int4_6763, %5561, %int32_6764, %int128_6765 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5566 = torch.aten._unsafe_view %5564, %5565 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %5566, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_6766 = torch.constant.int 1
    %int2_6767 = torch.constant.int 2
    %5567 = torch.aten.transpose.int %5484, %int1_6766, %int2_6767 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %5567, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_6768 = torch.constant.int 1
    %int2_6769 = torch.constant.int 2
    %5568 = torch.aten.transpose.int %5559, %int1_6768, %int2_6769 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %5568, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_6770 = torch.constant.int 1
    %int2_6771 = torch.constant.int 2
    %5569 = torch.aten.transpose.int %5566, %int1_6770, %int2_6771 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %5569, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_6772 = torch.constant.float 0.000000e+00
    %true_6773 = torch.constant.bool true
    %none_6774 = torch.constant.none
    %none_6775 = torch.constant.none
    %5570:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5567, %5568, %5569, %float0.000000e00_6772, %true_6773, %none_6774, %none_6775) : (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?],f32>) 
    torch.bind_symbolic_shape %5570#0, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_6776 = torch.constant.int 1
    %int2_6777 = torch.constant.int 2
    %5571 = torch.aten.transpose.int %5570#0, %int1_6776, %int2_6777 : !torch.vtensor<[4,32,?,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %5571, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_6778 = torch.constant.int 4
    %int4096_6779 = torch.constant.int 4096
    %5572 = torch.prim.ListConstruct %int4_6778, %5469, %int4096_6779 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5573 = torch.aten.view %5571, %5572 : !torch.vtensor<[4,?,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5573, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_6780 = torch.constant.int -2
    %int-1_6781 = torch.constant.int -1
    %5574 = torch.aten.transpose.int %239, %int-2_6780, %int-1_6781 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_6782 = torch.constant.int 4
    %5575 = torch.aten.mul.int %int4_6782, %5469 : !torch.int, !torch.int -> !torch.int
    %int4096_6783 = torch.constant.int 4096
    %5576 = torch.prim.ListConstruct %5575, %int4096_6783 : (!torch.int, !torch.int) -> !torch.list<int>
    %5577 = torch.aten.view %5573, %5576 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5577, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %5578 = torch.aten.mm %5577, %5574 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5578, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_6784 = torch.constant.int 4
    %int4096_6785 = torch.constant.int 4096
    %5579 = torch.prim.ListConstruct %int4_6784, %5469, %int4096_6785 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5580 = torch.aten.view %5578, %5579 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5580, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_6786 = torch.constant.int 1
    %5581 = torch.aten.add.Tensor %5419, %5580, %int1_6786 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5581, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_6787 = torch.constant.int 6
    %5582 = torch.prims.convert_element_type %5581, %int6_6787 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5582, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_6788 = torch.constant.int 2
    %5583 = torch.aten.pow.Tensor_Scalar %5582, %int2_6788 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5583, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_6789 = torch.constant.int -1
    %5584 = torch.prim.ListConstruct %int-1_6789 : (!torch.int) -> !torch.list<int>
    %true_6790 = torch.constant.bool true
    %none_6791 = torch.constant.none
    %5585 = torch.aten.mean.dim %5583, %5584, %true_6790, %none_6791 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5585, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_6792 = torch.constant.float 9.9999997473787516E-6
    %int1_6793 = torch.constant.int 1
    %5586 = torch.aten.add.Scalar %5585, %float9.999990e-06_6792, %int1_6793 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5586, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %5587 = torch.aten.rsqrt %5586 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5587, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %5588 = torch.aten.mul.Tensor %5582, %5587 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5588, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_6794 = torch.constant.int 5
    %5589 = torch.prims.convert_element_type %5588, %int5_6794 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5589, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %5590 = torch.aten.mul.Tensor %240, %5589 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5590, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_6795 = torch.constant.int 5
    %5591 = torch.prims.convert_element_type %5590, %int5_6795 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5591, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_6796 = torch.constant.int -2
    %int-1_6797 = torch.constant.int -1
    %5592 = torch.aten.transpose.int %241, %int-2_6796, %int-1_6797 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_6798 = torch.constant.int 4
    %5593 = torch.aten.mul.int %int4_6798, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_6799 = torch.constant.int 4096
    %5594 = torch.prim.ListConstruct %5593, %int4096_6799 : (!torch.int, !torch.int) -> !torch.list<int>
    %5595 = torch.aten.view %5591, %5594 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5595, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %5596 = torch.aten.mm %5595, %5592 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %5596, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_6800 = torch.constant.int 4
    %int14336_6801 = torch.constant.int 14336
    %5597 = torch.prim.ListConstruct %int4_6800, %306, %int14336_6801 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5598 = torch.aten.view %5596, %5597 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %5598, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %5599 = torch.aten.silu %5598 : !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %5599, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_6802 = torch.constant.int -2
    %int-1_6803 = torch.constant.int -1
    %5600 = torch.aten.transpose.int %242, %int-2_6802, %int-1_6803 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_6804 = torch.constant.int 4
    %5601 = torch.aten.mul.int %int4_6804, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_6805 = torch.constant.int 4096
    %5602 = torch.prim.ListConstruct %5601, %int4096_6805 : (!torch.int, !torch.int) -> !torch.list<int>
    %5603 = torch.aten.view %5591, %5602 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5603, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %5604 = torch.aten.mm %5603, %5600 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %5604, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_6806 = torch.constant.int 4
    %int14336_6807 = torch.constant.int 14336
    %5605 = torch.prim.ListConstruct %int4_6806, %306, %int14336_6807 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5606 = torch.aten.view %5604, %5605 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %5606, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %5607 = torch.aten.mul.Tensor %5599, %5606 : !torch.vtensor<[4,?,14336],f16>, !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %5607, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_6808 = torch.constant.int -2
    %int-1_6809 = torch.constant.int -1
    %5608 = torch.aten.transpose.int %243, %int-2_6808, %int-1_6809 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int1_6810 = torch.constant.int 1
    %5609 = torch.aten.size.int %5598, %int1_6810 : !torch.vtensor<[4,?,14336],f16>, !torch.int -> !torch.int
    %int4_6811 = torch.constant.int 4
    %5610 = torch.aten.mul.int %int4_6811, %5609 : !torch.int, !torch.int -> !torch.int
    %int14336_6812 = torch.constant.int 14336
    %5611 = torch.prim.ListConstruct %5610, %int14336_6812 : (!torch.int, !torch.int) -> !torch.list<int>
    %5612 = torch.aten.view %5607, %5611 : !torch.vtensor<[4,?,14336],f16>, !torch.list<int> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %5612, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %5613 = torch.aten.mm %5612, %5608 : !torch.vtensor<[?,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5613, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_6813 = torch.constant.int 4
    %int4096_6814 = torch.constant.int 4096
    %5614 = torch.prim.ListConstruct %int4_6813, %5609, %int4096_6814 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5615 = torch.aten.view %5613, %5614 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5615, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_6815 = torch.constant.int 1
    %5616 = torch.aten.add.Tensor %5581, %5615, %int1_6815 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5616, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_6816 = torch.constant.int 6
    %5617 = torch.prims.convert_element_type %5616, %int6_6816 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5617, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_6817 = torch.constant.int 2
    %5618 = torch.aten.pow.Tensor_Scalar %5617, %int2_6817 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5618, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_6818 = torch.constant.int -1
    %5619 = torch.prim.ListConstruct %int-1_6818 : (!torch.int) -> !torch.list<int>
    %true_6819 = torch.constant.bool true
    %none_6820 = torch.constant.none
    %5620 = torch.aten.mean.dim %5618, %5619, %true_6819, %none_6820 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5620, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_6821 = torch.constant.float 9.9999997473787516E-6
    %int1_6822 = torch.constant.int 1
    %5621 = torch.aten.add.Scalar %5620, %float9.999990e-06_6821, %int1_6822 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5621, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %5622 = torch.aten.rsqrt %5621 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5622, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %5623 = torch.aten.mul.Tensor %5617, %5622 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5623, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_6823 = torch.constant.int 5
    %5624 = torch.prims.convert_element_type %5623, %int5_6823 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5624, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %5625 = torch.aten.mul.Tensor %244, %5624 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5625, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_6824 = torch.constant.int 5
    %5626 = torch.prims.convert_element_type %5625, %int5_6824 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5626, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_6825 = torch.constant.int -2
    %int-1_6826 = torch.constant.int -1
    %5627 = torch.aten.transpose.int %245, %int-2_6825, %int-1_6826 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_6827 = torch.constant.int 4
    %5628 = torch.aten.mul.int %int4_6827, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_6828 = torch.constant.int 4096
    %5629 = torch.prim.ListConstruct %5628, %int4096_6828 : (!torch.int, !torch.int) -> !torch.list<int>
    %5630 = torch.aten.view %5626, %5629 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5630, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %5631 = torch.aten.mm %5630, %5627 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5631, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_6829 = torch.constant.int 4
    %int4096_6830 = torch.constant.int 4096
    %5632 = torch.prim.ListConstruct %int4_6829, %306, %int4096_6830 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5633 = torch.aten.view %5631, %5632 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5633, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_6831 = torch.constant.int -2
    %int-1_6832 = torch.constant.int -1
    %5634 = torch.aten.transpose.int %246, %int-2_6831, %int-1_6832 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_6833 = torch.constant.int 4
    %5635 = torch.aten.mul.int %int4_6833, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_6834 = torch.constant.int 4096
    %5636 = torch.prim.ListConstruct %5635, %int4096_6834 : (!torch.int, !torch.int) -> !torch.list<int>
    %5637 = torch.aten.view %5626, %5636 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5637, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %5638 = torch.aten.mm %5637, %5634 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %5638, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_6835 = torch.constant.int 4
    %int1024_6836 = torch.constant.int 1024
    %5639 = torch.prim.ListConstruct %int4_6835, %306, %int1024_6836 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5640 = torch.aten.view %5638, %5639 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %5640, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int-2_6837 = torch.constant.int -2
    %int-1_6838 = torch.constant.int -1
    %5641 = torch.aten.transpose.int %247, %int-2_6837, %int-1_6838 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_6839 = torch.constant.int 4
    %5642 = torch.aten.mul.int %int4_6839, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_6840 = torch.constant.int 4096
    %5643 = torch.prim.ListConstruct %5642, %int4096_6840 : (!torch.int, !torch.int) -> !torch.list<int>
    %5644 = torch.aten.view %5626, %5643 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5644, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %5645 = torch.aten.mm %5644, %5641 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %5645, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_6841 = torch.constant.int 4
    %int1024_6842 = torch.constant.int 1024
    %5646 = torch.prim.ListConstruct %int4_6841, %306, %int1024_6842 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5647 = torch.aten.view %5645, %5646 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %5647, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int4_6843 = torch.constant.int 4
    %int32_6844 = torch.constant.int 32
    %int128_6845 = torch.constant.int 128
    %5648 = torch.prim.ListConstruct %int4_6843, %306, %int32_6844, %int128_6845 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5649 = torch.aten.view %5633, %5648 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %5649, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_6846 = torch.constant.int 4
    %int8_6847 = torch.constant.int 8
    %int128_6848 = torch.constant.int 128
    %5650 = torch.prim.ListConstruct %int4_6846, %306, %int8_6847, %int128_6848 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5651 = torch.aten.view %5640, %5650 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %5651, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int4_6849 = torch.constant.int 4
    %int8_6850 = torch.constant.int 8
    %int128_6851 = torch.constant.int 128
    %5652 = torch.prim.ListConstruct %int4_6849, %306, %int8_6850, %int128_6851 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5653 = torch.aten.view %5647, %5652 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %5653, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int131072_6852 = torch.constant.int 131072
    %none_6853 = torch.constant.none
    %none_6854 = torch.constant.none
    %cpu_6855 = torch.constant.device "cpu"
    %false_6856 = torch.constant.bool false
    %5654 = torch.aten.arange %int131072_6852, %none_6853, %none_6854, %cpu_6855, %false_6856 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_6857 = torch.constant.int 0
    %int128_6858 = torch.constant.int 128
    %none_6859 = torch.constant.none
    %none_6860 = torch.constant.none
    %cpu_6861 = torch.constant.device "cpu"
    %false_6862 = torch.constant.bool false
    %5655 = torch.aten.arange.start %int0_6857, %int128_6858, %none_6859, %none_6860, %cpu_6861, %false_6862 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_6863 = torch.constant.int 2
    %5656 = torch.aten.floor_divide.Scalar %5655, %int2_6863 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_6864 = torch.constant.int 6
    %5657 = torch.prims.convert_element_type %5656, %int6_6864 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_6865 = torch.constant.int 128
    %5658 = torch.aten.div.Scalar %5657, %int128_6865 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_6866 = torch.constant.float 2.000000e+00
    %5659 = torch.aten.mul.Scalar %5658, %float2.000000e00_6866 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_6867 = torch.constant.float 5.000000e+05
    %5660 = torch.aten.pow.Scalar %float5.000000e05_6867, %5659 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %5661 = torch.aten.reciprocal %5660 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_6868 = torch.constant.float 1.000000e+00
    %5662 = torch.aten.mul.Scalar %5661, %float1.000000e00_6868 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_6869 = torch.constant.int 1
    %5663 = torch.aten.unsqueeze %5654, %int1_6869 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_6870 = torch.constant.int 0
    %5664 = torch.aten.unsqueeze %5662, %int0_6870 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %5665 = torch.aten.mul.Tensor %5663, %5664 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_6871 = torch.constant.int 1
    %5666 = torch.aten.size.int %5633, %int1_6871 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.int
    %int0_6872 = torch.constant.int 0
    %5667 = torch.aten.add.int %int0_6872, %5666 : !torch.int, !torch.int -> !torch.int
    %int0_6873 = torch.constant.int 0
    %int0_6874 = torch.constant.int 0
    %int1_6875 = torch.constant.int 1
    %5668 = torch.aten.slice.Tensor %5665, %int0_6873, %int0_6874, %5667, %int1_6875 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %5668, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_6876 = torch.constant.int 1
    %int0_6877 = torch.constant.int 0
    %int9223372036854775807_6878 = torch.constant.int 9223372036854775807
    %int1_6879 = torch.constant.int 1
    %5669 = torch.aten.slice.Tensor %5668, %int1_6876, %int0_6877, %int9223372036854775807_6878, %int1_6879 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %5669, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_6880 = torch.constant.int 1
    %int0_6881 = torch.constant.int 0
    %int9223372036854775807_6882 = torch.constant.int 9223372036854775807
    %int1_6883 = torch.constant.int 1
    %5670 = torch.aten.slice.Tensor %5669, %int1_6880, %int0_6881, %int9223372036854775807_6882, %int1_6883 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %5670, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_6884 = torch.constant.int 0
    %5671 = torch.aten.unsqueeze %5670, %int0_6884 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %5671, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_6885 = torch.constant.int 1
    %int0_6886 = torch.constant.int 0
    %int9223372036854775807_6887 = torch.constant.int 9223372036854775807
    %int1_6888 = torch.constant.int 1
    %5672 = torch.aten.slice.Tensor %5671, %int1_6885, %int0_6886, %int9223372036854775807_6887, %int1_6888 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %5672, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_6889 = torch.constant.int 2
    %int0_6890 = torch.constant.int 0
    %int9223372036854775807_6891 = torch.constant.int 9223372036854775807
    %int1_6892 = torch.constant.int 1
    %5673 = torch.aten.slice.Tensor %5672, %int2_6889, %int0_6890, %int9223372036854775807_6891, %int1_6892 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %5673, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_6893 = torch.constant.int 4
    %int1_6894 = torch.constant.int 1
    %int1_6895 = torch.constant.int 1
    %5674 = torch.prim.ListConstruct %int4_6893, %int1_6894, %int1_6895 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5675 = torch.aten.repeat %5673, %5674 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %5675, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_6896 = torch.constant.int 6
    %5676 = torch.prims.convert_element_type %5649, %int6_6896 : !torch.vtensor<[4,?,32,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %5676, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %5677 = torch_c.to_builtin_tensor %5676 : !torch.vtensor<[4,?,32,128],f32> -> tensor<4x?x32x128xf32>
    %5678 = torch_c.to_builtin_tensor %5675 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %5679 = util.call @sharktank_rotary_embedding_4_D_32_128_f32(%5677, %5678) : (tensor<4x?x32x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x32x128xf32>
    %5680 = torch_c.from_builtin_tensor %5679 : tensor<4x?x32x128xf32> -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %5680, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %int5_6897 = torch.constant.int 5
    %5681 = torch.prims.convert_element_type %5680, %int5_6897 : !torch.vtensor<[4,?,32,128],f32>, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %5681, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int131072_6898 = torch.constant.int 131072
    %none_6899 = torch.constant.none
    %none_6900 = torch.constant.none
    %cpu_6901 = torch.constant.device "cpu"
    %false_6902 = torch.constant.bool false
    %5682 = torch.aten.arange %int131072_6898, %none_6899, %none_6900, %cpu_6901, %false_6902 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_6903 = torch.constant.int 0
    %int128_6904 = torch.constant.int 128
    %none_6905 = torch.constant.none
    %none_6906 = torch.constant.none
    %cpu_6907 = torch.constant.device "cpu"
    %false_6908 = torch.constant.bool false
    %5683 = torch.aten.arange.start %int0_6903, %int128_6904, %none_6905, %none_6906, %cpu_6907, %false_6908 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_6909 = torch.constant.int 2
    %5684 = torch.aten.floor_divide.Scalar %5683, %int2_6909 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_6910 = torch.constant.int 6
    %5685 = torch.prims.convert_element_type %5684, %int6_6910 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_6911 = torch.constant.int 128
    %5686 = torch.aten.div.Scalar %5685, %int128_6911 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_6912 = torch.constant.float 2.000000e+00
    %5687 = torch.aten.mul.Scalar %5686, %float2.000000e00_6912 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_6913 = torch.constant.float 5.000000e+05
    %5688 = torch.aten.pow.Scalar %float5.000000e05_6913, %5687 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %5689 = torch.aten.reciprocal %5688 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_6914 = torch.constant.float 1.000000e+00
    %5690 = torch.aten.mul.Scalar %5689, %float1.000000e00_6914 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_6915 = torch.constant.int 1
    %5691 = torch.aten.unsqueeze %5682, %int1_6915 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_6916 = torch.constant.int 0
    %5692 = torch.aten.unsqueeze %5690, %int0_6916 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %5693 = torch.aten.mul.Tensor %5691, %5692 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_6917 = torch.constant.int 1
    %5694 = torch.aten.size.int %5640, %int1_6917 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int0_6918 = torch.constant.int 0
    %5695 = torch.aten.add.int %int0_6918, %5694 : !torch.int, !torch.int -> !torch.int
    %int0_6919 = torch.constant.int 0
    %int0_6920 = torch.constant.int 0
    %int1_6921 = torch.constant.int 1
    %5696 = torch.aten.slice.Tensor %5693, %int0_6919, %int0_6920, %5695, %int1_6921 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %5696, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_6922 = torch.constant.int 1
    %int0_6923 = torch.constant.int 0
    %int9223372036854775807_6924 = torch.constant.int 9223372036854775807
    %int1_6925 = torch.constant.int 1
    %5697 = torch.aten.slice.Tensor %5696, %int1_6922, %int0_6923, %int9223372036854775807_6924, %int1_6925 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %5697, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_6926 = torch.constant.int 1
    %int0_6927 = torch.constant.int 0
    %int9223372036854775807_6928 = torch.constant.int 9223372036854775807
    %int1_6929 = torch.constant.int 1
    %5698 = torch.aten.slice.Tensor %5697, %int1_6926, %int0_6927, %int9223372036854775807_6928, %int1_6929 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %5698, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_6930 = torch.constant.int 0
    %5699 = torch.aten.unsqueeze %5698, %int0_6930 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %5699, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_6931 = torch.constant.int 1
    %int0_6932 = torch.constant.int 0
    %int9223372036854775807_6933 = torch.constant.int 9223372036854775807
    %int1_6934 = torch.constant.int 1
    %5700 = torch.aten.slice.Tensor %5699, %int1_6931, %int0_6932, %int9223372036854775807_6933, %int1_6934 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %5700, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_6935 = torch.constant.int 2
    %int0_6936 = torch.constant.int 0
    %int9223372036854775807_6937 = torch.constant.int 9223372036854775807
    %int1_6938 = torch.constant.int 1
    %5701 = torch.aten.slice.Tensor %5700, %int2_6935, %int0_6936, %int9223372036854775807_6937, %int1_6938 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %5701, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_6939 = torch.constant.int 4
    %int1_6940 = torch.constant.int 1
    %int1_6941 = torch.constant.int 1
    %5702 = torch.prim.ListConstruct %int4_6939, %int1_6940, %int1_6941 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5703 = torch.aten.repeat %5701, %5702 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %5703, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_6942 = torch.constant.int 6
    %5704 = torch.prims.convert_element_type %5651, %int6_6942 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %5704, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %5705 = torch_c.to_builtin_tensor %5704 : !torch.vtensor<[4,?,8,128],f32> -> tensor<4x?x8x128xf32>
    %5706 = torch_c.to_builtin_tensor %5703 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %5707 = util.call @sharktank_rotary_embedding_4_D_8_128_f32(%5705, %5706) : (tensor<4x?x8x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x8x128xf32>
    %5708 = torch_c.from_builtin_tensor %5707 : tensor<4x?x8x128xf32> -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %5708, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %int5_6943 = torch.constant.int 5
    %5709 = torch.prims.convert_element_type %5708, %int5_6943 : !torch.vtensor<[4,?,8,128],f32>, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %5709, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int64_6944 = torch.constant.int 64
    %5710 = torch.aten.mul.Scalar %arg2, %int64_6944 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5710, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int54 = torch.constant.int 54
    %int1_6945 = torch.constant.int 1
    %5711 = torch.aten.add.Scalar %5710, %int54, %int1_6945 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5711, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_6946 = torch.constant.int 4
    %int32_6947 = torch.constant.int 32
    %int8_6948 = torch.constant.int 8
    %int128_6949 = torch.constant.int 128
    %5712 = torch.prim.ListConstruct %int4_6946, %398, %int32_6947, %int8_6948, %int128_6949 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5713 = torch.aten.view %5709, %5712 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5713, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_6950 = torch.constant.int 4
    %5714 = torch.aten.mul.int %int4_6950, %398 : !torch.int, !torch.int -> !torch.int
    %int32_6951 = torch.constant.int 32
    %int8_6952 = torch.constant.int 8
    %int128_6953 = torch.constant.int 128
    %5715 = torch.prim.ListConstruct %5714, %int32_6951, %int8_6952, %int128_6953 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5716 = torch.aten.view %5713, %5715 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %5716, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_6954 = torch.constant.int 4
    %5717 = torch.aten.mul.int %int4_6954, %398 : !torch.int, !torch.int -> !torch.int
    %5718 = torch.prim.ListConstruct %5717 : (!torch.int) -> !torch.list<int>
    %5719 = torch.aten.view %5711, %5718 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5719, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_6955 = torch.constant.int 32
    %int2_6956 = torch.constant.int 2
    %int32_6957 = torch.constant.int 32
    %int8_6958 = torch.constant.int 8
    %int128_6959 = torch.constant.int 128
    %5720 = torch.prim.ListConstruct %389, %int32_6955, %int2_6956, %int32_6957, %int8_6958, %int128_6959 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5721 = torch.aten.view %5553, %5720 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5721, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_6960 = torch.constant.int 32
    %5722 = torch.aten.mul.int %389, %int32_6960 : !torch.int, !torch.int -> !torch.int
    %int2_6961 = torch.constant.int 2
    %5723 = torch.aten.mul.int %5722, %int2_6961 : !torch.int, !torch.int -> !torch.int
    %int32_6962 = torch.constant.int 32
    %int8_6963 = torch.constant.int 8
    %int128_6964 = torch.constant.int 128
    %5724 = torch.prim.ListConstruct %5723, %int32_6962, %int8_6963, %int128_6964 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5725 = torch.aten.view %5721, %5724 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %5725, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %5726 = torch.prim.ListConstruct %5719 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_6965 = torch.constant.bool false
    %5727 = torch.aten.index_put %5725, %5726, %5716, %false_6965 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %5727, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_6966 = torch.constant.int 32
    %int2_6967 = torch.constant.int 2
    %int32_6968 = torch.constant.int 32
    %int8_6969 = torch.constant.int 8
    %int128_6970 = torch.constant.int 128
    %5728 = torch.prim.ListConstruct %389, %int32_6966, %int2_6967, %int32_6968, %int8_6969, %int128_6970 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5729 = torch.aten.view %5727, %5728 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5729, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_6971 = torch.constant.int 2097152
    %5730 = torch.prim.ListConstruct %389, %int2097152_6971 : (!torch.int, !torch.int) -> !torch.list<int>
    %5731 = torch.aten.view %5729, %5730 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %5731, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_6972 = torch.constant.int 32
    %int2_6973 = torch.constant.int 2
    %int32_6974 = torch.constant.int 32
    %int8_6975 = torch.constant.int 8
    %int128_6976 = torch.constant.int 128
    %5732 = torch.prim.ListConstruct %389, %int32_6972, %int2_6973, %int32_6974, %int8_6975, %int128_6976 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5733 = torch.aten.view %5731, %5732 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5733, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_6977 = torch.constant.int 32
    %int8_6978 = torch.constant.int 8
    %int128_6979 = torch.constant.int 128
    %5734 = torch.prim.ListConstruct %5723, %int32_6977, %int8_6978, %int128_6979 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5735 = torch.aten.view %5733, %5734 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %5735, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_6980 = torch.constant.int 4
    %int32_6981 = torch.constant.int 32
    %int8_6982 = torch.constant.int 8
    %int128_6983 = torch.constant.int 128
    %5736 = torch.prim.ListConstruct %int4_6980, %398, %int32_6981, %int8_6982, %int128_6983 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5737 = torch.aten.view %5653, %5736 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5737, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_6984 = torch.constant.int 4
    %5738 = torch.aten.mul.int %int4_6984, %398 : !torch.int, !torch.int -> !torch.int
    %int32_6985 = torch.constant.int 32
    %int8_6986 = torch.constant.int 8
    %int128_6987 = torch.constant.int 128
    %5739 = torch.prim.ListConstruct %5738, %int32_6985, %int8_6986, %int128_6987 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5740 = torch.aten.view %5737, %5739 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %5740, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int1_6988 = torch.constant.int 1
    %int1_6989 = torch.constant.int 1
    %5741 = torch.aten.add.Scalar %5711, %int1_6988, %int1_6989 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5741, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_6990 = torch.constant.int 4
    %5742 = torch.aten.mul.int %int4_6990, %398 : !torch.int, !torch.int -> !torch.int
    %5743 = torch.prim.ListConstruct %5742 : (!torch.int) -> !torch.list<int>
    %5744 = torch.aten.view %5741, %5743 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5744, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %5745 = torch.prim.ListConstruct %5744 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_6991 = torch.constant.bool false
    %5746 = torch.aten.index_put %5735, %5745, %5740, %false_6991 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %5746, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_6992 = torch.constant.int 32
    %int2_6993 = torch.constant.int 2
    %int32_6994 = torch.constant.int 32
    %int8_6995 = torch.constant.int 8
    %int128_6996 = torch.constant.int 128
    %5747 = torch.prim.ListConstruct %389, %int32_6992, %int2_6993, %int32_6994, %int8_6995, %int128_6996 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5748 = torch.aten.view %5746, %5747 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5748, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_6997 = torch.constant.int 2097152
    %5749 = torch.prim.ListConstruct %389, %int2097152_6997 : (!torch.int, !torch.int) -> !torch.list<int>
    %5750 = torch.aten.view %5748, %5749 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %5750, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int-2_6998 = torch.constant.int -2
    %5751 = torch.aten.unsqueeze %5709, %int-2_6998 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %5751, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int4_6999 = torch.constant.int 4
    %int8_7000 = torch.constant.int 8
    %int4_7001 = torch.constant.int 4
    %int128_7002 = torch.constant.int 128
    %5752 = torch.prim.ListConstruct %int4_6999, %5694, %int8_7000, %int4_7001, %int128_7002 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_7003 = torch.constant.bool false
    %5753 = torch.aten.expand %5751, %5752, %false_7003 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5753, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_7004 = torch.constant.int 0
    %5754 = torch.aten.clone %5753, %int0_7004 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5754, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_7005 = torch.constant.int 4
    %int32_7006 = torch.constant.int 32
    %int128_7007 = torch.constant.int 128
    %5755 = torch.prim.ListConstruct %int4_7005, %5694, %int32_7006, %int128_7007 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5756 = torch.aten._unsafe_view %5754, %5755 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %5756, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_7008 = torch.constant.int -2
    %5757 = torch.aten.unsqueeze %5653, %int-2_7008 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %5757, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_7009 = torch.constant.int 1
    %5758 = torch.aten.size.int %5647, %int1_7009 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int4_7010 = torch.constant.int 4
    %int8_7011 = torch.constant.int 8
    %int4_7012 = torch.constant.int 4
    %int128_7013 = torch.constant.int 128
    %5759 = torch.prim.ListConstruct %int4_7010, %5758, %int8_7011, %int4_7012, %int128_7013 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_7014 = torch.constant.bool false
    %5760 = torch.aten.expand %5757, %5759, %false_7014 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5760, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_7015 = torch.constant.int 0
    %5761 = torch.aten.clone %5760, %int0_7015 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5761, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_7016 = torch.constant.int 4
    %int32_7017 = torch.constant.int 32
    %int128_7018 = torch.constant.int 128
    %5762 = torch.prim.ListConstruct %int4_7016, %5758, %int32_7017, %int128_7018 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5763 = torch.aten._unsafe_view %5761, %5762 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %5763, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_7019 = torch.constant.int 1
    %int2_7020 = torch.constant.int 2
    %5764 = torch.aten.transpose.int %5681, %int1_7019, %int2_7020 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %5764, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_7021 = torch.constant.int 1
    %int2_7022 = torch.constant.int 2
    %5765 = torch.aten.transpose.int %5756, %int1_7021, %int2_7022 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %5765, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_7023 = torch.constant.int 1
    %int2_7024 = torch.constant.int 2
    %5766 = torch.aten.transpose.int %5763, %int1_7023, %int2_7024 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %5766, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_7025 = torch.constant.float 0.000000e+00
    %true_7026 = torch.constant.bool true
    %none_7027 = torch.constant.none
    %none_7028 = torch.constant.none
    %5767:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5764, %5765, %5766, %float0.000000e00_7025, %true_7026, %none_7027, %none_7028) : (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?],f32>) 
    torch.bind_symbolic_shape %5767#0, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_7029 = torch.constant.int 1
    %int2_7030 = torch.constant.int 2
    %5768 = torch.aten.transpose.int %5767#0, %int1_7029, %int2_7030 : !torch.vtensor<[4,32,?,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %5768, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_7031 = torch.constant.int 4
    %int4096_7032 = torch.constant.int 4096
    %5769 = torch.prim.ListConstruct %int4_7031, %5666, %int4096_7032 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5770 = torch.aten.view %5768, %5769 : !torch.vtensor<[4,?,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5770, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_7033 = torch.constant.int -2
    %int-1_7034 = torch.constant.int -1
    %5771 = torch.aten.transpose.int %248, %int-2_7033, %int-1_7034 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_7035 = torch.constant.int 4
    %5772 = torch.aten.mul.int %int4_7035, %5666 : !torch.int, !torch.int -> !torch.int
    %int4096_7036 = torch.constant.int 4096
    %5773 = torch.prim.ListConstruct %5772, %int4096_7036 : (!torch.int, !torch.int) -> !torch.list<int>
    %5774 = torch.aten.view %5770, %5773 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5774, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %5775 = torch.aten.mm %5774, %5771 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5775, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_7037 = torch.constant.int 4
    %int4096_7038 = torch.constant.int 4096
    %5776 = torch.prim.ListConstruct %int4_7037, %5666, %int4096_7038 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5777 = torch.aten.view %5775, %5776 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5777, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_7039 = torch.constant.int 1
    %5778 = torch.aten.add.Tensor %5616, %5777, %int1_7039 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5778, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_7040 = torch.constant.int 6
    %5779 = torch.prims.convert_element_type %5778, %int6_7040 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5779, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_7041 = torch.constant.int 2
    %5780 = torch.aten.pow.Tensor_Scalar %5779, %int2_7041 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5780, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_7042 = torch.constant.int -1
    %5781 = torch.prim.ListConstruct %int-1_7042 : (!torch.int) -> !torch.list<int>
    %true_7043 = torch.constant.bool true
    %none_7044 = torch.constant.none
    %5782 = torch.aten.mean.dim %5780, %5781, %true_7043, %none_7044 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5782, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_7045 = torch.constant.float 9.9999997473787516E-6
    %int1_7046 = torch.constant.int 1
    %5783 = torch.aten.add.Scalar %5782, %float9.999990e-06_7045, %int1_7046 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5783, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %5784 = torch.aten.rsqrt %5783 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5784, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %5785 = torch.aten.mul.Tensor %5779, %5784 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5785, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_7047 = torch.constant.int 5
    %5786 = torch.prims.convert_element_type %5785, %int5_7047 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5786, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %5787 = torch.aten.mul.Tensor %249, %5786 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5787, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_7048 = torch.constant.int 5
    %5788 = torch.prims.convert_element_type %5787, %int5_7048 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5788, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_7049 = torch.constant.int -2
    %int-1_7050 = torch.constant.int -1
    %5789 = torch.aten.transpose.int %250, %int-2_7049, %int-1_7050 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_7051 = torch.constant.int 4
    %5790 = torch.aten.mul.int %int4_7051, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_7052 = torch.constant.int 4096
    %5791 = torch.prim.ListConstruct %5790, %int4096_7052 : (!torch.int, !torch.int) -> !torch.list<int>
    %5792 = torch.aten.view %5788, %5791 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5792, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %5793 = torch.aten.mm %5792, %5789 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %5793, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_7053 = torch.constant.int 4
    %int14336_7054 = torch.constant.int 14336
    %5794 = torch.prim.ListConstruct %int4_7053, %306, %int14336_7054 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5795 = torch.aten.view %5793, %5794 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %5795, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %5796 = torch.aten.silu %5795 : !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %5796, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_7055 = torch.constant.int -2
    %int-1_7056 = torch.constant.int -1
    %5797 = torch.aten.transpose.int %251, %int-2_7055, %int-1_7056 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_7057 = torch.constant.int 4
    %5798 = torch.aten.mul.int %int4_7057, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_7058 = torch.constant.int 4096
    %5799 = torch.prim.ListConstruct %5798, %int4096_7058 : (!torch.int, !torch.int) -> !torch.list<int>
    %5800 = torch.aten.view %5788, %5799 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5800, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %5801 = torch.aten.mm %5800, %5797 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %5801, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_7059 = torch.constant.int 4
    %int14336_7060 = torch.constant.int 14336
    %5802 = torch.prim.ListConstruct %int4_7059, %306, %int14336_7060 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5803 = torch.aten.view %5801, %5802 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %5803, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %5804 = torch.aten.mul.Tensor %5796, %5803 : !torch.vtensor<[4,?,14336],f16>, !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %5804, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_7061 = torch.constant.int -2
    %int-1_7062 = torch.constant.int -1
    %5805 = torch.aten.transpose.int %252, %int-2_7061, %int-1_7062 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int1_7063 = torch.constant.int 1
    %5806 = torch.aten.size.int %5795, %int1_7063 : !torch.vtensor<[4,?,14336],f16>, !torch.int -> !torch.int
    %int4_7064 = torch.constant.int 4
    %5807 = torch.aten.mul.int %int4_7064, %5806 : !torch.int, !torch.int -> !torch.int
    %int14336_7065 = torch.constant.int 14336
    %5808 = torch.prim.ListConstruct %5807, %int14336_7065 : (!torch.int, !torch.int) -> !torch.list<int>
    %5809 = torch.aten.view %5804, %5808 : !torch.vtensor<[4,?,14336],f16>, !torch.list<int> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %5809, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %5810 = torch.aten.mm %5809, %5805 : !torch.vtensor<[?,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5810, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_7066 = torch.constant.int 4
    %int4096_7067 = torch.constant.int 4096
    %5811 = torch.prim.ListConstruct %int4_7066, %5806, %int4096_7067 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5812 = torch.aten.view %5810, %5811 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5812, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_7068 = torch.constant.int 1
    %5813 = torch.aten.add.Tensor %5778, %5812, %int1_7068 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5813, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_7069 = torch.constant.int 6
    %5814 = torch.prims.convert_element_type %5813, %int6_7069 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5814, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_7070 = torch.constant.int 2
    %5815 = torch.aten.pow.Tensor_Scalar %5814, %int2_7070 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5815, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_7071 = torch.constant.int -1
    %5816 = torch.prim.ListConstruct %int-1_7071 : (!torch.int) -> !torch.list<int>
    %true_7072 = torch.constant.bool true
    %none_7073 = torch.constant.none
    %5817 = torch.aten.mean.dim %5815, %5816, %true_7072, %none_7073 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5817, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_7074 = torch.constant.float 9.9999997473787516E-6
    %int1_7075 = torch.constant.int 1
    %5818 = torch.aten.add.Scalar %5817, %float9.999990e-06_7074, %int1_7075 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5818, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %5819 = torch.aten.rsqrt %5818 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5819, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %5820 = torch.aten.mul.Tensor %5814, %5819 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5820, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_7076 = torch.constant.int 5
    %5821 = torch.prims.convert_element_type %5820, %int5_7076 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5821, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %5822 = torch.aten.mul.Tensor %253, %5821 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5822, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_7077 = torch.constant.int 5
    %5823 = torch.prims.convert_element_type %5822, %int5_7077 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5823, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_7078 = torch.constant.int -2
    %int-1_7079 = torch.constant.int -1
    %5824 = torch.aten.transpose.int %254, %int-2_7078, %int-1_7079 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_7080 = torch.constant.int 4
    %5825 = torch.aten.mul.int %int4_7080, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_7081 = torch.constant.int 4096
    %5826 = torch.prim.ListConstruct %5825, %int4096_7081 : (!torch.int, !torch.int) -> !torch.list<int>
    %5827 = torch.aten.view %5823, %5826 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5827, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %5828 = torch.aten.mm %5827, %5824 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5828, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_7082 = torch.constant.int 4
    %int4096_7083 = torch.constant.int 4096
    %5829 = torch.prim.ListConstruct %int4_7082, %306, %int4096_7083 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5830 = torch.aten.view %5828, %5829 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5830, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_7084 = torch.constant.int -2
    %int-1_7085 = torch.constant.int -1
    %5831 = torch.aten.transpose.int %255, %int-2_7084, %int-1_7085 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_7086 = torch.constant.int 4
    %5832 = torch.aten.mul.int %int4_7086, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_7087 = torch.constant.int 4096
    %5833 = torch.prim.ListConstruct %5832, %int4096_7087 : (!torch.int, !torch.int) -> !torch.list<int>
    %5834 = torch.aten.view %5823, %5833 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5834, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %5835 = torch.aten.mm %5834, %5831 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %5835, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_7088 = torch.constant.int 4
    %int1024_7089 = torch.constant.int 1024
    %5836 = torch.prim.ListConstruct %int4_7088, %306, %int1024_7089 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5837 = torch.aten.view %5835, %5836 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %5837, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int-2_7090 = torch.constant.int -2
    %int-1_7091 = torch.constant.int -1
    %5838 = torch.aten.transpose.int %256, %int-2_7090, %int-1_7091 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_7092 = torch.constant.int 4
    %5839 = torch.aten.mul.int %int4_7092, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_7093 = torch.constant.int 4096
    %5840 = torch.prim.ListConstruct %5839, %int4096_7093 : (!torch.int, !torch.int) -> !torch.list<int>
    %5841 = torch.aten.view %5823, %5840 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5841, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %5842 = torch.aten.mm %5841, %5838 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %5842, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_7094 = torch.constant.int 4
    %int1024_7095 = torch.constant.int 1024
    %5843 = torch.prim.ListConstruct %int4_7094, %306, %int1024_7095 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5844 = torch.aten.view %5842, %5843 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %5844, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int4_7096 = torch.constant.int 4
    %int32_7097 = torch.constant.int 32
    %int128_7098 = torch.constant.int 128
    %5845 = torch.prim.ListConstruct %int4_7096, %306, %int32_7097, %int128_7098 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5846 = torch.aten.view %5830, %5845 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %5846, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_7099 = torch.constant.int 4
    %int8_7100 = torch.constant.int 8
    %int128_7101 = torch.constant.int 128
    %5847 = torch.prim.ListConstruct %int4_7099, %306, %int8_7100, %int128_7101 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5848 = torch.aten.view %5837, %5847 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %5848, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int4_7102 = torch.constant.int 4
    %int8_7103 = torch.constant.int 8
    %int128_7104 = torch.constant.int 128
    %5849 = torch.prim.ListConstruct %int4_7102, %306, %int8_7103, %int128_7104 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5850 = torch.aten.view %5844, %5849 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %5850, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int131072_7105 = torch.constant.int 131072
    %none_7106 = torch.constant.none
    %none_7107 = torch.constant.none
    %cpu_7108 = torch.constant.device "cpu"
    %false_7109 = torch.constant.bool false
    %5851 = torch.aten.arange %int131072_7105, %none_7106, %none_7107, %cpu_7108, %false_7109 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_7110 = torch.constant.int 0
    %int128_7111 = torch.constant.int 128
    %none_7112 = torch.constant.none
    %none_7113 = torch.constant.none
    %cpu_7114 = torch.constant.device "cpu"
    %false_7115 = torch.constant.bool false
    %5852 = torch.aten.arange.start %int0_7110, %int128_7111, %none_7112, %none_7113, %cpu_7114, %false_7115 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_7116 = torch.constant.int 2
    %5853 = torch.aten.floor_divide.Scalar %5852, %int2_7116 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_7117 = torch.constant.int 6
    %5854 = torch.prims.convert_element_type %5853, %int6_7117 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_7118 = torch.constant.int 128
    %5855 = torch.aten.div.Scalar %5854, %int128_7118 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_7119 = torch.constant.float 2.000000e+00
    %5856 = torch.aten.mul.Scalar %5855, %float2.000000e00_7119 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_7120 = torch.constant.float 5.000000e+05
    %5857 = torch.aten.pow.Scalar %float5.000000e05_7120, %5856 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %5858 = torch.aten.reciprocal %5857 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_7121 = torch.constant.float 1.000000e+00
    %5859 = torch.aten.mul.Scalar %5858, %float1.000000e00_7121 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_7122 = torch.constant.int 1
    %5860 = torch.aten.unsqueeze %5851, %int1_7122 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_7123 = torch.constant.int 0
    %5861 = torch.aten.unsqueeze %5859, %int0_7123 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %5862 = torch.aten.mul.Tensor %5860, %5861 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_7124 = torch.constant.int 1
    %5863 = torch.aten.size.int %5830, %int1_7124 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.int
    %int0_7125 = torch.constant.int 0
    %5864 = torch.aten.add.int %int0_7125, %5863 : !torch.int, !torch.int -> !torch.int
    %int0_7126 = torch.constant.int 0
    %int0_7127 = torch.constant.int 0
    %int1_7128 = torch.constant.int 1
    %5865 = torch.aten.slice.Tensor %5862, %int0_7126, %int0_7127, %5864, %int1_7128 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %5865, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_7129 = torch.constant.int 1
    %int0_7130 = torch.constant.int 0
    %int9223372036854775807_7131 = torch.constant.int 9223372036854775807
    %int1_7132 = torch.constant.int 1
    %5866 = torch.aten.slice.Tensor %5865, %int1_7129, %int0_7130, %int9223372036854775807_7131, %int1_7132 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %5866, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_7133 = torch.constant.int 1
    %int0_7134 = torch.constant.int 0
    %int9223372036854775807_7135 = torch.constant.int 9223372036854775807
    %int1_7136 = torch.constant.int 1
    %5867 = torch.aten.slice.Tensor %5866, %int1_7133, %int0_7134, %int9223372036854775807_7135, %int1_7136 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %5867, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_7137 = torch.constant.int 0
    %5868 = torch.aten.unsqueeze %5867, %int0_7137 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %5868, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_7138 = torch.constant.int 1
    %int0_7139 = torch.constant.int 0
    %int9223372036854775807_7140 = torch.constant.int 9223372036854775807
    %int1_7141 = torch.constant.int 1
    %5869 = torch.aten.slice.Tensor %5868, %int1_7138, %int0_7139, %int9223372036854775807_7140, %int1_7141 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %5869, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_7142 = torch.constant.int 2
    %int0_7143 = torch.constant.int 0
    %int9223372036854775807_7144 = torch.constant.int 9223372036854775807
    %int1_7145 = torch.constant.int 1
    %5870 = torch.aten.slice.Tensor %5869, %int2_7142, %int0_7143, %int9223372036854775807_7144, %int1_7145 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %5870, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_7146 = torch.constant.int 4
    %int1_7147 = torch.constant.int 1
    %int1_7148 = torch.constant.int 1
    %5871 = torch.prim.ListConstruct %int4_7146, %int1_7147, %int1_7148 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5872 = torch.aten.repeat %5870, %5871 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %5872, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_7149 = torch.constant.int 6
    %5873 = torch.prims.convert_element_type %5846, %int6_7149 : !torch.vtensor<[4,?,32,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %5873, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %5874 = torch_c.to_builtin_tensor %5873 : !torch.vtensor<[4,?,32,128],f32> -> tensor<4x?x32x128xf32>
    %5875 = torch_c.to_builtin_tensor %5872 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %5876 = util.call @sharktank_rotary_embedding_4_D_32_128_f32(%5874, %5875) : (tensor<4x?x32x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x32x128xf32>
    %5877 = torch_c.from_builtin_tensor %5876 : tensor<4x?x32x128xf32> -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %5877, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %int5_7150 = torch.constant.int 5
    %5878 = torch.prims.convert_element_type %5877, %int5_7150 : !torch.vtensor<[4,?,32,128],f32>, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %5878, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int131072_7151 = torch.constant.int 131072
    %none_7152 = torch.constant.none
    %none_7153 = torch.constant.none
    %cpu_7154 = torch.constant.device "cpu"
    %false_7155 = torch.constant.bool false
    %5879 = torch.aten.arange %int131072_7151, %none_7152, %none_7153, %cpu_7154, %false_7155 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_7156 = torch.constant.int 0
    %int128_7157 = torch.constant.int 128
    %none_7158 = torch.constant.none
    %none_7159 = torch.constant.none
    %cpu_7160 = torch.constant.device "cpu"
    %false_7161 = torch.constant.bool false
    %5880 = torch.aten.arange.start %int0_7156, %int128_7157, %none_7158, %none_7159, %cpu_7160, %false_7161 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_7162 = torch.constant.int 2
    %5881 = torch.aten.floor_divide.Scalar %5880, %int2_7162 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_7163 = torch.constant.int 6
    %5882 = torch.prims.convert_element_type %5881, %int6_7163 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_7164 = torch.constant.int 128
    %5883 = torch.aten.div.Scalar %5882, %int128_7164 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_7165 = torch.constant.float 2.000000e+00
    %5884 = torch.aten.mul.Scalar %5883, %float2.000000e00_7165 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_7166 = torch.constant.float 5.000000e+05
    %5885 = torch.aten.pow.Scalar %float5.000000e05_7166, %5884 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %5886 = torch.aten.reciprocal %5885 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_7167 = torch.constant.float 1.000000e+00
    %5887 = torch.aten.mul.Scalar %5886, %float1.000000e00_7167 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_7168 = torch.constant.int 1
    %5888 = torch.aten.unsqueeze %5879, %int1_7168 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_7169 = torch.constant.int 0
    %5889 = torch.aten.unsqueeze %5887, %int0_7169 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %5890 = torch.aten.mul.Tensor %5888, %5889 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_7170 = torch.constant.int 1
    %5891 = torch.aten.size.int %5837, %int1_7170 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int0_7171 = torch.constant.int 0
    %5892 = torch.aten.add.int %int0_7171, %5891 : !torch.int, !torch.int -> !torch.int
    %int0_7172 = torch.constant.int 0
    %int0_7173 = torch.constant.int 0
    %int1_7174 = torch.constant.int 1
    %5893 = torch.aten.slice.Tensor %5890, %int0_7172, %int0_7173, %5892, %int1_7174 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %5893, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_7175 = torch.constant.int 1
    %int0_7176 = torch.constant.int 0
    %int9223372036854775807_7177 = torch.constant.int 9223372036854775807
    %int1_7178 = torch.constant.int 1
    %5894 = torch.aten.slice.Tensor %5893, %int1_7175, %int0_7176, %int9223372036854775807_7177, %int1_7178 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %5894, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_7179 = torch.constant.int 1
    %int0_7180 = torch.constant.int 0
    %int9223372036854775807_7181 = torch.constant.int 9223372036854775807
    %int1_7182 = torch.constant.int 1
    %5895 = torch.aten.slice.Tensor %5894, %int1_7179, %int0_7180, %int9223372036854775807_7181, %int1_7182 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %5895, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_7183 = torch.constant.int 0
    %5896 = torch.aten.unsqueeze %5895, %int0_7183 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %5896, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_7184 = torch.constant.int 1
    %int0_7185 = torch.constant.int 0
    %int9223372036854775807_7186 = torch.constant.int 9223372036854775807
    %int1_7187 = torch.constant.int 1
    %5897 = torch.aten.slice.Tensor %5896, %int1_7184, %int0_7185, %int9223372036854775807_7186, %int1_7187 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %5897, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_7188 = torch.constant.int 2
    %int0_7189 = torch.constant.int 0
    %int9223372036854775807_7190 = torch.constant.int 9223372036854775807
    %int1_7191 = torch.constant.int 1
    %5898 = torch.aten.slice.Tensor %5897, %int2_7188, %int0_7189, %int9223372036854775807_7190, %int1_7191 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %5898, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_7192 = torch.constant.int 4
    %int1_7193 = torch.constant.int 1
    %int1_7194 = torch.constant.int 1
    %5899 = torch.prim.ListConstruct %int4_7192, %int1_7193, %int1_7194 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5900 = torch.aten.repeat %5898, %5899 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %5900, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_7195 = torch.constant.int 6
    %5901 = torch.prims.convert_element_type %5848, %int6_7195 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %5901, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %5902 = torch_c.to_builtin_tensor %5901 : !torch.vtensor<[4,?,8,128],f32> -> tensor<4x?x8x128xf32>
    %5903 = torch_c.to_builtin_tensor %5900 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %5904 = util.call @sharktank_rotary_embedding_4_D_8_128_f32(%5902, %5903) : (tensor<4x?x8x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x8x128xf32>
    %5905 = torch_c.from_builtin_tensor %5904 : tensor<4x?x8x128xf32> -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %5905, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %int5_7196 = torch.constant.int 5
    %5906 = torch.prims.convert_element_type %5905, %int5_7196 : !torch.vtensor<[4,?,8,128],f32>, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %5906, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int64_7197 = torch.constant.int 64
    %5907 = torch.aten.mul.Scalar %arg2, %int64_7197 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5907, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int56 = torch.constant.int 56
    %int1_7198 = torch.constant.int 1
    %5908 = torch.aten.add.Scalar %5907, %int56, %int1_7198 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5908, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_7199 = torch.constant.int 4
    %int32_7200 = torch.constant.int 32
    %int8_7201 = torch.constant.int 8
    %int128_7202 = torch.constant.int 128
    %5909 = torch.prim.ListConstruct %int4_7199, %398, %int32_7200, %int8_7201, %int128_7202 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5910 = torch.aten.view %5906, %5909 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5910, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_7203 = torch.constant.int 4
    %5911 = torch.aten.mul.int %int4_7203, %398 : !torch.int, !torch.int -> !torch.int
    %int32_7204 = torch.constant.int 32
    %int8_7205 = torch.constant.int 8
    %int128_7206 = torch.constant.int 128
    %5912 = torch.prim.ListConstruct %5911, %int32_7204, %int8_7205, %int128_7206 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5913 = torch.aten.view %5910, %5912 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %5913, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_7207 = torch.constant.int 4
    %5914 = torch.aten.mul.int %int4_7207, %398 : !torch.int, !torch.int -> !torch.int
    %5915 = torch.prim.ListConstruct %5914 : (!torch.int) -> !torch.list<int>
    %5916 = torch.aten.view %5908, %5915 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5916, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_7208 = torch.constant.int 32
    %int2_7209 = torch.constant.int 2
    %int32_7210 = torch.constant.int 32
    %int8_7211 = torch.constant.int 8
    %int128_7212 = torch.constant.int 128
    %5917 = torch.prim.ListConstruct %389, %int32_7208, %int2_7209, %int32_7210, %int8_7211, %int128_7212 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5918 = torch.aten.view %5750, %5917 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5918, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_7213 = torch.constant.int 32
    %5919 = torch.aten.mul.int %389, %int32_7213 : !torch.int, !torch.int -> !torch.int
    %int2_7214 = torch.constant.int 2
    %5920 = torch.aten.mul.int %5919, %int2_7214 : !torch.int, !torch.int -> !torch.int
    %int32_7215 = torch.constant.int 32
    %int8_7216 = torch.constant.int 8
    %int128_7217 = torch.constant.int 128
    %5921 = torch.prim.ListConstruct %5920, %int32_7215, %int8_7216, %int128_7217 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5922 = torch.aten.view %5918, %5921 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %5922, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %5923 = torch.prim.ListConstruct %5916 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_7218 = torch.constant.bool false
    %5924 = torch.aten.index_put %5922, %5923, %5913, %false_7218 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %5924, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_7219 = torch.constant.int 32
    %int2_7220 = torch.constant.int 2
    %int32_7221 = torch.constant.int 32
    %int8_7222 = torch.constant.int 8
    %int128_7223 = torch.constant.int 128
    %5925 = torch.prim.ListConstruct %389, %int32_7219, %int2_7220, %int32_7221, %int8_7222, %int128_7223 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5926 = torch.aten.view %5924, %5925 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5926, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_7224 = torch.constant.int 2097152
    %5927 = torch.prim.ListConstruct %389, %int2097152_7224 : (!torch.int, !torch.int) -> !torch.list<int>
    %5928 = torch.aten.view %5926, %5927 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %5928, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_7225 = torch.constant.int 32
    %int2_7226 = torch.constant.int 2
    %int32_7227 = torch.constant.int 32
    %int8_7228 = torch.constant.int 8
    %int128_7229 = torch.constant.int 128
    %5929 = torch.prim.ListConstruct %389, %int32_7225, %int2_7226, %int32_7227, %int8_7228, %int128_7229 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5930 = torch.aten.view %5928, %5929 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5930, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_7230 = torch.constant.int 32
    %int8_7231 = torch.constant.int 8
    %int128_7232 = torch.constant.int 128
    %5931 = torch.prim.ListConstruct %5920, %int32_7230, %int8_7231, %int128_7232 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5932 = torch.aten.view %5930, %5931 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %5932, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_7233 = torch.constant.int 4
    %int32_7234 = torch.constant.int 32
    %int8_7235 = torch.constant.int 8
    %int128_7236 = torch.constant.int 128
    %5933 = torch.prim.ListConstruct %int4_7233, %398, %int32_7234, %int8_7235, %int128_7236 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5934 = torch.aten.view %5850, %5933 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5934, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_7237 = torch.constant.int 4
    %5935 = torch.aten.mul.int %int4_7237, %398 : !torch.int, !torch.int -> !torch.int
    %int32_7238 = torch.constant.int 32
    %int8_7239 = torch.constant.int 8
    %int128_7240 = torch.constant.int 128
    %5936 = torch.prim.ListConstruct %5935, %int32_7238, %int8_7239, %int128_7240 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5937 = torch.aten.view %5934, %5936 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %5937, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int1_7241 = torch.constant.int 1
    %int1_7242 = torch.constant.int 1
    %5938 = torch.aten.add.Scalar %5908, %int1_7241, %int1_7242 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5938, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_7243 = torch.constant.int 4
    %5939 = torch.aten.mul.int %int4_7243, %398 : !torch.int, !torch.int -> !torch.int
    %5940 = torch.prim.ListConstruct %5939 : (!torch.int) -> !torch.list<int>
    %5941 = torch.aten.view %5938, %5940 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5941, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %5942 = torch.prim.ListConstruct %5941 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_7244 = torch.constant.bool false
    %5943 = torch.aten.index_put %5932, %5942, %5937, %false_7244 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %5943, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_7245 = torch.constant.int 32
    %int2_7246 = torch.constant.int 2
    %int32_7247 = torch.constant.int 32
    %int8_7248 = torch.constant.int 8
    %int128_7249 = torch.constant.int 128
    %5944 = torch.prim.ListConstruct %389, %int32_7245, %int2_7246, %int32_7247, %int8_7248, %int128_7249 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5945 = torch.aten.view %5943, %5944 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5945, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_7250 = torch.constant.int 2097152
    %5946 = torch.prim.ListConstruct %389, %int2097152_7250 : (!torch.int, !torch.int) -> !torch.list<int>
    %5947 = torch.aten.view %5945, %5946 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %5947, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int-2_7251 = torch.constant.int -2
    %5948 = torch.aten.unsqueeze %5906, %int-2_7251 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %5948, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int4_7252 = torch.constant.int 4
    %int8_7253 = torch.constant.int 8
    %int4_7254 = torch.constant.int 4
    %int128_7255 = torch.constant.int 128
    %5949 = torch.prim.ListConstruct %int4_7252, %5891, %int8_7253, %int4_7254, %int128_7255 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_7256 = torch.constant.bool false
    %5950 = torch.aten.expand %5948, %5949, %false_7256 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5950, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_7257 = torch.constant.int 0
    %5951 = torch.aten.clone %5950, %int0_7257 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5951, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_7258 = torch.constant.int 4
    %int32_7259 = torch.constant.int 32
    %int128_7260 = torch.constant.int 128
    %5952 = torch.prim.ListConstruct %int4_7258, %5891, %int32_7259, %int128_7260 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5953 = torch.aten._unsafe_view %5951, %5952 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %5953, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_7261 = torch.constant.int -2
    %5954 = torch.aten.unsqueeze %5850, %int-2_7261 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %5954, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_7262 = torch.constant.int 1
    %5955 = torch.aten.size.int %5844, %int1_7262 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int4_7263 = torch.constant.int 4
    %int8_7264 = torch.constant.int 8
    %int4_7265 = torch.constant.int 4
    %int128_7266 = torch.constant.int 128
    %5956 = torch.prim.ListConstruct %int4_7263, %5955, %int8_7264, %int4_7265, %int128_7266 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_7267 = torch.constant.bool false
    %5957 = torch.aten.expand %5954, %5956, %false_7267 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5957, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_7268 = torch.constant.int 0
    %5958 = torch.aten.clone %5957, %int0_7268 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5958, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_7269 = torch.constant.int 4
    %int32_7270 = torch.constant.int 32
    %int128_7271 = torch.constant.int 128
    %5959 = torch.prim.ListConstruct %int4_7269, %5955, %int32_7270, %int128_7271 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5960 = torch.aten._unsafe_view %5958, %5959 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %5960, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_7272 = torch.constant.int 1
    %int2_7273 = torch.constant.int 2
    %5961 = torch.aten.transpose.int %5878, %int1_7272, %int2_7273 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %5961, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_7274 = torch.constant.int 1
    %int2_7275 = torch.constant.int 2
    %5962 = torch.aten.transpose.int %5953, %int1_7274, %int2_7275 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %5962, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_7276 = torch.constant.int 1
    %int2_7277 = torch.constant.int 2
    %5963 = torch.aten.transpose.int %5960, %int1_7276, %int2_7277 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %5963, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_7278 = torch.constant.float 0.000000e+00
    %true_7279 = torch.constant.bool true
    %none_7280 = torch.constant.none
    %none_7281 = torch.constant.none
    %5964:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5961, %5962, %5963, %float0.000000e00_7278, %true_7279, %none_7280, %none_7281) : (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?],f32>) 
    torch.bind_symbolic_shape %5964#0, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_7282 = torch.constant.int 1
    %int2_7283 = torch.constant.int 2
    %5965 = torch.aten.transpose.int %5964#0, %int1_7282, %int2_7283 : !torch.vtensor<[4,32,?,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %5965, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_7284 = torch.constant.int 4
    %int4096_7285 = torch.constant.int 4096
    %5966 = torch.prim.ListConstruct %int4_7284, %5863, %int4096_7285 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5967 = torch.aten.view %5965, %5966 : !torch.vtensor<[4,?,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5967, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_7286 = torch.constant.int -2
    %int-1_7287 = torch.constant.int -1
    %5968 = torch.aten.transpose.int %257, %int-2_7286, %int-1_7287 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_7288 = torch.constant.int 4
    %5969 = torch.aten.mul.int %int4_7288, %5863 : !torch.int, !torch.int -> !torch.int
    %int4096_7289 = torch.constant.int 4096
    %5970 = torch.prim.ListConstruct %5969, %int4096_7289 : (!torch.int, !torch.int) -> !torch.list<int>
    %5971 = torch.aten.view %5967, %5970 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5971, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %5972 = torch.aten.mm %5971, %5968 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5972, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_7290 = torch.constant.int 4
    %int4096_7291 = torch.constant.int 4096
    %5973 = torch.prim.ListConstruct %int4_7290, %5863, %int4096_7291 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5974 = torch.aten.view %5972, %5973 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5974, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_7292 = torch.constant.int 1
    %5975 = torch.aten.add.Tensor %5813, %5974, %int1_7292 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5975, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_7293 = torch.constant.int 6
    %5976 = torch.prims.convert_element_type %5975, %int6_7293 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5976, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_7294 = torch.constant.int 2
    %5977 = torch.aten.pow.Tensor_Scalar %5976, %int2_7294 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5977, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_7295 = torch.constant.int -1
    %5978 = torch.prim.ListConstruct %int-1_7295 : (!torch.int) -> !torch.list<int>
    %true_7296 = torch.constant.bool true
    %none_7297 = torch.constant.none
    %5979 = torch.aten.mean.dim %5977, %5978, %true_7296, %none_7297 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5979, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_7298 = torch.constant.float 9.9999997473787516E-6
    %int1_7299 = torch.constant.int 1
    %5980 = torch.aten.add.Scalar %5979, %float9.999990e-06_7298, %int1_7299 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5980, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %5981 = torch.aten.rsqrt %5980 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %5981, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %5982 = torch.aten.mul.Tensor %5976, %5981 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5982, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_7300 = torch.constant.int 5
    %5983 = torch.prims.convert_element_type %5982, %int5_7300 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5983, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %5984 = torch.aten.mul.Tensor %258, %5983 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %5984, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_7301 = torch.constant.int 5
    %5985 = torch.prims.convert_element_type %5984, %int5_7301 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %5985, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_7302 = torch.constant.int -2
    %int-1_7303 = torch.constant.int -1
    %5986 = torch.aten.transpose.int %259, %int-2_7302, %int-1_7303 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_7304 = torch.constant.int 4
    %5987 = torch.aten.mul.int %int4_7304, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_7305 = torch.constant.int 4096
    %5988 = torch.prim.ListConstruct %5987, %int4096_7305 : (!torch.int, !torch.int) -> !torch.list<int>
    %5989 = torch.aten.view %5985, %5988 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5989, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %5990 = torch.aten.mm %5989, %5986 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %5990, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_7306 = torch.constant.int 4
    %int14336_7307 = torch.constant.int 14336
    %5991 = torch.prim.ListConstruct %int4_7306, %306, %int14336_7307 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5992 = torch.aten.view %5990, %5991 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %5992, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %5993 = torch.aten.silu %5992 : !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %5993, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_7308 = torch.constant.int -2
    %int-1_7309 = torch.constant.int -1
    %5994 = torch.aten.transpose.int %260, %int-2_7308, %int-1_7309 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_7310 = torch.constant.int 4
    %5995 = torch.aten.mul.int %int4_7310, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_7311 = torch.constant.int 4096
    %5996 = torch.prim.ListConstruct %5995, %int4096_7311 : (!torch.int, !torch.int) -> !torch.list<int>
    %5997 = torch.aten.view %5985, %5996 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %5997, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %5998 = torch.aten.mm %5997, %5994 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %5998, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_7312 = torch.constant.int 4
    %int14336_7313 = torch.constant.int 14336
    %5999 = torch.prim.ListConstruct %int4_7312, %306, %int14336_7313 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6000 = torch.aten.view %5998, %5999 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %6000, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %6001 = torch.aten.mul.Tensor %5993, %6000 : !torch.vtensor<[4,?,14336],f16>, !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %6001, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_7314 = torch.constant.int -2
    %int-1_7315 = torch.constant.int -1
    %6002 = torch.aten.transpose.int %261, %int-2_7314, %int-1_7315 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int1_7316 = torch.constant.int 1
    %6003 = torch.aten.size.int %5992, %int1_7316 : !torch.vtensor<[4,?,14336],f16>, !torch.int -> !torch.int
    %int4_7317 = torch.constant.int 4
    %6004 = torch.aten.mul.int %int4_7317, %6003 : !torch.int, !torch.int -> !torch.int
    %int14336_7318 = torch.constant.int 14336
    %6005 = torch.prim.ListConstruct %6004, %int14336_7318 : (!torch.int, !torch.int) -> !torch.list<int>
    %6006 = torch.aten.view %6001, %6005 : !torch.vtensor<[4,?,14336],f16>, !torch.list<int> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %6006, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %6007 = torch.aten.mm %6006, %6002 : !torch.vtensor<[?,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %6007, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_7319 = torch.constant.int 4
    %int4096_7320 = torch.constant.int 4096
    %6008 = torch.prim.ListConstruct %int4_7319, %6003, %int4096_7320 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6009 = torch.aten.view %6007, %6008 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %6009, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_7321 = torch.constant.int 1
    %6010 = torch.aten.add.Tensor %5975, %6009, %int1_7321 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %6010, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_7322 = torch.constant.int 6
    %6011 = torch.prims.convert_element_type %6010, %int6_7322 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %6011, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_7323 = torch.constant.int 2
    %6012 = torch.aten.pow.Tensor_Scalar %6011, %int2_7323 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %6012, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_7324 = torch.constant.int -1
    %6013 = torch.prim.ListConstruct %int-1_7324 : (!torch.int) -> !torch.list<int>
    %true_7325 = torch.constant.bool true
    %none_7326 = torch.constant.none
    %6014 = torch.aten.mean.dim %6012, %6013, %true_7325, %none_7326 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6014, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_7327 = torch.constant.float 9.9999997473787516E-6
    %int1_7328 = torch.constant.int 1
    %6015 = torch.aten.add.Scalar %6014, %float9.999990e-06_7327, %int1_7328 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6015, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %6016 = torch.aten.rsqrt %6015 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6016, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %6017 = torch.aten.mul.Tensor %6011, %6016 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %6017, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_7329 = torch.constant.int 5
    %6018 = torch.prims.convert_element_type %6017, %int5_7329 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %6018, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %6019 = torch.aten.mul.Tensor %262, %6018 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %6019, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_7330 = torch.constant.int 5
    %6020 = torch.prims.convert_element_type %6019, %int5_7330 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %6020, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_7331 = torch.constant.int -2
    %int-1_7332 = torch.constant.int -1
    %6021 = torch.aten.transpose.int %263, %int-2_7331, %int-1_7332 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_7333 = torch.constant.int 4
    %6022 = torch.aten.mul.int %int4_7333, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_7334 = torch.constant.int 4096
    %6023 = torch.prim.ListConstruct %6022, %int4096_7334 : (!torch.int, !torch.int) -> !torch.list<int>
    %6024 = torch.aten.view %6020, %6023 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %6024, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %6025 = torch.aten.mm %6024, %6021 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %6025, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_7335 = torch.constant.int 4
    %int4096_7336 = torch.constant.int 4096
    %6026 = torch.prim.ListConstruct %int4_7335, %306, %int4096_7336 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6027 = torch.aten.view %6025, %6026 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %6027, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_7337 = torch.constant.int -2
    %int-1_7338 = torch.constant.int -1
    %6028 = torch.aten.transpose.int %264, %int-2_7337, %int-1_7338 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_7339 = torch.constant.int 4
    %6029 = torch.aten.mul.int %int4_7339, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_7340 = torch.constant.int 4096
    %6030 = torch.prim.ListConstruct %6029, %int4096_7340 : (!torch.int, !torch.int) -> !torch.list<int>
    %6031 = torch.aten.view %6020, %6030 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %6031, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %6032 = torch.aten.mm %6031, %6028 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %6032, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_7341 = torch.constant.int 4
    %int1024_7342 = torch.constant.int 1024
    %6033 = torch.prim.ListConstruct %int4_7341, %306, %int1024_7342 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6034 = torch.aten.view %6032, %6033 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %6034, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int-2_7343 = torch.constant.int -2
    %int-1_7344 = torch.constant.int -1
    %6035 = torch.aten.transpose.int %265, %int-2_7343, %int-1_7344 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_7345 = torch.constant.int 4
    %6036 = torch.aten.mul.int %int4_7345, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_7346 = torch.constant.int 4096
    %6037 = torch.prim.ListConstruct %6036, %int4096_7346 : (!torch.int, !torch.int) -> !torch.list<int>
    %6038 = torch.aten.view %6020, %6037 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %6038, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %6039 = torch.aten.mm %6038, %6035 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %6039, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_7347 = torch.constant.int 4
    %int1024_7348 = torch.constant.int 1024
    %6040 = torch.prim.ListConstruct %int4_7347, %306, %int1024_7348 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6041 = torch.aten.view %6039, %6040 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %6041, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int4_7349 = torch.constant.int 4
    %int32_7350 = torch.constant.int 32
    %int128_7351 = torch.constant.int 128
    %6042 = torch.prim.ListConstruct %int4_7349, %306, %int32_7350, %int128_7351 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6043 = torch.aten.view %6027, %6042 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %6043, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_7352 = torch.constant.int 4
    %int8_7353 = torch.constant.int 8
    %int128_7354 = torch.constant.int 128
    %6044 = torch.prim.ListConstruct %int4_7352, %306, %int8_7353, %int128_7354 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6045 = torch.aten.view %6034, %6044 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %6045, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int4_7355 = torch.constant.int 4
    %int8_7356 = torch.constant.int 8
    %int128_7357 = torch.constant.int 128
    %6046 = torch.prim.ListConstruct %int4_7355, %306, %int8_7356, %int128_7357 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6047 = torch.aten.view %6041, %6046 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %6047, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int131072_7358 = torch.constant.int 131072
    %none_7359 = torch.constant.none
    %none_7360 = torch.constant.none
    %cpu_7361 = torch.constant.device "cpu"
    %false_7362 = torch.constant.bool false
    %6048 = torch.aten.arange %int131072_7358, %none_7359, %none_7360, %cpu_7361, %false_7362 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_7363 = torch.constant.int 0
    %int128_7364 = torch.constant.int 128
    %none_7365 = torch.constant.none
    %none_7366 = torch.constant.none
    %cpu_7367 = torch.constant.device "cpu"
    %false_7368 = torch.constant.bool false
    %6049 = torch.aten.arange.start %int0_7363, %int128_7364, %none_7365, %none_7366, %cpu_7367, %false_7368 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_7369 = torch.constant.int 2
    %6050 = torch.aten.floor_divide.Scalar %6049, %int2_7369 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_7370 = torch.constant.int 6
    %6051 = torch.prims.convert_element_type %6050, %int6_7370 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_7371 = torch.constant.int 128
    %6052 = torch.aten.div.Scalar %6051, %int128_7371 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_7372 = torch.constant.float 2.000000e+00
    %6053 = torch.aten.mul.Scalar %6052, %float2.000000e00_7372 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_7373 = torch.constant.float 5.000000e+05
    %6054 = torch.aten.pow.Scalar %float5.000000e05_7373, %6053 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %6055 = torch.aten.reciprocal %6054 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_7374 = torch.constant.float 1.000000e+00
    %6056 = torch.aten.mul.Scalar %6055, %float1.000000e00_7374 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_7375 = torch.constant.int 1
    %6057 = torch.aten.unsqueeze %6048, %int1_7375 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_7376 = torch.constant.int 0
    %6058 = torch.aten.unsqueeze %6056, %int0_7376 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %6059 = torch.aten.mul.Tensor %6057, %6058 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_7377 = torch.constant.int 1
    %6060 = torch.aten.size.int %6027, %int1_7377 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.int
    %int0_7378 = torch.constant.int 0
    %6061 = torch.aten.add.int %int0_7378, %6060 : !torch.int, !torch.int -> !torch.int
    %int0_7379 = torch.constant.int 0
    %int0_7380 = torch.constant.int 0
    %int1_7381 = torch.constant.int 1
    %6062 = torch.aten.slice.Tensor %6059, %int0_7379, %int0_7380, %6061, %int1_7381 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %6062, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_7382 = torch.constant.int 1
    %int0_7383 = torch.constant.int 0
    %int9223372036854775807_7384 = torch.constant.int 9223372036854775807
    %int1_7385 = torch.constant.int 1
    %6063 = torch.aten.slice.Tensor %6062, %int1_7382, %int0_7383, %int9223372036854775807_7384, %int1_7385 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %6063, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_7386 = torch.constant.int 1
    %int0_7387 = torch.constant.int 0
    %int9223372036854775807_7388 = torch.constant.int 9223372036854775807
    %int1_7389 = torch.constant.int 1
    %6064 = torch.aten.slice.Tensor %6063, %int1_7386, %int0_7387, %int9223372036854775807_7388, %int1_7389 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %6064, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_7390 = torch.constant.int 0
    %6065 = torch.aten.unsqueeze %6064, %int0_7390 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %6065, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_7391 = torch.constant.int 1
    %int0_7392 = torch.constant.int 0
    %int9223372036854775807_7393 = torch.constant.int 9223372036854775807
    %int1_7394 = torch.constant.int 1
    %6066 = torch.aten.slice.Tensor %6065, %int1_7391, %int0_7392, %int9223372036854775807_7393, %int1_7394 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %6066, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_7395 = torch.constant.int 2
    %int0_7396 = torch.constant.int 0
    %int9223372036854775807_7397 = torch.constant.int 9223372036854775807
    %int1_7398 = torch.constant.int 1
    %6067 = torch.aten.slice.Tensor %6066, %int2_7395, %int0_7396, %int9223372036854775807_7397, %int1_7398 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %6067, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_7399 = torch.constant.int 4
    %int1_7400 = torch.constant.int 1
    %int1_7401 = torch.constant.int 1
    %6068 = torch.prim.ListConstruct %int4_7399, %int1_7400, %int1_7401 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6069 = torch.aten.repeat %6067, %6068 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %6069, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_7402 = torch.constant.int 6
    %6070 = torch.prims.convert_element_type %6043, %int6_7402 : !torch.vtensor<[4,?,32,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %6070, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %6071 = torch_c.to_builtin_tensor %6070 : !torch.vtensor<[4,?,32,128],f32> -> tensor<4x?x32x128xf32>
    %6072 = torch_c.to_builtin_tensor %6069 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %6073 = util.call @sharktank_rotary_embedding_4_D_32_128_f32(%6071, %6072) : (tensor<4x?x32x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x32x128xf32>
    %6074 = torch_c.from_builtin_tensor %6073 : tensor<4x?x32x128xf32> -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %6074, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %int5_7403 = torch.constant.int 5
    %6075 = torch.prims.convert_element_type %6074, %int5_7403 : !torch.vtensor<[4,?,32,128],f32>, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %6075, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int131072_7404 = torch.constant.int 131072
    %none_7405 = torch.constant.none
    %none_7406 = torch.constant.none
    %cpu_7407 = torch.constant.device "cpu"
    %false_7408 = torch.constant.bool false
    %6076 = torch.aten.arange %int131072_7404, %none_7405, %none_7406, %cpu_7407, %false_7408 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_7409 = torch.constant.int 0
    %int128_7410 = torch.constant.int 128
    %none_7411 = torch.constant.none
    %none_7412 = torch.constant.none
    %cpu_7413 = torch.constant.device "cpu"
    %false_7414 = torch.constant.bool false
    %6077 = torch.aten.arange.start %int0_7409, %int128_7410, %none_7411, %none_7412, %cpu_7413, %false_7414 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_7415 = torch.constant.int 2
    %6078 = torch.aten.floor_divide.Scalar %6077, %int2_7415 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_7416 = torch.constant.int 6
    %6079 = torch.prims.convert_element_type %6078, %int6_7416 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_7417 = torch.constant.int 128
    %6080 = torch.aten.div.Scalar %6079, %int128_7417 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_7418 = torch.constant.float 2.000000e+00
    %6081 = torch.aten.mul.Scalar %6080, %float2.000000e00_7418 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_7419 = torch.constant.float 5.000000e+05
    %6082 = torch.aten.pow.Scalar %float5.000000e05_7419, %6081 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %6083 = torch.aten.reciprocal %6082 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_7420 = torch.constant.float 1.000000e+00
    %6084 = torch.aten.mul.Scalar %6083, %float1.000000e00_7420 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_7421 = torch.constant.int 1
    %6085 = torch.aten.unsqueeze %6076, %int1_7421 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_7422 = torch.constant.int 0
    %6086 = torch.aten.unsqueeze %6084, %int0_7422 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %6087 = torch.aten.mul.Tensor %6085, %6086 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_7423 = torch.constant.int 1
    %6088 = torch.aten.size.int %6034, %int1_7423 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int0_7424 = torch.constant.int 0
    %6089 = torch.aten.add.int %int0_7424, %6088 : !torch.int, !torch.int -> !torch.int
    %int0_7425 = torch.constant.int 0
    %int0_7426 = torch.constant.int 0
    %int1_7427 = torch.constant.int 1
    %6090 = torch.aten.slice.Tensor %6087, %int0_7425, %int0_7426, %6089, %int1_7427 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %6090, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_7428 = torch.constant.int 1
    %int0_7429 = torch.constant.int 0
    %int9223372036854775807_7430 = torch.constant.int 9223372036854775807
    %int1_7431 = torch.constant.int 1
    %6091 = torch.aten.slice.Tensor %6090, %int1_7428, %int0_7429, %int9223372036854775807_7430, %int1_7431 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %6091, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_7432 = torch.constant.int 1
    %int0_7433 = torch.constant.int 0
    %int9223372036854775807_7434 = torch.constant.int 9223372036854775807
    %int1_7435 = torch.constant.int 1
    %6092 = torch.aten.slice.Tensor %6091, %int1_7432, %int0_7433, %int9223372036854775807_7434, %int1_7435 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %6092, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_7436 = torch.constant.int 0
    %6093 = torch.aten.unsqueeze %6092, %int0_7436 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %6093, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_7437 = torch.constant.int 1
    %int0_7438 = torch.constant.int 0
    %int9223372036854775807_7439 = torch.constant.int 9223372036854775807
    %int1_7440 = torch.constant.int 1
    %6094 = torch.aten.slice.Tensor %6093, %int1_7437, %int0_7438, %int9223372036854775807_7439, %int1_7440 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %6094, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_7441 = torch.constant.int 2
    %int0_7442 = torch.constant.int 0
    %int9223372036854775807_7443 = torch.constant.int 9223372036854775807
    %int1_7444 = torch.constant.int 1
    %6095 = torch.aten.slice.Tensor %6094, %int2_7441, %int0_7442, %int9223372036854775807_7443, %int1_7444 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %6095, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_7445 = torch.constant.int 4
    %int1_7446 = torch.constant.int 1
    %int1_7447 = torch.constant.int 1
    %6096 = torch.prim.ListConstruct %int4_7445, %int1_7446, %int1_7447 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6097 = torch.aten.repeat %6095, %6096 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %6097, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_7448 = torch.constant.int 6
    %6098 = torch.prims.convert_element_type %6045, %int6_7448 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %6098, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %6099 = torch_c.to_builtin_tensor %6098 : !torch.vtensor<[4,?,8,128],f32> -> tensor<4x?x8x128xf32>
    %6100 = torch_c.to_builtin_tensor %6097 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %6101 = util.call @sharktank_rotary_embedding_4_D_8_128_f32(%6099, %6100) : (tensor<4x?x8x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x8x128xf32>
    %6102 = torch_c.from_builtin_tensor %6101 : tensor<4x?x8x128xf32> -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %6102, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %int5_7449 = torch.constant.int 5
    %6103 = torch.prims.convert_element_type %6102, %int5_7449 : !torch.vtensor<[4,?,8,128],f32>, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %6103, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int64_7450 = torch.constant.int 64
    %6104 = torch.aten.mul.Scalar %arg2, %int64_7450 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6104, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int58 = torch.constant.int 58
    %int1_7451 = torch.constant.int 1
    %6105 = torch.aten.add.Scalar %6104, %int58, %int1_7451 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6105, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_7452 = torch.constant.int 4
    %int32_7453 = torch.constant.int 32
    %int8_7454 = torch.constant.int 8
    %int128_7455 = torch.constant.int 128
    %6106 = torch.prim.ListConstruct %int4_7452, %398, %int32_7453, %int8_7454, %int128_7455 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6107 = torch.aten.view %6103, %6106 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6107, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_7456 = torch.constant.int 4
    %6108 = torch.aten.mul.int %int4_7456, %398 : !torch.int, !torch.int -> !torch.int
    %int32_7457 = torch.constant.int 32
    %int8_7458 = torch.constant.int 8
    %int128_7459 = torch.constant.int 128
    %6109 = torch.prim.ListConstruct %6108, %int32_7457, %int8_7458, %int128_7459 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6110 = torch.aten.view %6107, %6109 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %6110, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_7460 = torch.constant.int 4
    %6111 = torch.aten.mul.int %int4_7460, %398 : !torch.int, !torch.int -> !torch.int
    %6112 = torch.prim.ListConstruct %6111 : (!torch.int) -> !torch.list<int>
    %6113 = torch.aten.view %6105, %6112 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %6113, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_7461 = torch.constant.int 32
    %int2_7462 = torch.constant.int 2
    %int32_7463 = torch.constant.int 32
    %int8_7464 = torch.constant.int 8
    %int128_7465 = torch.constant.int 128
    %6114 = torch.prim.ListConstruct %389, %int32_7461, %int2_7462, %int32_7463, %int8_7464, %int128_7465 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6115 = torch.aten.view %5947, %6114 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %6115, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_7466 = torch.constant.int 32
    %6116 = torch.aten.mul.int %389, %int32_7466 : !torch.int, !torch.int -> !torch.int
    %int2_7467 = torch.constant.int 2
    %6117 = torch.aten.mul.int %6116, %int2_7467 : !torch.int, !torch.int -> !torch.int
    %int32_7468 = torch.constant.int 32
    %int8_7469 = torch.constant.int 8
    %int128_7470 = torch.constant.int 128
    %6118 = torch.prim.ListConstruct %6117, %int32_7468, %int8_7469, %int128_7470 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6119 = torch.aten.view %6115, %6118 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %6119, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %6120 = torch.prim.ListConstruct %6113 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_7471 = torch.constant.bool false
    %6121 = torch.aten.index_put %6119, %6120, %6110, %false_7471 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %6121, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_7472 = torch.constant.int 32
    %int2_7473 = torch.constant.int 2
    %int32_7474 = torch.constant.int 32
    %int8_7475 = torch.constant.int 8
    %int128_7476 = torch.constant.int 128
    %6122 = torch.prim.ListConstruct %389, %int32_7472, %int2_7473, %int32_7474, %int8_7475, %int128_7476 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6123 = torch.aten.view %6121, %6122 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %6123, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_7477 = torch.constant.int 2097152
    %6124 = torch.prim.ListConstruct %389, %int2097152_7477 : (!torch.int, !torch.int) -> !torch.list<int>
    %6125 = torch.aten.view %6123, %6124 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %6125, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_7478 = torch.constant.int 32
    %int2_7479 = torch.constant.int 2
    %int32_7480 = torch.constant.int 32
    %int8_7481 = torch.constant.int 8
    %int128_7482 = torch.constant.int 128
    %6126 = torch.prim.ListConstruct %389, %int32_7478, %int2_7479, %int32_7480, %int8_7481, %int128_7482 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6127 = torch.aten.view %6125, %6126 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %6127, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_7483 = torch.constant.int 32
    %int8_7484 = torch.constant.int 8
    %int128_7485 = torch.constant.int 128
    %6128 = torch.prim.ListConstruct %6117, %int32_7483, %int8_7484, %int128_7485 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6129 = torch.aten.view %6127, %6128 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %6129, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_7486 = torch.constant.int 4
    %int32_7487 = torch.constant.int 32
    %int8_7488 = torch.constant.int 8
    %int128_7489 = torch.constant.int 128
    %6130 = torch.prim.ListConstruct %int4_7486, %398, %int32_7487, %int8_7488, %int128_7489 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6131 = torch.aten.view %6047, %6130 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6131, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_7490 = torch.constant.int 4
    %6132 = torch.aten.mul.int %int4_7490, %398 : !torch.int, !torch.int -> !torch.int
    %int32_7491 = torch.constant.int 32
    %int8_7492 = torch.constant.int 8
    %int128_7493 = torch.constant.int 128
    %6133 = torch.prim.ListConstruct %6132, %int32_7491, %int8_7492, %int128_7493 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6134 = torch.aten.view %6131, %6133 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %6134, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int1_7494 = torch.constant.int 1
    %int1_7495 = torch.constant.int 1
    %6135 = torch.aten.add.Scalar %6105, %int1_7494, %int1_7495 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6135, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_7496 = torch.constant.int 4
    %6136 = torch.aten.mul.int %int4_7496, %398 : !torch.int, !torch.int -> !torch.int
    %6137 = torch.prim.ListConstruct %6136 : (!torch.int) -> !torch.list<int>
    %6138 = torch.aten.view %6135, %6137 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %6138, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %6139 = torch.prim.ListConstruct %6138 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_7497 = torch.constant.bool false
    %6140 = torch.aten.index_put %6129, %6139, %6134, %false_7497 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %6140, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_7498 = torch.constant.int 32
    %int2_7499 = torch.constant.int 2
    %int32_7500 = torch.constant.int 32
    %int8_7501 = torch.constant.int 8
    %int128_7502 = torch.constant.int 128
    %6141 = torch.prim.ListConstruct %389, %int32_7498, %int2_7499, %int32_7500, %int8_7501, %int128_7502 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6142 = torch.aten.view %6140, %6141 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %6142, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_7503 = torch.constant.int 2097152
    %6143 = torch.prim.ListConstruct %389, %int2097152_7503 : (!torch.int, !torch.int) -> !torch.list<int>
    %6144 = torch.aten.view %6142, %6143 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %6144, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int-2_7504 = torch.constant.int -2
    %6145 = torch.aten.unsqueeze %6103, %int-2_7504 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %6145, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int4_7505 = torch.constant.int 4
    %int8_7506 = torch.constant.int 8
    %int4_7507 = torch.constant.int 4
    %int128_7508 = torch.constant.int 128
    %6146 = torch.prim.ListConstruct %int4_7505, %6088, %int8_7506, %int4_7507, %int128_7508 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_7509 = torch.constant.bool false
    %6147 = torch.aten.expand %6145, %6146, %false_7509 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %6147, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_7510 = torch.constant.int 0
    %6148 = torch.aten.clone %6147, %int0_7510 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %6148, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_7511 = torch.constant.int 4
    %int32_7512 = torch.constant.int 32
    %int128_7513 = torch.constant.int 128
    %6149 = torch.prim.ListConstruct %int4_7511, %6088, %int32_7512, %int128_7513 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6150 = torch.aten._unsafe_view %6148, %6149 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %6150, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_7514 = torch.constant.int -2
    %6151 = torch.aten.unsqueeze %6047, %int-2_7514 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %6151, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_7515 = torch.constant.int 1
    %6152 = torch.aten.size.int %6041, %int1_7515 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int4_7516 = torch.constant.int 4
    %int8_7517 = torch.constant.int 8
    %int4_7518 = torch.constant.int 4
    %int128_7519 = torch.constant.int 128
    %6153 = torch.prim.ListConstruct %int4_7516, %6152, %int8_7517, %int4_7518, %int128_7519 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_7520 = torch.constant.bool false
    %6154 = torch.aten.expand %6151, %6153, %false_7520 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %6154, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_7521 = torch.constant.int 0
    %6155 = torch.aten.clone %6154, %int0_7521 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %6155, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_7522 = torch.constant.int 4
    %int32_7523 = torch.constant.int 32
    %int128_7524 = torch.constant.int 128
    %6156 = torch.prim.ListConstruct %int4_7522, %6152, %int32_7523, %int128_7524 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6157 = torch.aten._unsafe_view %6155, %6156 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %6157, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_7525 = torch.constant.int 1
    %int2_7526 = torch.constant.int 2
    %6158 = torch.aten.transpose.int %6075, %int1_7525, %int2_7526 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %6158, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_7527 = torch.constant.int 1
    %int2_7528 = torch.constant.int 2
    %6159 = torch.aten.transpose.int %6150, %int1_7527, %int2_7528 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %6159, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_7529 = torch.constant.int 1
    %int2_7530 = torch.constant.int 2
    %6160 = torch.aten.transpose.int %6157, %int1_7529, %int2_7530 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %6160, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_7531 = torch.constant.float 0.000000e+00
    %true_7532 = torch.constant.bool true
    %none_7533 = torch.constant.none
    %none_7534 = torch.constant.none
    %6161:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6158, %6159, %6160, %float0.000000e00_7531, %true_7532, %none_7533, %none_7534) : (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?],f32>) 
    torch.bind_symbolic_shape %6161#0, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_7535 = torch.constant.int 1
    %int2_7536 = torch.constant.int 2
    %6162 = torch.aten.transpose.int %6161#0, %int1_7535, %int2_7536 : !torch.vtensor<[4,32,?,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %6162, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_7537 = torch.constant.int 4
    %int4096_7538 = torch.constant.int 4096
    %6163 = torch.prim.ListConstruct %int4_7537, %6060, %int4096_7538 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6164 = torch.aten.view %6162, %6163 : !torch.vtensor<[4,?,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %6164, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_7539 = torch.constant.int -2
    %int-1_7540 = torch.constant.int -1
    %6165 = torch.aten.transpose.int %266, %int-2_7539, %int-1_7540 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_7541 = torch.constant.int 4
    %6166 = torch.aten.mul.int %int4_7541, %6060 : !torch.int, !torch.int -> !torch.int
    %int4096_7542 = torch.constant.int 4096
    %6167 = torch.prim.ListConstruct %6166, %int4096_7542 : (!torch.int, !torch.int) -> !torch.list<int>
    %6168 = torch.aten.view %6164, %6167 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %6168, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %6169 = torch.aten.mm %6168, %6165 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %6169, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_7543 = torch.constant.int 4
    %int4096_7544 = torch.constant.int 4096
    %6170 = torch.prim.ListConstruct %int4_7543, %6060, %int4096_7544 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6171 = torch.aten.view %6169, %6170 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %6171, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_7545 = torch.constant.int 1
    %6172 = torch.aten.add.Tensor %6010, %6171, %int1_7545 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %6172, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_7546 = torch.constant.int 6
    %6173 = torch.prims.convert_element_type %6172, %int6_7546 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %6173, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_7547 = torch.constant.int 2
    %6174 = torch.aten.pow.Tensor_Scalar %6173, %int2_7547 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %6174, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_7548 = torch.constant.int -1
    %6175 = torch.prim.ListConstruct %int-1_7548 : (!torch.int) -> !torch.list<int>
    %true_7549 = torch.constant.bool true
    %none_7550 = torch.constant.none
    %6176 = torch.aten.mean.dim %6174, %6175, %true_7549, %none_7550 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6176, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_7551 = torch.constant.float 9.9999997473787516E-6
    %int1_7552 = torch.constant.int 1
    %6177 = torch.aten.add.Scalar %6176, %float9.999990e-06_7551, %int1_7552 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6177, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %6178 = torch.aten.rsqrt %6177 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6178, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %6179 = torch.aten.mul.Tensor %6173, %6178 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %6179, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_7553 = torch.constant.int 5
    %6180 = torch.prims.convert_element_type %6179, %int5_7553 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %6180, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %6181 = torch.aten.mul.Tensor %267, %6180 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %6181, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_7554 = torch.constant.int 5
    %6182 = torch.prims.convert_element_type %6181, %int5_7554 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %6182, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_7555 = torch.constant.int -2
    %int-1_7556 = torch.constant.int -1
    %6183 = torch.aten.transpose.int %268, %int-2_7555, %int-1_7556 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_7557 = torch.constant.int 4
    %6184 = torch.aten.mul.int %int4_7557, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_7558 = torch.constant.int 4096
    %6185 = torch.prim.ListConstruct %6184, %int4096_7558 : (!torch.int, !torch.int) -> !torch.list<int>
    %6186 = torch.aten.view %6182, %6185 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %6186, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %6187 = torch.aten.mm %6186, %6183 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %6187, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_7559 = torch.constant.int 4
    %int14336_7560 = torch.constant.int 14336
    %6188 = torch.prim.ListConstruct %int4_7559, %306, %int14336_7560 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6189 = torch.aten.view %6187, %6188 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %6189, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %6190 = torch.aten.silu %6189 : !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %6190, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_7561 = torch.constant.int -2
    %int-1_7562 = torch.constant.int -1
    %6191 = torch.aten.transpose.int %269, %int-2_7561, %int-1_7562 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_7563 = torch.constant.int 4
    %6192 = torch.aten.mul.int %int4_7563, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_7564 = torch.constant.int 4096
    %6193 = torch.prim.ListConstruct %6192, %int4096_7564 : (!torch.int, !torch.int) -> !torch.list<int>
    %6194 = torch.aten.view %6182, %6193 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %6194, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %6195 = torch.aten.mm %6194, %6191 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %6195, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_7565 = torch.constant.int 4
    %int14336_7566 = torch.constant.int 14336
    %6196 = torch.prim.ListConstruct %int4_7565, %306, %int14336_7566 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6197 = torch.aten.view %6195, %6196 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %6197, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %6198 = torch.aten.mul.Tensor %6190, %6197 : !torch.vtensor<[4,?,14336],f16>, !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %6198, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_7567 = torch.constant.int -2
    %int-1_7568 = torch.constant.int -1
    %6199 = torch.aten.transpose.int %270, %int-2_7567, %int-1_7568 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int1_7569 = torch.constant.int 1
    %6200 = torch.aten.size.int %6189, %int1_7569 : !torch.vtensor<[4,?,14336],f16>, !torch.int -> !torch.int
    %int4_7570 = torch.constant.int 4
    %6201 = torch.aten.mul.int %int4_7570, %6200 : !torch.int, !torch.int -> !torch.int
    %int14336_7571 = torch.constant.int 14336
    %6202 = torch.prim.ListConstruct %6201, %int14336_7571 : (!torch.int, !torch.int) -> !torch.list<int>
    %6203 = torch.aten.view %6198, %6202 : !torch.vtensor<[4,?,14336],f16>, !torch.list<int> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %6203, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %6204 = torch.aten.mm %6203, %6199 : !torch.vtensor<[?,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %6204, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_7572 = torch.constant.int 4
    %int4096_7573 = torch.constant.int 4096
    %6205 = torch.prim.ListConstruct %int4_7572, %6200, %int4096_7573 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6206 = torch.aten.view %6204, %6205 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %6206, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_7574 = torch.constant.int 1
    %6207 = torch.aten.add.Tensor %6172, %6206, %int1_7574 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %6207, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_7575 = torch.constant.int 6
    %6208 = torch.prims.convert_element_type %6207, %int6_7575 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %6208, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_7576 = torch.constant.int 2
    %6209 = torch.aten.pow.Tensor_Scalar %6208, %int2_7576 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %6209, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_7577 = torch.constant.int -1
    %6210 = torch.prim.ListConstruct %int-1_7577 : (!torch.int) -> !torch.list<int>
    %true_7578 = torch.constant.bool true
    %none_7579 = torch.constant.none
    %6211 = torch.aten.mean.dim %6209, %6210, %true_7578, %none_7579 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6211, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_7580 = torch.constant.float 9.9999997473787516E-6
    %int1_7581 = torch.constant.int 1
    %6212 = torch.aten.add.Scalar %6211, %float9.999990e-06_7580, %int1_7581 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6212, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %6213 = torch.aten.rsqrt %6212 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6213, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %6214 = torch.aten.mul.Tensor %6208, %6213 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %6214, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_7582 = torch.constant.int 5
    %6215 = torch.prims.convert_element_type %6214, %int5_7582 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %6215, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %6216 = torch.aten.mul.Tensor %271, %6215 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %6216, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_7583 = torch.constant.int 5
    %6217 = torch.prims.convert_element_type %6216, %int5_7583 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %6217, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_7584 = torch.constant.int -2
    %int-1_7585 = torch.constant.int -1
    %6218 = torch.aten.transpose.int %272, %int-2_7584, %int-1_7585 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_7586 = torch.constant.int 4
    %6219 = torch.aten.mul.int %int4_7586, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_7587 = torch.constant.int 4096
    %6220 = torch.prim.ListConstruct %6219, %int4096_7587 : (!torch.int, !torch.int) -> !torch.list<int>
    %6221 = torch.aten.view %6217, %6220 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %6221, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %6222 = torch.aten.mm %6221, %6218 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %6222, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_7588 = torch.constant.int 4
    %int4096_7589 = torch.constant.int 4096
    %6223 = torch.prim.ListConstruct %int4_7588, %306, %int4096_7589 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6224 = torch.aten.view %6222, %6223 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %6224, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_7590 = torch.constant.int -2
    %int-1_7591 = torch.constant.int -1
    %6225 = torch.aten.transpose.int %273, %int-2_7590, %int-1_7591 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_7592 = torch.constant.int 4
    %6226 = torch.aten.mul.int %int4_7592, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_7593 = torch.constant.int 4096
    %6227 = torch.prim.ListConstruct %6226, %int4096_7593 : (!torch.int, !torch.int) -> !torch.list<int>
    %6228 = torch.aten.view %6217, %6227 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %6228, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %6229 = torch.aten.mm %6228, %6225 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %6229, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_7594 = torch.constant.int 4
    %int1024_7595 = torch.constant.int 1024
    %6230 = torch.prim.ListConstruct %int4_7594, %306, %int1024_7595 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6231 = torch.aten.view %6229, %6230 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %6231, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int-2_7596 = torch.constant.int -2
    %int-1_7597 = torch.constant.int -1
    %6232 = torch.aten.transpose.int %274, %int-2_7596, %int-1_7597 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_7598 = torch.constant.int 4
    %6233 = torch.aten.mul.int %int4_7598, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_7599 = torch.constant.int 4096
    %6234 = torch.prim.ListConstruct %6233, %int4096_7599 : (!torch.int, !torch.int) -> !torch.list<int>
    %6235 = torch.aten.view %6217, %6234 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %6235, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %6236 = torch.aten.mm %6235, %6232 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %6236, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_7600 = torch.constant.int 4
    %int1024_7601 = torch.constant.int 1024
    %6237 = torch.prim.ListConstruct %int4_7600, %306, %int1024_7601 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6238 = torch.aten.view %6236, %6237 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %6238, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int4_7602 = torch.constant.int 4
    %int32_7603 = torch.constant.int 32
    %int128_7604 = torch.constant.int 128
    %6239 = torch.prim.ListConstruct %int4_7602, %306, %int32_7603, %int128_7604 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6240 = torch.aten.view %6224, %6239 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %6240, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_7605 = torch.constant.int 4
    %int8_7606 = torch.constant.int 8
    %int128_7607 = torch.constant.int 128
    %6241 = torch.prim.ListConstruct %int4_7605, %306, %int8_7606, %int128_7607 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6242 = torch.aten.view %6231, %6241 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %6242, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int4_7608 = torch.constant.int 4
    %int8_7609 = torch.constant.int 8
    %int128_7610 = torch.constant.int 128
    %6243 = torch.prim.ListConstruct %int4_7608, %306, %int8_7609, %int128_7610 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6244 = torch.aten.view %6238, %6243 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %6244, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int131072_7611 = torch.constant.int 131072
    %none_7612 = torch.constant.none
    %none_7613 = torch.constant.none
    %cpu_7614 = torch.constant.device "cpu"
    %false_7615 = torch.constant.bool false
    %6245 = torch.aten.arange %int131072_7611, %none_7612, %none_7613, %cpu_7614, %false_7615 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_7616 = torch.constant.int 0
    %int128_7617 = torch.constant.int 128
    %none_7618 = torch.constant.none
    %none_7619 = torch.constant.none
    %cpu_7620 = torch.constant.device "cpu"
    %false_7621 = torch.constant.bool false
    %6246 = torch.aten.arange.start %int0_7616, %int128_7617, %none_7618, %none_7619, %cpu_7620, %false_7621 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_7622 = torch.constant.int 2
    %6247 = torch.aten.floor_divide.Scalar %6246, %int2_7622 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_7623 = torch.constant.int 6
    %6248 = torch.prims.convert_element_type %6247, %int6_7623 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_7624 = torch.constant.int 128
    %6249 = torch.aten.div.Scalar %6248, %int128_7624 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_7625 = torch.constant.float 2.000000e+00
    %6250 = torch.aten.mul.Scalar %6249, %float2.000000e00_7625 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_7626 = torch.constant.float 5.000000e+05
    %6251 = torch.aten.pow.Scalar %float5.000000e05_7626, %6250 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %6252 = torch.aten.reciprocal %6251 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_7627 = torch.constant.float 1.000000e+00
    %6253 = torch.aten.mul.Scalar %6252, %float1.000000e00_7627 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_7628 = torch.constant.int 1
    %6254 = torch.aten.unsqueeze %6245, %int1_7628 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_7629 = torch.constant.int 0
    %6255 = torch.aten.unsqueeze %6253, %int0_7629 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %6256 = torch.aten.mul.Tensor %6254, %6255 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_7630 = torch.constant.int 1
    %6257 = torch.aten.size.int %6224, %int1_7630 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.int
    %int0_7631 = torch.constant.int 0
    %6258 = torch.aten.add.int %int0_7631, %6257 : !torch.int, !torch.int -> !torch.int
    %int0_7632 = torch.constant.int 0
    %int0_7633 = torch.constant.int 0
    %int1_7634 = torch.constant.int 1
    %6259 = torch.aten.slice.Tensor %6256, %int0_7632, %int0_7633, %6258, %int1_7634 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %6259, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_7635 = torch.constant.int 1
    %int0_7636 = torch.constant.int 0
    %int9223372036854775807_7637 = torch.constant.int 9223372036854775807
    %int1_7638 = torch.constant.int 1
    %6260 = torch.aten.slice.Tensor %6259, %int1_7635, %int0_7636, %int9223372036854775807_7637, %int1_7638 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %6260, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_7639 = torch.constant.int 1
    %int0_7640 = torch.constant.int 0
    %int9223372036854775807_7641 = torch.constant.int 9223372036854775807
    %int1_7642 = torch.constant.int 1
    %6261 = torch.aten.slice.Tensor %6260, %int1_7639, %int0_7640, %int9223372036854775807_7641, %int1_7642 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %6261, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_7643 = torch.constant.int 0
    %6262 = torch.aten.unsqueeze %6261, %int0_7643 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %6262, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_7644 = torch.constant.int 1
    %int0_7645 = torch.constant.int 0
    %int9223372036854775807_7646 = torch.constant.int 9223372036854775807
    %int1_7647 = torch.constant.int 1
    %6263 = torch.aten.slice.Tensor %6262, %int1_7644, %int0_7645, %int9223372036854775807_7646, %int1_7647 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %6263, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_7648 = torch.constant.int 2
    %int0_7649 = torch.constant.int 0
    %int9223372036854775807_7650 = torch.constant.int 9223372036854775807
    %int1_7651 = torch.constant.int 1
    %6264 = torch.aten.slice.Tensor %6263, %int2_7648, %int0_7649, %int9223372036854775807_7650, %int1_7651 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %6264, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_7652 = torch.constant.int 4
    %int1_7653 = torch.constant.int 1
    %int1_7654 = torch.constant.int 1
    %6265 = torch.prim.ListConstruct %int4_7652, %int1_7653, %int1_7654 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6266 = torch.aten.repeat %6264, %6265 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %6266, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_7655 = torch.constant.int 6
    %6267 = torch.prims.convert_element_type %6240, %int6_7655 : !torch.vtensor<[4,?,32,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %6267, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %6268 = torch_c.to_builtin_tensor %6267 : !torch.vtensor<[4,?,32,128],f32> -> tensor<4x?x32x128xf32>
    %6269 = torch_c.to_builtin_tensor %6266 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %6270 = util.call @sharktank_rotary_embedding_4_D_32_128_f32(%6268, %6269) : (tensor<4x?x32x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x32x128xf32>
    %6271 = torch_c.from_builtin_tensor %6270 : tensor<4x?x32x128xf32> -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %6271, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %int5_7656 = torch.constant.int 5
    %6272 = torch.prims.convert_element_type %6271, %int5_7656 : !torch.vtensor<[4,?,32,128],f32>, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %6272, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int131072_7657 = torch.constant.int 131072
    %none_7658 = torch.constant.none
    %none_7659 = torch.constant.none
    %cpu_7660 = torch.constant.device "cpu"
    %false_7661 = torch.constant.bool false
    %6273 = torch.aten.arange %int131072_7657, %none_7658, %none_7659, %cpu_7660, %false_7661 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_7662 = torch.constant.int 0
    %int128_7663 = torch.constant.int 128
    %none_7664 = torch.constant.none
    %none_7665 = torch.constant.none
    %cpu_7666 = torch.constant.device "cpu"
    %false_7667 = torch.constant.bool false
    %6274 = torch.aten.arange.start %int0_7662, %int128_7663, %none_7664, %none_7665, %cpu_7666, %false_7667 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_7668 = torch.constant.int 2
    %6275 = torch.aten.floor_divide.Scalar %6274, %int2_7668 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_7669 = torch.constant.int 6
    %6276 = torch.prims.convert_element_type %6275, %int6_7669 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_7670 = torch.constant.int 128
    %6277 = torch.aten.div.Scalar %6276, %int128_7670 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_7671 = torch.constant.float 2.000000e+00
    %6278 = torch.aten.mul.Scalar %6277, %float2.000000e00_7671 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_7672 = torch.constant.float 5.000000e+05
    %6279 = torch.aten.pow.Scalar %float5.000000e05_7672, %6278 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %6280 = torch.aten.reciprocal %6279 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_7673 = torch.constant.float 1.000000e+00
    %6281 = torch.aten.mul.Scalar %6280, %float1.000000e00_7673 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_7674 = torch.constant.int 1
    %6282 = torch.aten.unsqueeze %6273, %int1_7674 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_7675 = torch.constant.int 0
    %6283 = torch.aten.unsqueeze %6281, %int0_7675 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %6284 = torch.aten.mul.Tensor %6282, %6283 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_7676 = torch.constant.int 1
    %6285 = torch.aten.size.int %6231, %int1_7676 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int0_7677 = torch.constant.int 0
    %6286 = torch.aten.add.int %int0_7677, %6285 : !torch.int, !torch.int -> !torch.int
    %int0_7678 = torch.constant.int 0
    %int0_7679 = torch.constant.int 0
    %int1_7680 = torch.constant.int 1
    %6287 = torch.aten.slice.Tensor %6284, %int0_7678, %int0_7679, %6286, %int1_7680 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %6287, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_7681 = torch.constant.int 1
    %int0_7682 = torch.constant.int 0
    %int9223372036854775807_7683 = torch.constant.int 9223372036854775807
    %int1_7684 = torch.constant.int 1
    %6288 = torch.aten.slice.Tensor %6287, %int1_7681, %int0_7682, %int9223372036854775807_7683, %int1_7684 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %6288, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_7685 = torch.constant.int 1
    %int0_7686 = torch.constant.int 0
    %int9223372036854775807_7687 = torch.constant.int 9223372036854775807
    %int1_7688 = torch.constant.int 1
    %6289 = torch.aten.slice.Tensor %6288, %int1_7685, %int0_7686, %int9223372036854775807_7687, %int1_7688 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %6289, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_7689 = torch.constant.int 0
    %6290 = torch.aten.unsqueeze %6289, %int0_7689 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %6290, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_7690 = torch.constant.int 1
    %int0_7691 = torch.constant.int 0
    %int9223372036854775807_7692 = torch.constant.int 9223372036854775807
    %int1_7693 = torch.constant.int 1
    %6291 = torch.aten.slice.Tensor %6290, %int1_7690, %int0_7691, %int9223372036854775807_7692, %int1_7693 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %6291, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_7694 = torch.constant.int 2
    %int0_7695 = torch.constant.int 0
    %int9223372036854775807_7696 = torch.constant.int 9223372036854775807
    %int1_7697 = torch.constant.int 1
    %6292 = torch.aten.slice.Tensor %6291, %int2_7694, %int0_7695, %int9223372036854775807_7696, %int1_7697 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %6292, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_7698 = torch.constant.int 4
    %int1_7699 = torch.constant.int 1
    %int1_7700 = torch.constant.int 1
    %6293 = torch.prim.ListConstruct %int4_7698, %int1_7699, %int1_7700 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6294 = torch.aten.repeat %6292, %6293 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %6294, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_7701 = torch.constant.int 6
    %6295 = torch.prims.convert_element_type %6242, %int6_7701 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %6295, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %6296 = torch_c.to_builtin_tensor %6295 : !torch.vtensor<[4,?,8,128],f32> -> tensor<4x?x8x128xf32>
    %6297 = torch_c.to_builtin_tensor %6294 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %6298 = util.call @sharktank_rotary_embedding_4_D_8_128_f32(%6296, %6297) : (tensor<4x?x8x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x8x128xf32>
    %6299 = torch_c.from_builtin_tensor %6298 : tensor<4x?x8x128xf32> -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %6299, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %int5_7702 = torch.constant.int 5
    %6300 = torch.prims.convert_element_type %6299, %int5_7702 : !torch.vtensor<[4,?,8,128],f32>, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %6300, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int64_7703 = torch.constant.int 64
    %6301 = torch.aten.mul.Scalar %arg2, %int64_7703 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6301, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int60 = torch.constant.int 60
    %int1_7704 = torch.constant.int 1
    %6302 = torch.aten.add.Scalar %6301, %int60, %int1_7704 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6302, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_7705 = torch.constant.int 4
    %int32_7706 = torch.constant.int 32
    %int8_7707 = torch.constant.int 8
    %int128_7708 = torch.constant.int 128
    %6303 = torch.prim.ListConstruct %int4_7705, %398, %int32_7706, %int8_7707, %int128_7708 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6304 = torch.aten.view %6300, %6303 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6304, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_7709 = torch.constant.int 4
    %6305 = torch.aten.mul.int %int4_7709, %398 : !torch.int, !torch.int -> !torch.int
    %int32_7710 = torch.constant.int 32
    %int8_7711 = torch.constant.int 8
    %int128_7712 = torch.constant.int 128
    %6306 = torch.prim.ListConstruct %6305, %int32_7710, %int8_7711, %int128_7712 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6307 = torch.aten.view %6304, %6306 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %6307, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_7713 = torch.constant.int 4
    %6308 = torch.aten.mul.int %int4_7713, %398 : !torch.int, !torch.int -> !torch.int
    %6309 = torch.prim.ListConstruct %6308 : (!torch.int) -> !torch.list<int>
    %6310 = torch.aten.view %6302, %6309 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %6310, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_7714 = torch.constant.int 32
    %int2_7715 = torch.constant.int 2
    %int32_7716 = torch.constant.int 32
    %int8_7717 = torch.constant.int 8
    %int128_7718 = torch.constant.int 128
    %6311 = torch.prim.ListConstruct %389, %int32_7714, %int2_7715, %int32_7716, %int8_7717, %int128_7718 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6312 = torch.aten.view %6144, %6311 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %6312, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_7719 = torch.constant.int 32
    %6313 = torch.aten.mul.int %389, %int32_7719 : !torch.int, !torch.int -> !torch.int
    %int2_7720 = torch.constant.int 2
    %6314 = torch.aten.mul.int %6313, %int2_7720 : !torch.int, !torch.int -> !torch.int
    %int32_7721 = torch.constant.int 32
    %int8_7722 = torch.constant.int 8
    %int128_7723 = torch.constant.int 128
    %6315 = torch.prim.ListConstruct %6314, %int32_7721, %int8_7722, %int128_7723 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6316 = torch.aten.view %6312, %6315 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %6316, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %6317 = torch.prim.ListConstruct %6310 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_7724 = torch.constant.bool false
    %6318 = torch.aten.index_put %6316, %6317, %6307, %false_7724 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %6318, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_7725 = torch.constant.int 32
    %int2_7726 = torch.constant.int 2
    %int32_7727 = torch.constant.int 32
    %int8_7728 = torch.constant.int 8
    %int128_7729 = torch.constant.int 128
    %6319 = torch.prim.ListConstruct %389, %int32_7725, %int2_7726, %int32_7727, %int8_7728, %int128_7729 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6320 = torch.aten.view %6318, %6319 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %6320, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_7730 = torch.constant.int 2097152
    %6321 = torch.prim.ListConstruct %389, %int2097152_7730 : (!torch.int, !torch.int) -> !torch.list<int>
    %6322 = torch.aten.view %6320, %6321 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %6322, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_7731 = torch.constant.int 32
    %int2_7732 = torch.constant.int 2
    %int32_7733 = torch.constant.int 32
    %int8_7734 = torch.constant.int 8
    %int128_7735 = torch.constant.int 128
    %6323 = torch.prim.ListConstruct %389, %int32_7731, %int2_7732, %int32_7733, %int8_7734, %int128_7735 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6324 = torch.aten.view %6322, %6323 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %6324, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_7736 = torch.constant.int 32
    %int8_7737 = torch.constant.int 8
    %int128_7738 = torch.constant.int 128
    %6325 = torch.prim.ListConstruct %6314, %int32_7736, %int8_7737, %int128_7738 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6326 = torch.aten.view %6324, %6325 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %6326, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_7739 = torch.constant.int 4
    %int32_7740 = torch.constant.int 32
    %int8_7741 = torch.constant.int 8
    %int128_7742 = torch.constant.int 128
    %6327 = torch.prim.ListConstruct %int4_7739, %398, %int32_7740, %int8_7741, %int128_7742 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6328 = torch.aten.view %6244, %6327 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6328, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_7743 = torch.constant.int 4
    %6329 = torch.aten.mul.int %int4_7743, %398 : !torch.int, !torch.int -> !torch.int
    %int32_7744 = torch.constant.int 32
    %int8_7745 = torch.constant.int 8
    %int128_7746 = torch.constant.int 128
    %6330 = torch.prim.ListConstruct %6329, %int32_7744, %int8_7745, %int128_7746 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6331 = torch.aten.view %6328, %6330 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %6331, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int1_7747 = torch.constant.int 1
    %int1_7748 = torch.constant.int 1
    %6332 = torch.aten.add.Scalar %6302, %int1_7747, %int1_7748 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6332, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_7749 = torch.constant.int 4
    %6333 = torch.aten.mul.int %int4_7749, %398 : !torch.int, !torch.int -> !torch.int
    %6334 = torch.prim.ListConstruct %6333 : (!torch.int) -> !torch.list<int>
    %6335 = torch.aten.view %6332, %6334 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %6335, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %6336 = torch.prim.ListConstruct %6335 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_7750 = torch.constant.bool false
    %6337 = torch.aten.index_put %6326, %6336, %6331, %false_7750 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %6337, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_7751 = torch.constant.int 32
    %int2_7752 = torch.constant.int 2
    %int32_7753 = torch.constant.int 32
    %int8_7754 = torch.constant.int 8
    %int128_7755 = torch.constant.int 128
    %6338 = torch.prim.ListConstruct %389, %int32_7751, %int2_7752, %int32_7753, %int8_7754, %int128_7755 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6339 = torch.aten.view %6337, %6338 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %6339, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_7756 = torch.constant.int 2097152
    %6340 = torch.prim.ListConstruct %389, %int2097152_7756 : (!torch.int, !torch.int) -> !torch.list<int>
    %6341 = torch.aten.view %6339, %6340 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %6341, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int-2_7757 = torch.constant.int -2
    %6342 = torch.aten.unsqueeze %6300, %int-2_7757 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %6342, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int4_7758 = torch.constant.int 4
    %int8_7759 = torch.constant.int 8
    %int4_7760 = torch.constant.int 4
    %int128_7761 = torch.constant.int 128
    %6343 = torch.prim.ListConstruct %int4_7758, %6285, %int8_7759, %int4_7760, %int128_7761 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_7762 = torch.constant.bool false
    %6344 = torch.aten.expand %6342, %6343, %false_7762 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %6344, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_7763 = torch.constant.int 0
    %6345 = torch.aten.clone %6344, %int0_7763 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %6345, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_7764 = torch.constant.int 4
    %int32_7765 = torch.constant.int 32
    %int128_7766 = torch.constant.int 128
    %6346 = torch.prim.ListConstruct %int4_7764, %6285, %int32_7765, %int128_7766 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6347 = torch.aten._unsafe_view %6345, %6346 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %6347, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_7767 = torch.constant.int -2
    %6348 = torch.aten.unsqueeze %6244, %int-2_7767 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %6348, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_7768 = torch.constant.int 1
    %6349 = torch.aten.size.int %6238, %int1_7768 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int4_7769 = torch.constant.int 4
    %int8_7770 = torch.constant.int 8
    %int4_7771 = torch.constant.int 4
    %int128_7772 = torch.constant.int 128
    %6350 = torch.prim.ListConstruct %int4_7769, %6349, %int8_7770, %int4_7771, %int128_7772 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_7773 = torch.constant.bool false
    %6351 = torch.aten.expand %6348, %6350, %false_7773 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %6351, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_7774 = torch.constant.int 0
    %6352 = torch.aten.clone %6351, %int0_7774 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %6352, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_7775 = torch.constant.int 4
    %int32_7776 = torch.constant.int 32
    %int128_7777 = torch.constant.int 128
    %6353 = torch.prim.ListConstruct %int4_7775, %6349, %int32_7776, %int128_7777 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6354 = torch.aten._unsafe_view %6352, %6353 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %6354, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_7778 = torch.constant.int 1
    %int2_7779 = torch.constant.int 2
    %6355 = torch.aten.transpose.int %6272, %int1_7778, %int2_7779 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %6355, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_7780 = torch.constant.int 1
    %int2_7781 = torch.constant.int 2
    %6356 = torch.aten.transpose.int %6347, %int1_7780, %int2_7781 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %6356, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_7782 = torch.constant.int 1
    %int2_7783 = torch.constant.int 2
    %6357 = torch.aten.transpose.int %6354, %int1_7782, %int2_7783 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %6357, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_7784 = torch.constant.float 0.000000e+00
    %true_7785 = torch.constant.bool true
    %none_7786 = torch.constant.none
    %none_7787 = torch.constant.none
    %6358:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6355, %6356, %6357, %float0.000000e00_7784, %true_7785, %none_7786, %none_7787) : (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?],f32>) 
    torch.bind_symbolic_shape %6358#0, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_7788 = torch.constant.int 1
    %int2_7789 = torch.constant.int 2
    %6359 = torch.aten.transpose.int %6358#0, %int1_7788, %int2_7789 : !torch.vtensor<[4,32,?,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %6359, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_7790 = torch.constant.int 4
    %int4096_7791 = torch.constant.int 4096
    %6360 = torch.prim.ListConstruct %int4_7790, %6257, %int4096_7791 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6361 = torch.aten.view %6359, %6360 : !torch.vtensor<[4,?,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %6361, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_7792 = torch.constant.int -2
    %int-1_7793 = torch.constant.int -1
    %6362 = torch.aten.transpose.int %275, %int-2_7792, %int-1_7793 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_7794 = torch.constant.int 4
    %6363 = torch.aten.mul.int %int4_7794, %6257 : !torch.int, !torch.int -> !torch.int
    %int4096_7795 = torch.constant.int 4096
    %6364 = torch.prim.ListConstruct %6363, %int4096_7795 : (!torch.int, !torch.int) -> !torch.list<int>
    %6365 = torch.aten.view %6361, %6364 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %6365, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %6366 = torch.aten.mm %6365, %6362 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %6366, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_7796 = torch.constant.int 4
    %int4096_7797 = torch.constant.int 4096
    %6367 = torch.prim.ListConstruct %int4_7796, %6257, %int4096_7797 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6368 = torch.aten.view %6366, %6367 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %6368, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_7798 = torch.constant.int 1
    %6369 = torch.aten.add.Tensor %6207, %6368, %int1_7798 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %6369, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_7799 = torch.constant.int 6
    %6370 = torch.prims.convert_element_type %6369, %int6_7799 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %6370, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_7800 = torch.constant.int 2
    %6371 = torch.aten.pow.Tensor_Scalar %6370, %int2_7800 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %6371, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_7801 = torch.constant.int -1
    %6372 = torch.prim.ListConstruct %int-1_7801 : (!torch.int) -> !torch.list<int>
    %true_7802 = torch.constant.bool true
    %none_7803 = torch.constant.none
    %6373 = torch.aten.mean.dim %6371, %6372, %true_7802, %none_7803 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6373, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_7804 = torch.constant.float 9.9999997473787516E-6
    %int1_7805 = torch.constant.int 1
    %6374 = torch.aten.add.Scalar %6373, %float9.999990e-06_7804, %int1_7805 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6374, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %6375 = torch.aten.rsqrt %6374 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6375, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %6376 = torch.aten.mul.Tensor %6370, %6375 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %6376, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_7806 = torch.constant.int 5
    %6377 = torch.prims.convert_element_type %6376, %int5_7806 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %6377, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %6378 = torch.aten.mul.Tensor %276, %6377 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %6378, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_7807 = torch.constant.int 5
    %6379 = torch.prims.convert_element_type %6378, %int5_7807 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %6379, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_7808 = torch.constant.int -2
    %int-1_7809 = torch.constant.int -1
    %6380 = torch.aten.transpose.int %277, %int-2_7808, %int-1_7809 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_7810 = torch.constant.int 4
    %6381 = torch.aten.mul.int %int4_7810, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_7811 = torch.constant.int 4096
    %6382 = torch.prim.ListConstruct %6381, %int4096_7811 : (!torch.int, !torch.int) -> !torch.list<int>
    %6383 = torch.aten.view %6379, %6382 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %6383, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %6384 = torch.aten.mm %6383, %6380 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %6384, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_7812 = torch.constant.int 4
    %int14336_7813 = torch.constant.int 14336
    %6385 = torch.prim.ListConstruct %int4_7812, %306, %int14336_7813 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6386 = torch.aten.view %6384, %6385 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %6386, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %6387 = torch.aten.silu %6386 : !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %6387, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_7814 = torch.constant.int -2
    %int-1_7815 = torch.constant.int -1
    %6388 = torch.aten.transpose.int %278, %int-2_7814, %int-1_7815 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_7816 = torch.constant.int 4
    %6389 = torch.aten.mul.int %int4_7816, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_7817 = torch.constant.int 4096
    %6390 = torch.prim.ListConstruct %6389, %int4096_7817 : (!torch.int, !torch.int) -> !torch.list<int>
    %6391 = torch.aten.view %6379, %6390 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %6391, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %6392 = torch.aten.mm %6391, %6388 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %6392, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_7818 = torch.constant.int 4
    %int14336_7819 = torch.constant.int 14336
    %6393 = torch.prim.ListConstruct %int4_7818, %306, %int14336_7819 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6394 = torch.aten.view %6392, %6393 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %6394, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %6395 = torch.aten.mul.Tensor %6387, %6394 : !torch.vtensor<[4,?,14336],f16>, !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %6395, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_7820 = torch.constant.int -2
    %int-1_7821 = torch.constant.int -1
    %6396 = torch.aten.transpose.int %279, %int-2_7820, %int-1_7821 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int1_7822 = torch.constant.int 1
    %6397 = torch.aten.size.int %6386, %int1_7822 : !torch.vtensor<[4,?,14336],f16>, !torch.int -> !torch.int
    %int4_7823 = torch.constant.int 4
    %6398 = torch.aten.mul.int %int4_7823, %6397 : !torch.int, !torch.int -> !torch.int
    %int14336_7824 = torch.constant.int 14336
    %6399 = torch.prim.ListConstruct %6398, %int14336_7824 : (!torch.int, !torch.int) -> !torch.list<int>
    %6400 = torch.aten.view %6395, %6399 : !torch.vtensor<[4,?,14336],f16>, !torch.list<int> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %6400, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %6401 = torch.aten.mm %6400, %6396 : !torch.vtensor<[?,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %6401, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_7825 = torch.constant.int 4
    %int4096_7826 = torch.constant.int 4096
    %6402 = torch.prim.ListConstruct %int4_7825, %6397, %int4096_7826 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6403 = torch.aten.view %6401, %6402 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %6403, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_7827 = torch.constant.int 1
    %6404 = torch.aten.add.Tensor %6369, %6403, %int1_7827 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %6404, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_7828 = torch.constant.int 6
    %6405 = torch.prims.convert_element_type %6404, %int6_7828 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %6405, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_7829 = torch.constant.int 2
    %6406 = torch.aten.pow.Tensor_Scalar %6405, %int2_7829 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %6406, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_7830 = torch.constant.int -1
    %6407 = torch.prim.ListConstruct %int-1_7830 : (!torch.int) -> !torch.list<int>
    %true_7831 = torch.constant.bool true
    %none_7832 = torch.constant.none
    %6408 = torch.aten.mean.dim %6406, %6407, %true_7831, %none_7832 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6408, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_7833 = torch.constant.float 9.9999997473787516E-6
    %int1_7834 = torch.constant.int 1
    %6409 = torch.aten.add.Scalar %6408, %float9.999990e-06_7833, %int1_7834 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6409, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %6410 = torch.aten.rsqrt %6409 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6410, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %6411 = torch.aten.mul.Tensor %6405, %6410 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %6411, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_7835 = torch.constant.int 5
    %6412 = torch.prims.convert_element_type %6411, %int5_7835 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %6412, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %6413 = torch.aten.mul.Tensor %280, %6412 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %6413, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_7836 = torch.constant.int 5
    %6414 = torch.prims.convert_element_type %6413, %int5_7836 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %6414, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_7837 = torch.constant.int -2
    %int-1_7838 = torch.constant.int -1
    %6415 = torch.aten.transpose.int %281, %int-2_7837, %int-1_7838 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_7839 = torch.constant.int 4
    %6416 = torch.aten.mul.int %int4_7839, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_7840 = torch.constant.int 4096
    %6417 = torch.prim.ListConstruct %6416, %int4096_7840 : (!torch.int, !torch.int) -> !torch.list<int>
    %6418 = torch.aten.view %6414, %6417 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %6418, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %6419 = torch.aten.mm %6418, %6415 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %6419, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_7841 = torch.constant.int 4
    %int4096_7842 = torch.constant.int 4096
    %6420 = torch.prim.ListConstruct %int4_7841, %306, %int4096_7842 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6421 = torch.aten.view %6419, %6420 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %6421, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_7843 = torch.constant.int -2
    %int-1_7844 = torch.constant.int -1
    %6422 = torch.aten.transpose.int %282, %int-2_7843, %int-1_7844 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_7845 = torch.constant.int 4
    %6423 = torch.aten.mul.int %int4_7845, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_7846 = torch.constant.int 4096
    %6424 = torch.prim.ListConstruct %6423, %int4096_7846 : (!torch.int, !torch.int) -> !torch.list<int>
    %6425 = torch.aten.view %6414, %6424 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %6425, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %6426 = torch.aten.mm %6425, %6422 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %6426, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_7847 = torch.constant.int 4
    %int1024_7848 = torch.constant.int 1024
    %6427 = torch.prim.ListConstruct %int4_7847, %306, %int1024_7848 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6428 = torch.aten.view %6426, %6427 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %6428, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int-2_7849 = torch.constant.int -2
    %int-1_7850 = torch.constant.int -1
    %6429 = torch.aten.transpose.int %283, %int-2_7849, %int-1_7850 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_7851 = torch.constant.int 4
    %6430 = torch.aten.mul.int %int4_7851, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_7852 = torch.constant.int 4096
    %6431 = torch.prim.ListConstruct %6430, %int4096_7852 : (!torch.int, !torch.int) -> !torch.list<int>
    %6432 = torch.aten.view %6414, %6431 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %6432, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %6433 = torch.aten.mm %6432, %6429 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[?,1024],f16>
    torch.bind_symbolic_shape %6433, [%292], affine_map<()[s0] -> (s0 * 128, 1024)> : !torch.vtensor<[?,1024],f16>
    %int4_7853 = torch.constant.int 4
    %int1024_7854 = torch.constant.int 1024
    %6434 = torch.prim.ListConstruct %int4_7853, %306, %int1024_7854 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6435 = torch.aten.view %6433, %6434 : !torch.vtensor<[?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,1024],f16>
    torch.bind_symbolic_shape %6435, [%292], affine_map<()[s0] -> (4, s0 * 32, 1024)> : !torch.vtensor<[4,?,1024],f16>
    %int4_7855 = torch.constant.int 4
    %int32_7856 = torch.constant.int 32
    %int128_7857 = torch.constant.int 128
    %6436 = torch.prim.ListConstruct %int4_7855, %306, %int32_7856, %int128_7857 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6437 = torch.aten.view %6421, %6436 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %6437, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_7858 = torch.constant.int 4
    %int8_7859 = torch.constant.int 8
    %int128_7860 = torch.constant.int 128
    %6438 = torch.prim.ListConstruct %int4_7858, %306, %int8_7859, %int128_7860 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6439 = torch.aten.view %6428, %6438 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %6439, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int4_7861 = torch.constant.int 4
    %int8_7862 = torch.constant.int 8
    %int128_7863 = torch.constant.int 128
    %6440 = torch.prim.ListConstruct %int4_7861, %306, %int8_7862, %int128_7863 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6441 = torch.aten.view %6435, %6440 : !torch.vtensor<[4,?,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %6441, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int131072_7864 = torch.constant.int 131072
    %none_7865 = torch.constant.none
    %none_7866 = torch.constant.none
    %cpu_7867 = torch.constant.device "cpu"
    %false_7868 = torch.constant.bool false
    %6442 = torch.aten.arange %int131072_7864, %none_7865, %none_7866, %cpu_7867, %false_7868 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_7869 = torch.constant.int 0
    %int128_7870 = torch.constant.int 128
    %none_7871 = torch.constant.none
    %none_7872 = torch.constant.none
    %cpu_7873 = torch.constant.device "cpu"
    %false_7874 = torch.constant.bool false
    %6443 = torch.aten.arange.start %int0_7869, %int128_7870, %none_7871, %none_7872, %cpu_7873, %false_7874 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_7875 = torch.constant.int 2
    %6444 = torch.aten.floor_divide.Scalar %6443, %int2_7875 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_7876 = torch.constant.int 6
    %6445 = torch.prims.convert_element_type %6444, %int6_7876 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_7877 = torch.constant.int 128
    %6446 = torch.aten.div.Scalar %6445, %int128_7877 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_7878 = torch.constant.float 2.000000e+00
    %6447 = torch.aten.mul.Scalar %6446, %float2.000000e00_7878 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_7879 = torch.constant.float 5.000000e+05
    %6448 = torch.aten.pow.Scalar %float5.000000e05_7879, %6447 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %6449 = torch.aten.reciprocal %6448 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_7880 = torch.constant.float 1.000000e+00
    %6450 = torch.aten.mul.Scalar %6449, %float1.000000e00_7880 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_7881 = torch.constant.int 1
    %6451 = torch.aten.unsqueeze %6442, %int1_7881 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_7882 = torch.constant.int 0
    %6452 = torch.aten.unsqueeze %6450, %int0_7882 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %6453 = torch.aten.mul.Tensor %6451, %6452 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_7883 = torch.constant.int 1
    %6454 = torch.aten.size.int %6421, %int1_7883 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.int
    %int0_7884 = torch.constant.int 0
    %6455 = torch.aten.add.int %int0_7884, %6454 : !torch.int, !torch.int -> !torch.int
    %int0_7885 = torch.constant.int 0
    %int0_7886 = torch.constant.int 0
    %int1_7887 = torch.constant.int 1
    %6456 = torch.aten.slice.Tensor %6453, %int0_7885, %int0_7886, %6455, %int1_7887 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %6456, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_7888 = torch.constant.int 1
    %int0_7889 = torch.constant.int 0
    %int9223372036854775807_7890 = torch.constant.int 9223372036854775807
    %int1_7891 = torch.constant.int 1
    %6457 = torch.aten.slice.Tensor %6456, %int1_7888, %int0_7889, %int9223372036854775807_7890, %int1_7891 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %6457, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_7892 = torch.constant.int 1
    %int0_7893 = torch.constant.int 0
    %int9223372036854775807_7894 = torch.constant.int 9223372036854775807
    %int1_7895 = torch.constant.int 1
    %6458 = torch.aten.slice.Tensor %6457, %int1_7892, %int0_7893, %int9223372036854775807_7894, %int1_7895 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %6458, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_7896 = torch.constant.int 0
    %6459 = torch.aten.unsqueeze %6458, %int0_7896 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %6459, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_7897 = torch.constant.int 1
    %int0_7898 = torch.constant.int 0
    %int9223372036854775807_7899 = torch.constant.int 9223372036854775807
    %int1_7900 = torch.constant.int 1
    %6460 = torch.aten.slice.Tensor %6459, %int1_7897, %int0_7898, %int9223372036854775807_7899, %int1_7900 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %6460, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_7901 = torch.constant.int 2
    %int0_7902 = torch.constant.int 0
    %int9223372036854775807_7903 = torch.constant.int 9223372036854775807
    %int1_7904 = torch.constant.int 1
    %6461 = torch.aten.slice.Tensor %6460, %int2_7901, %int0_7902, %int9223372036854775807_7903, %int1_7904 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %6461, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_7905 = torch.constant.int 4
    %int1_7906 = torch.constant.int 1
    %int1_7907 = torch.constant.int 1
    %6462 = torch.prim.ListConstruct %int4_7905, %int1_7906, %int1_7907 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6463 = torch.aten.repeat %6461, %6462 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %6463, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_7908 = torch.constant.int 6
    %6464 = torch.prims.convert_element_type %6437, %int6_7908 : !torch.vtensor<[4,?,32,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %6464, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %6465 = torch_c.to_builtin_tensor %6464 : !torch.vtensor<[4,?,32,128],f32> -> tensor<4x?x32x128xf32>
    %6466 = torch_c.to_builtin_tensor %6463 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %6467 = util.call @sharktank_rotary_embedding_4_D_32_128_f32(%6465, %6466) : (tensor<4x?x32x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x32x128xf32>
    %6468 = torch_c.from_builtin_tensor %6467 : tensor<4x?x32x128xf32> -> !torch.vtensor<[4,?,32,128],f32>
    torch.bind_symbolic_shape %6468, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f32>
    %int5_7909 = torch.constant.int 5
    %6469 = torch.prims.convert_element_type %6468, %int5_7909 : !torch.vtensor<[4,?,32,128],f32>, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %6469, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int131072_7910 = torch.constant.int 131072
    %none_7911 = torch.constant.none
    %none_7912 = torch.constant.none
    %cpu_7913 = torch.constant.device "cpu"
    %false_7914 = torch.constant.bool false
    %6470 = torch.aten.arange %int131072_7910, %none_7911, %none_7912, %cpu_7913, %false_7914 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_7915 = torch.constant.int 0
    %int128_7916 = torch.constant.int 128
    %none_7917 = torch.constant.none
    %none_7918 = torch.constant.none
    %cpu_7919 = torch.constant.device "cpu"
    %false_7920 = torch.constant.bool false
    %6471 = torch.aten.arange.start %int0_7915, %int128_7916, %none_7917, %none_7918, %cpu_7919, %false_7920 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2_7921 = torch.constant.int 2
    %6472 = torch.aten.floor_divide.Scalar %6471, %int2_7921 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_7922 = torch.constant.int 6
    %6473 = torch.prims.convert_element_type %6472, %int6_7922 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_7923 = torch.constant.int 128
    %6474 = torch.aten.div.Scalar %6473, %int128_7923 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00_7924 = torch.constant.float 2.000000e+00
    %6475 = torch.aten.mul.Scalar %6474, %float2.000000e00_7924 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05_7925 = torch.constant.float 5.000000e+05
    %6476 = torch.aten.pow.Scalar %float5.000000e05_7925, %6475 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %6477 = torch.aten.reciprocal %6476 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00_7926 = torch.constant.float 1.000000e+00
    %6478 = torch.aten.mul.Scalar %6477, %float1.000000e00_7926 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_7927 = torch.constant.int 1
    %6479 = torch.aten.unsqueeze %6470, %int1_7927 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_7928 = torch.constant.int 0
    %6480 = torch.aten.unsqueeze %6478, %int0_7928 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %6481 = torch.aten.mul.Tensor %6479, %6480 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int1_7929 = torch.constant.int 1
    %6482 = torch.aten.size.int %6428, %int1_7929 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int0_7930 = torch.constant.int 0
    %6483 = torch.aten.add.int %int0_7930, %6482 : !torch.int, !torch.int -> !torch.int
    %int0_7931 = torch.constant.int 0
    %int0_7932 = torch.constant.int 0
    %int1_7933 = torch.constant.int 1
    %6484 = torch.aten.slice.Tensor %6481, %int0_7931, %int0_7932, %6483, %int1_7933 : !torch.vtensor<[131072,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %6484, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_7934 = torch.constant.int 1
    %int0_7935 = torch.constant.int 0
    %int9223372036854775807_7936 = torch.constant.int 9223372036854775807
    %int1_7937 = torch.constant.int 1
    %6485 = torch.aten.slice.Tensor %6484, %int1_7934, %int0_7935, %int9223372036854775807_7936, %int1_7937 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %6485, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int1_7938 = torch.constant.int 1
    %int0_7939 = torch.constant.int 0
    %int9223372036854775807_7940 = torch.constant.int 9223372036854775807
    %int1_7941 = torch.constant.int 1
    %6486 = torch.aten.slice.Tensor %6485, %int1_7938, %int0_7939, %int9223372036854775807_7940, %int1_7941 : !torch.vtensor<[?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[?,128],f32>
    torch.bind_symbolic_shape %6486, [%292], affine_map<()[s0] -> (s0 * 32, 128)> : !torch.vtensor<[?,128],f32>
    %int0_7942 = torch.constant.int 0
    %6487 = torch.aten.unsqueeze %6486, %int0_7942 : !torch.vtensor<[?,128],f32>, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %6487, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int1_7943 = torch.constant.int 1
    %int0_7944 = torch.constant.int 0
    %int9223372036854775807_7945 = torch.constant.int 9223372036854775807
    %int1_7946 = torch.constant.int 1
    %6488 = torch.aten.slice.Tensor %6487, %int1_7943, %int0_7944, %int9223372036854775807_7945, %int1_7946 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %6488, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int2_7947 = torch.constant.int 2
    %int0_7948 = torch.constant.int 0
    %int9223372036854775807_7949 = torch.constant.int 9223372036854775807
    %int1_7950 = torch.constant.int 1
    %6489 = torch.aten.slice.Tensor %6488, %int2_7947, %int0_7948, %int9223372036854775807_7949, %int1_7950 : !torch.vtensor<[1,?,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,?,128],f32>
    torch.bind_symbolic_shape %6489, [%292], affine_map<()[s0] -> (1, s0 * 32, 128)> : !torch.vtensor<[1,?,128],f32>
    %int4_7951 = torch.constant.int 4
    %int1_7952 = torch.constant.int 1
    %int1_7953 = torch.constant.int 1
    %6490 = torch.prim.ListConstruct %int4_7951, %int1_7952, %int1_7953 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6491 = torch.aten.repeat %6489, %6490 : !torch.vtensor<[1,?,128],f32>, !torch.list<int> -> !torch.vtensor<[4,?,128],f32>
    torch.bind_symbolic_shape %6491, [%292], affine_map<()[s0] -> (4, s0 * 32, 128)> : !torch.vtensor<[4,?,128],f32>
    %int6_7954 = torch.constant.int 6
    %6492 = torch.prims.convert_element_type %6439, %int6_7954 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %6492, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %6493 = torch_c.to_builtin_tensor %6492 : !torch.vtensor<[4,?,8,128],f32> -> tensor<4x?x8x128xf32>
    %6494 = torch_c.to_builtin_tensor %6491 : !torch.vtensor<[4,?,128],f32> -> tensor<4x?x128xf32>
    %6495 = util.call @sharktank_rotary_embedding_4_D_8_128_f32(%6493, %6494) : (tensor<4x?x8x128xf32>, tensor<4x?x128xf32>) -> tensor<4x?x8x128xf32>
    %6496 = torch_c.from_builtin_tensor %6495 : tensor<4x?x8x128xf32> -> !torch.vtensor<[4,?,8,128],f32>
    torch.bind_symbolic_shape %6496, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f32>
    %int5_7955 = torch.constant.int 5
    %6497 = torch.prims.convert_element_type %6496, %int5_7955 : !torch.vtensor<[4,?,8,128],f32>, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %6497, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int64_7956 = torch.constant.int 64
    %6498 = torch.aten.mul.Scalar %arg2, %int64_7956 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6498, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int62 = torch.constant.int 62
    %int1_7957 = torch.constant.int 1
    %6499 = torch.aten.add.Scalar %6498, %int62, %int1_7957 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6499, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_7958 = torch.constant.int 4
    %int32_7959 = torch.constant.int 32
    %int8_7960 = torch.constant.int 8
    %int128_7961 = torch.constant.int 128
    %6500 = torch.prim.ListConstruct %int4_7958, %398, %int32_7959, %int8_7960, %int128_7961 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6501 = torch.aten.view %6497, %6500 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6501, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_7962 = torch.constant.int 4
    %6502 = torch.aten.mul.int %int4_7962, %398 : !torch.int, !torch.int -> !torch.int
    %int32_7963 = torch.constant.int 32
    %int8_7964 = torch.constant.int 8
    %int128_7965 = torch.constant.int 128
    %6503 = torch.prim.ListConstruct %6502, %int32_7963, %int8_7964, %int128_7965 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6504 = torch.aten.view %6501, %6503 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %6504, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_7966 = torch.constant.int 4
    %6505 = torch.aten.mul.int %int4_7966, %398 : !torch.int, !torch.int -> !torch.int
    %6506 = torch.prim.ListConstruct %6505 : (!torch.int) -> !torch.list<int>
    %6507 = torch.aten.view %6499, %6506 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %6507, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_7967 = torch.constant.int 32
    %int2_7968 = torch.constant.int 2
    %int32_7969 = torch.constant.int 32
    %int8_7970 = torch.constant.int 8
    %int128_7971 = torch.constant.int 128
    %6508 = torch.prim.ListConstruct %389, %int32_7967, %int2_7968, %int32_7969, %int8_7970, %int128_7971 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6509 = torch.aten.view %6341, %6508 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %6509, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_7972 = torch.constant.int 32
    %6510 = torch.aten.mul.int %389, %int32_7972 : !torch.int, !torch.int -> !torch.int
    %int2_7973 = torch.constant.int 2
    %6511 = torch.aten.mul.int %6510, %int2_7973 : !torch.int, !torch.int -> !torch.int
    %int32_7974 = torch.constant.int 32
    %int8_7975 = torch.constant.int 8
    %int128_7976 = torch.constant.int 128
    %6512 = torch.prim.ListConstruct %6511, %int32_7974, %int8_7975, %int128_7976 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6513 = torch.aten.view %6509, %6512 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %6513, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %6514 = torch.prim.ListConstruct %6507 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_7977 = torch.constant.bool false
    %6515 = torch.aten.index_put %6513, %6514, %6504, %false_7977 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %6515, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_7978 = torch.constant.int 32
    %int2_7979 = torch.constant.int 2
    %int32_7980 = torch.constant.int 32
    %int8_7981 = torch.constant.int 8
    %int128_7982 = torch.constant.int 128
    %6516 = torch.prim.ListConstruct %389, %int32_7978, %int2_7979, %int32_7980, %int8_7981, %int128_7982 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6517 = torch.aten.view %6515, %6516 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %6517, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_7983 = torch.constant.int 2097152
    %6518 = torch.prim.ListConstruct %389, %int2097152_7983 : (!torch.int, !torch.int) -> !torch.list<int>
    %6519 = torch.aten.view %6517, %6518 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %6519, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_7984 = torch.constant.int 32
    %int2_7985 = torch.constant.int 2
    %int32_7986 = torch.constant.int 32
    %int8_7987 = torch.constant.int 8
    %int128_7988 = torch.constant.int 128
    %6520 = torch.prim.ListConstruct %389, %int32_7984, %int2_7985, %int32_7986, %int8_7987, %int128_7988 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6521 = torch.aten.view %6519, %6520 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %6521, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_7989 = torch.constant.int 32
    %int8_7990 = torch.constant.int 8
    %int128_7991 = torch.constant.int 128
    %6522 = torch.prim.ListConstruct %6511, %int32_7989, %int8_7990, %int128_7991 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6523 = torch.aten.view %6521, %6522 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %6523, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int4_7992 = torch.constant.int 4
    %int32_7993 = torch.constant.int 32
    %int8_7994 = torch.constant.int 8
    %int128_7995 = torch.constant.int 128
    %6524 = torch.prim.ListConstruct %int4_7992, %398, %int32_7993, %int8_7994, %int128_7995 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6525 = torch.aten.view %6441, %6524 : !torch.vtensor<[4,?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6525, [%292], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int4_7996 = torch.constant.int 4
    %6526 = torch.aten.mul.int %int4_7996, %398 : !torch.int, !torch.int -> !torch.int
    %int32_7997 = torch.constant.int 32
    %int8_7998 = torch.constant.int 8
    %int128_7999 = torch.constant.int 128
    %6527 = torch.prim.ListConstruct %6526, %int32_7997, %int8_7998, %int128_7999 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6528 = torch.aten.view %6525, %6527 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %6528, [%292], affine_map<()[s0] -> (s0 * 4, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int1_8000 = torch.constant.int 1
    %int1_8001 = torch.constant.int 1
    %6529 = torch.aten.add.Scalar %6499, %int1_8000, %int1_8001 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6529, [%292], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_8002 = torch.constant.int 4
    %6530 = torch.aten.mul.int %int4_8002, %398 : !torch.int, !torch.int -> !torch.int
    %6531 = torch.prim.ListConstruct %6530 : (!torch.int) -> !torch.list<int>
    %6532 = torch.aten.view %6529, %6531 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %6532, [%292], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %6533 = torch.prim.ListConstruct %6532 : (!torch.vtensor<[?],si64>) -> !torch.list<optional<vtensor>>
    %false_8003 = torch.constant.bool false
    %6534 = torch.aten.index_put %6523, %6533, %6528, %false_8003 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[?,32,8,128],f16>, !torch.bool -> !torch.vtensor<[?,32,8,128],f16>
    torch.bind_symbolic_shape %6534, [%293], affine_map<()[s0] -> (s0 * 64, 32, 8, 128)> : !torch.vtensor<[?,32,8,128],f16>
    %int32_8004 = torch.constant.int 32
    %int2_8005 = torch.constant.int 2
    %int32_8006 = torch.constant.int 32
    %int8_8007 = torch.constant.int 8
    %int128_8008 = torch.constant.int 128
    %6535 = torch.prim.ListConstruct %389, %int32_8004, %int2_8005, %int32_8006, %int8_8007, %int128_8008 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6536 = torch.aten.view %6534, %6535 : !torch.vtensor<[?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %6536, [%293], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_8009 = torch.constant.int 2097152
    %6537 = torch.prim.ListConstruct %389, %int2097152_8009 : (!torch.int, !torch.int) -> !torch.list<int>
    %6538 = torch.aten.view %6536, %6537 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.overwrite.tensor.contents %6538 overwrites %arg3 : !torch.vtensor<[?,2097152],f16>, !torch.tensor<[?,2097152],f16>
    torch.bind_symbolic_shape %6538, [%293], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int-2_8010 = torch.constant.int -2
    %6539 = torch.aten.unsqueeze %6497, %int-2_8010 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %6539, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int4_8011 = torch.constant.int 4
    %int8_8012 = torch.constant.int 8
    %int4_8013 = torch.constant.int 4
    %int128_8014 = torch.constant.int 128
    %6540 = torch.prim.ListConstruct %int4_8011, %6482, %int8_8012, %int4_8013, %int128_8014 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_8015 = torch.constant.bool false
    %6541 = torch.aten.expand %6539, %6540, %false_8015 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %6541, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_8016 = torch.constant.int 0
    %6542 = torch.aten.clone %6541, %int0_8016 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %6542, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_8017 = torch.constant.int 4
    %int32_8018 = torch.constant.int 32
    %int128_8019 = torch.constant.int 128
    %6543 = torch.prim.ListConstruct %int4_8017, %6482, %int32_8018, %int128_8019 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6544 = torch.aten._unsafe_view %6542, %6543 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %6544, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_8020 = torch.constant.int -2
    %6545 = torch.aten.unsqueeze %6441, %int-2_8020 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %6545, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_8021 = torch.constant.int 1
    %6546 = torch.aten.size.int %6435, %int1_8021 : !torch.vtensor<[4,?,1024],f16>, !torch.int -> !torch.int
    %int4_8022 = torch.constant.int 4
    %int8_8023 = torch.constant.int 8
    %int4_8024 = torch.constant.int 4
    %int128_8025 = torch.constant.int 128
    %6547 = torch.prim.ListConstruct %int4_8022, %6546, %int8_8023, %int4_8024, %int128_8025 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_8026 = torch.constant.bool false
    %6548 = torch.aten.expand %6545, %6547, %false_8026 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %6548, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_8027 = torch.constant.int 0
    %6549 = torch.aten.clone %6548, %int0_8027 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %6549, [%292], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_8028 = torch.constant.int 4
    %int32_8029 = torch.constant.int 32
    %int128_8030 = torch.constant.int 128
    %6550 = torch.prim.ListConstruct %int4_8028, %6546, %int32_8029, %int128_8030 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6551 = torch.aten._unsafe_view %6549, %6550 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %6551, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_8031 = torch.constant.int 1
    %int2_8032 = torch.constant.int 2
    %6552 = torch.aten.transpose.int %6469, %int1_8031, %int2_8032 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %6552, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_8033 = torch.constant.int 1
    %int2_8034 = torch.constant.int 2
    %6553 = torch.aten.transpose.int %6544, %int1_8033, %int2_8034 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %6553, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_8035 = torch.constant.int 1
    %int2_8036 = torch.constant.int 2
    %6554 = torch.aten.transpose.int %6551, %int1_8035, %int2_8036 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %6554, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_8037 = torch.constant.float 0.000000e+00
    %true_8038 = torch.constant.bool true
    %none_8039 = torch.constant.none
    %none_8040 = torch.constant.none
    %6555:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6552, %6553, %6554, %float0.000000e00_8037, %true_8038, %none_8039, %none_8040) : (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?],f32>) 
    torch.bind_symbolic_shape %6555#0, [%292], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_8041 = torch.constant.int 1
    %int2_8042 = torch.constant.int 2
    %6556 = torch.aten.transpose.int %6555#0, %int1_8041, %int2_8042 : !torch.vtensor<[4,32,?,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %6556, [%292], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int4_8043 = torch.constant.int 4
    %int4096_8044 = torch.constant.int 4096
    %6557 = torch.prim.ListConstruct %int4_8043, %6454, %int4096_8044 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6558 = torch.aten.view %6556, %6557 : !torch.vtensor<[4,?,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %6558, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_8045 = torch.constant.int -2
    %int-1_8046 = torch.constant.int -1
    %6559 = torch.aten.transpose.int %284, %int-2_8045, %int-1_8046 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_8047 = torch.constant.int 4
    %6560 = torch.aten.mul.int %int4_8047, %6454 : !torch.int, !torch.int -> !torch.int
    %int4096_8048 = torch.constant.int 4096
    %6561 = torch.prim.ListConstruct %6560, %int4096_8048 : (!torch.int, !torch.int) -> !torch.list<int>
    %6562 = torch.aten.view %6558, %6561 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %6562, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %6563 = torch.aten.mm %6562, %6559 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %6563, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_8049 = torch.constant.int 4
    %int4096_8050 = torch.constant.int 4096
    %6564 = torch.prim.ListConstruct %int4_8049, %6454, %int4096_8050 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6565 = torch.aten.view %6563, %6564 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %6565, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_8051 = torch.constant.int 1
    %6566 = torch.aten.add.Tensor %6404, %6565, %int1_8051 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %6566, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_8052 = torch.constant.int 6
    %6567 = torch.prims.convert_element_type %6566, %int6_8052 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %6567, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_8053 = torch.constant.int 2
    %6568 = torch.aten.pow.Tensor_Scalar %6567, %int2_8053 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %6568, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_8054 = torch.constant.int -1
    %6569 = torch.prim.ListConstruct %int-1_8054 : (!torch.int) -> !torch.list<int>
    %true_8055 = torch.constant.bool true
    %none_8056 = torch.constant.none
    %6570 = torch.aten.mean.dim %6568, %6569, %true_8055, %none_8056 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6570, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_8057 = torch.constant.float 9.9999997473787516E-6
    %int1_8058 = torch.constant.int 1
    %6571 = torch.aten.add.Scalar %6570, %float9.999990e-06_8057, %int1_8058 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6571, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %6572 = torch.aten.rsqrt %6571 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6572, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %6573 = torch.aten.mul.Tensor %6567, %6572 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %6573, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_8059 = torch.constant.int 5
    %6574 = torch.prims.convert_element_type %6573, %int5_8059 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %6574, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %6575 = torch.aten.mul.Tensor %285, %6574 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %6575, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_8060 = torch.constant.int 5
    %6576 = torch.prims.convert_element_type %6575, %int5_8060 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %6576, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_8061 = torch.constant.int -2
    %int-1_8062 = torch.constant.int -1
    %6577 = torch.aten.transpose.int %286, %int-2_8061, %int-1_8062 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_8063 = torch.constant.int 4
    %6578 = torch.aten.mul.int %int4_8063, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_8064 = torch.constant.int 4096
    %6579 = torch.prim.ListConstruct %6578, %int4096_8064 : (!torch.int, !torch.int) -> !torch.list<int>
    %6580 = torch.aten.view %6576, %6579 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %6580, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %6581 = torch.aten.mm %6580, %6577 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %6581, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_8065 = torch.constant.int 4
    %int14336_8066 = torch.constant.int 14336
    %6582 = torch.prim.ListConstruct %int4_8065, %306, %int14336_8066 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6583 = torch.aten.view %6581, %6582 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %6583, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %6584 = torch.aten.silu %6583 : !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %6584, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_8067 = torch.constant.int -2
    %int-1_8068 = torch.constant.int -1
    %6585 = torch.aten.transpose.int %287, %int-2_8067, %int-1_8068 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_8069 = torch.constant.int 4
    %6586 = torch.aten.mul.int %int4_8069, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_8070 = torch.constant.int 4096
    %6587 = torch.prim.ListConstruct %6586, %int4096_8070 : (!torch.int, !torch.int) -> !torch.list<int>
    %6588 = torch.aten.view %6576, %6587 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %6588, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %6589 = torch.aten.mm %6588, %6585 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %6589, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %int4_8071 = torch.constant.int 4
    %int14336_8072 = torch.constant.int 14336
    %6590 = torch.prim.ListConstruct %int4_8071, %306, %int14336_8072 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6591 = torch.aten.view %6589, %6590 : !torch.vtensor<[?,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %6591, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %6592 = torch.aten.mul.Tensor %6584, %6591 : !torch.vtensor<[4,?,14336],f16>, !torch.vtensor<[4,?,14336],f16> -> !torch.vtensor<[4,?,14336],f16>
    torch.bind_symbolic_shape %6592, [%292], affine_map<()[s0] -> (4, s0 * 32, 14336)> : !torch.vtensor<[4,?,14336],f16>
    %int-2_8073 = torch.constant.int -2
    %int-1_8074 = torch.constant.int -1
    %6593 = torch.aten.transpose.int %288, %int-2_8073, %int-1_8074 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int1_8075 = torch.constant.int 1
    %6594 = torch.aten.size.int %6583, %int1_8075 : !torch.vtensor<[4,?,14336],f16>, !torch.int -> !torch.int
    %int4_8076 = torch.constant.int 4
    %6595 = torch.aten.mul.int %int4_8076, %6594 : !torch.int, !torch.int -> !torch.int
    %int14336_8077 = torch.constant.int 14336
    %6596 = torch.prim.ListConstruct %6595, %int14336_8077 : (!torch.int, !torch.int) -> !torch.list<int>
    %6597 = torch.aten.view %6592, %6596 : !torch.vtensor<[4,?,14336],f16>, !torch.list<int> -> !torch.vtensor<[?,14336],f16>
    torch.bind_symbolic_shape %6597, [%292], affine_map<()[s0] -> (s0 * 128, 14336)> : !torch.vtensor<[?,14336],f16>
    %6598 = torch.aten.mm %6597, %6593 : !torch.vtensor<[?,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %6598, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %int4_8078 = torch.constant.int 4
    %int4096_8079 = torch.constant.int 4096
    %6599 = torch.prim.ListConstruct %int4_8078, %6594, %int4096_8079 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6600 = torch.aten.view %6598, %6599 : !torch.vtensor<[?,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %6600, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int1_8080 = torch.constant.int 1
    %6601 = torch.aten.add.Tensor %6566, %6600, %int1_8080 : !torch.vtensor<[4,?,4096],f16>, !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %6601, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int6_8081 = torch.constant.int 6
    %6602 = torch.prims.convert_element_type %6601, %int6_8081 : !torch.vtensor<[4,?,4096],f16>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %6602, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int2_8082 = torch.constant.int 2
    %6603 = torch.aten.pow.Tensor_Scalar %6602, %int2_8082 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %6603, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int-1_8083 = torch.constant.int -1
    %6604 = torch.prim.ListConstruct %int-1_8083 : (!torch.int) -> !torch.list<int>
    %true_8084 = torch.constant.bool true
    %none_8085 = torch.constant.none
    %6605 = torch.aten.mean.dim %6603, %6604, %true_8084, %none_8085 : !torch.vtensor<[4,?,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6605, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %float9.999990e-06_8086 = torch.constant.float 9.9999997473787516E-6
    %int1_8087 = torch.constant.int 1
    %6606 = torch.aten.add.Scalar %6605, %float9.999990e-06_8086, %int1_8087 : !torch.vtensor<[4,?,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6606, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %6607 = torch.aten.rsqrt %6606 : !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,1],f32>
    torch.bind_symbolic_shape %6607, [%292], affine_map<()[s0] -> (4, s0 * 32, 1)> : !torch.vtensor<[4,?,1],f32>
    %6608 = torch.aten.mul.Tensor %6602, %6607 : !torch.vtensor<[4,?,4096],f32>, !torch.vtensor<[4,?,1],f32> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %6608, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_8088 = torch.constant.int 5
    %6609 = torch.prims.convert_element_type %6608, %int5_8088 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %6609, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %6610 = torch.aten.mul.Tensor %289, %6609 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,?,4096],f16> -> !torch.vtensor<[4,?,4096],f32>
    torch.bind_symbolic_shape %6610, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f32>
    %int5_8089 = torch.constant.int 5
    %6611 = torch.prims.convert_element_type %6610, %int5_8089 : !torch.vtensor<[4,?,4096],f32>, !torch.int -> !torch.vtensor<[4,?,4096],f16>
    torch.bind_symbolic_shape %6611, [%292], affine_map<()[s0] -> (4, s0 * 32, 4096)> : !torch.vtensor<[4,?,4096],f16>
    %int-2_8090 = torch.constant.int -2
    %int-1_8091 = torch.constant.int -1
    %6612 = torch.aten.transpose.int %290, %int-2_8090, %int-1_8091 : !torch.vtensor<[128256,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,128256],f16>
    %int4_8092 = torch.constant.int 4
    %6613 = torch.aten.mul.int %int4_8092, %306 : !torch.int, !torch.int -> !torch.int
    %int4096_8093 = torch.constant.int 4096
    %6614 = torch.prim.ListConstruct %6613, %int4096_8093 : (!torch.int, !torch.int) -> !torch.list<int>
    %6615 = torch.aten.view %6611, %6614 : !torch.vtensor<[4,?,4096],f16>, !torch.list<int> -> !torch.vtensor<[?,4096],f16>
    torch.bind_symbolic_shape %6615, [%292], affine_map<()[s0] -> (s0 * 128, 4096)> : !torch.vtensor<[?,4096],f16>
    %6616 = torch.aten.mm %6615, %6612 : !torch.vtensor<[?,4096],f16>, !torch.vtensor<[4096,128256],f16> -> !torch.vtensor<[?,128256],f16>
    torch.bind_symbolic_shape %6616, [%292], affine_map<()[s0] -> (s0 * 128, 128256)> : !torch.vtensor<[?,128256],f16>
    %int4_8094 = torch.constant.int 4
    %int128256 = torch.constant.int 128256
    %6617 = torch.prim.ListConstruct %int4_8094, %306, %int128256 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6618 = torch.aten.view %6616, %6617 : !torch.vtensor<[?,128256],f16>, !torch.list<int> -> !torch.vtensor<[4,?,128256],f16>
    torch.bind_symbolic_shape %6618, [%292], affine_map<()[s0] -> (4, s0 * 32, 128256)> : !torch.vtensor<[4,?,128256],f16>
    return %6618 : !torch.vtensor<[4,?,128256],f16>
  }
  func.func @decode_bs4(%arg0: !torch.vtensor<[4,1],si64>, %arg1: !torch.vtensor<[4],si64>, %arg2: !torch.vtensor<[4],si64>, %arg3: !torch.vtensor<[4,?],si64>, %arg4: !torch.tensor<[?,2097152],f16>) -> !torch.vtensor<[4,1,128256],f16> attributes {torch.assume_strict_symbolic_shapes} {
    %__auto.token_embd.weight = util.global.load @__auto.token_embd.weight : tensor<128256x4096xf16>
    %0 = torch_c.from_builtin_tensor %__auto.token_embd.weight : tensor<128256x4096xf16> -> !torch.vtensor<[128256,4096],f16>
    %__auto.blk.0.attn_norm.weight = util.global.load @__auto.blk.0.attn_norm.weight : tensor<4096xf32>
    %1 = torch_c.from_builtin_tensor %__auto.blk.0.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.0.attn_q.weight = util.global.load @__auto.blk.0.attn_q.weight : tensor<4096x4096xf16>
    %2 = torch_c.from_builtin_tensor %__auto.blk.0.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.0.attn_k.weight = util.global.load @__auto.blk.0.attn_k.weight : tensor<1024x4096xf16>
    %3 = torch_c.from_builtin_tensor %__auto.blk.0.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.0.attn_v.weight = util.global.load @__auto.blk.0.attn_v.weight : tensor<1024x4096xf16>
    %4 = torch_c.from_builtin_tensor %__auto.blk.0.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %5 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %6 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %__auto.blk.0.attn_output.weight = util.global.load @__auto.blk.0.attn_output.weight : tensor<4096x4096xf16>
    %7 = torch_c.from_builtin_tensor %__auto.blk.0.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.0.ffn_norm.weight = util.global.load @__auto.blk.0.ffn_norm.weight : tensor<4096xf32>
    %8 = torch_c.from_builtin_tensor %__auto.blk.0.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.0.ffn_gate.weight = util.global.load @__auto.blk.0.ffn_gate.weight : tensor<14336x4096xf16>
    %9 = torch_c.from_builtin_tensor %__auto.blk.0.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.0.ffn_up.weight = util.global.load @__auto.blk.0.ffn_up.weight : tensor<14336x4096xf16>
    %10 = torch_c.from_builtin_tensor %__auto.blk.0.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.0.ffn_down.weight = util.global.load @__auto.blk.0.ffn_down.weight : tensor<4096x14336xf16>
    %11 = torch_c.from_builtin_tensor %__auto.blk.0.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.1.attn_norm.weight = util.global.load @__auto.blk.1.attn_norm.weight : tensor<4096xf32>
    %12 = torch_c.from_builtin_tensor %__auto.blk.1.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.1.attn_q.weight = util.global.load @__auto.blk.1.attn_q.weight : tensor<4096x4096xf16>
    %13 = torch_c.from_builtin_tensor %__auto.blk.1.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.1.attn_k.weight = util.global.load @__auto.blk.1.attn_k.weight : tensor<1024x4096xf16>
    %14 = torch_c.from_builtin_tensor %__auto.blk.1.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.1.attn_v.weight = util.global.load @__auto.blk.1.attn_v.weight : tensor<1024x4096xf16>
    %15 = torch_c.from_builtin_tensor %__auto.blk.1.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %16 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %17 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %__auto.blk.1.attn_output.weight = util.global.load @__auto.blk.1.attn_output.weight : tensor<4096x4096xf16>
    %18 = torch_c.from_builtin_tensor %__auto.blk.1.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.1.ffn_norm.weight = util.global.load @__auto.blk.1.ffn_norm.weight : tensor<4096xf32>
    %19 = torch_c.from_builtin_tensor %__auto.blk.1.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.1.ffn_gate.weight = util.global.load @__auto.blk.1.ffn_gate.weight : tensor<14336x4096xf16>
    %20 = torch_c.from_builtin_tensor %__auto.blk.1.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.1.ffn_up.weight = util.global.load @__auto.blk.1.ffn_up.weight : tensor<14336x4096xf16>
    %21 = torch_c.from_builtin_tensor %__auto.blk.1.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.1.ffn_down.weight = util.global.load @__auto.blk.1.ffn_down.weight : tensor<4096x14336xf16>
    %22 = torch_c.from_builtin_tensor %__auto.blk.1.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.2.attn_norm.weight = util.global.load @__auto.blk.2.attn_norm.weight : tensor<4096xf32>
    %23 = torch_c.from_builtin_tensor %__auto.blk.2.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.2.attn_q.weight = util.global.load @__auto.blk.2.attn_q.weight : tensor<4096x4096xf16>
    %24 = torch_c.from_builtin_tensor %__auto.blk.2.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.2.attn_k.weight = util.global.load @__auto.blk.2.attn_k.weight : tensor<1024x4096xf16>
    %25 = torch_c.from_builtin_tensor %__auto.blk.2.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.2.attn_v.weight = util.global.load @__auto.blk.2.attn_v.weight : tensor<1024x4096xf16>
    %26 = torch_c.from_builtin_tensor %__auto.blk.2.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %27 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %28 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %__auto.blk.2.attn_output.weight = util.global.load @__auto.blk.2.attn_output.weight : tensor<4096x4096xf16>
    %29 = torch_c.from_builtin_tensor %__auto.blk.2.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.2.ffn_norm.weight = util.global.load @__auto.blk.2.ffn_norm.weight : tensor<4096xf32>
    %30 = torch_c.from_builtin_tensor %__auto.blk.2.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.2.ffn_gate.weight = util.global.load @__auto.blk.2.ffn_gate.weight : tensor<14336x4096xf16>
    %31 = torch_c.from_builtin_tensor %__auto.blk.2.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.2.ffn_up.weight = util.global.load @__auto.blk.2.ffn_up.weight : tensor<14336x4096xf16>
    %32 = torch_c.from_builtin_tensor %__auto.blk.2.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.2.ffn_down.weight = util.global.load @__auto.blk.2.ffn_down.weight : tensor<4096x14336xf16>
    %33 = torch_c.from_builtin_tensor %__auto.blk.2.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.3.attn_norm.weight = util.global.load @__auto.blk.3.attn_norm.weight : tensor<4096xf32>
    %34 = torch_c.from_builtin_tensor %__auto.blk.3.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.3.attn_q.weight = util.global.load @__auto.blk.3.attn_q.weight : tensor<4096x4096xf16>
    %35 = torch_c.from_builtin_tensor %__auto.blk.3.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.3.attn_k.weight = util.global.load @__auto.blk.3.attn_k.weight : tensor<1024x4096xf16>
    %36 = torch_c.from_builtin_tensor %__auto.blk.3.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.3.attn_v.weight = util.global.load @__auto.blk.3.attn_v.weight : tensor<1024x4096xf16>
    %37 = torch_c.from_builtin_tensor %__auto.blk.3.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %38 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %39 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %__auto.blk.3.attn_output.weight = util.global.load @__auto.blk.3.attn_output.weight : tensor<4096x4096xf16>
    %40 = torch_c.from_builtin_tensor %__auto.blk.3.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.3.ffn_norm.weight = util.global.load @__auto.blk.3.ffn_norm.weight : tensor<4096xf32>
    %41 = torch_c.from_builtin_tensor %__auto.blk.3.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.3.ffn_gate.weight = util.global.load @__auto.blk.3.ffn_gate.weight : tensor<14336x4096xf16>
    %42 = torch_c.from_builtin_tensor %__auto.blk.3.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.3.ffn_up.weight = util.global.load @__auto.blk.3.ffn_up.weight : tensor<14336x4096xf16>
    %43 = torch_c.from_builtin_tensor %__auto.blk.3.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.3.ffn_down.weight = util.global.load @__auto.blk.3.ffn_down.weight : tensor<4096x14336xf16>
    %44 = torch_c.from_builtin_tensor %__auto.blk.3.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.4.attn_norm.weight = util.global.load @__auto.blk.4.attn_norm.weight : tensor<4096xf32>
    %45 = torch_c.from_builtin_tensor %__auto.blk.4.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.4.attn_q.weight = util.global.load @__auto.blk.4.attn_q.weight : tensor<4096x4096xf16>
    %46 = torch_c.from_builtin_tensor %__auto.blk.4.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.4.attn_k.weight = util.global.load @__auto.blk.4.attn_k.weight : tensor<1024x4096xf16>
    %47 = torch_c.from_builtin_tensor %__auto.blk.4.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.4.attn_v.weight = util.global.load @__auto.blk.4.attn_v.weight : tensor<1024x4096xf16>
    %48 = torch_c.from_builtin_tensor %__auto.blk.4.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %49 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %50 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %__auto.blk.4.attn_output.weight = util.global.load @__auto.blk.4.attn_output.weight : tensor<4096x4096xf16>
    %51 = torch_c.from_builtin_tensor %__auto.blk.4.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.4.ffn_norm.weight = util.global.load @__auto.blk.4.ffn_norm.weight : tensor<4096xf32>
    %52 = torch_c.from_builtin_tensor %__auto.blk.4.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.4.ffn_gate.weight = util.global.load @__auto.blk.4.ffn_gate.weight : tensor<14336x4096xf16>
    %53 = torch_c.from_builtin_tensor %__auto.blk.4.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.4.ffn_up.weight = util.global.load @__auto.blk.4.ffn_up.weight : tensor<14336x4096xf16>
    %54 = torch_c.from_builtin_tensor %__auto.blk.4.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.4.ffn_down.weight = util.global.load @__auto.blk.4.ffn_down.weight : tensor<4096x14336xf16>
    %55 = torch_c.from_builtin_tensor %__auto.blk.4.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.5.attn_norm.weight = util.global.load @__auto.blk.5.attn_norm.weight : tensor<4096xf32>
    %56 = torch_c.from_builtin_tensor %__auto.blk.5.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.5.attn_q.weight = util.global.load @__auto.blk.5.attn_q.weight : tensor<4096x4096xf16>
    %57 = torch_c.from_builtin_tensor %__auto.blk.5.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.5.attn_k.weight = util.global.load @__auto.blk.5.attn_k.weight : tensor<1024x4096xf16>
    %58 = torch_c.from_builtin_tensor %__auto.blk.5.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.5.attn_v.weight = util.global.load @__auto.blk.5.attn_v.weight : tensor<1024x4096xf16>
    %59 = torch_c.from_builtin_tensor %__auto.blk.5.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %60 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %61 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %__auto.blk.5.attn_output.weight = util.global.load @__auto.blk.5.attn_output.weight : tensor<4096x4096xf16>
    %62 = torch_c.from_builtin_tensor %__auto.blk.5.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.5.ffn_norm.weight = util.global.load @__auto.blk.5.ffn_norm.weight : tensor<4096xf32>
    %63 = torch_c.from_builtin_tensor %__auto.blk.5.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.5.ffn_gate.weight = util.global.load @__auto.blk.5.ffn_gate.weight : tensor<14336x4096xf16>
    %64 = torch_c.from_builtin_tensor %__auto.blk.5.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.5.ffn_up.weight = util.global.load @__auto.blk.5.ffn_up.weight : tensor<14336x4096xf16>
    %65 = torch_c.from_builtin_tensor %__auto.blk.5.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.5.ffn_down.weight = util.global.load @__auto.blk.5.ffn_down.weight : tensor<4096x14336xf16>
    %66 = torch_c.from_builtin_tensor %__auto.blk.5.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.6.attn_norm.weight = util.global.load @__auto.blk.6.attn_norm.weight : tensor<4096xf32>
    %67 = torch_c.from_builtin_tensor %__auto.blk.6.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.6.attn_q.weight = util.global.load @__auto.blk.6.attn_q.weight : tensor<4096x4096xf16>
    %68 = torch_c.from_builtin_tensor %__auto.blk.6.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.6.attn_k.weight = util.global.load @__auto.blk.6.attn_k.weight : tensor<1024x4096xf16>
    %69 = torch_c.from_builtin_tensor %__auto.blk.6.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.6.attn_v.weight = util.global.load @__auto.blk.6.attn_v.weight : tensor<1024x4096xf16>
    %70 = torch_c.from_builtin_tensor %__auto.blk.6.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %71 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %72 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %__auto.blk.6.attn_output.weight = util.global.load @__auto.blk.6.attn_output.weight : tensor<4096x4096xf16>
    %73 = torch_c.from_builtin_tensor %__auto.blk.6.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.6.ffn_norm.weight = util.global.load @__auto.blk.6.ffn_norm.weight : tensor<4096xf32>
    %74 = torch_c.from_builtin_tensor %__auto.blk.6.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.6.ffn_gate.weight = util.global.load @__auto.blk.6.ffn_gate.weight : tensor<14336x4096xf16>
    %75 = torch_c.from_builtin_tensor %__auto.blk.6.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.6.ffn_up.weight = util.global.load @__auto.blk.6.ffn_up.weight : tensor<14336x4096xf16>
    %76 = torch_c.from_builtin_tensor %__auto.blk.6.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.6.ffn_down.weight = util.global.load @__auto.blk.6.ffn_down.weight : tensor<4096x14336xf16>
    %77 = torch_c.from_builtin_tensor %__auto.blk.6.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.7.attn_norm.weight = util.global.load @__auto.blk.7.attn_norm.weight : tensor<4096xf32>
    %78 = torch_c.from_builtin_tensor %__auto.blk.7.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.7.attn_q.weight = util.global.load @__auto.blk.7.attn_q.weight : tensor<4096x4096xf16>
    %79 = torch_c.from_builtin_tensor %__auto.blk.7.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.7.attn_k.weight = util.global.load @__auto.blk.7.attn_k.weight : tensor<1024x4096xf16>
    %80 = torch_c.from_builtin_tensor %__auto.blk.7.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.7.attn_v.weight = util.global.load @__auto.blk.7.attn_v.weight : tensor<1024x4096xf16>
    %81 = torch_c.from_builtin_tensor %__auto.blk.7.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %82 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %83 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %__auto.blk.7.attn_output.weight = util.global.load @__auto.blk.7.attn_output.weight : tensor<4096x4096xf16>
    %84 = torch_c.from_builtin_tensor %__auto.blk.7.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.7.ffn_norm.weight = util.global.load @__auto.blk.7.ffn_norm.weight : tensor<4096xf32>
    %85 = torch_c.from_builtin_tensor %__auto.blk.7.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.7.ffn_gate.weight = util.global.load @__auto.blk.7.ffn_gate.weight : tensor<14336x4096xf16>
    %86 = torch_c.from_builtin_tensor %__auto.blk.7.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.7.ffn_up.weight = util.global.load @__auto.blk.7.ffn_up.weight : tensor<14336x4096xf16>
    %87 = torch_c.from_builtin_tensor %__auto.blk.7.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.7.ffn_down.weight = util.global.load @__auto.blk.7.ffn_down.weight : tensor<4096x14336xf16>
    %88 = torch_c.from_builtin_tensor %__auto.blk.7.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.8.attn_norm.weight = util.global.load @__auto.blk.8.attn_norm.weight : tensor<4096xf32>
    %89 = torch_c.from_builtin_tensor %__auto.blk.8.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.8.attn_q.weight = util.global.load @__auto.blk.8.attn_q.weight : tensor<4096x4096xf16>
    %90 = torch_c.from_builtin_tensor %__auto.blk.8.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.8.attn_k.weight = util.global.load @__auto.blk.8.attn_k.weight : tensor<1024x4096xf16>
    %91 = torch_c.from_builtin_tensor %__auto.blk.8.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.8.attn_v.weight = util.global.load @__auto.blk.8.attn_v.weight : tensor<1024x4096xf16>
    %92 = torch_c.from_builtin_tensor %__auto.blk.8.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %93 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %94 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %__auto.blk.8.attn_output.weight = util.global.load @__auto.blk.8.attn_output.weight : tensor<4096x4096xf16>
    %95 = torch_c.from_builtin_tensor %__auto.blk.8.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.8.ffn_norm.weight = util.global.load @__auto.blk.8.ffn_norm.weight : tensor<4096xf32>
    %96 = torch_c.from_builtin_tensor %__auto.blk.8.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.8.ffn_gate.weight = util.global.load @__auto.blk.8.ffn_gate.weight : tensor<14336x4096xf16>
    %97 = torch_c.from_builtin_tensor %__auto.blk.8.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.8.ffn_up.weight = util.global.load @__auto.blk.8.ffn_up.weight : tensor<14336x4096xf16>
    %98 = torch_c.from_builtin_tensor %__auto.blk.8.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.8.ffn_down.weight = util.global.load @__auto.blk.8.ffn_down.weight : tensor<4096x14336xf16>
    %99 = torch_c.from_builtin_tensor %__auto.blk.8.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.9.attn_norm.weight = util.global.load @__auto.blk.9.attn_norm.weight : tensor<4096xf32>
    %100 = torch_c.from_builtin_tensor %__auto.blk.9.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.9.attn_q.weight = util.global.load @__auto.blk.9.attn_q.weight : tensor<4096x4096xf16>
    %101 = torch_c.from_builtin_tensor %__auto.blk.9.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.9.attn_k.weight = util.global.load @__auto.blk.9.attn_k.weight : tensor<1024x4096xf16>
    %102 = torch_c.from_builtin_tensor %__auto.blk.9.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.9.attn_v.weight = util.global.load @__auto.blk.9.attn_v.weight : tensor<1024x4096xf16>
    %103 = torch_c.from_builtin_tensor %__auto.blk.9.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %104 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %105 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %__auto.blk.9.attn_output.weight = util.global.load @__auto.blk.9.attn_output.weight : tensor<4096x4096xf16>
    %106 = torch_c.from_builtin_tensor %__auto.blk.9.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.9.ffn_norm.weight = util.global.load @__auto.blk.9.ffn_norm.weight : tensor<4096xf32>
    %107 = torch_c.from_builtin_tensor %__auto.blk.9.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.9.ffn_gate.weight = util.global.load @__auto.blk.9.ffn_gate.weight : tensor<14336x4096xf16>
    %108 = torch_c.from_builtin_tensor %__auto.blk.9.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.9.ffn_up.weight = util.global.load @__auto.blk.9.ffn_up.weight : tensor<14336x4096xf16>
    %109 = torch_c.from_builtin_tensor %__auto.blk.9.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.9.ffn_down.weight = util.global.load @__auto.blk.9.ffn_down.weight : tensor<4096x14336xf16>
    %110 = torch_c.from_builtin_tensor %__auto.blk.9.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.10.attn_norm.weight = util.global.load @__auto.blk.10.attn_norm.weight : tensor<4096xf32>
    %111 = torch_c.from_builtin_tensor %__auto.blk.10.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.10.attn_q.weight = util.global.load @__auto.blk.10.attn_q.weight : tensor<4096x4096xf16>
    %112 = torch_c.from_builtin_tensor %__auto.blk.10.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.10.attn_k.weight = util.global.load @__auto.blk.10.attn_k.weight : tensor<1024x4096xf16>
    %113 = torch_c.from_builtin_tensor %__auto.blk.10.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.10.attn_v.weight = util.global.load @__auto.blk.10.attn_v.weight : tensor<1024x4096xf16>
    %114 = torch_c.from_builtin_tensor %__auto.blk.10.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %115 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %116 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %__auto.blk.10.attn_output.weight = util.global.load @__auto.blk.10.attn_output.weight : tensor<4096x4096xf16>
    %117 = torch_c.from_builtin_tensor %__auto.blk.10.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.10.ffn_norm.weight = util.global.load @__auto.blk.10.ffn_norm.weight : tensor<4096xf32>
    %118 = torch_c.from_builtin_tensor %__auto.blk.10.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.10.ffn_gate.weight = util.global.load @__auto.blk.10.ffn_gate.weight : tensor<14336x4096xf16>
    %119 = torch_c.from_builtin_tensor %__auto.blk.10.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.10.ffn_up.weight = util.global.load @__auto.blk.10.ffn_up.weight : tensor<14336x4096xf16>
    %120 = torch_c.from_builtin_tensor %__auto.blk.10.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.10.ffn_down.weight = util.global.load @__auto.blk.10.ffn_down.weight : tensor<4096x14336xf16>
    %121 = torch_c.from_builtin_tensor %__auto.blk.10.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.11.attn_norm.weight = util.global.load @__auto.blk.11.attn_norm.weight : tensor<4096xf32>
    %122 = torch_c.from_builtin_tensor %__auto.blk.11.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.11.attn_q.weight = util.global.load @__auto.blk.11.attn_q.weight : tensor<4096x4096xf16>
    %123 = torch_c.from_builtin_tensor %__auto.blk.11.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.11.attn_k.weight = util.global.load @__auto.blk.11.attn_k.weight : tensor<1024x4096xf16>
    %124 = torch_c.from_builtin_tensor %__auto.blk.11.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.11.attn_v.weight = util.global.load @__auto.blk.11.attn_v.weight : tensor<1024x4096xf16>
    %125 = torch_c.from_builtin_tensor %__auto.blk.11.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %126 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %127 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %__auto.blk.11.attn_output.weight = util.global.load @__auto.blk.11.attn_output.weight : tensor<4096x4096xf16>
    %128 = torch_c.from_builtin_tensor %__auto.blk.11.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.11.ffn_norm.weight = util.global.load @__auto.blk.11.ffn_norm.weight : tensor<4096xf32>
    %129 = torch_c.from_builtin_tensor %__auto.blk.11.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.11.ffn_gate.weight = util.global.load @__auto.blk.11.ffn_gate.weight : tensor<14336x4096xf16>
    %130 = torch_c.from_builtin_tensor %__auto.blk.11.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.11.ffn_up.weight = util.global.load @__auto.blk.11.ffn_up.weight : tensor<14336x4096xf16>
    %131 = torch_c.from_builtin_tensor %__auto.blk.11.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.11.ffn_down.weight = util.global.load @__auto.blk.11.ffn_down.weight : tensor<4096x14336xf16>
    %132 = torch_c.from_builtin_tensor %__auto.blk.11.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.12.attn_norm.weight = util.global.load @__auto.blk.12.attn_norm.weight : tensor<4096xf32>
    %133 = torch_c.from_builtin_tensor %__auto.blk.12.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.12.attn_q.weight = util.global.load @__auto.blk.12.attn_q.weight : tensor<4096x4096xf16>
    %134 = torch_c.from_builtin_tensor %__auto.blk.12.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.12.attn_k.weight = util.global.load @__auto.blk.12.attn_k.weight : tensor<1024x4096xf16>
    %135 = torch_c.from_builtin_tensor %__auto.blk.12.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.12.attn_v.weight = util.global.load @__auto.blk.12.attn_v.weight : tensor<1024x4096xf16>
    %136 = torch_c.from_builtin_tensor %__auto.blk.12.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %137 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %138 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %__auto.blk.12.attn_output.weight = util.global.load @__auto.blk.12.attn_output.weight : tensor<4096x4096xf16>
    %139 = torch_c.from_builtin_tensor %__auto.blk.12.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.12.ffn_norm.weight = util.global.load @__auto.blk.12.ffn_norm.weight : tensor<4096xf32>
    %140 = torch_c.from_builtin_tensor %__auto.blk.12.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.12.ffn_gate.weight = util.global.load @__auto.blk.12.ffn_gate.weight : tensor<14336x4096xf16>
    %141 = torch_c.from_builtin_tensor %__auto.blk.12.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.12.ffn_up.weight = util.global.load @__auto.blk.12.ffn_up.weight : tensor<14336x4096xf16>
    %142 = torch_c.from_builtin_tensor %__auto.blk.12.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.12.ffn_down.weight = util.global.load @__auto.blk.12.ffn_down.weight : tensor<4096x14336xf16>
    %143 = torch_c.from_builtin_tensor %__auto.blk.12.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.13.attn_norm.weight = util.global.load @__auto.blk.13.attn_norm.weight : tensor<4096xf32>
    %144 = torch_c.from_builtin_tensor %__auto.blk.13.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.13.attn_q.weight = util.global.load @__auto.blk.13.attn_q.weight : tensor<4096x4096xf16>
    %145 = torch_c.from_builtin_tensor %__auto.blk.13.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.13.attn_k.weight = util.global.load @__auto.blk.13.attn_k.weight : tensor<1024x4096xf16>
    %146 = torch_c.from_builtin_tensor %__auto.blk.13.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.13.attn_v.weight = util.global.load @__auto.blk.13.attn_v.weight : tensor<1024x4096xf16>
    %147 = torch_c.from_builtin_tensor %__auto.blk.13.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %148 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %149 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %__auto.blk.13.attn_output.weight = util.global.load @__auto.blk.13.attn_output.weight : tensor<4096x4096xf16>
    %150 = torch_c.from_builtin_tensor %__auto.blk.13.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.13.ffn_norm.weight = util.global.load @__auto.blk.13.ffn_norm.weight : tensor<4096xf32>
    %151 = torch_c.from_builtin_tensor %__auto.blk.13.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.13.ffn_gate.weight = util.global.load @__auto.blk.13.ffn_gate.weight : tensor<14336x4096xf16>
    %152 = torch_c.from_builtin_tensor %__auto.blk.13.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.13.ffn_up.weight = util.global.load @__auto.blk.13.ffn_up.weight : tensor<14336x4096xf16>
    %153 = torch_c.from_builtin_tensor %__auto.blk.13.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.13.ffn_down.weight = util.global.load @__auto.blk.13.ffn_down.weight : tensor<4096x14336xf16>
    %154 = torch_c.from_builtin_tensor %__auto.blk.13.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.14.attn_norm.weight = util.global.load @__auto.blk.14.attn_norm.weight : tensor<4096xf32>
    %155 = torch_c.from_builtin_tensor %__auto.blk.14.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.14.attn_q.weight = util.global.load @__auto.blk.14.attn_q.weight : tensor<4096x4096xf16>
    %156 = torch_c.from_builtin_tensor %__auto.blk.14.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.14.attn_k.weight = util.global.load @__auto.blk.14.attn_k.weight : tensor<1024x4096xf16>
    %157 = torch_c.from_builtin_tensor %__auto.blk.14.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.14.attn_v.weight = util.global.load @__auto.blk.14.attn_v.weight : tensor<1024x4096xf16>
    %158 = torch_c.from_builtin_tensor %__auto.blk.14.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %159 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %160 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %__auto.blk.14.attn_output.weight = util.global.load @__auto.blk.14.attn_output.weight : tensor<4096x4096xf16>
    %161 = torch_c.from_builtin_tensor %__auto.blk.14.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.14.ffn_norm.weight = util.global.load @__auto.blk.14.ffn_norm.weight : tensor<4096xf32>
    %162 = torch_c.from_builtin_tensor %__auto.blk.14.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.14.ffn_gate.weight = util.global.load @__auto.blk.14.ffn_gate.weight : tensor<14336x4096xf16>
    %163 = torch_c.from_builtin_tensor %__auto.blk.14.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.14.ffn_up.weight = util.global.load @__auto.blk.14.ffn_up.weight : tensor<14336x4096xf16>
    %164 = torch_c.from_builtin_tensor %__auto.blk.14.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.14.ffn_down.weight = util.global.load @__auto.blk.14.ffn_down.weight : tensor<4096x14336xf16>
    %165 = torch_c.from_builtin_tensor %__auto.blk.14.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.15.attn_norm.weight = util.global.load @__auto.blk.15.attn_norm.weight : tensor<4096xf32>
    %166 = torch_c.from_builtin_tensor %__auto.blk.15.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.15.attn_q.weight = util.global.load @__auto.blk.15.attn_q.weight : tensor<4096x4096xf16>
    %167 = torch_c.from_builtin_tensor %__auto.blk.15.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.15.attn_k.weight = util.global.load @__auto.blk.15.attn_k.weight : tensor<1024x4096xf16>
    %168 = torch_c.from_builtin_tensor %__auto.blk.15.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.15.attn_v.weight = util.global.load @__auto.blk.15.attn_v.weight : tensor<1024x4096xf16>
    %169 = torch_c.from_builtin_tensor %__auto.blk.15.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %170 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %171 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %__auto.blk.15.attn_output.weight = util.global.load @__auto.blk.15.attn_output.weight : tensor<4096x4096xf16>
    %172 = torch_c.from_builtin_tensor %__auto.blk.15.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.15.ffn_norm.weight = util.global.load @__auto.blk.15.ffn_norm.weight : tensor<4096xf32>
    %173 = torch_c.from_builtin_tensor %__auto.blk.15.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.15.ffn_gate.weight = util.global.load @__auto.blk.15.ffn_gate.weight : tensor<14336x4096xf16>
    %174 = torch_c.from_builtin_tensor %__auto.blk.15.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.15.ffn_up.weight = util.global.load @__auto.blk.15.ffn_up.weight : tensor<14336x4096xf16>
    %175 = torch_c.from_builtin_tensor %__auto.blk.15.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.15.ffn_down.weight = util.global.load @__auto.blk.15.ffn_down.weight : tensor<4096x14336xf16>
    %176 = torch_c.from_builtin_tensor %__auto.blk.15.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.16.attn_norm.weight = util.global.load @__auto.blk.16.attn_norm.weight : tensor<4096xf32>
    %177 = torch_c.from_builtin_tensor %__auto.blk.16.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.16.attn_q.weight = util.global.load @__auto.blk.16.attn_q.weight : tensor<4096x4096xf16>
    %178 = torch_c.from_builtin_tensor %__auto.blk.16.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.16.attn_k.weight = util.global.load @__auto.blk.16.attn_k.weight : tensor<1024x4096xf16>
    %179 = torch_c.from_builtin_tensor %__auto.blk.16.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.16.attn_v.weight = util.global.load @__auto.blk.16.attn_v.weight : tensor<1024x4096xf16>
    %180 = torch_c.from_builtin_tensor %__auto.blk.16.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %181 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %182 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %__auto.blk.16.attn_output.weight = util.global.load @__auto.blk.16.attn_output.weight : tensor<4096x4096xf16>
    %183 = torch_c.from_builtin_tensor %__auto.blk.16.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.16.ffn_norm.weight = util.global.load @__auto.blk.16.ffn_norm.weight : tensor<4096xf32>
    %184 = torch_c.from_builtin_tensor %__auto.blk.16.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.16.ffn_gate.weight = util.global.load @__auto.blk.16.ffn_gate.weight : tensor<14336x4096xf16>
    %185 = torch_c.from_builtin_tensor %__auto.blk.16.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.16.ffn_up.weight = util.global.load @__auto.blk.16.ffn_up.weight : tensor<14336x4096xf16>
    %186 = torch_c.from_builtin_tensor %__auto.blk.16.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.16.ffn_down.weight = util.global.load @__auto.blk.16.ffn_down.weight : tensor<4096x14336xf16>
    %187 = torch_c.from_builtin_tensor %__auto.blk.16.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.17.attn_norm.weight = util.global.load @__auto.blk.17.attn_norm.weight : tensor<4096xf32>
    %188 = torch_c.from_builtin_tensor %__auto.blk.17.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.17.attn_q.weight = util.global.load @__auto.blk.17.attn_q.weight : tensor<4096x4096xf16>
    %189 = torch_c.from_builtin_tensor %__auto.blk.17.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.17.attn_k.weight = util.global.load @__auto.blk.17.attn_k.weight : tensor<1024x4096xf16>
    %190 = torch_c.from_builtin_tensor %__auto.blk.17.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.17.attn_v.weight = util.global.load @__auto.blk.17.attn_v.weight : tensor<1024x4096xf16>
    %191 = torch_c.from_builtin_tensor %__auto.blk.17.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %192 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %193 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %__auto.blk.17.attn_output.weight = util.global.load @__auto.blk.17.attn_output.weight : tensor<4096x4096xf16>
    %194 = torch_c.from_builtin_tensor %__auto.blk.17.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.17.ffn_norm.weight = util.global.load @__auto.blk.17.ffn_norm.weight : tensor<4096xf32>
    %195 = torch_c.from_builtin_tensor %__auto.blk.17.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.17.ffn_gate.weight = util.global.load @__auto.blk.17.ffn_gate.weight : tensor<14336x4096xf16>
    %196 = torch_c.from_builtin_tensor %__auto.blk.17.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.17.ffn_up.weight = util.global.load @__auto.blk.17.ffn_up.weight : tensor<14336x4096xf16>
    %197 = torch_c.from_builtin_tensor %__auto.blk.17.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.17.ffn_down.weight = util.global.load @__auto.blk.17.ffn_down.weight : tensor<4096x14336xf16>
    %198 = torch_c.from_builtin_tensor %__auto.blk.17.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.18.attn_norm.weight = util.global.load @__auto.blk.18.attn_norm.weight : tensor<4096xf32>
    %199 = torch_c.from_builtin_tensor %__auto.blk.18.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.18.attn_q.weight = util.global.load @__auto.blk.18.attn_q.weight : tensor<4096x4096xf16>
    %200 = torch_c.from_builtin_tensor %__auto.blk.18.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.18.attn_k.weight = util.global.load @__auto.blk.18.attn_k.weight : tensor<1024x4096xf16>
    %201 = torch_c.from_builtin_tensor %__auto.blk.18.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.18.attn_v.weight = util.global.load @__auto.blk.18.attn_v.weight : tensor<1024x4096xf16>
    %202 = torch_c.from_builtin_tensor %__auto.blk.18.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %203 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %204 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %__auto.blk.18.attn_output.weight = util.global.load @__auto.blk.18.attn_output.weight : tensor<4096x4096xf16>
    %205 = torch_c.from_builtin_tensor %__auto.blk.18.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.18.ffn_norm.weight = util.global.load @__auto.blk.18.ffn_norm.weight : tensor<4096xf32>
    %206 = torch_c.from_builtin_tensor %__auto.blk.18.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.18.ffn_gate.weight = util.global.load @__auto.blk.18.ffn_gate.weight : tensor<14336x4096xf16>
    %207 = torch_c.from_builtin_tensor %__auto.blk.18.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.18.ffn_up.weight = util.global.load @__auto.blk.18.ffn_up.weight : tensor<14336x4096xf16>
    %208 = torch_c.from_builtin_tensor %__auto.blk.18.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.18.ffn_down.weight = util.global.load @__auto.blk.18.ffn_down.weight : tensor<4096x14336xf16>
    %209 = torch_c.from_builtin_tensor %__auto.blk.18.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.19.attn_norm.weight = util.global.load @__auto.blk.19.attn_norm.weight : tensor<4096xf32>
    %210 = torch_c.from_builtin_tensor %__auto.blk.19.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.19.attn_q.weight = util.global.load @__auto.blk.19.attn_q.weight : tensor<4096x4096xf16>
    %211 = torch_c.from_builtin_tensor %__auto.blk.19.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.19.attn_k.weight = util.global.load @__auto.blk.19.attn_k.weight : tensor<1024x4096xf16>
    %212 = torch_c.from_builtin_tensor %__auto.blk.19.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.19.attn_v.weight = util.global.load @__auto.blk.19.attn_v.weight : tensor<1024x4096xf16>
    %213 = torch_c.from_builtin_tensor %__auto.blk.19.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %214 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %215 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %__auto.blk.19.attn_output.weight = util.global.load @__auto.blk.19.attn_output.weight : tensor<4096x4096xf16>
    %216 = torch_c.from_builtin_tensor %__auto.blk.19.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.19.ffn_norm.weight = util.global.load @__auto.blk.19.ffn_norm.weight : tensor<4096xf32>
    %217 = torch_c.from_builtin_tensor %__auto.blk.19.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.19.ffn_gate.weight = util.global.load @__auto.blk.19.ffn_gate.weight : tensor<14336x4096xf16>
    %218 = torch_c.from_builtin_tensor %__auto.blk.19.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.19.ffn_up.weight = util.global.load @__auto.blk.19.ffn_up.weight : tensor<14336x4096xf16>
    %219 = torch_c.from_builtin_tensor %__auto.blk.19.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.19.ffn_down.weight = util.global.load @__auto.blk.19.ffn_down.weight : tensor<4096x14336xf16>
    %220 = torch_c.from_builtin_tensor %__auto.blk.19.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.20.attn_norm.weight = util.global.load @__auto.blk.20.attn_norm.weight : tensor<4096xf32>
    %221 = torch_c.from_builtin_tensor %__auto.blk.20.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.20.attn_q.weight = util.global.load @__auto.blk.20.attn_q.weight : tensor<4096x4096xf16>
    %222 = torch_c.from_builtin_tensor %__auto.blk.20.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.20.attn_k.weight = util.global.load @__auto.blk.20.attn_k.weight : tensor<1024x4096xf16>
    %223 = torch_c.from_builtin_tensor %__auto.blk.20.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.20.attn_v.weight = util.global.load @__auto.blk.20.attn_v.weight : tensor<1024x4096xf16>
    %224 = torch_c.from_builtin_tensor %__auto.blk.20.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %225 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %226 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %__auto.blk.20.attn_output.weight = util.global.load @__auto.blk.20.attn_output.weight : tensor<4096x4096xf16>
    %227 = torch_c.from_builtin_tensor %__auto.blk.20.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.20.ffn_norm.weight = util.global.load @__auto.blk.20.ffn_norm.weight : tensor<4096xf32>
    %228 = torch_c.from_builtin_tensor %__auto.blk.20.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.20.ffn_gate.weight = util.global.load @__auto.blk.20.ffn_gate.weight : tensor<14336x4096xf16>
    %229 = torch_c.from_builtin_tensor %__auto.blk.20.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.20.ffn_up.weight = util.global.load @__auto.blk.20.ffn_up.weight : tensor<14336x4096xf16>
    %230 = torch_c.from_builtin_tensor %__auto.blk.20.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.20.ffn_down.weight = util.global.load @__auto.blk.20.ffn_down.weight : tensor<4096x14336xf16>
    %231 = torch_c.from_builtin_tensor %__auto.blk.20.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.21.attn_norm.weight = util.global.load @__auto.blk.21.attn_norm.weight : tensor<4096xf32>
    %232 = torch_c.from_builtin_tensor %__auto.blk.21.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.21.attn_q.weight = util.global.load @__auto.blk.21.attn_q.weight : tensor<4096x4096xf16>
    %233 = torch_c.from_builtin_tensor %__auto.blk.21.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.21.attn_k.weight = util.global.load @__auto.blk.21.attn_k.weight : tensor<1024x4096xf16>
    %234 = torch_c.from_builtin_tensor %__auto.blk.21.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.21.attn_v.weight = util.global.load @__auto.blk.21.attn_v.weight : tensor<1024x4096xf16>
    %235 = torch_c.from_builtin_tensor %__auto.blk.21.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %236 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %237 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %__auto.blk.21.attn_output.weight = util.global.load @__auto.blk.21.attn_output.weight : tensor<4096x4096xf16>
    %238 = torch_c.from_builtin_tensor %__auto.blk.21.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.21.ffn_norm.weight = util.global.load @__auto.blk.21.ffn_norm.weight : tensor<4096xf32>
    %239 = torch_c.from_builtin_tensor %__auto.blk.21.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.21.ffn_gate.weight = util.global.load @__auto.blk.21.ffn_gate.weight : tensor<14336x4096xf16>
    %240 = torch_c.from_builtin_tensor %__auto.blk.21.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.21.ffn_up.weight = util.global.load @__auto.blk.21.ffn_up.weight : tensor<14336x4096xf16>
    %241 = torch_c.from_builtin_tensor %__auto.blk.21.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.21.ffn_down.weight = util.global.load @__auto.blk.21.ffn_down.weight : tensor<4096x14336xf16>
    %242 = torch_c.from_builtin_tensor %__auto.blk.21.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.22.attn_norm.weight = util.global.load @__auto.blk.22.attn_norm.weight : tensor<4096xf32>
    %243 = torch_c.from_builtin_tensor %__auto.blk.22.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.22.attn_q.weight = util.global.load @__auto.blk.22.attn_q.weight : tensor<4096x4096xf16>
    %244 = torch_c.from_builtin_tensor %__auto.blk.22.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.22.attn_k.weight = util.global.load @__auto.blk.22.attn_k.weight : tensor<1024x4096xf16>
    %245 = torch_c.from_builtin_tensor %__auto.blk.22.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.22.attn_v.weight = util.global.load @__auto.blk.22.attn_v.weight : tensor<1024x4096xf16>
    %246 = torch_c.from_builtin_tensor %__auto.blk.22.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %247 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %248 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %__auto.blk.22.attn_output.weight = util.global.load @__auto.blk.22.attn_output.weight : tensor<4096x4096xf16>
    %249 = torch_c.from_builtin_tensor %__auto.blk.22.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.22.ffn_norm.weight = util.global.load @__auto.blk.22.ffn_norm.weight : tensor<4096xf32>
    %250 = torch_c.from_builtin_tensor %__auto.blk.22.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.22.ffn_gate.weight = util.global.load @__auto.blk.22.ffn_gate.weight : tensor<14336x4096xf16>
    %251 = torch_c.from_builtin_tensor %__auto.blk.22.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.22.ffn_up.weight = util.global.load @__auto.blk.22.ffn_up.weight : tensor<14336x4096xf16>
    %252 = torch_c.from_builtin_tensor %__auto.blk.22.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.22.ffn_down.weight = util.global.load @__auto.blk.22.ffn_down.weight : tensor<4096x14336xf16>
    %253 = torch_c.from_builtin_tensor %__auto.blk.22.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.23.attn_norm.weight = util.global.load @__auto.blk.23.attn_norm.weight : tensor<4096xf32>
    %254 = torch_c.from_builtin_tensor %__auto.blk.23.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.23.attn_q.weight = util.global.load @__auto.blk.23.attn_q.weight : tensor<4096x4096xf16>
    %255 = torch_c.from_builtin_tensor %__auto.blk.23.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.23.attn_k.weight = util.global.load @__auto.blk.23.attn_k.weight : tensor<1024x4096xf16>
    %256 = torch_c.from_builtin_tensor %__auto.blk.23.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.23.attn_v.weight = util.global.load @__auto.blk.23.attn_v.weight : tensor<1024x4096xf16>
    %257 = torch_c.from_builtin_tensor %__auto.blk.23.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %258 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %259 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %__auto.blk.23.attn_output.weight = util.global.load @__auto.blk.23.attn_output.weight : tensor<4096x4096xf16>
    %260 = torch_c.from_builtin_tensor %__auto.blk.23.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.23.ffn_norm.weight = util.global.load @__auto.blk.23.ffn_norm.weight : tensor<4096xf32>
    %261 = torch_c.from_builtin_tensor %__auto.blk.23.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.23.ffn_gate.weight = util.global.load @__auto.blk.23.ffn_gate.weight : tensor<14336x4096xf16>
    %262 = torch_c.from_builtin_tensor %__auto.blk.23.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.23.ffn_up.weight = util.global.load @__auto.blk.23.ffn_up.weight : tensor<14336x4096xf16>
    %263 = torch_c.from_builtin_tensor %__auto.blk.23.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.23.ffn_down.weight = util.global.load @__auto.blk.23.ffn_down.weight : tensor<4096x14336xf16>
    %264 = torch_c.from_builtin_tensor %__auto.blk.23.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.24.attn_norm.weight = util.global.load @__auto.blk.24.attn_norm.weight : tensor<4096xf32>
    %265 = torch_c.from_builtin_tensor %__auto.blk.24.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.24.attn_q.weight = util.global.load @__auto.blk.24.attn_q.weight : tensor<4096x4096xf16>
    %266 = torch_c.from_builtin_tensor %__auto.blk.24.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.24.attn_k.weight = util.global.load @__auto.blk.24.attn_k.weight : tensor<1024x4096xf16>
    %267 = torch_c.from_builtin_tensor %__auto.blk.24.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.24.attn_v.weight = util.global.load @__auto.blk.24.attn_v.weight : tensor<1024x4096xf16>
    %268 = torch_c.from_builtin_tensor %__auto.blk.24.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %269 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %270 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %__auto.blk.24.attn_output.weight = util.global.load @__auto.blk.24.attn_output.weight : tensor<4096x4096xf16>
    %271 = torch_c.from_builtin_tensor %__auto.blk.24.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.24.ffn_norm.weight = util.global.load @__auto.blk.24.ffn_norm.weight : tensor<4096xf32>
    %272 = torch_c.from_builtin_tensor %__auto.blk.24.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.24.ffn_gate.weight = util.global.load @__auto.blk.24.ffn_gate.weight : tensor<14336x4096xf16>
    %273 = torch_c.from_builtin_tensor %__auto.blk.24.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.24.ffn_up.weight = util.global.load @__auto.blk.24.ffn_up.weight : tensor<14336x4096xf16>
    %274 = torch_c.from_builtin_tensor %__auto.blk.24.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.24.ffn_down.weight = util.global.load @__auto.blk.24.ffn_down.weight : tensor<4096x14336xf16>
    %275 = torch_c.from_builtin_tensor %__auto.blk.24.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.25.attn_norm.weight = util.global.load @__auto.blk.25.attn_norm.weight : tensor<4096xf32>
    %276 = torch_c.from_builtin_tensor %__auto.blk.25.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.25.attn_q.weight = util.global.load @__auto.blk.25.attn_q.weight : tensor<4096x4096xf16>
    %277 = torch_c.from_builtin_tensor %__auto.blk.25.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.25.attn_k.weight = util.global.load @__auto.blk.25.attn_k.weight : tensor<1024x4096xf16>
    %278 = torch_c.from_builtin_tensor %__auto.blk.25.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.25.attn_v.weight = util.global.load @__auto.blk.25.attn_v.weight : tensor<1024x4096xf16>
    %279 = torch_c.from_builtin_tensor %__auto.blk.25.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %280 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %281 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %__auto.blk.25.attn_output.weight = util.global.load @__auto.blk.25.attn_output.weight : tensor<4096x4096xf16>
    %282 = torch_c.from_builtin_tensor %__auto.blk.25.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.25.ffn_norm.weight = util.global.load @__auto.blk.25.ffn_norm.weight : tensor<4096xf32>
    %283 = torch_c.from_builtin_tensor %__auto.blk.25.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.25.ffn_gate.weight = util.global.load @__auto.blk.25.ffn_gate.weight : tensor<14336x4096xf16>
    %284 = torch_c.from_builtin_tensor %__auto.blk.25.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.25.ffn_up.weight = util.global.load @__auto.blk.25.ffn_up.weight : tensor<14336x4096xf16>
    %285 = torch_c.from_builtin_tensor %__auto.blk.25.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.25.ffn_down.weight = util.global.load @__auto.blk.25.ffn_down.weight : tensor<4096x14336xf16>
    %286 = torch_c.from_builtin_tensor %__auto.blk.25.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.26.attn_norm.weight = util.global.load @__auto.blk.26.attn_norm.weight : tensor<4096xf32>
    %287 = torch_c.from_builtin_tensor %__auto.blk.26.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.26.attn_q.weight = util.global.load @__auto.blk.26.attn_q.weight : tensor<4096x4096xf16>
    %288 = torch_c.from_builtin_tensor %__auto.blk.26.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.26.attn_k.weight = util.global.load @__auto.blk.26.attn_k.weight : tensor<1024x4096xf16>
    %289 = torch_c.from_builtin_tensor %__auto.blk.26.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.26.attn_v.weight = util.global.load @__auto.blk.26.attn_v.weight : tensor<1024x4096xf16>
    %290 = torch_c.from_builtin_tensor %__auto.blk.26.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %291 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %292 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %__auto.blk.26.attn_output.weight = util.global.load @__auto.blk.26.attn_output.weight : tensor<4096x4096xf16>
    %293 = torch_c.from_builtin_tensor %__auto.blk.26.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.26.ffn_norm.weight = util.global.load @__auto.blk.26.ffn_norm.weight : tensor<4096xf32>
    %294 = torch_c.from_builtin_tensor %__auto.blk.26.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.26.ffn_gate.weight = util.global.load @__auto.blk.26.ffn_gate.weight : tensor<14336x4096xf16>
    %295 = torch_c.from_builtin_tensor %__auto.blk.26.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.26.ffn_up.weight = util.global.load @__auto.blk.26.ffn_up.weight : tensor<14336x4096xf16>
    %296 = torch_c.from_builtin_tensor %__auto.blk.26.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.26.ffn_down.weight = util.global.load @__auto.blk.26.ffn_down.weight : tensor<4096x14336xf16>
    %297 = torch_c.from_builtin_tensor %__auto.blk.26.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.27.attn_norm.weight = util.global.load @__auto.blk.27.attn_norm.weight : tensor<4096xf32>
    %298 = torch_c.from_builtin_tensor %__auto.blk.27.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.27.attn_q.weight = util.global.load @__auto.blk.27.attn_q.weight : tensor<4096x4096xf16>
    %299 = torch_c.from_builtin_tensor %__auto.blk.27.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.27.attn_k.weight = util.global.load @__auto.blk.27.attn_k.weight : tensor<1024x4096xf16>
    %300 = torch_c.from_builtin_tensor %__auto.blk.27.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.27.attn_v.weight = util.global.load @__auto.blk.27.attn_v.weight : tensor<1024x4096xf16>
    %301 = torch_c.from_builtin_tensor %__auto.blk.27.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %302 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %303 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %__auto.blk.27.attn_output.weight = util.global.load @__auto.blk.27.attn_output.weight : tensor<4096x4096xf16>
    %304 = torch_c.from_builtin_tensor %__auto.blk.27.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.27.ffn_norm.weight = util.global.load @__auto.blk.27.ffn_norm.weight : tensor<4096xf32>
    %305 = torch_c.from_builtin_tensor %__auto.blk.27.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.27.ffn_gate.weight = util.global.load @__auto.blk.27.ffn_gate.weight : tensor<14336x4096xf16>
    %306 = torch_c.from_builtin_tensor %__auto.blk.27.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.27.ffn_up.weight = util.global.load @__auto.blk.27.ffn_up.weight : tensor<14336x4096xf16>
    %307 = torch_c.from_builtin_tensor %__auto.blk.27.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.27.ffn_down.weight = util.global.load @__auto.blk.27.ffn_down.weight : tensor<4096x14336xf16>
    %308 = torch_c.from_builtin_tensor %__auto.blk.27.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.28.attn_norm.weight = util.global.load @__auto.blk.28.attn_norm.weight : tensor<4096xf32>
    %309 = torch_c.from_builtin_tensor %__auto.blk.28.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.28.attn_q.weight = util.global.load @__auto.blk.28.attn_q.weight : tensor<4096x4096xf16>
    %310 = torch_c.from_builtin_tensor %__auto.blk.28.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.28.attn_k.weight = util.global.load @__auto.blk.28.attn_k.weight : tensor<1024x4096xf16>
    %311 = torch_c.from_builtin_tensor %__auto.blk.28.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.28.attn_v.weight = util.global.load @__auto.blk.28.attn_v.weight : tensor<1024x4096xf16>
    %312 = torch_c.from_builtin_tensor %__auto.blk.28.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %313 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %314 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %__auto.blk.28.attn_output.weight = util.global.load @__auto.blk.28.attn_output.weight : tensor<4096x4096xf16>
    %315 = torch_c.from_builtin_tensor %__auto.blk.28.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.28.ffn_norm.weight = util.global.load @__auto.blk.28.ffn_norm.weight : tensor<4096xf32>
    %316 = torch_c.from_builtin_tensor %__auto.blk.28.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.28.ffn_gate.weight = util.global.load @__auto.blk.28.ffn_gate.weight : tensor<14336x4096xf16>
    %317 = torch_c.from_builtin_tensor %__auto.blk.28.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.28.ffn_up.weight = util.global.load @__auto.blk.28.ffn_up.weight : tensor<14336x4096xf16>
    %318 = torch_c.from_builtin_tensor %__auto.blk.28.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.28.ffn_down.weight = util.global.load @__auto.blk.28.ffn_down.weight : tensor<4096x14336xf16>
    %319 = torch_c.from_builtin_tensor %__auto.blk.28.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.29.attn_norm.weight = util.global.load @__auto.blk.29.attn_norm.weight : tensor<4096xf32>
    %320 = torch_c.from_builtin_tensor %__auto.blk.29.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.29.attn_q.weight = util.global.load @__auto.blk.29.attn_q.weight : tensor<4096x4096xf16>
    %321 = torch_c.from_builtin_tensor %__auto.blk.29.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.29.attn_k.weight = util.global.load @__auto.blk.29.attn_k.weight : tensor<1024x4096xf16>
    %322 = torch_c.from_builtin_tensor %__auto.blk.29.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.29.attn_v.weight = util.global.load @__auto.blk.29.attn_v.weight : tensor<1024x4096xf16>
    %323 = torch_c.from_builtin_tensor %__auto.blk.29.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %324 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %325 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %__auto.blk.29.attn_output.weight = util.global.load @__auto.blk.29.attn_output.weight : tensor<4096x4096xf16>
    %326 = torch_c.from_builtin_tensor %__auto.blk.29.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.29.ffn_norm.weight = util.global.load @__auto.blk.29.ffn_norm.weight : tensor<4096xf32>
    %327 = torch_c.from_builtin_tensor %__auto.blk.29.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.29.ffn_gate.weight = util.global.load @__auto.blk.29.ffn_gate.weight : tensor<14336x4096xf16>
    %328 = torch_c.from_builtin_tensor %__auto.blk.29.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.29.ffn_up.weight = util.global.load @__auto.blk.29.ffn_up.weight : tensor<14336x4096xf16>
    %329 = torch_c.from_builtin_tensor %__auto.blk.29.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.29.ffn_down.weight = util.global.load @__auto.blk.29.ffn_down.weight : tensor<4096x14336xf16>
    %330 = torch_c.from_builtin_tensor %__auto.blk.29.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.30.attn_norm.weight = util.global.load @__auto.blk.30.attn_norm.weight : tensor<4096xf32>
    %331 = torch_c.from_builtin_tensor %__auto.blk.30.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.30.attn_q.weight = util.global.load @__auto.blk.30.attn_q.weight : tensor<4096x4096xf16>
    %332 = torch_c.from_builtin_tensor %__auto.blk.30.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.30.attn_k.weight = util.global.load @__auto.blk.30.attn_k.weight : tensor<1024x4096xf16>
    %333 = torch_c.from_builtin_tensor %__auto.blk.30.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.30.attn_v.weight = util.global.load @__auto.blk.30.attn_v.weight : tensor<1024x4096xf16>
    %334 = torch_c.from_builtin_tensor %__auto.blk.30.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %335 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %336 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %__auto.blk.30.attn_output.weight = util.global.load @__auto.blk.30.attn_output.weight : tensor<4096x4096xf16>
    %337 = torch_c.from_builtin_tensor %__auto.blk.30.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.30.ffn_norm.weight = util.global.load @__auto.blk.30.ffn_norm.weight : tensor<4096xf32>
    %338 = torch_c.from_builtin_tensor %__auto.blk.30.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.30.ffn_gate.weight = util.global.load @__auto.blk.30.ffn_gate.weight : tensor<14336x4096xf16>
    %339 = torch_c.from_builtin_tensor %__auto.blk.30.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.30.ffn_up.weight = util.global.load @__auto.blk.30.ffn_up.weight : tensor<14336x4096xf16>
    %340 = torch_c.from_builtin_tensor %__auto.blk.30.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.30.ffn_down.weight = util.global.load @__auto.blk.30.ffn_down.weight : tensor<4096x14336xf16>
    %341 = torch_c.from_builtin_tensor %__auto.blk.30.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.blk.31.attn_norm.weight = util.global.load @__auto.blk.31.attn_norm.weight : tensor<4096xf32>
    %342 = torch_c.from_builtin_tensor %__auto.blk.31.attn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.31.attn_q.weight = util.global.load @__auto.blk.31.attn_q.weight : tensor<4096x4096xf16>
    %343 = torch_c.from_builtin_tensor %__auto.blk.31.attn_q.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.31.attn_k.weight = util.global.load @__auto.blk.31.attn_k.weight : tensor<1024x4096xf16>
    %344 = torch_c.from_builtin_tensor %__auto.blk.31.attn_k.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %__auto.blk.31.attn_v.weight = util.global.load @__auto.blk.31.attn_v.weight : tensor<1024x4096xf16>
    %345 = torch_c.from_builtin_tensor %__auto.blk.31.attn_v.weight : tensor<1024x4096xf16> -> !torch.vtensor<[1024,4096],f16>
    %346 = torch.vtensor.literal(dense<0> : tensor<si64>) : !torch.vtensor<[],si64>
    %347 = torch.vtensor.literal(dense<1> : tensor<si64>) : !torch.vtensor<[],si64>
    %__auto.blk.31.attn_output.weight = util.global.load @__auto.blk.31.attn_output.weight : tensor<4096x4096xf16>
    %348 = torch_c.from_builtin_tensor %__auto.blk.31.attn_output.weight : tensor<4096x4096xf16> -> !torch.vtensor<[4096,4096],f16>
    %__auto.blk.31.ffn_norm.weight = util.global.load @__auto.blk.31.ffn_norm.weight : tensor<4096xf32>
    %349 = torch_c.from_builtin_tensor %__auto.blk.31.ffn_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.blk.31.ffn_gate.weight = util.global.load @__auto.blk.31.ffn_gate.weight : tensor<14336x4096xf16>
    %350 = torch_c.from_builtin_tensor %__auto.blk.31.ffn_gate.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.31.ffn_up.weight = util.global.load @__auto.blk.31.ffn_up.weight : tensor<14336x4096xf16>
    %351 = torch_c.from_builtin_tensor %__auto.blk.31.ffn_up.weight : tensor<14336x4096xf16> -> !torch.vtensor<[14336,4096],f16>
    %__auto.blk.31.ffn_down.weight = util.global.load @__auto.blk.31.ffn_down.weight : tensor<4096x14336xf16>
    %352 = torch_c.from_builtin_tensor %__auto.blk.31.ffn_down.weight : tensor<4096x14336xf16> -> !torch.vtensor<[4096,14336],f16>
    %__auto.output_norm.weight = util.global.load @__auto.output_norm.weight : tensor<4096xf32>
    %353 = torch_c.from_builtin_tensor %__auto.output_norm.weight : tensor<4096xf32> -> !torch.vtensor<[4096],f32>
    %__auto.output.weight = util.global.load @__auto.output.weight : tensor<128256x4096xf16>
    %354 = torch_c.from_builtin_tensor %__auto.output.weight : tensor<128256x4096xf16> -> !torch.vtensor<[128256,4096],f16>
    %355 = torch.copy.to_vtensor %arg4 : !torch.vtensor<[?,2097152],f16>
    %356 = torch.symbolic_int "s0" {min_val = 2, max_val = 4095} : !torch.int
    %357 = torch.symbolic_int "s1" {min_val = 2, max_val = 9223372036854775806} : !torch.int
    torch.bind_symbolic_shape %arg3, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %355, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int1 = torch.constant.int 1
    %358 = torch.aten.size.int %arg3, %int1 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.int
    %int32 = torch.constant.int 32
    %359 = torch.aten.mul.int %358, %int32 : !torch.int, !torch.int -> !torch.int
    %int0 = torch.constant.int 0
    %int1_0 = torch.constant.int 1
    %none = torch.constant.none
    %none_1 = torch.constant.none
    %cpu = torch.constant.device "cpu"
    %false = torch.constant.bool false
    %360 = torch.aten.arange.start_step %int0, %359, %int1_0, %none, %none_1, %cpu, %false : !torch.int, !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %360, [%356], affine_map<()[s0] -> (s0 * 32)> : !torch.vtensor<[?],si64>
    %int-1 = torch.constant.int -1
    %361 = torch.aten.unsqueeze %arg1, %int-1 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %362 = torch.aten.ge.Tensor %360, %361 : !torch.vtensor<[?],si64>, !torch.vtensor<[4,1],si64> -> !torch.vtensor<[4,?],i1>
    torch.bind_symbolic_shape %362, [%356], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],i1>
    %int0_2 = torch.constant.int 0
    %int6 = torch.constant.int 6
    %int0_3 = torch.constant.int 0
    %cpu_4 = torch.constant.device "cpu"
    %none_5 = torch.constant.none
    %363 = torch.aten.scalar_tensor %int0_2, %int6, %int0_3, %cpu_4, %none_5 : !torch.int, !torch.int, !torch.int, !torch.Device, !torch.none -> !torch.vtensor<[],f32>
    %float-Inf = torch.constant.float 0xFFF0000000000000
    %int6_6 = torch.constant.int 6
    %int0_7 = torch.constant.int 0
    %cpu_8 = torch.constant.device "cpu"
    %none_9 = torch.constant.none
    %364 = torch.aten.scalar_tensor %float-Inf, %int6_6, %int0_7, %cpu_8, %none_9 : !torch.float, !torch.int, !torch.int, !torch.Device, !torch.none -> !torch.vtensor<[],f32>
    %365 = torch.aten.where.self %362, %364, %363 : !torch.vtensor<[4,?],i1>, !torch.vtensor<[],f32>, !torch.vtensor<[],f32> -> !torch.vtensor<[4,?],f32>
    torch.bind_symbolic_shape %365, [%356], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],f32>
    %int5 = torch.constant.int 5
    %366 = torch.prims.convert_element_type %365, %int5 : !torch.vtensor<[4,?],f32>, !torch.int -> !torch.vtensor<[4,?],f16>
    torch.bind_symbolic_shape %366, [%356], affine_map<()[s0] -> (4, s0 * 32)> : !torch.vtensor<[4,?],f16>
    %int1_10 = torch.constant.int 1
    %367 = torch.aten.unsqueeze %366, %int1_10 : !torch.vtensor<[4,?],f16>, !torch.int -> !torch.vtensor<[4,1,?],f16>
    torch.bind_symbolic_shape %367, [%356], affine_map<()[s0] -> (4, 1, s0 * 32)> : !torch.vtensor<[4,1,?],f16>
    %int1_11 = torch.constant.int 1
    %368 = torch.aten.unsqueeze %367, %int1_11 : !torch.vtensor<[4,1,?],f16>, !torch.int -> !torch.vtensor<[4,1,1,?],f16>
    torch.bind_symbolic_shape %368, [%356], affine_map<()[s0] -> (4, 1, 1, s0 * 32)> : !torch.vtensor<[4,1,1,?],f16>
    %int0_12 = torch.constant.int 0
    %int1_13 = torch.constant.int 1
    %none_14 = torch.constant.none
    %none_15 = torch.constant.none
    %cpu_16 = torch.constant.device "cpu"
    %false_17 = torch.constant.bool false
    %369 = torch.aten.arange.start %int0_12, %int1_13, %none_14, %none_15, %cpu_16, %false_17 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[1],si64>
    %int0_18 = torch.constant.int 0
    %370 = torch.aten.unsqueeze %369, %int0_18 : !torch.vtensor<[1],si64>, !torch.int -> !torch.vtensor<[1,1],si64>
    %int1_19 = torch.constant.int 1
    %371 = torch.aten.unsqueeze %arg2, %int1_19 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_20 = torch.constant.int 1
    %372 = torch.aten.add.Tensor %370, %371, %int1_20 : !torch.vtensor<[1,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int131072 = torch.constant.int 131072
    %none_21 = torch.constant.none
    %none_22 = torch.constant.none
    %cpu_23 = torch.constant.device "cpu"
    %false_24 = torch.constant.bool false
    %373 = torch.aten.arange %int131072, %none_21, %none_22, %cpu_23, %false_24 : !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[131072],si64>
    %int0_25 = torch.constant.int 0
    %int128 = torch.constant.int 128
    %none_26 = torch.constant.none
    %none_27 = torch.constant.none
    %cpu_28 = torch.constant.device "cpu"
    %false_29 = torch.constant.bool false
    %374 = torch.aten.arange.start %int0_25, %int128, %none_26, %none_27, %cpu_28, %false_29 : !torch.int, !torch.int, !torch.none, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],si64>
    %int2 = torch.constant.int 2
    %375 = torch.aten.floor_divide.Scalar %374, %int2 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %int6_30 = torch.constant.int 6
    %376 = torch.prims.convert_element_type %375, %int6_30 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128],f32>
    %int128_31 = torch.constant.int 128
    %377 = torch.aten.div.Scalar %376, %int128_31 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %float2.000000e00 = torch.constant.float 2.000000e+00
    %378 = torch.aten.mul.Scalar %377, %float2.000000e00 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %float5.000000e05 = torch.constant.float 5.000000e+05
    %379 = torch.aten.pow.Scalar %float5.000000e05, %378 : !torch.float, !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %380 = torch.aten.reciprocal %379 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %float1.000000e00 = torch.constant.float 1.000000e+00
    %381 = torch.aten.mul.Scalar %380, %float1.000000e00 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int1_32 = torch.constant.int 1
    %382 = torch.aten.unsqueeze %373, %int1_32 : !torch.vtensor<[131072],si64>, !torch.int -> !torch.vtensor<[131072,1],si64>
    %int0_33 = torch.constant.int 0
    %383 = torch.aten.unsqueeze %381, %int0_33 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %384 = torch.aten.mul.Tensor %382, %383 : !torch.vtensor<[131072,1],si64>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[131072,128],f32>
    %int4 = torch.constant.int 4
    %385 = torch.prim.ListConstruct %int4 : (!torch.int) -> !torch.list<int>
    %386 = torch.aten.view %372, %385 : !torch.vtensor<[4,1],si64>, !torch.list<int> -> !torch.vtensor<[4],si64>
    %387 = torch.prim.ListConstruct %386 : (!torch.vtensor<[4],si64>) -> !torch.list<optional<vtensor>>
    %388 = torch.aten.index.Tensor %384, %387 : !torch.vtensor<[131072,128],f32>, !torch.list<optional<vtensor>> -> !torch.vtensor<[4,128],f32>
    %int1_34 = torch.constant.int 1
    %389 = torch.aten.unsqueeze %388, %int1_34 : !torch.vtensor<[4,128],f32>, !torch.int -> !torch.vtensor<[4,1,128],f32>
    %int-1_35 = torch.constant.int -1
    %false_36 = torch.constant.bool false
    %false_37 = torch.constant.bool false
    %390 = torch.aten.embedding %0, %arg0, %int-1_35, %false_36, %false_37 : !torch.vtensor<[128256,4096],f16>, !torch.vtensor<[4,1],si64>, !torch.int, !torch.bool, !torch.bool -> !torch.vtensor<[4,1,4096],f16>
    %int6_38 = torch.constant.int 6
    %391 = torch.prims.convert_element_type %390, %int6_38 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_39 = torch.constant.int 2
    %392 = torch.aten.pow.Tensor_Scalar %391, %int2_39 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_40 = torch.constant.int -1
    %393 = torch.prim.ListConstruct %int-1_40 : (!torch.int) -> !torch.list<int>
    %true = torch.constant.bool true
    %none_41 = torch.constant.none
    %394 = torch.aten.mean.dim %392, %393, %true, %none_41 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06 = torch.constant.float 9.9999997473787516E-6
    %int1_42 = torch.constant.int 1
    %395 = torch.aten.add.Scalar %394, %float9.999990e-06, %int1_42 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %396 = torch.aten.rsqrt %395 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %397 = torch.aten.mul.Tensor %391, %396 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_43 = torch.constant.int 5
    %398 = torch.prims.convert_element_type %397, %int5_43 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %399 = torch.aten.mul.Tensor %1, %398 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_44 = torch.constant.int 5
    %400 = torch.prims.convert_element_type %399, %int5_44 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2 = torch.constant.int -2
    %int-1_45 = torch.constant.int -1
    %401 = torch.aten.transpose.int %2, %int-2, %int-1_45 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_46 = torch.constant.int 4
    %int4096 = torch.constant.int 4096
    %402 = torch.prim.ListConstruct %int4_46, %int4096 : (!torch.int, !torch.int) -> !torch.list<int>
    %403 = torch.aten.view %400, %402 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %404 = torch.aten.mm %403, %401 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_47 = torch.constant.int 4
    %int1_48 = torch.constant.int 1
    %int4096_49 = torch.constant.int 4096
    %405 = torch.prim.ListConstruct %int4_47, %int1_48, %int4096_49 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %406 = torch.aten.view %404, %405 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_50 = torch.constant.int -2
    %int-1_51 = torch.constant.int -1
    %407 = torch.aten.transpose.int %3, %int-2_50, %int-1_51 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_52 = torch.constant.int 4
    %int4096_53 = torch.constant.int 4096
    %408 = torch.prim.ListConstruct %int4_52, %int4096_53 : (!torch.int, !torch.int) -> !torch.list<int>
    %409 = torch.aten.view %400, %408 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %410 = torch.aten.mm %409, %407 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_54 = torch.constant.int 4
    %int1_55 = torch.constant.int 1
    %int1024 = torch.constant.int 1024
    %411 = torch.prim.ListConstruct %int4_54, %int1_55, %int1024 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %412 = torch.aten.view %410, %411 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int-2_56 = torch.constant.int -2
    %int-1_57 = torch.constant.int -1
    %413 = torch.aten.transpose.int %4, %int-2_56, %int-1_57 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_58 = torch.constant.int 4
    %int4096_59 = torch.constant.int 4096
    %414 = torch.prim.ListConstruct %int4_58, %int4096_59 : (!torch.int, !torch.int) -> !torch.list<int>
    %415 = torch.aten.view %400, %414 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %416 = torch.aten.mm %415, %413 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_60 = torch.constant.int 4
    %int1_61 = torch.constant.int 1
    %int1024_62 = torch.constant.int 1024
    %417 = torch.prim.ListConstruct %int4_60, %int1_61, %int1024_62 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %418 = torch.aten.view %416, %417 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int4_63 = torch.constant.int 4
    %int1_64 = torch.constant.int 1
    %int32_65 = torch.constant.int 32
    %int128_66 = torch.constant.int 128
    %419 = torch.prim.ListConstruct %int4_63, %int1_64, %int32_65, %int128_66 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %420 = torch.aten.view %406, %419 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,128],f16>
    %int4_67 = torch.constant.int 4
    %int1_68 = torch.constant.int 1
    %int8 = torch.constant.int 8
    %int128_69 = torch.constant.int 128
    %421 = torch.prim.ListConstruct %int4_67, %int1_68, %int8, %int128_69 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %422 = torch.aten.view %412, %421 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int4_70 = torch.constant.int 4
    %int1_71 = torch.constant.int 1
    %int8_72 = torch.constant.int 8
    %int128_73 = torch.constant.int 128
    %423 = torch.prim.ListConstruct %int4_70, %int1_71, %int8_72, %int128_73 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %424 = torch.aten.view %418, %423 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int6_74 = torch.constant.int 6
    %425 = torch.prims.convert_element_type %420, %int6_74 : !torch.vtensor<[4,1,32,128],f16>, !torch.int -> !torch.vtensor<[4,1,32,128],f32>
    %426 = torch_c.to_builtin_tensor %425 : !torch.vtensor<[4,1,32,128],f32> -> tensor<4x1x32x128xf32>
    %427 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %428 = util.call @sharktank_rotary_embedding_4_1_32_128_f32(%426, %427) : (tensor<4x1x32x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x32x128xf32>
    %429 = torch_c.from_builtin_tensor %428 : tensor<4x1x32x128xf32> -> !torch.vtensor<[4,1,32,128],f32>
    %int5_75 = torch.constant.int 5
    %430 = torch.prims.convert_element_type %429, %int5_75 : !torch.vtensor<[4,1,32,128],f32>, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int6_76 = torch.constant.int 6
    %431 = torch.prims.convert_element_type %422, %int6_76 : !torch.vtensor<[4,1,8,128],f16>, !torch.int -> !torch.vtensor<[4,1,8,128],f32>
    %432 = torch_c.to_builtin_tensor %431 : !torch.vtensor<[4,1,8,128],f32> -> tensor<4x1x8x128xf32>
    %433 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %434 = util.call @sharktank_rotary_embedding_4_1_8_128_f32(%432, %433) : (tensor<4x1x8x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x8x128xf32>
    %435 = torch_c.from_builtin_tensor %434 : tensor<4x1x8x128xf32> -> !torch.vtensor<[4,1,8,128],f32>
    %int5_77 = torch.constant.int 5
    %436 = torch.prims.convert_element_type %435, %int5_77 : !torch.vtensor<[4,1,8,128],f32>, !torch.int -> !torch.vtensor<[4,1,8,128],f16>
    %int0_78 = torch.constant.int 0
    %437 = torch.aten.size.int %355, %int0_78 : !torch.vtensor<[?,2097152],f16>, !torch.int -> !torch.int
    %int32_79 = torch.constant.int 32
    %int2_80 = torch.constant.int 2
    %int32_81 = torch.constant.int 32
    %int8_82 = torch.constant.int 8
    %int128_83 = torch.constant.int 128
    %438 = torch.prim.ListConstruct %437, %int32_79, %int2_80, %int32_81, %int8_82, %int128_83 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %439 = torch.aten.view %355, %438 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %439, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_84 = torch.constant.int 32
    %440 = torch.aten.mul.int %437, %int32_84 : !torch.int, !torch.int -> !torch.int
    %int2_85 = torch.constant.int 2
    %441 = torch.aten.mul.int %440, %int2_85 : !torch.int, !torch.int -> !torch.int
    %int32_86 = torch.constant.int 32
    %442 = torch.aten.mul.int %441, %int32_86 : !torch.int, !torch.int -> !torch.int
    %int8_87 = torch.constant.int 8
    %int128_88 = torch.constant.int 128
    %443 = torch.prim.ListConstruct %442, %int8_87, %int128_88 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %444 = torch.aten.view %439, %443 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %444, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_89 = torch.constant.int 32
    %445 = torch.aten.floor_divide.Scalar %arg2, %int32_89 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_90 = torch.constant.int 1
    %446 = torch.aten.unsqueeze %445, %int1_90 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_91 = torch.constant.int 1
    %false_92 = torch.constant.bool false
    %447 = torch.aten.gather %arg3, %int1_91, %446, %false_92 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_93 = torch.constant.int 32
    %448 = torch.aten.remainder.Scalar %arg2, %int32_93 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_94 = torch.constant.int 1
    %449 = torch.aten.unsqueeze %448, %int1_94 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_95 = torch.constant.none
    %450 = torch.aten.clone %5, %none_95 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_96 = torch.constant.int 0
    %451 = torch.aten.unsqueeze %450, %int0_96 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_97 = torch.constant.int 4
    %int1_98 = torch.constant.int 1
    %452 = torch.prim.ListConstruct %int4_97, %int1_98 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_99 = torch.constant.int 1
    %int1_100 = torch.constant.int 1
    %453 = torch.prim.ListConstruct %int1_99, %int1_100 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_101 = torch.constant.int 4
    %int0_102 = torch.constant.int 0
    %cpu_103 = torch.constant.device "cpu"
    %false_104 = torch.constant.bool false
    %454 = torch.aten.empty_strided %452, %453, %int4_101, %int0_102, %cpu_103, %false_104 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int0_105 = torch.constant.int 0
    %455 = torch.aten.fill.Scalar %454, %int0_105 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_106 = torch.constant.int 4
    %int1_107 = torch.constant.int 1
    %456 = torch.prim.ListConstruct %int4_106, %int1_107 : (!torch.int, !torch.int) -> !torch.list<int>
    %457 = torch.aten.repeat %451, %456 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_108 = torch.constant.int 32
    %458 = torch.aten.mul.Scalar %447, %int32_108 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_109 = torch.constant.int 1
    %459 = torch.aten.add.Tensor %458, %455, %int1_109 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_110 = torch.constant.int 2
    %460 = torch.aten.mul.Scalar %459, %int2_110 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_111 = torch.constant.int 1
    %461 = torch.aten.add.Tensor %460, %457, %int1_111 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_112 = torch.constant.int 32
    %462 = torch.aten.mul.Scalar %461, %int32_112 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_113 = torch.constant.int 1
    %463 = torch.aten.add.Tensor %462, %449, %int1_113 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %464 = torch.prim.ListConstruct %463 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_114 = torch.constant.bool false
    %465 = torch.aten.index_put %444, %464, %436, %false_114 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %465, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_115 = torch.constant.int 32
    %int2_116 = torch.constant.int 2
    %int32_117 = torch.constant.int 32
    %int8_118 = torch.constant.int 8
    %int128_119 = torch.constant.int 128
    %466 = torch.prim.ListConstruct %437, %int32_115, %int2_116, %int32_117, %int8_118, %int128_119 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %467 = torch.aten.view %465, %466 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %467, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152 = torch.constant.int 2097152
    %468 = torch.prim.ListConstruct %437, %int2097152 : (!torch.int, !torch.int) -> !torch.list<int>
    %469 = torch.aten.view %467, %468 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %469, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_120 = torch.constant.int 32
    %int2_121 = torch.constant.int 2
    %int32_122 = torch.constant.int 32
    %int8_123 = torch.constant.int 8
    %int128_124 = torch.constant.int 128
    %470 = torch.prim.ListConstruct %437, %int32_120, %int2_121, %int32_122, %int8_123, %int128_124 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %471 = torch.aten.view %469, %470 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %471, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int8_125 = torch.constant.int 8
    %int128_126 = torch.constant.int 128
    %472 = torch.prim.ListConstruct %442, %int8_125, %int128_126 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %473 = torch.aten.view %471, %472 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %473, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_127 = torch.constant.int 32
    %474 = torch.aten.floor_divide.Scalar %arg2, %int32_127 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_128 = torch.constant.int 1
    %475 = torch.aten.unsqueeze %474, %int1_128 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_129 = torch.constant.int 1
    %false_130 = torch.constant.bool false
    %476 = torch.aten.gather %arg3, %int1_129, %475, %false_130 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_131 = torch.constant.int 32
    %477 = torch.aten.remainder.Scalar %arg2, %int32_131 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_132 = torch.constant.int 1
    %478 = torch.aten.unsqueeze %477, %int1_132 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_133 = torch.constant.none
    %479 = torch.aten.clone %6, %none_133 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_134 = torch.constant.int 0
    %480 = torch.aten.unsqueeze %479, %int0_134 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_135 = torch.constant.int 4
    %int1_136 = torch.constant.int 1
    %481 = torch.prim.ListConstruct %int4_135, %int1_136 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_137 = torch.constant.int 1
    %int1_138 = torch.constant.int 1
    %482 = torch.prim.ListConstruct %int1_137, %int1_138 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_139 = torch.constant.int 4
    %int0_140 = torch.constant.int 0
    %cpu_141 = torch.constant.device "cpu"
    %false_142 = torch.constant.bool false
    %483 = torch.aten.empty_strided %481, %482, %int4_139, %int0_140, %cpu_141, %false_142 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int0_143 = torch.constant.int 0
    %484 = torch.aten.fill.Scalar %483, %int0_143 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_144 = torch.constant.int 4
    %int1_145 = torch.constant.int 1
    %485 = torch.prim.ListConstruct %int4_144, %int1_145 : (!torch.int, !torch.int) -> !torch.list<int>
    %486 = torch.aten.repeat %480, %485 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_146 = torch.constant.int 32
    %487 = torch.aten.mul.Scalar %476, %int32_146 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_147 = torch.constant.int 1
    %488 = torch.aten.add.Tensor %487, %484, %int1_147 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_148 = torch.constant.int 2
    %489 = torch.aten.mul.Scalar %488, %int2_148 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_149 = torch.constant.int 1
    %490 = torch.aten.add.Tensor %489, %486, %int1_149 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_150 = torch.constant.int 32
    %491 = torch.aten.mul.Scalar %490, %int32_150 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_151 = torch.constant.int 1
    %492 = torch.aten.add.Tensor %491, %478, %int1_151 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %493 = torch.prim.ListConstruct %492 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_152 = torch.constant.bool false
    %494 = torch.aten.index_put %473, %493, %424, %false_152 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %494, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_153 = torch.constant.int 32
    %int2_154 = torch.constant.int 2
    %int32_155 = torch.constant.int 32
    %int8_156 = torch.constant.int 8
    %int128_157 = torch.constant.int 128
    %495 = torch.prim.ListConstruct %437, %int32_153, %int2_154, %int32_155, %int8_156, %int128_157 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %496 = torch.aten.view %494, %495 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %496, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_158 = torch.constant.int 2097152
    %497 = torch.prim.ListConstruct %437, %int2097152_158 : (!torch.int, !torch.int) -> !torch.list<int>
    %498 = torch.aten.view %496, %497 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %498, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int4_159 = torch.constant.int 4
    %499 = torch.prim.ListConstruct %int4_159, %358 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_160 = torch.constant.int 1
    %500 = torch.prim.ListConstruct %358, %int1_160 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_161 = torch.constant.int 4
    %int0_162 = torch.constant.int 0
    %cpu_163 = torch.constant.device "cpu"
    %false_164 = torch.constant.bool false
    %501 = torch.aten.empty_strided %499, %500, %int4_161, %int0_162, %cpu_163, %false_164 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %501, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int0_165 = torch.constant.int 0
    %502 = torch.aten.fill.Scalar %501, %int0_165 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %502, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int32_166 = torch.constant.int 32
    %503 = torch.aten.mul.Scalar %arg3, %int32_166 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %503, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_167 = torch.constant.int 1
    %504 = torch.aten.add.Tensor %503, %502, %int1_167 : !torch.vtensor<[4,?],si64>, !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %504, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_168 = torch.constant.int 4
    %505 = torch.aten.mul.int %int4_168, %358 : !torch.int, !torch.int -> !torch.int
    %506 = torch.prim.ListConstruct %505 : (!torch.int) -> !torch.list<int>
    %507 = torch.aten.view %504, %506 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %507, [%356], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_169 = torch.constant.int 32
    %int2_170 = torch.constant.int 2
    %int32_171 = torch.constant.int 32
    %int8_172 = torch.constant.int 8
    %int128_173 = torch.constant.int 128
    %508 = torch.prim.ListConstruct %437, %int32_169, %int2_170, %int32_171, %int8_172, %int128_173 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %509 = torch.aten.view %498, %508 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %509, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_174 = torch.constant.int 32
    %510 = torch.aten.mul.int %437, %int32_174 : !torch.int, !torch.int -> !torch.int
    %int2_175 = torch.constant.int 2
    %int32_176 = torch.constant.int 32
    %int8_177 = torch.constant.int 8
    %int128_178 = torch.constant.int 128
    %511 = torch.prim.ListConstruct %510, %int2_175, %int32_176, %int8_177, %int128_178 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %512 = torch.aten.view %509, %511 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %512, [%357], affine_map<()[s0] -> (s0 * 32, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int0_179 = torch.constant.int 0
    %513 = torch.aten.index_select %512, %int0_179, %507 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.int, !torch.vtensor<[?],si64> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %513, [%356], affine_map<()[s0] -> (s0 * 4, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int4_180 = torch.constant.int 4
    %int2_181 = torch.constant.int 2
    %int32_182 = torch.constant.int 32
    %int8_183 = torch.constant.int 8
    %int128_184 = torch.constant.int 128
    %514 = torch.prim.ListConstruct %int4_180, %358, %int2_181, %int32_182, %int8_183, %int128_184 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %515 = torch.aten.view %513, %514 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %515, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int0_185 = torch.constant.int 0
    %int0_186 = torch.constant.int 0
    %int9223372036854775807 = torch.constant.int 9223372036854775807
    %int1_187 = torch.constant.int 1
    %516 = torch.aten.slice.Tensor %515, %int0_185, %int0_186, %int9223372036854775807, %int1_187 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %516, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_188 = torch.constant.int 1
    %int0_189 = torch.constant.int 0
    %int9223372036854775807_190 = torch.constant.int 9223372036854775807
    %int1_191 = torch.constant.int 1
    %517 = torch.aten.slice.Tensor %516, %int1_188, %int0_189, %int9223372036854775807_190, %int1_191 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %517, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_192 = torch.constant.int 2
    %int0_193 = torch.constant.int 0
    %518 = torch.aten.select.int %517, %int2_192, %int0_193 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %518, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int32_194 = torch.constant.int 32
    %519 = torch.aten.mul.int %358, %int32_194 : !torch.int, !torch.int -> !torch.int
    %int2_195 = torch.constant.int 2
    %int0_196 = torch.constant.int 0
    %int1_197 = torch.constant.int 1
    %520 = torch.aten.slice.Tensor %518, %int2_195, %int0_196, %519, %int1_197 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %520, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_198 = torch.constant.int 0
    %521 = torch.aten.clone %520, %int0_198 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %521, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_199 = torch.constant.int 1
    %522 = torch.aten.size.int %517, %int1_199 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_200 = torch.constant.int 32
    %523 = torch.aten.mul.int %522, %int32_200 : !torch.int, !torch.int -> !torch.int
    %int4_201 = torch.constant.int 4
    %int8_202 = torch.constant.int 8
    %int128_203 = torch.constant.int 128
    %524 = torch.prim.ListConstruct %int4_201, %523, %int8_202, %int128_203 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %525 = torch.aten._unsafe_view %521, %524 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %525, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_204 = torch.constant.int 0
    %int0_205 = torch.constant.int 0
    %int9223372036854775807_206 = torch.constant.int 9223372036854775807
    %int1_207 = torch.constant.int 1
    %526 = torch.aten.slice.Tensor %525, %int0_204, %int0_205, %int9223372036854775807_206, %int1_207 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %526, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_208 = torch.constant.int 0
    %int0_209 = torch.constant.int 0
    %int9223372036854775807_210 = torch.constant.int 9223372036854775807
    %int1_211 = torch.constant.int 1
    %527 = torch.aten.slice.Tensor %515, %int0_208, %int0_209, %int9223372036854775807_210, %int1_211 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %527, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_212 = torch.constant.int 1
    %int0_213 = torch.constant.int 0
    %int9223372036854775807_214 = torch.constant.int 9223372036854775807
    %int1_215 = torch.constant.int 1
    %528 = torch.aten.slice.Tensor %527, %int1_212, %int0_213, %int9223372036854775807_214, %int1_215 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %528, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_216 = torch.constant.int 2
    %int1_217 = torch.constant.int 1
    %529 = torch.aten.select.int %528, %int2_216, %int1_217 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %529, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int2_218 = torch.constant.int 2
    %int0_219 = torch.constant.int 0
    %int1_220 = torch.constant.int 1
    %530 = torch.aten.slice.Tensor %529, %int2_218, %int0_219, %519, %int1_220 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %530, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_221 = torch.constant.int 0
    %531 = torch.aten.clone %530, %int0_221 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %531, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_222 = torch.constant.int 1
    %532 = torch.aten.size.int %528, %int1_222 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_223 = torch.constant.int 32
    %533 = torch.aten.mul.int %532, %int32_223 : !torch.int, !torch.int -> !torch.int
    %int4_224 = torch.constant.int 4
    %int8_225 = torch.constant.int 8
    %int128_226 = torch.constant.int 128
    %534 = torch.prim.ListConstruct %int4_224, %533, %int8_225, %int128_226 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %535 = torch.aten._unsafe_view %531, %534 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %535, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_227 = torch.constant.int 0
    %int0_228 = torch.constant.int 0
    %int9223372036854775807_229 = torch.constant.int 9223372036854775807
    %int1_230 = torch.constant.int 1
    %536 = torch.aten.slice.Tensor %535, %int0_227, %int0_228, %int9223372036854775807_229, %int1_230 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %536, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int-2_231 = torch.constant.int -2
    %537 = torch.aten.unsqueeze %526, %int-2_231 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %537, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_232 = torch.constant.int 1
    %538 = torch.aten.size.int %525, %int1_232 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_233 = torch.constant.int 4
    %int8_234 = torch.constant.int 8
    %int4_235 = torch.constant.int 4
    %int128_236 = torch.constant.int 128
    %539 = torch.prim.ListConstruct %int4_233, %538, %int8_234, %int4_235, %int128_236 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_237 = torch.constant.bool false
    %540 = torch.aten.expand %537, %539, %false_237 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %540, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_238 = torch.constant.int 0
    %541 = torch.aten.clone %540, %int0_238 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %541, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_239 = torch.constant.int 4
    %int32_240 = torch.constant.int 32
    %int128_241 = torch.constant.int 128
    %542 = torch.prim.ListConstruct %int4_239, %538, %int32_240, %int128_241 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %543 = torch.aten._unsafe_view %541, %542 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %543, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_242 = torch.constant.int -2
    %544 = torch.aten.unsqueeze %536, %int-2_242 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %544, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_243 = torch.constant.int 1
    %545 = torch.aten.size.int %535, %int1_243 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_244 = torch.constant.int 4
    %int8_245 = torch.constant.int 8
    %int4_246 = torch.constant.int 4
    %int128_247 = torch.constant.int 128
    %546 = torch.prim.ListConstruct %int4_244, %545, %int8_245, %int4_246, %int128_247 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_248 = torch.constant.bool false
    %547 = torch.aten.expand %544, %546, %false_248 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %547, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_249 = torch.constant.int 0
    %548 = torch.aten.clone %547, %int0_249 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %548, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_250 = torch.constant.int 4
    %int32_251 = torch.constant.int 32
    %int128_252 = torch.constant.int 128
    %549 = torch.prim.ListConstruct %int4_250, %545, %int32_251, %int128_252 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %550 = torch.aten._unsafe_view %548, %549 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %550, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_253 = torch.constant.int 1
    %int2_254 = torch.constant.int 2
    %551 = torch.aten.transpose.int %430, %int1_253, %int2_254 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,128],f16>
    %int1_255 = torch.constant.int 1
    %int2_256 = torch.constant.int 2
    %552 = torch.aten.transpose.int %543, %int1_255, %int2_256 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %552, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_257 = torch.constant.int 1
    %int2_258 = torch.constant.int 2
    %553 = torch.aten.transpose.int %550, %int1_257, %int2_258 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %553, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00 = torch.constant.float 0.000000e+00
    %false_259 = torch.constant.bool false
    %none_260 = torch.constant.none
    %554:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%551, %552, %553, %float0.000000e00, %false_259, %368, %none_260) : (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.vtensor<[4,1,1,?],f16>, !torch.none) -> (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,1],f32>) 
    %int1_261 = torch.constant.int 1
    %int2_262 = torch.constant.int 2
    %555 = torch.aten.transpose.int %554#0, %int1_261, %int2_262 : !torch.vtensor<[4,32,1,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int4_263 = torch.constant.int 4
    %int1_264 = torch.constant.int 1
    %int4096_265 = torch.constant.int 4096
    %556 = torch.prim.ListConstruct %int4_263, %int1_264, %int4096_265 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %557 = torch.aten.view %555, %556 : !torch.vtensor<[4,1,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_266 = torch.constant.int -2
    %int-1_267 = torch.constant.int -1
    %558 = torch.aten.transpose.int %7, %int-2_266, %int-1_267 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_268 = torch.constant.int 4
    %int4096_269 = torch.constant.int 4096
    %559 = torch.prim.ListConstruct %int4_268, %int4096_269 : (!torch.int, !torch.int) -> !torch.list<int>
    %560 = torch.aten.view %557, %559 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %561 = torch.aten.mm %560, %558 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_270 = torch.constant.int 4
    %int1_271 = torch.constant.int 1
    %int4096_272 = torch.constant.int 4096
    %562 = torch.prim.ListConstruct %int4_270, %int1_271, %int4096_272 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %563 = torch.aten.view %561, %562 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_273 = torch.constant.int 1
    %564 = torch.aten.add.Tensor %390, %563, %int1_273 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_274 = torch.constant.int 6
    %565 = torch.prims.convert_element_type %564, %int6_274 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_275 = torch.constant.int 2
    %566 = torch.aten.pow.Tensor_Scalar %565, %int2_275 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_276 = torch.constant.int -1
    %567 = torch.prim.ListConstruct %int-1_276 : (!torch.int) -> !torch.list<int>
    %true_277 = torch.constant.bool true
    %none_278 = torch.constant.none
    %568 = torch.aten.mean.dim %566, %567, %true_277, %none_278 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_279 = torch.constant.float 9.9999997473787516E-6
    %int1_280 = torch.constant.int 1
    %569 = torch.aten.add.Scalar %568, %float9.999990e-06_279, %int1_280 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %570 = torch.aten.rsqrt %569 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %571 = torch.aten.mul.Tensor %565, %570 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_281 = torch.constant.int 5
    %572 = torch.prims.convert_element_type %571, %int5_281 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %573 = torch.aten.mul.Tensor %8, %572 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_282 = torch.constant.int 5
    %574 = torch.prims.convert_element_type %573, %int5_282 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_283 = torch.constant.int -2
    %int-1_284 = torch.constant.int -1
    %575 = torch.aten.transpose.int %9, %int-2_283, %int-1_284 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_285 = torch.constant.int 4
    %int4096_286 = torch.constant.int 4096
    %576 = torch.prim.ListConstruct %int4_285, %int4096_286 : (!torch.int, !torch.int) -> !torch.list<int>
    %577 = torch.aten.view %574, %576 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %578 = torch.aten.mm %577, %575 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_287 = torch.constant.int 4
    %int1_288 = torch.constant.int 1
    %int14336 = torch.constant.int 14336
    %579 = torch.prim.ListConstruct %int4_287, %int1_288, %int14336 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %580 = torch.aten.view %578, %579 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %581 = torch.aten.silu %580 : !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_289 = torch.constant.int -2
    %int-1_290 = torch.constant.int -1
    %582 = torch.aten.transpose.int %10, %int-2_289, %int-1_290 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_291 = torch.constant.int 4
    %int4096_292 = torch.constant.int 4096
    %583 = torch.prim.ListConstruct %int4_291, %int4096_292 : (!torch.int, !torch.int) -> !torch.list<int>
    %584 = torch.aten.view %574, %583 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %585 = torch.aten.mm %584, %582 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_293 = torch.constant.int 4
    %int1_294 = torch.constant.int 1
    %int14336_295 = torch.constant.int 14336
    %586 = torch.prim.ListConstruct %int4_293, %int1_294, %int14336_295 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %587 = torch.aten.view %585, %586 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %588 = torch.aten.mul.Tensor %581, %587 : !torch.vtensor<[4,1,14336],f16>, !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_296 = torch.constant.int -2
    %int-1_297 = torch.constant.int -1
    %589 = torch.aten.transpose.int %11, %int-2_296, %int-1_297 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int4_298 = torch.constant.int 4
    %int14336_299 = torch.constant.int 14336
    %590 = torch.prim.ListConstruct %int4_298, %int14336_299 : (!torch.int, !torch.int) -> !torch.list<int>
    %591 = torch.aten.view %588, %590 : !torch.vtensor<[4,1,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,14336],f16>
    %592 = torch.aten.mm %591, %589 : !torch.vtensor<[4,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_300 = torch.constant.int 4
    %int1_301 = torch.constant.int 1
    %int4096_302 = torch.constant.int 4096
    %593 = torch.prim.ListConstruct %int4_300, %int1_301, %int4096_302 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %594 = torch.aten.view %592, %593 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_303 = torch.constant.int 1
    %595 = torch.aten.add.Tensor %564, %594, %int1_303 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_304 = torch.constant.int 6
    %596 = torch.prims.convert_element_type %595, %int6_304 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_305 = torch.constant.int 2
    %597 = torch.aten.pow.Tensor_Scalar %596, %int2_305 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_306 = torch.constant.int -1
    %598 = torch.prim.ListConstruct %int-1_306 : (!torch.int) -> !torch.list<int>
    %true_307 = torch.constant.bool true
    %none_308 = torch.constant.none
    %599 = torch.aten.mean.dim %597, %598, %true_307, %none_308 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_309 = torch.constant.float 9.9999997473787516E-6
    %int1_310 = torch.constant.int 1
    %600 = torch.aten.add.Scalar %599, %float9.999990e-06_309, %int1_310 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %601 = torch.aten.rsqrt %600 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %602 = torch.aten.mul.Tensor %596, %601 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_311 = torch.constant.int 5
    %603 = torch.prims.convert_element_type %602, %int5_311 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %604 = torch.aten.mul.Tensor %12, %603 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_312 = torch.constant.int 5
    %605 = torch.prims.convert_element_type %604, %int5_312 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_313 = torch.constant.int -2
    %int-1_314 = torch.constant.int -1
    %606 = torch.aten.transpose.int %13, %int-2_313, %int-1_314 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_315 = torch.constant.int 4
    %int4096_316 = torch.constant.int 4096
    %607 = torch.prim.ListConstruct %int4_315, %int4096_316 : (!torch.int, !torch.int) -> !torch.list<int>
    %608 = torch.aten.view %605, %607 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %609 = torch.aten.mm %608, %606 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_317 = torch.constant.int 4
    %int1_318 = torch.constant.int 1
    %int4096_319 = torch.constant.int 4096
    %610 = torch.prim.ListConstruct %int4_317, %int1_318, %int4096_319 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %611 = torch.aten.view %609, %610 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_320 = torch.constant.int -2
    %int-1_321 = torch.constant.int -1
    %612 = torch.aten.transpose.int %14, %int-2_320, %int-1_321 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_322 = torch.constant.int 4
    %int4096_323 = torch.constant.int 4096
    %613 = torch.prim.ListConstruct %int4_322, %int4096_323 : (!torch.int, !torch.int) -> !torch.list<int>
    %614 = torch.aten.view %605, %613 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %615 = torch.aten.mm %614, %612 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_324 = torch.constant.int 4
    %int1_325 = torch.constant.int 1
    %int1024_326 = torch.constant.int 1024
    %616 = torch.prim.ListConstruct %int4_324, %int1_325, %int1024_326 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %617 = torch.aten.view %615, %616 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int-2_327 = torch.constant.int -2
    %int-1_328 = torch.constant.int -1
    %618 = torch.aten.transpose.int %15, %int-2_327, %int-1_328 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_329 = torch.constant.int 4
    %int4096_330 = torch.constant.int 4096
    %619 = torch.prim.ListConstruct %int4_329, %int4096_330 : (!torch.int, !torch.int) -> !torch.list<int>
    %620 = torch.aten.view %605, %619 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %621 = torch.aten.mm %620, %618 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_331 = torch.constant.int 4
    %int1_332 = torch.constant.int 1
    %int1024_333 = torch.constant.int 1024
    %622 = torch.prim.ListConstruct %int4_331, %int1_332, %int1024_333 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %623 = torch.aten.view %621, %622 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int4_334 = torch.constant.int 4
    %int1_335 = torch.constant.int 1
    %int32_336 = torch.constant.int 32
    %int128_337 = torch.constant.int 128
    %624 = torch.prim.ListConstruct %int4_334, %int1_335, %int32_336, %int128_337 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %625 = torch.aten.view %611, %624 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,128],f16>
    %int4_338 = torch.constant.int 4
    %int1_339 = torch.constant.int 1
    %int8_340 = torch.constant.int 8
    %int128_341 = torch.constant.int 128
    %626 = torch.prim.ListConstruct %int4_338, %int1_339, %int8_340, %int128_341 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %627 = torch.aten.view %617, %626 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int4_342 = torch.constant.int 4
    %int1_343 = torch.constant.int 1
    %int8_344 = torch.constant.int 8
    %int128_345 = torch.constant.int 128
    %628 = torch.prim.ListConstruct %int4_342, %int1_343, %int8_344, %int128_345 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %629 = torch.aten.view %623, %628 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int6_346 = torch.constant.int 6
    %630 = torch.prims.convert_element_type %625, %int6_346 : !torch.vtensor<[4,1,32,128],f16>, !torch.int -> !torch.vtensor<[4,1,32,128],f32>
    %631 = torch_c.to_builtin_tensor %630 : !torch.vtensor<[4,1,32,128],f32> -> tensor<4x1x32x128xf32>
    %632 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %633 = util.call @sharktank_rotary_embedding_4_1_32_128_f32(%631, %632) : (tensor<4x1x32x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x32x128xf32>
    %634 = torch_c.from_builtin_tensor %633 : tensor<4x1x32x128xf32> -> !torch.vtensor<[4,1,32,128],f32>
    %int5_347 = torch.constant.int 5
    %635 = torch.prims.convert_element_type %634, %int5_347 : !torch.vtensor<[4,1,32,128],f32>, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int6_348 = torch.constant.int 6
    %636 = torch.prims.convert_element_type %627, %int6_348 : !torch.vtensor<[4,1,8,128],f16>, !torch.int -> !torch.vtensor<[4,1,8,128],f32>
    %637 = torch_c.to_builtin_tensor %636 : !torch.vtensor<[4,1,8,128],f32> -> tensor<4x1x8x128xf32>
    %638 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %639 = util.call @sharktank_rotary_embedding_4_1_8_128_f32(%637, %638) : (tensor<4x1x8x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x8x128xf32>
    %640 = torch_c.from_builtin_tensor %639 : tensor<4x1x8x128xf32> -> !torch.vtensor<[4,1,8,128],f32>
    %int5_349 = torch.constant.int 5
    %641 = torch.prims.convert_element_type %640, %int5_349 : !torch.vtensor<[4,1,8,128],f32>, !torch.int -> !torch.vtensor<[4,1,8,128],f16>
    %int32_350 = torch.constant.int 32
    %642 = torch.aten.floor_divide.Scalar %arg2, %int32_350 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_351 = torch.constant.int 1
    %643 = torch.aten.unsqueeze %642, %int1_351 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_352 = torch.constant.int 1
    %false_353 = torch.constant.bool false
    %644 = torch.aten.gather %arg3, %int1_352, %643, %false_353 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_354 = torch.constant.int 32
    %645 = torch.aten.remainder.Scalar %arg2, %int32_354 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_355 = torch.constant.int 1
    %646 = torch.aten.unsqueeze %645, %int1_355 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_356 = torch.constant.none
    %647 = torch.aten.clone %16, %none_356 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_357 = torch.constant.int 0
    %648 = torch.aten.unsqueeze %647, %int0_357 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_358 = torch.constant.int 4
    %int1_359 = torch.constant.int 1
    %649 = torch.prim.ListConstruct %int4_358, %int1_359 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_360 = torch.constant.int 1
    %int1_361 = torch.constant.int 1
    %650 = torch.prim.ListConstruct %int1_360, %int1_361 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_362 = torch.constant.int 4
    %int0_363 = torch.constant.int 0
    %cpu_364 = torch.constant.device "cpu"
    %false_365 = torch.constant.bool false
    %651 = torch.aten.empty_strided %649, %650, %int4_362, %int0_363, %cpu_364, %false_365 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int1_366 = torch.constant.int 1
    %652 = torch.aten.fill.Scalar %651, %int1_366 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_367 = torch.constant.int 4
    %int1_368 = torch.constant.int 1
    %653 = torch.prim.ListConstruct %int4_367, %int1_368 : (!torch.int, !torch.int) -> !torch.list<int>
    %654 = torch.aten.repeat %648, %653 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_369 = torch.constant.int 32
    %655 = torch.aten.mul.Scalar %644, %int32_369 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_370 = torch.constant.int 1
    %656 = torch.aten.add.Tensor %655, %652, %int1_370 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_371 = torch.constant.int 2
    %657 = torch.aten.mul.Scalar %656, %int2_371 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_372 = torch.constant.int 1
    %658 = torch.aten.add.Tensor %657, %654, %int1_372 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_373 = torch.constant.int 32
    %659 = torch.aten.mul.Scalar %658, %int32_373 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_374 = torch.constant.int 1
    %660 = torch.aten.add.Tensor %659, %646, %int1_374 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_375 = torch.constant.int 32
    %int2_376 = torch.constant.int 2
    %int32_377 = torch.constant.int 32
    %int8_378 = torch.constant.int 8
    %int128_379 = torch.constant.int 128
    %661 = torch.prim.ListConstruct %437, %int32_375, %int2_376, %int32_377, %int8_378, %int128_379 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %662 = torch.aten.view %498, %661 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %662, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_380 = torch.constant.int 32
    %663 = torch.aten.mul.int %437, %int32_380 : !torch.int, !torch.int -> !torch.int
    %int2_381 = torch.constant.int 2
    %664 = torch.aten.mul.int %663, %int2_381 : !torch.int, !torch.int -> !torch.int
    %int32_382 = torch.constant.int 32
    %665 = torch.aten.mul.int %664, %int32_382 : !torch.int, !torch.int -> !torch.int
    %int8_383 = torch.constant.int 8
    %int128_384 = torch.constant.int 128
    %666 = torch.prim.ListConstruct %665, %int8_383, %int128_384 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %667 = torch.aten.view %662, %666 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %667, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %668 = torch.prim.ListConstruct %660 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_385 = torch.constant.bool false
    %669 = torch.aten.index_put %667, %668, %641, %false_385 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %669, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_386 = torch.constant.int 32
    %int2_387 = torch.constant.int 2
    %int32_388 = torch.constant.int 32
    %int8_389 = torch.constant.int 8
    %int128_390 = torch.constant.int 128
    %670 = torch.prim.ListConstruct %437, %int32_386, %int2_387, %int32_388, %int8_389, %int128_390 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %671 = torch.aten.view %669, %670 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %671, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_391 = torch.constant.int 2097152
    %672 = torch.prim.ListConstruct %437, %int2097152_391 : (!torch.int, !torch.int) -> !torch.list<int>
    %673 = torch.aten.view %671, %672 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %673, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_392 = torch.constant.int 32
    %int2_393 = torch.constant.int 2
    %int32_394 = torch.constant.int 32
    %int8_395 = torch.constant.int 8
    %int128_396 = torch.constant.int 128
    %674 = torch.prim.ListConstruct %437, %int32_392, %int2_393, %int32_394, %int8_395, %int128_396 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %675 = torch.aten.view %673, %674 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %675, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int8_397 = torch.constant.int 8
    %int128_398 = torch.constant.int 128
    %676 = torch.prim.ListConstruct %665, %int8_397, %int128_398 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %677 = torch.aten.view %675, %676 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %677, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_399 = torch.constant.int 32
    %678 = torch.aten.floor_divide.Scalar %arg2, %int32_399 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_400 = torch.constant.int 1
    %679 = torch.aten.unsqueeze %678, %int1_400 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_401 = torch.constant.int 1
    %false_402 = torch.constant.bool false
    %680 = torch.aten.gather %arg3, %int1_401, %679, %false_402 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_403 = torch.constant.int 32
    %681 = torch.aten.remainder.Scalar %arg2, %int32_403 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_404 = torch.constant.int 1
    %682 = torch.aten.unsqueeze %681, %int1_404 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_405 = torch.constant.none
    %683 = torch.aten.clone %17, %none_405 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_406 = torch.constant.int 0
    %684 = torch.aten.unsqueeze %683, %int0_406 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_407 = torch.constant.int 4
    %int1_408 = torch.constant.int 1
    %685 = torch.prim.ListConstruct %int4_407, %int1_408 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_409 = torch.constant.int 1
    %int1_410 = torch.constant.int 1
    %686 = torch.prim.ListConstruct %int1_409, %int1_410 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_411 = torch.constant.int 4
    %int0_412 = torch.constant.int 0
    %cpu_413 = torch.constant.device "cpu"
    %false_414 = torch.constant.bool false
    %687 = torch.aten.empty_strided %685, %686, %int4_411, %int0_412, %cpu_413, %false_414 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int1_415 = torch.constant.int 1
    %688 = torch.aten.fill.Scalar %687, %int1_415 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_416 = torch.constant.int 4
    %int1_417 = torch.constant.int 1
    %689 = torch.prim.ListConstruct %int4_416, %int1_417 : (!torch.int, !torch.int) -> !torch.list<int>
    %690 = torch.aten.repeat %684, %689 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_418 = torch.constant.int 32
    %691 = torch.aten.mul.Scalar %680, %int32_418 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_419 = torch.constant.int 1
    %692 = torch.aten.add.Tensor %691, %688, %int1_419 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_420 = torch.constant.int 2
    %693 = torch.aten.mul.Scalar %692, %int2_420 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_421 = torch.constant.int 1
    %694 = torch.aten.add.Tensor %693, %690, %int1_421 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_422 = torch.constant.int 32
    %695 = torch.aten.mul.Scalar %694, %int32_422 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_423 = torch.constant.int 1
    %696 = torch.aten.add.Tensor %695, %682, %int1_423 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %697 = torch.prim.ListConstruct %696 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_424 = torch.constant.bool false
    %698 = torch.aten.index_put %677, %697, %629, %false_424 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %698, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_425 = torch.constant.int 32
    %int2_426 = torch.constant.int 2
    %int32_427 = torch.constant.int 32
    %int8_428 = torch.constant.int 8
    %int128_429 = torch.constant.int 128
    %699 = torch.prim.ListConstruct %437, %int32_425, %int2_426, %int32_427, %int8_428, %int128_429 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %700 = torch.aten.view %698, %699 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %700, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_430 = torch.constant.int 2097152
    %701 = torch.prim.ListConstruct %437, %int2097152_430 : (!torch.int, !torch.int) -> !torch.list<int>
    %702 = torch.aten.view %700, %701 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %702, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int4_431 = torch.constant.int 4
    %703 = torch.prim.ListConstruct %int4_431, %358 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_432 = torch.constant.int 1
    %704 = torch.prim.ListConstruct %358, %int1_432 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_433 = torch.constant.int 4
    %int0_434 = torch.constant.int 0
    %cpu_435 = torch.constant.device "cpu"
    %false_436 = torch.constant.bool false
    %705 = torch.aten.empty_strided %703, %704, %int4_433, %int0_434, %cpu_435, %false_436 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %705, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_437 = torch.constant.int 1
    %706 = torch.aten.fill.Scalar %705, %int1_437 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %706, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int32_438 = torch.constant.int 32
    %707 = torch.aten.mul.Scalar %arg3, %int32_438 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %707, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_439 = torch.constant.int 1
    %708 = torch.aten.add.Tensor %707, %706, %int1_439 : !torch.vtensor<[4,?],si64>, !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %708, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_440 = torch.constant.int 4
    %709 = torch.aten.mul.int %int4_440, %358 : !torch.int, !torch.int -> !torch.int
    %710 = torch.prim.ListConstruct %709 : (!torch.int) -> !torch.list<int>
    %711 = torch.aten.view %708, %710 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %711, [%356], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_441 = torch.constant.int 32
    %int2_442 = torch.constant.int 2
    %int32_443 = torch.constant.int 32
    %int8_444 = torch.constant.int 8
    %int128_445 = torch.constant.int 128
    %712 = torch.prim.ListConstruct %437, %int32_441, %int2_442, %int32_443, %int8_444, %int128_445 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %713 = torch.aten.view %702, %712 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %713, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_446 = torch.constant.int 32
    %714 = torch.aten.mul.int %437, %int32_446 : !torch.int, !torch.int -> !torch.int
    %int2_447 = torch.constant.int 2
    %int32_448 = torch.constant.int 32
    %int8_449 = torch.constant.int 8
    %int128_450 = torch.constant.int 128
    %715 = torch.prim.ListConstruct %714, %int2_447, %int32_448, %int8_449, %int128_450 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %716 = torch.aten.view %713, %715 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %716, [%357], affine_map<()[s0] -> (s0 * 32, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int0_451 = torch.constant.int 0
    %717 = torch.aten.index_select %716, %int0_451, %711 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.int, !torch.vtensor<[?],si64> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %717, [%356], affine_map<()[s0] -> (s0 * 4, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int4_452 = torch.constant.int 4
    %int2_453 = torch.constant.int 2
    %int32_454 = torch.constant.int 32
    %int8_455 = torch.constant.int 8
    %int128_456 = torch.constant.int 128
    %718 = torch.prim.ListConstruct %int4_452, %358, %int2_453, %int32_454, %int8_455, %int128_456 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %719 = torch.aten.view %717, %718 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %719, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int0_457 = torch.constant.int 0
    %int0_458 = torch.constant.int 0
    %int9223372036854775807_459 = torch.constant.int 9223372036854775807
    %int1_460 = torch.constant.int 1
    %720 = torch.aten.slice.Tensor %719, %int0_457, %int0_458, %int9223372036854775807_459, %int1_460 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %720, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_461 = torch.constant.int 1
    %int0_462 = torch.constant.int 0
    %int9223372036854775807_463 = torch.constant.int 9223372036854775807
    %int1_464 = torch.constant.int 1
    %721 = torch.aten.slice.Tensor %720, %int1_461, %int0_462, %int9223372036854775807_463, %int1_464 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %721, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_465 = torch.constant.int 2
    %int0_466 = torch.constant.int 0
    %722 = torch.aten.select.int %721, %int2_465, %int0_466 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %722, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int32_467 = torch.constant.int 32
    %723 = torch.aten.mul.int %358, %int32_467 : !torch.int, !torch.int -> !torch.int
    %int2_468 = torch.constant.int 2
    %int0_469 = torch.constant.int 0
    %int1_470 = torch.constant.int 1
    %724 = torch.aten.slice.Tensor %722, %int2_468, %int0_469, %723, %int1_470 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %724, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_471 = torch.constant.int 0
    %725 = torch.aten.clone %724, %int0_471 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %725, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_472 = torch.constant.int 1
    %726 = torch.aten.size.int %721, %int1_472 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_473 = torch.constant.int 32
    %727 = torch.aten.mul.int %726, %int32_473 : !torch.int, !torch.int -> !torch.int
    %int4_474 = torch.constant.int 4
    %int8_475 = torch.constant.int 8
    %int128_476 = torch.constant.int 128
    %728 = torch.prim.ListConstruct %int4_474, %727, %int8_475, %int128_476 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %729 = torch.aten._unsafe_view %725, %728 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %729, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_477 = torch.constant.int 0
    %int0_478 = torch.constant.int 0
    %int9223372036854775807_479 = torch.constant.int 9223372036854775807
    %int1_480 = torch.constant.int 1
    %730 = torch.aten.slice.Tensor %729, %int0_477, %int0_478, %int9223372036854775807_479, %int1_480 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %730, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_481 = torch.constant.int 0
    %int0_482 = torch.constant.int 0
    %int9223372036854775807_483 = torch.constant.int 9223372036854775807
    %int1_484 = torch.constant.int 1
    %731 = torch.aten.slice.Tensor %719, %int0_481, %int0_482, %int9223372036854775807_483, %int1_484 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %731, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_485 = torch.constant.int 1
    %int0_486 = torch.constant.int 0
    %int9223372036854775807_487 = torch.constant.int 9223372036854775807
    %int1_488 = torch.constant.int 1
    %732 = torch.aten.slice.Tensor %731, %int1_485, %int0_486, %int9223372036854775807_487, %int1_488 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %732, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_489 = torch.constant.int 2
    %int1_490 = torch.constant.int 1
    %733 = torch.aten.select.int %732, %int2_489, %int1_490 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %733, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int2_491 = torch.constant.int 2
    %int0_492 = torch.constant.int 0
    %int1_493 = torch.constant.int 1
    %734 = torch.aten.slice.Tensor %733, %int2_491, %int0_492, %723, %int1_493 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %734, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_494 = torch.constant.int 0
    %735 = torch.aten.clone %734, %int0_494 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %735, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_495 = torch.constant.int 1
    %736 = torch.aten.size.int %732, %int1_495 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_496 = torch.constant.int 32
    %737 = torch.aten.mul.int %736, %int32_496 : !torch.int, !torch.int -> !torch.int
    %int4_497 = torch.constant.int 4
    %int8_498 = torch.constant.int 8
    %int128_499 = torch.constant.int 128
    %738 = torch.prim.ListConstruct %int4_497, %737, %int8_498, %int128_499 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %739 = torch.aten._unsafe_view %735, %738 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %739, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_500 = torch.constant.int 0
    %int0_501 = torch.constant.int 0
    %int9223372036854775807_502 = torch.constant.int 9223372036854775807
    %int1_503 = torch.constant.int 1
    %740 = torch.aten.slice.Tensor %739, %int0_500, %int0_501, %int9223372036854775807_502, %int1_503 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %740, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int-2_504 = torch.constant.int -2
    %741 = torch.aten.unsqueeze %730, %int-2_504 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %741, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_505 = torch.constant.int 1
    %742 = torch.aten.size.int %729, %int1_505 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_506 = torch.constant.int 4
    %int8_507 = torch.constant.int 8
    %int4_508 = torch.constant.int 4
    %int128_509 = torch.constant.int 128
    %743 = torch.prim.ListConstruct %int4_506, %742, %int8_507, %int4_508, %int128_509 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_510 = torch.constant.bool false
    %744 = torch.aten.expand %741, %743, %false_510 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %744, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_511 = torch.constant.int 0
    %745 = torch.aten.clone %744, %int0_511 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %745, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_512 = torch.constant.int 4
    %int32_513 = torch.constant.int 32
    %int128_514 = torch.constant.int 128
    %746 = torch.prim.ListConstruct %int4_512, %742, %int32_513, %int128_514 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %747 = torch.aten._unsafe_view %745, %746 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %747, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_515 = torch.constant.int -2
    %748 = torch.aten.unsqueeze %740, %int-2_515 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %748, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_516 = torch.constant.int 1
    %749 = torch.aten.size.int %739, %int1_516 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_517 = torch.constant.int 4
    %int8_518 = torch.constant.int 8
    %int4_519 = torch.constant.int 4
    %int128_520 = torch.constant.int 128
    %750 = torch.prim.ListConstruct %int4_517, %749, %int8_518, %int4_519, %int128_520 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_521 = torch.constant.bool false
    %751 = torch.aten.expand %748, %750, %false_521 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %751, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_522 = torch.constant.int 0
    %752 = torch.aten.clone %751, %int0_522 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %752, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_523 = torch.constant.int 4
    %int32_524 = torch.constant.int 32
    %int128_525 = torch.constant.int 128
    %753 = torch.prim.ListConstruct %int4_523, %749, %int32_524, %int128_525 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %754 = torch.aten._unsafe_view %752, %753 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %754, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_526 = torch.constant.int 1
    %int2_527 = torch.constant.int 2
    %755 = torch.aten.transpose.int %635, %int1_526, %int2_527 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,128],f16>
    %int1_528 = torch.constant.int 1
    %int2_529 = torch.constant.int 2
    %756 = torch.aten.transpose.int %747, %int1_528, %int2_529 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %756, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_530 = torch.constant.int 1
    %int2_531 = torch.constant.int 2
    %757 = torch.aten.transpose.int %754, %int1_530, %int2_531 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %757, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_532 = torch.constant.float 0.000000e+00
    %false_533 = torch.constant.bool false
    %none_534 = torch.constant.none
    %758:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%755, %756, %757, %float0.000000e00_532, %false_533, %368, %none_534) : (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.vtensor<[4,1,1,?],f16>, !torch.none) -> (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,1],f32>) 
    %int1_535 = torch.constant.int 1
    %int2_536 = torch.constant.int 2
    %759 = torch.aten.transpose.int %758#0, %int1_535, %int2_536 : !torch.vtensor<[4,32,1,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int4_537 = torch.constant.int 4
    %int1_538 = torch.constant.int 1
    %int4096_539 = torch.constant.int 4096
    %760 = torch.prim.ListConstruct %int4_537, %int1_538, %int4096_539 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %761 = torch.aten.view %759, %760 : !torch.vtensor<[4,1,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_540 = torch.constant.int -2
    %int-1_541 = torch.constant.int -1
    %762 = torch.aten.transpose.int %18, %int-2_540, %int-1_541 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_542 = torch.constant.int 4
    %int4096_543 = torch.constant.int 4096
    %763 = torch.prim.ListConstruct %int4_542, %int4096_543 : (!torch.int, !torch.int) -> !torch.list<int>
    %764 = torch.aten.view %761, %763 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %765 = torch.aten.mm %764, %762 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_544 = torch.constant.int 4
    %int1_545 = torch.constant.int 1
    %int4096_546 = torch.constant.int 4096
    %766 = torch.prim.ListConstruct %int4_544, %int1_545, %int4096_546 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %767 = torch.aten.view %765, %766 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_547 = torch.constant.int 1
    %768 = torch.aten.add.Tensor %595, %767, %int1_547 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_548 = torch.constant.int 6
    %769 = torch.prims.convert_element_type %768, %int6_548 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_549 = torch.constant.int 2
    %770 = torch.aten.pow.Tensor_Scalar %769, %int2_549 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_550 = torch.constant.int -1
    %771 = torch.prim.ListConstruct %int-1_550 : (!torch.int) -> !torch.list<int>
    %true_551 = torch.constant.bool true
    %none_552 = torch.constant.none
    %772 = torch.aten.mean.dim %770, %771, %true_551, %none_552 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_553 = torch.constant.float 9.9999997473787516E-6
    %int1_554 = torch.constant.int 1
    %773 = torch.aten.add.Scalar %772, %float9.999990e-06_553, %int1_554 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %774 = torch.aten.rsqrt %773 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %775 = torch.aten.mul.Tensor %769, %774 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_555 = torch.constant.int 5
    %776 = torch.prims.convert_element_type %775, %int5_555 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %777 = torch.aten.mul.Tensor %19, %776 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_556 = torch.constant.int 5
    %778 = torch.prims.convert_element_type %777, %int5_556 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_557 = torch.constant.int -2
    %int-1_558 = torch.constant.int -1
    %779 = torch.aten.transpose.int %20, %int-2_557, %int-1_558 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_559 = torch.constant.int 4
    %int4096_560 = torch.constant.int 4096
    %780 = torch.prim.ListConstruct %int4_559, %int4096_560 : (!torch.int, !torch.int) -> !torch.list<int>
    %781 = torch.aten.view %778, %780 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %782 = torch.aten.mm %781, %779 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_561 = torch.constant.int 4
    %int1_562 = torch.constant.int 1
    %int14336_563 = torch.constant.int 14336
    %783 = torch.prim.ListConstruct %int4_561, %int1_562, %int14336_563 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %784 = torch.aten.view %782, %783 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %785 = torch.aten.silu %784 : !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_564 = torch.constant.int -2
    %int-1_565 = torch.constant.int -1
    %786 = torch.aten.transpose.int %21, %int-2_564, %int-1_565 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_566 = torch.constant.int 4
    %int4096_567 = torch.constant.int 4096
    %787 = torch.prim.ListConstruct %int4_566, %int4096_567 : (!torch.int, !torch.int) -> !torch.list<int>
    %788 = torch.aten.view %778, %787 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %789 = torch.aten.mm %788, %786 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_568 = torch.constant.int 4
    %int1_569 = torch.constant.int 1
    %int14336_570 = torch.constant.int 14336
    %790 = torch.prim.ListConstruct %int4_568, %int1_569, %int14336_570 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %791 = torch.aten.view %789, %790 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %792 = torch.aten.mul.Tensor %785, %791 : !torch.vtensor<[4,1,14336],f16>, !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_571 = torch.constant.int -2
    %int-1_572 = torch.constant.int -1
    %793 = torch.aten.transpose.int %22, %int-2_571, %int-1_572 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int4_573 = torch.constant.int 4
    %int14336_574 = torch.constant.int 14336
    %794 = torch.prim.ListConstruct %int4_573, %int14336_574 : (!torch.int, !torch.int) -> !torch.list<int>
    %795 = torch.aten.view %792, %794 : !torch.vtensor<[4,1,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,14336],f16>
    %796 = torch.aten.mm %795, %793 : !torch.vtensor<[4,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_575 = torch.constant.int 4
    %int1_576 = torch.constant.int 1
    %int4096_577 = torch.constant.int 4096
    %797 = torch.prim.ListConstruct %int4_575, %int1_576, %int4096_577 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %798 = torch.aten.view %796, %797 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_578 = torch.constant.int 1
    %799 = torch.aten.add.Tensor %768, %798, %int1_578 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_579 = torch.constant.int 6
    %800 = torch.prims.convert_element_type %799, %int6_579 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_580 = torch.constant.int 2
    %801 = torch.aten.pow.Tensor_Scalar %800, %int2_580 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_581 = torch.constant.int -1
    %802 = torch.prim.ListConstruct %int-1_581 : (!torch.int) -> !torch.list<int>
    %true_582 = torch.constant.bool true
    %none_583 = torch.constant.none
    %803 = torch.aten.mean.dim %801, %802, %true_582, %none_583 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_584 = torch.constant.float 9.9999997473787516E-6
    %int1_585 = torch.constant.int 1
    %804 = torch.aten.add.Scalar %803, %float9.999990e-06_584, %int1_585 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %805 = torch.aten.rsqrt %804 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %806 = torch.aten.mul.Tensor %800, %805 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_586 = torch.constant.int 5
    %807 = torch.prims.convert_element_type %806, %int5_586 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %808 = torch.aten.mul.Tensor %23, %807 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_587 = torch.constant.int 5
    %809 = torch.prims.convert_element_type %808, %int5_587 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_588 = torch.constant.int -2
    %int-1_589 = torch.constant.int -1
    %810 = torch.aten.transpose.int %24, %int-2_588, %int-1_589 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_590 = torch.constant.int 4
    %int4096_591 = torch.constant.int 4096
    %811 = torch.prim.ListConstruct %int4_590, %int4096_591 : (!torch.int, !torch.int) -> !torch.list<int>
    %812 = torch.aten.view %809, %811 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %813 = torch.aten.mm %812, %810 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_592 = torch.constant.int 4
    %int1_593 = torch.constant.int 1
    %int4096_594 = torch.constant.int 4096
    %814 = torch.prim.ListConstruct %int4_592, %int1_593, %int4096_594 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %815 = torch.aten.view %813, %814 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_595 = torch.constant.int -2
    %int-1_596 = torch.constant.int -1
    %816 = torch.aten.transpose.int %25, %int-2_595, %int-1_596 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_597 = torch.constant.int 4
    %int4096_598 = torch.constant.int 4096
    %817 = torch.prim.ListConstruct %int4_597, %int4096_598 : (!torch.int, !torch.int) -> !torch.list<int>
    %818 = torch.aten.view %809, %817 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %819 = torch.aten.mm %818, %816 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_599 = torch.constant.int 4
    %int1_600 = torch.constant.int 1
    %int1024_601 = torch.constant.int 1024
    %820 = torch.prim.ListConstruct %int4_599, %int1_600, %int1024_601 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %821 = torch.aten.view %819, %820 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int-2_602 = torch.constant.int -2
    %int-1_603 = torch.constant.int -1
    %822 = torch.aten.transpose.int %26, %int-2_602, %int-1_603 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_604 = torch.constant.int 4
    %int4096_605 = torch.constant.int 4096
    %823 = torch.prim.ListConstruct %int4_604, %int4096_605 : (!torch.int, !torch.int) -> !torch.list<int>
    %824 = torch.aten.view %809, %823 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %825 = torch.aten.mm %824, %822 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_606 = torch.constant.int 4
    %int1_607 = torch.constant.int 1
    %int1024_608 = torch.constant.int 1024
    %826 = torch.prim.ListConstruct %int4_606, %int1_607, %int1024_608 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %827 = torch.aten.view %825, %826 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int4_609 = torch.constant.int 4
    %int1_610 = torch.constant.int 1
    %int32_611 = torch.constant.int 32
    %int128_612 = torch.constant.int 128
    %828 = torch.prim.ListConstruct %int4_609, %int1_610, %int32_611, %int128_612 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %829 = torch.aten.view %815, %828 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,128],f16>
    %int4_613 = torch.constant.int 4
    %int1_614 = torch.constant.int 1
    %int8_615 = torch.constant.int 8
    %int128_616 = torch.constant.int 128
    %830 = torch.prim.ListConstruct %int4_613, %int1_614, %int8_615, %int128_616 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %831 = torch.aten.view %821, %830 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int4_617 = torch.constant.int 4
    %int1_618 = torch.constant.int 1
    %int8_619 = torch.constant.int 8
    %int128_620 = torch.constant.int 128
    %832 = torch.prim.ListConstruct %int4_617, %int1_618, %int8_619, %int128_620 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %833 = torch.aten.view %827, %832 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int6_621 = torch.constant.int 6
    %834 = torch.prims.convert_element_type %829, %int6_621 : !torch.vtensor<[4,1,32,128],f16>, !torch.int -> !torch.vtensor<[4,1,32,128],f32>
    %835 = torch_c.to_builtin_tensor %834 : !torch.vtensor<[4,1,32,128],f32> -> tensor<4x1x32x128xf32>
    %836 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %837 = util.call @sharktank_rotary_embedding_4_1_32_128_f32(%835, %836) : (tensor<4x1x32x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x32x128xf32>
    %838 = torch_c.from_builtin_tensor %837 : tensor<4x1x32x128xf32> -> !torch.vtensor<[4,1,32,128],f32>
    %int5_622 = torch.constant.int 5
    %839 = torch.prims.convert_element_type %838, %int5_622 : !torch.vtensor<[4,1,32,128],f32>, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int6_623 = torch.constant.int 6
    %840 = torch.prims.convert_element_type %831, %int6_623 : !torch.vtensor<[4,1,8,128],f16>, !torch.int -> !torch.vtensor<[4,1,8,128],f32>
    %841 = torch_c.to_builtin_tensor %840 : !torch.vtensor<[4,1,8,128],f32> -> tensor<4x1x8x128xf32>
    %842 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %843 = util.call @sharktank_rotary_embedding_4_1_8_128_f32(%841, %842) : (tensor<4x1x8x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x8x128xf32>
    %844 = torch_c.from_builtin_tensor %843 : tensor<4x1x8x128xf32> -> !torch.vtensor<[4,1,8,128],f32>
    %int5_624 = torch.constant.int 5
    %845 = torch.prims.convert_element_type %844, %int5_624 : !torch.vtensor<[4,1,8,128],f32>, !torch.int -> !torch.vtensor<[4,1,8,128],f16>
    %int32_625 = torch.constant.int 32
    %846 = torch.aten.floor_divide.Scalar %arg2, %int32_625 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_626 = torch.constant.int 1
    %847 = torch.aten.unsqueeze %846, %int1_626 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_627 = torch.constant.int 1
    %false_628 = torch.constant.bool false
    %848 = torch.aten.gather %arg3, %int1_627, %847, %false_628 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_629 = torch.constant.int 32
    %849 = torch.aten.remainder.Scalar %arg2, %int32_629 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_630 = torch.constant.int 1
    %850 = torch.aten.unsqueeze %849, %int1_630 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_631 = torch.constant.none
    %851 = torch.aten.clone %27, %none_631 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_632 = torch.constant.int 0
    %852 = torch.aten.unsqueeze %851, %int0_632 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_633 = torch.constant.int 4
    %int1_634 = torch.constant.int 1
    %853 = torch.prim.ListConstruct %int4_633, %int1_634 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_635 = torch.constant.int 1
    %int1_636 = torch.constant.int 1
    %854 = torch.prim.ListConstruct %int1_635, %int1_636 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_637 = torch.constant.int 4
    %int0_638 = torch.constant.int 0
    %cpu_639 = torch.constant.device "cpu"
    %false_640 = torch.constant.bool false
    %855 = torch.aten.empty_strided %853, %854, %int4_637, %int0_638, %cpu_639, %false_640 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int2_641 = torch.constant.int 2
    %856 = torch.aten.fill.Scalar %855, %int2_641 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_642 = torch.constant.int 4
    %int1_643 = torch.constant.int 1
    %857 = torch.prim.ListConstruct %int4_642, %int1_643 : (!torch.int, !torch.int) -> !torch.list<int>
    %858 = torch.aten.repeat %852, %857 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_644 = torch.constant.int 32
    %859 = torch.aten.mul.Scalar %848, %int32_644 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_645 = torch.constant.int 1
    %860 = torch.aten.add.Tensor %859, %856, %int1_645 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_646 = torch.constant.int 2
    %861 = torch.aten.mul.Scalar %860, %int2_646 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_647 = torch.constant.int 1
    %862 = torch.aten.add.Tensor %861, %858, %int1_647 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_648 = torch.constant.int 32
    %863 = torch.aten.mul.Scalar %862, %int32_648 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_649 = torch.constant.int 1
    %864 = torch.aten.add.Tensor %863, %850, %int1_649 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_650 = torch.constant.int 32
    %int2_651 = torch.constant.int 2
    %int32_652 = torch.constant.int 32
    %int8_653 = torch.constant.int 8
    %int128_654 = torch.constant.int 128
    %865 = torch.prim.ListConstruct %437, %int32_650, %int2_651, %int32_652, %int8_653, %int128_654 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %866 = torch.aten.view %702, %865 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %866, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_655 = torch.constant.int 32
    %867 = torch.aten.mul.int %437, %int32_655 : !torch.int, !torch.int -> !torch.int
    %int2_656 = torch.constant.int 2
    %868 = torch.aten.mul.int %867, %int2_656 : !torch.int, !torch.int -> !torch.int
    %int32_657 = torch.constant.int 32
    %869 = torch.aten.mul.int %868, %int32_657 : !torch.int, !torch.int -> !torch.int
    %int8_658 = torch.constant.int 8
    %int128_659 = torch.constant.int 128
    %870 = torch.prim.ListConstruct %869, %int8_658, %int128_659 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %871 = torch.aten.view %866, %870 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %871, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %872 = torch.prim.ListConstruct %864 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_660 = torch.constant.bool false
    %873 = torch.aten.index_put %871, %872, %845, %false_660 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %873, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_661 = torch.constant.int 32
    %int2_662 = torch.constant.int 2
    %int32_663 = torch.constant.int 32
    %int8_664 = torch.constant.int 8
    %int128_665 = torch.constant.int 128
    %874 = torch.prim.ListConstruct %437, %int32_661, %int2_662, %int32_663, %int8_664, %int128_665 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %875 = torch.aten.view %873, %874 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %875, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_666 = torch.constant.int 2097152
    %876 = torch.prim.ListConstruct %437, %int2097152_666 : (!torch.int, !torch.int) -> !torch.list<int>
    %877 = torch.aten.view %875, %876 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %877, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_667 = torch.constant.int 32
    %int2_668 = torch.constant.int 2
    %int32_669 = torch.constant.int 32
    %int8_670 = torch.constant.int 8
    %int128_671 = torch.constant.int 128
    %878 = torch.prim.ListConstruct %437, %int32_667, %int2_668, %int32_669, %int8_670, %int128_671 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %879 = torch.aten.view %877, %878 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %879, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int8_672 = torch.constant.int 8
    %int128_673 = torch.constant.int 128
    %880 = torch.prim.ListConstruct %869, %int8_672, %int128_673 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %881 = torch.aten.view %879, %880 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %881, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_674 = torch.constant.int 32
    %882 = torch.aten.floor_divide.Scalar %arg2, %int32_674 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_675 = torch.constant.int 1
    %883 = torch.aten.unsqueeze %882, %int1_675 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_676 = torch.constant.int 1
    %false_677 = torch.constant.bool false
    %884 = torch.aten.gather %arg3, %int1_676, %883, %false_677 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_678 = torch.constant.int 32
    %885 = torch.aten.remainder.Scalar %arg2, %int32_678 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_679 = torch.constant.int 1
    %886 = torch.aten.unsqueeze %885, %int1_679 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_680 = torch.constant.none
    %887 = torch.aten.clone %28, %none_680 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_681 = torch.constant.int 0
    %888 = torch.aten.unsqueeze %887, %int0_681 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_682 = torch.constant.int 4
    %int1_683 = torch.constant.int 1
    %889 = torch.prim.ListConstruct %int4_682, %int1_683 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_684 = torch.constant.int 1
    %int1_685 = torch.constant.int 1
    %890 = torch.prim.ListConstruct %int1_684, %int1_685 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_686 = torch.constant.int 4
    %int0_687 = torch.constant.int 0
    %cpu_688 = torch.constant.device "cpu"
    %false_689 = torch.constant.bool false
    %891 = torch.aten.empty_strided %889, %890, %int4_686, %int0_687, %cpu_688, %false_689 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int2_690 = torch.constant.int 2
    %892 = torch.aten.fill.Scalar %891, %int2_690 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_691 = torch.constant.int 4
    %int1_692 = torch.constant.int 1
    %893 = torch.prim.ListConstruct %int4_691, %int1_692 : (!torch.int, !torch.int) -> !torch.list<int>
    %894 = torch.aten.repeat %888, %893 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_693 = torch.constant.int 32
    %895 = torch.aten.mul.Scalar %884, %int32_693 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_694 = torch.constant.int 1
    %896 = torch.aten.add.Tensor %895, %892, %int1_694 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_695 = torch.constant.int 2
    %897 = torch.aten.mul.Scalar %896, %int2_695 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_696 = torch.constant.int 1
    %898 = torch.aten.add.Tensor %897, %894, %int1_696 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_697 = torch.constant.int 32
    %899 = torch.aten.mul.Scalar %898, %int32_697 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_698 = torch.constant.int 1
    %900 = torch.aten.add.Tensor %899, %886, %int1_698 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %901 = torch.prim.ListConstruct %900 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_699 = torch.constant.bool false
    %902 = torch.aten.index_put %881, %901, %833, %false_699 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %902, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_700 = torch.constant.int 32
    %int2_701 = torch.constant.int 2
    %int32_702 = torch.constant.int 32
    %int8_703 = torch.constant.int 8
    %int128_704 = torch.constant.int 128
    %903 = torch.prim.ListConstruct %437, %int32_700, %int2_701, %int32_702, %int8_703, %int128_704 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %904 = torch.aten.view %902, %903 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %904, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_705 = torch.constant.int 2097152
    %905 = torch.prim.ListConstruct %437, %int2097152_705 : (!torch.int, !torch.int) -> !torch.list<int>
    %906 = torch.aten.view %904, %905 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %906, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int4_706 = torch.constant.int 4
    %907 = torch.prim.ListConstruct %int4_706, %358 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_707 = torch.constant.int 1
    %908 = torch.prim.ListConstruct %358, %int1_707 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_708 = torch.constant.int 4
    %int0_709 = torch.constant.int 0
    %cpu_710 = torch.constant.device "cpu"
    %false_711 = torch.constant.bool false
    %909 = torch.aten.empty_strided %907, %908, %int4_708, %int0_709, %cpu_710, %false_711 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %909, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int2_712 = torch.constant.int 2
    %910 = torch.aten.fill.Scalar %909, %int2_712 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %910, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int32_713 = torch.constant.int 32
    %911 = torch.aten.mul.Scalar %arg3, %int32_713 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %911, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_714 = torch.constant.int 1
    %912 = torch.aten.add.Tensor %911, %910, %int1_714 : !torch.vtensor<[4,?],si64>, !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %912, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_715 = torch.constant.int 4
    %913 = torch.aten.mul.int %int4_715, %358 : !torch.int, !torch.int -> !torch.int
    %914 = torch.prim.ListConstruct %913 : (!torch.int) -> !torch.list<int>
    %915 = torch.aten.view %912, %914 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %915, [%356], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_716 = torch.constant.int 32
    %int2_717 = torch.constant.int 2
    %int32_718 = torch.constant.int 32
    %int8_719 = torch.constant.int 8
    %int128_720 = torch.constant.int 128
    %916 = torch.prim.ListConstruct %437, %int32_716, %int2_717, %int32_718, %int8_719, %int128_720 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %917 = torch.aten.view %906, %916 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %917, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_721 = torch.constant.int 32
    %918 = torch.aten.mul.int %437, %int32_721 : !torch.int, !torch.int -> !torch.int
    %int2_722 = torch.constant.int 2
    %int32_723 = torch.constant.int 32
    %int8_724 = torch.constant.int 8
    %int128_725 = torch.constant.int 128
    %919 = torch.prim.ListConstruct %918, %int2_722, %int32_723, %int8_724, %int128_725 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %920 = torch.aten.view %917, %919 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %920, [%357], affine_map<()[s0] -> (s0 * 32, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int0_726 = torch.constant.int 0
    %921 = torch.aten.index_select %920, %int0_726, %915 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.int, !torch.vtensor<[?],si64> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %921, [%356], affine_map<()[s0] -> (s0 * 4, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int4_727 = torch.constant.int 4
    %int2_728 = torch.constant.int 2
    %int32_729 = torch.constant.int 32
    %int8_730 = torch.constant.int 8
    %int128_731 = torch.constant.int 128
    %922 = torch.prim.ListConstruct %int4_727, %358, %int2_728, %int32_729, %int8_730, %int128_731 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %923 = torch.aten.view %921, %922 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %923, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int0_732 = torch.constant.int 0
    %int0_733 = torch.constant.int 0
    %int9223372036854775807_734 = torch.constant.int 9223372036854775807
    %int1_735 = torch.constant.int 1
    %924 = torch.aten.slice.Tensor %923, %int0_732, %int0_733, %int9223372036854775807_734, %int1_735 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %924, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_736 = torch.constant.int 1
    %int0_737 = torch.constant.int 0
    %int9223372036854775807_738 = torch.constant.int 9223372036854775807
    %int1_739 = torch.constant.int 1
    %925 = torch.aten.slice.Tensor %924, %int1_736, %int0_737, %int9223372036854775807_738, %int1_739 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %925, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_740 = torch.constant.int 2
    %int0_741 = torch.constant.int 0
    %926 = torch.aten.select.int %925, %int2_740, %int0_741 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %926, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int32_742 = torch.constant.int 32
    %927 = torch.aten.mul.int %358, %int32_742 : !torch.int, !torch.int -> !torch.int
    %int2_743 = torch.constant.int 2
    %int0_744 = torch.constant.int 0
    %int1_745 = torch.constant.int 1
    %928 = torch.aten.slice.Tensor %926, %int2_743, %int0_744, %927, %int1_745 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %928, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_746 = torch.constant.int 0
    %929 = torch.aten.clone %928, %int0_746 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %929, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_747 = torch.constant.int 1
    %930 = torch.aten.size.int %925, %int1_747 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_748 = torch.constant.int 32
    %931 = torch.aten.mul.int %930, %int32_748 : !torch.int, !torch.int -> !torch.int
    %int4_749 = torch.constant.int 4
    %int8_750 = torch.constant.int 8
    %int128_751 = torch.constant.int 128
    %932 = torch.prim.ListConstruct %int4_749, %931, %int8_750, %int128_751 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %933 = torch.aten._unsafe_view %929, %932 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %933, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_752 = torch.constant.int 0
    %int0_753 = torch.constant.int 0
    %int9223372036854775807_754 = torch.constant.int 9223372036854775807
    %int1_755 = torch.constant.int 1
    %934 = torch.aten.slice.Tensor %933, %int0_752, %int0_753, %int9223372036854775807_754, %int1_755 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %934, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_756 = torch.constant.int 0
    %int0_757 = torch.constant.int 0
    %int9223372036854775807_758 = torch.constant.int 9223372036854775807
    %int1_759 = torch.constant.int 1
    %935 = torch.aten.slice.Tensor %923, %int0_756, %int0_757, %int9223372036854775807_758, %int1_759 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %935, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_760 = torch.constant.int 1
    %int0_761 = torch.constant.int 0
    %int9223372036854775807_762 = torch.constant.int 9223372036854775807
    %int1_763 = torch.constant.int 1
    %936 = torch.aten.slice.Tensor %935, %int1_760, %int0_761, %int9223372036854775807_762, %int1_763 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %936, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_764 = torch.constant.int 2
    %int1_765 = torch.constant.int 1
    %937 = torch.aten.select.int %936, %int2_764, %int1_765 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %937, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int2_766 = torch.constant.int 2
    %int0_767 = torch.constant.int 0
    %int1_768 = torch.constant.int 1
    %938 = torch.aten.slice.Tensor %937, %int2_766, %int0_767, %927, %int1_768 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %938, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_769 = torch.constant.int 0
    %939 = torch.aten.clone %938, %int0_769 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %939, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_770 = torch.constant.int 1
    %940 = torch.aten.size.int %936, %int1_770 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_771 = torch.constant.int 32
    %941 = torch.aten.mul.int %940, %int32_771 : !torch.int, !torch.int -> !torch.int
    %int4_772 = torch.constant.int 4
    %int8_773 = torch.constant.int 8
    %int128_774 = torch.constant.int 128
    %942 = torch.prim.ListConstruct %int4_772, %941, %int8_773, %int128_774 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %943 = torch.aten._unsafe_view %939, %942 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %943, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_775 = torch.constant.int 0
    %int0_776 = torch.constant.int 0
    %int9223372036854775807_777 = torch.constant.int 9223372036854775807
    %int1_778 = torch.constant.int 1
    %944 = torch.aten.slice.Tensor %943, %int0_775, %int0_776, %int9223372036854775807_777, %int1_778 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %944, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int-2_779 = torch.constant.int -2
    %945 = torch.aten.unsqueeze %934, %int-2_779 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %945, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_780 = torch.constant.int 1
    %946 = torch.aten.size.int %933, %int1_780 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_781 = torch.constant.int 4
    %int8_782 = torch.constant.int 8
    %int4_783 = torch.constant.int 4
    %int128_784 = torch.constant.int 128
    %947 = torch.prim.ListConstruct %int4_781, %946, %int8_782, %int4_783, %int128_784 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_785 = torch.constant.bool false
    %948 = torch.aten.expand %945, %947, %false_785 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %948, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_786 = torch.constant.int 0
    %949 = torch.aten.clone %948, %int0_786 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %949, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_787 = torch.constant.int 4
    %int32_788 = torch.constant.int 32
    %int128_789 = torch.constant.int 128
    %950 = torch.prim.ListConstruct %int4_787, %946, %int32_788, %int128_789 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %951 = torch.aten._unsafe_view %949, %950 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %951, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_790 = torch.constant.int -2
    %952 = torch.aten.unsqueeze %944, %int-2_790 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %952, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_791 = torch.constant.int 1
    %953 = torch.aten.size.int %943, %int1_791 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_792 = torch.constant.int 4
    %int8_793 = torch.constant.int 8
    %int4_794 = torch.constant.int 4
    %int128_795 = torch.constant.int 128
    %954 = torch.prim.ListConstruct %int4_792, %953, %int8_793, %int4_794, %int128_795 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_796 = torch.constant.bool false
    %955 = torch.aten.expand %952, %954, %false_796 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %955, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_797 = torch.constant.int 0
    %956 = torch.aten.clone %955, %int0_797 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %956, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_798 = torch.constant.int 4
    %int32_799 = torch.constant.int 32
    %int128_800 = torch.constant.int 128
    %957 = torch.prim.ListConstruct %int4_798, %953, %int32_799, %int128_800 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %958 = torch.aten._unsafe_view %956, %957 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %958, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_801 = torch.constant.int 1
    %int2_802 = torch.constant.int 2
    %959 = torch.aten.transpose.int %839, %int1_801, %int2_802 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,128],f16>
    %int1_803 = torch.constant.int 1
    %int2_804 = torch.constant.int 2
    %960 = torch.aten.transpose.int %951, %int1_803, %int2_804 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %960, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_805 = torch.constant.int 1
    %int2_806 = torch.constant.int 2
    %961 = torch.aten.transpose.int %958, %int1_805, %int2_806 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %961, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_807 = torch.constant.float 0.000000e+00
    %false_808 = torch.constant.bool false
    %none_809 = torch.constant.none
    %962:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%959, %960, %961, %float0.000000e00_807, %false_808, %368, %none_809) : (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.vtensor<[4,1,1,?],f16>, !torch.none) -> (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,1],f32>) 
    %int1_810 = torch.constant.int 1
    %int2_811 = torch.constant.int 2
    %963 = torch.aten.transpose.int %962#0, %int1_810, %int2_811 : !torch.vtensor<[4,32,1,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int4_812 = torch.constant.int 4
    %int1_813 = torch.constant.int 1
    %int4096_814 = torch.constant.int 4096
    %964 = torch.prim.ListConstruct %int4_812, %int1_813, %int4096_814 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %965 = torch.aten.view %963, %964 : !torch.vtensor<[4,1,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_815 = torch.constant.int -2
    %int-1_816 = torch.constant.int -1
    %966 = torch.aten.transpose.int %29, %int-2_815, %int-1_816 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_817 = torch.constant.int 4
    %int4096_818 = torch.constant.int 4096
    %967 = torch.prim.ListConstruct %int4_817, %int4096_818 : (!torch.int, !torch.int) -> !torch.list<int>
    %968 = torch.aten.view %965, %967 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %969 = torch.aten.mm %968, %966 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_819 = torch.constant.int 4
    %int1_820 = torch.constant.int 1
    %int4096_821 = torch.constant.int 4096
    %970 = torch.prim.ListConstruct %int4_819, %int1_820, %int4096_821 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %971 = torch.aten.view %969, %970 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_822 = torch.constant.int 1
    %972 = torch.aten.add.Tensor %799, %971, %int1_822 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_823 = torch.constant.int 6
    %973 = torch.prims.convert_element_type %972, %int6_823 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_824 = torch.constant.int 2
    %974 = torch.aten.pow.Tensor_Scalar %973, %int2_824 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_825 = torch.constant.int -1
    %975 = torch.prim.ListConstruct %int-1_825 : (!torch.int) -> !torch.list<int>
    %true_826 = torch.constant.bool true
    %none_827 = torch.constant.none
    %976 = torch.aten.mean.dim %974, %975, %true_826, %none_827 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_828 = torch.constant.float 9.9999997473787516E-6
    %int1_829 = torch.constant.int 1
    %977 = torch.aten.add.Scalar %976, %float9.999990e-06_828, %int1_829 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %978 = torch.aten.rsqrt %977 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %979 = torch.aten.mul.Tensor %973, %978 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_830 = torch.constant.int 5
    %980 = torch.prims.convert_element_type %979, %int5_830 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %981 = torch.aten.mul.Tensor %30, %980 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_831 = torch.constant.int 5
    %982 = torch.prims.convert_element_type %981, %int5_831 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_832 = torch.constant.int -2
    %int-1_833 = torch.constant.int -1
    %983 = torch.aten.transpose.int %31, %int-2_832, %int-1_833 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_834 = torch.constant.int 4
    %int4096_835 = torch.constant.int 4096
    %984 = torch.prim.ListConstruct %int4_834, %int4096_835 : (!torch.int, !torch.int) -> !torch.list<int>
    %985 = torch.aten.view %982, %984 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %986 = torch.aten.mm %985, %983 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_836 = torch.constant.int 4
    %int1_837 = torch.constant.int 1
    %int14336_838 = torch.constant.int 14336
    %987 = torch.prim.ListConstruct %int4_836, %int1_837, %int14336_838 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %988 = torch.aten.view %986, %987 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %989 = torch.aten.silu %988 : !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_839 = torch.constant.int -2
    %int-1_840 = torch.constant.int -1
    %990 = torch.aten.transpose.int %32, %int-2_839, %int-1_840 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_841 = torch.constant.int 4
    %int4096_842 = torch.constant.int 4096
    %991 = torch.prim.ListConstruct %int4_841, %int4096_842 : (!torch.int, !torch.int) -> !torch.list<int>
    %992 = torch.aten.view %982, %991 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %993 = torch.aten.mm %992, %990 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_843 = torch.constant.int 4
    %int1_844 = torch.constant.int 1
    %int14336_845 = torch.constant.int 14336
    %994 = torch.prim.ListConstruct %int4_843, %int1_844, %int14336_845 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %995 = torch.aten.view %993, %994 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %996 = torch.aten.mul.Tensor %989, %995 : !torch.vtensor<[4,1,14336],f16>, !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_846 = torch.constant.int -2
    %int-1_847 = torch.constant.int -1
    %997 = torch.aten.transpose.int %33, %int-2_846, %int-1_847 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int4_848 = torch.constant.int 4
    %int14336_849 = torch.constant.int 14336
    %998 = torch.prim.ListConstruct %int4_848, %int14336_849 : (!torch.int, !torch.int) -> !torch.list<int>
    %999 = torch.aten.view %996, %998 : !torch.vtensor<[4,1,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,14336],f16>
    %1000 = torch.aten.mm %999, %997 : !torch.vtensor<[4,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_850 = torch.constant.int 4
    %int1_851 = torch.constant.int 1
    %int4096_852 = torch.constant.int 4096
    %1001 = torch.prim.ListConstruct %int4_850, %int1_851, %int4096_852 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1002 = torch.aten.view %1000, %1001 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_853 = torch.constant.int 1
    %1003 = torch.aten.add.Tensor %972, %1002, %int1_853 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_854 = torch.constant.int 6
    %1004 = torch.prims.convert_element_type %1003, %int6_854 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_855 = torch.constant.int 2
    %1005 = torch.aten.pow.Tensor_Scalar %1004, %int2_855 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_856 = torch.constant.int -1
    %1006 = torch.prim.ListConstruct %int-1_856 : (!torch.int) -> !torch.list<int>
    %true_857 = torch.constant.bool true
    %none_858 = torch.constant.none
    %1007 = torch.aten.mean.dim %1005, %1006, %true_857, %none_858 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_859 = torch.constant.float 9.9999997473787516E-6
    %int1_860 = torch.constant.int 1
    %1008 = torch.aten.add.Scalar %1007, %float9.999990e-06_859, %int1_860 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %1009 = torch.aten.rsqrt %1008 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %1010 = torch.aten.mul.Tensor %1004, %1009 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_861 = torch.constant.int 5
    %1011 = torch.prims.convert_element_type %1010, %int5_861 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %1012 = torch.aten.mul.Tensor %34, %1011 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_862 = torch.constant.int 5
    %1013 = torch.prims.convert_element_type %1012, %int5_862 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_863 = torch.constant.int -2
    %int-1_864 = torch.constant.int -1
    %1014 = torch.aten.transpose.int %35, %int-2_863, %int-1_864 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_865 = torch.constant.int 4
    %int4096_866 = torch.constant.int 4096
    %1015 = torch.prim.ListConstruct %int4_865, %int4096_866 : (!torch.int, !torch.int) -> !torch.list<int>
    %1016 = torch.aten.view %1013, %1015 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %1017 = torch.aten.mm %1016, %1014 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_867 = torch.constant.int 4
    %int1_868 = torch.constant.int 1
    %int4096_869 = torch.constant.int 4096
    %1018 = torch.prim.ListConstruct %int4_867, %int1_868, %int4096_869 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1019 = torch.aten.view %1017, %1018 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_870 = torch.constant.int -2
    %int-1_871 = torch.constant.int -1
    %1020 = torch.aten.transpose.int %36, %int-2_870, %int-1_871 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_872 = torch.constant.int 4
    %int4096_873 = torch.constant.int 4096
    %1021 = torch.prim.ListConstruct %int4_872, %int4096_873 : (!torch.int, !torch.int) -> !torch.list<int>
    %1022 = torch.aten.view %1013, %1021 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %1023 = torch.aten.mm %1022, %1020 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_874 = torch.constant.int 4
    %int1_875 = torch.constant.int 1
    %int1024_876 = torch.constant.int 1024
    %1024 = torch.prim.ListConstruct %int4_874, %int1_875, %int1024_876 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1025 = torch.aten.view %1023, %1024 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int-2_877 = torch.constant.int -2
    %int-1_878 = torch.constant.int -1
    %1026 = torch.aten.transpose.int %37, %int-2_877, %int-1_878 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_879 = torch.constant.int 4
    %int4096_880 = torch.constant.int 4096
    %1027 = torch.prim.ListConstruct %int4_879, %int4096_880 : (!torch.int, !torch.int) -> !torch.list<int>
    %1028 = torch.aten.view %1013, %1027 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %1029 = torch.aten.mm %1028, %1026 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_881 = torch.constant.int 4
    %int1_882 = torch.constant.int 1
    %int1024_883 = torch.constant.int 1024
    %1030 = torch.prim.ListConstruct %int4_881, %int1_882, %int1024_883 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1031 = torch.aten.view %1029, %1030 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int4_884 = torch.constant.int 4
    %int1_885 = torch.constant.int 1
    %int32_886 = torch.constant.int 32
    %int128_887 = torch.constant.int 128
    %1032 = torch.prim.ListConstruct %int4_884, %int1_885, %int32_886, %int128_887 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1033 = torch.aten.view %1019, %1032 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,128],f16>
    %int4_888 = torch.constant.int 4
    %int1_889 = torch.constant.int 1
    %int8_890 = torch.constant.int 8
    %int128_891 = torch.constant.int 128
    %1034 = torch.prim.ListConstruct %int4_888, %int1_889, %int8_890, %int128_891 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1035 = torch.aten.view %1025, %1034 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int4_892 = torch.constant.int 4
    %int1_893 = torch.constant.int 1
    %int8_894 = torch.constant.int 8
    %int128_895 = torch.constant.int 128
    %1036 = torch.prim.ListConstruct %int4_892, %int1_893, %int8_894, %int128_895 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1037 = torch.aten.view %1031, %1036 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int6_896 = torch.constant.int 6
    %1038 = torch.prims.convert_element_type %1033, %int6_896 : !torch.vtensor<[4,1,32,128],f16>, !torch.int -> !torch.vtensor<[4,1,32,128],f32>
    %1039 = torch_c.to_builtin_tensor %1038 : !torch.vtensor<[4,1,32,128],f32> -> tensor<4x1x32x128xf32>
    %1040 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %1041 = util.call @sharktank_rotary_embedding_4_1_32_128_f32(%1039, %1040) : (tensor<4x1x32x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x32x128xf32>
    %1042 = torch_c.from_builtin_tensor %1041 : tensor<4x1x32x128xf32> -> !torch.vtensor<[4,1,32,128],f32>
    %int5_897 = torch.constant.int 5
    %1043 = torch.prims.convert_element_type %1042, %int5_897 : !torch.vtensor<[4,1,32,128],f32>, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int6_898 = torch.constant.int 6
    %1044 = torch.prims.convert_element_type %1035, %int6_898 : !torch.vtensor<[4,1,8,128],f16>, !torch.int -> !torch.vtensor<[4,1,8,128],f32>
    %1045 = torch_c.to_builtin_tensor %1044 : !torch.vtensor<[4,1,8,128],f32> -> tensor<4x1x8x128xf32>
    %1046 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %1047 = util.call @sharktank_rotary_embedding_4_1_8_128_f32(%1045, %1046) : (tensor<4x1x8x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x8x128xf32>
    %1048 = torch_c.from_builtin_tensor %1047 : tensor<4x1x8x128xf32> -> !torch.vtensor<[4,1,8,128],f32>
    %int5_899 = torch.constant.int 5
    %1049 = torch.prims.convert_element_type %1048, %int5_899 : !torch.vtensor<[4,1,8,128],f32>, !torch.int -> !torch.vtensor<[4,1,8,128],f16>
    %int32_900 = torch.constant.int 32
    %1050 = torch.aten.floor_divide.Scalar %arg2, %int32_900 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_901 = torch.constant.int 1
    %1051 = torch.aten.unsqueeze %1050, %int1_901 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_902 = torch.constant.int 1
    %false_903 = torch.constant.bool false
    %1052 = torch.aten.gather %arg3, %int1_902, %1051, %false_903 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_904 = torch.constant.int 32
    %1053 = torch.aten.remainder.Scalar %arg2, %int32_904 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_905 = torch.constant.int 1
    %1054 = torch.aten.unsqueeze %1053, %int1_905 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_906 = torch.constant.none
    %1055 = torch.aten.clone %38, %none_906 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_907 = torch.constant.int 0
    %1056 = torch.aten.unsqueeze %1055, %int0_907 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_908 = torch.constant.int 4
    %int1_909 = torch.constant.int 1
    %1057 = torch.prim.ListConstruct %int4_908, %int1_909 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_910 = torch.constant.int 1
    %int1_911 = torch.constant.int 1
    %1058 = torch.prim.ListConstruct %int1_910, %int1_911 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_912 = torch.constant.int 4
    %int0_913 = torch.constant.int 0
    %cpu_914 = torch.constant.device "cpu"
    %false_915 = torch.constant.bool false
    %1059 = torch.aten.empty_strided %1057, %1058, %int4_912, %int0_913, %cpu_914, %false_915 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int3 = torch.constant.int 3
    %1060 = torch.aten.fill.Scalar %1059, %int3 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_916 = torch.constant.int 4
    %int1_917 = torch.constant.int 1
    %1061 = torch.prim.ListConstruct %int4_916, %int1_917 : (!torch.int, !torch.int) -> !torch.list<int>
    %1062 = torch.aten.repeat %1056, %1061 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_918 = torch.constant.int 32
    %1063 = torch.aten.mul.Scalar %1052, %int32_918 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_919 = torch.constant.int 1
    %1064 = torch.aten.add.Tensor %1063, %1060, %int1_919 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_920 = torch.constant.int 2
    %1065 = torch.aten.mul.Scalar %1064, %int2_920 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_921 = torch.constant.int 1
    %1066 = torch.aten.add.Tensor %1065, %1062, %int1_921 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_922 = torch.constant.int 32
    %1067 = torch.aten.mul.Scalar %1066, %int32_922 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_923 = torch.constant.int 1
    %1068 = torch.aten.add.Tensor %1067, %1054, %int1_923 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_924 = torch.constant.int 32
    %int2_925 = torch.constant.int 2
    %int32_926 = torch.constant.int 32
    %int8_927 = torch.constant.int 8
    %int128_928 = torch.constant.int 128
    %1069 = torch.prim.ListConstruct %437, %int32_924, %int2_925, %int32_926, %int8_927, %int128_928 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1070 = torch.aten.view %906, %1069 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1070, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_929 = torch.constant.int 32
    %1071 = torch.aten.mul.int %437, %int32_929 : !torch.int, !torch.int -> !torch.int
    %int2_930 = torch.constant.int 2
    %1072 = torch.aten.mul.int %1071, %int2_930 : !torch.int, !torch.int -> !torch.int
    %int32_931 = torch.constant.int 32
    %1073 = torch.aten.mul.int %1072, %int32_931 : !torch.int, !torch.int -> !torch.int
    %int8_932 = torch.constant.int 8
    %int128_933 = torch.constant.int 128
    %1074 = torch.prim.ListConstruct %1073, %int8_932, %int128_933 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1075 = torch.aten.view %1070, %1074 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %1075, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %1076 = torch.prim.ListConstruct %1068 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_934 = torch.constant.bool false
    %1077 = torch.aten.index_put %1075, %1076, %1049, %false_934 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %1077, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_935 = torch.constant.int 32
    %int2_936 = torch.constant.int 2
    %int32_937 = torch.constant.int 32
    %int8_938 = torch.constant.int 8
    %int128_939 = torch.constant.int 128
    %1078 = torch.prim.ListConstruct %437, %int32_935, %int2_936, %int32_937, %int8_938, %int128_939 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1079 = torch.aten.view %1077, %1078 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1079, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_940 = torch.constant.int 2097152
    %1080 = torch.prim.ListConstruct %437, %int2097152_940 : (!torch.int, !torch.int) -> !torch.list<int>
    %1081 = torch.aten.view %1079, %1080 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %1081, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_941 = torch.constant.int 32
    %int2_942 = torch.constant.int 2
    %int32_943 = torch.constant.int 32
    %int8_944 = torch.constant.int 8
    %int128_945 = torch.constant.int 128
    %1082 = torch.prim.ListConstruct %437, %int32_941, %int2_942, %int32_943, %int8_944, %int128_945 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1083 = torch.aten.view %1081, %1082 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1083, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int8_946 = torch.constant.int 8
    %int128_947 = torch.constant.int 128
    %1084 = torch.prim.ListConstruct %1073, %int8_946, %int128_947 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1085 = torch.aten.view %1083, %1084 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %1085, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_948 = torch.constant.int 32
    %1086 = torch.aten.floor_divide.Scalar %arg2, %int32_948 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_949 = torch.constant.int 1
    %1087 = torch.aten.unsqueeze %1086, %int1_949 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_950 = torch.constant.int 1
    %false_951 = torch.constant.bool false
    %1088 = torch.aten.gather %arg3, %int1_950, %1087, %false_951 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_952 = torch.constant.int 32
    %1089 = torch.aten.remainder.Scalar %arg2, %int32_952 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_953 = torch.constant.int 1
    %1090 = torch.aten.unsqueeze %1089, %int1_953 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_954 = torch.constant.none
    %1091 = torch.aten.clone %39, %none_954 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_955 = torch.constant.int 0
    %1092 = torch.aten.unsqueeze %1091, %int0_955 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_956 = torch.constant.int 4
    %int1_957 = torch.constant.int 1
    %1093 = torch.prim.ListConstruct %int4_956, %int1_957 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_958 = torch.constant.int 1
    %int1_959 = torch.constant.int 1
    %1094 = torch.prim.ListConstruct %int1_958, %int1_959 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_960 = torch.constant.int 4
    %int0_961 = torch.constant.int 0
    %cpu_962 = torch.constant.device "cpu"
    %false_963 = torch.constant.bool false
    %1095 = torch.aten.empty_strided %1093, %1094, %int4_960, %int0_961, %cpu_962, %false_963 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int3_964 = torch.constant.int 3
    %1096 = torch.aten.fill.Scalar %1095, %int3_964 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_965 = torch.constant.int 4
    %int1_966 = torch.constant.int 1
    %1097 = torch.prim.ListConstruct %int4_965, %int1_966 : (!torch.int, !torch.int) -> !torch.list<int>
    %1098 = torch.aten.repeat %1092, %1097 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_967 = torch.constant.int 32
    %1099 = torch.aten.mul.Scalar %1088, %int32_967 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_968 = torch.constant.int 1
    %1100 = torch.aten.add.Tensor %1099, %1096, %int1_968 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_969 = torch.constant.int 2
    %1101 = torch.aten.mul.Scalar %1100, %int2_969 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_970 = torch.constant.int 1
    %1102 = torch.aten.add.Tensor %1101, %1098, %int1_970 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_971 = torch.constant.int 32
    %1103 = torch.aten.mul.Scalar %1102, %int32_971 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_972 = torch.constant.int 1
    %1104 = torch.aten.add.Tensor %1103, %1090, %int1_972 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %1105 = torch.prim.ListConstruct %1104 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_973 = torch.constant.bool false
    %1106 = torch.aten.index_put %1085, %1105, %1037, %false_973 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %1106, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_974 = torch.constant.int 32
    %int2_975 = torch.constant.int 2
    %int32_976 = torch.constant.int 32
    %int8_977 = torch.constant.int 8
    %int128_978 = torch.constant.int 128
    %1107 = torch.prim.ListConstruct %437, %int32_974, %int2_975, %int32_976, %int8_977, %int128_978 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1108 = torch.aten.view %1106, %1107 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1108, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_979 = torch.constant.int 2097152
    %1109 = torch.prim.ListConstruct %437, %int2097152_979 : (!torch.int, !torch.int) -> !torch.list<int>
    %1110 = torch.aten.view %1108, %1109 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %1110, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int4_980 = torch.constant.int 4
    %1111 = torch.prim.ListConstruct %int4_980, %358 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_981 = torch.constant.int 1
    %1112 = torch.prim.ListConstruct %358, %int1_981 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_982 = torch.constant.int 4
    %int0_983 = torch.constant.int 0
    %cpu_984 = torch.constant.device "cpu"
    %false_985 = torch.constant.bool false
    %1113 = torch.aten.empty_strided %1111, %1112, %int4_982, %int0_983, %cpu_984, %false_985 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1113, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int3_986 = torch.constant.int 3
    %1114 = torch.aten.fill.Scalar %1113, %int3_986 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1114, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int32_987 = torch.constant.int 32
    %1115 = torch.aten.mul.Scalar %arg3, %int32_987 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1115, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_988 = torch.constant.int 1
    %1116 = torch.aten.add.Tensor %1115, %1114, %int1_988 : !torch.vtensor<[4,?],si64>, !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1116, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_989 = torch.constant.int 4
    %1117 = torch.aten.mul.int %int4_989, %358 : !torch.int, !torch.int -> !torch.int
    %1118 = torch.prim.ListConstruct %1117 : (!torch.int) -> !torch.list<int>
    %1119 = torch.aten.view %1116, %1118 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1119, [%356], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_990 = torch.constant.int 32
    %int2_991 = torch.constant.int 2
    %int32_992 = torch.constant.int 32
    %int8_993 = torch.constant.int 8
    %int128_994 = torch.constant.int 128
    %1120 = torch.prim.ListConstruct %437, %int32_990, %int2_991, %int32_992, %int8_993, %int128_994 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1121 = torch.aten.view %1110, %1120 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1121, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_995 = torch.constant.int 32
    %1122 = torch.aten.mul.int %437, %int32_995 : !torch.int, !torch.int -> !torch.int
    %int2_996 = torch.constant.int 2
    %int32_997 = torch.constant.int 32
    %int8_998 = torch.constant.int 8
    %int128_999 = torch.constant.int 128
    %1123 = torch.prim.ListConstruct %1122, %int2_996, %int32_997, %int8_998, %int128_999 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1124 = torch.aten.view %1121, %1123 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %1124, [%357], affine_map<()[s0] -> (s0 * 32, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int0_1000 = torch.constant.int 0
    %1125 = torch.aten.index_select %1124, %int0_1000, %1119 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.int, !torch.vtensor<[?],si64> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %1125, [%356], affine_map<()[s0] -> (s0 * 4, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int4_1001 = torch.constant.int 4
    %int2_1002 = torch.constant.int 2
    %int32_1003 = torch.constant.int 32
    %int8_1004 = torch.constant.int 8
    %int128_1005 = torch.constant.int 128
    %1126 = torch.prim.ListConstruct %int4_1001, %358, %int2_1002, %int32_1003, %int8_1004, %int128_1005 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1127 = torch.aten.view %1125, %1126 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %1127, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int0_1006 = torch.constant.int 0
    %int0_1007 = torch.constant.int 0
    %int9223372036854775807_1008 = torch.constant.int 9223372036854775807
    %int1_1009 = torch.constant.int 1
    %1128 = torch.aten.slice.Tensor %1127, %int0_1006, %int0_1007, %int9223372036854775807_1008, %int1_1009 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %1128, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_1010 = torch.constant.int 1
    %int0_1011 = torch.constant.int 0
    %int9223372036854775807_1012 = torch.constant.int 9223372036854775807
    %int1_1013 = torch.constant.int 1
    %1129 = torch.aten.slice.Tensor %1128, %int1_1010, %int0_1011, %int9223372036854775807_1012, %int1_1013 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %1129, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_1014 = torch.constant.int 2
    %int0_1015 = torch.constant.int 0
    %1130 = torch.aten.select.int %1129, %int2_1014, %int0_1015 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1130, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int32_1016 = torch.constant.int 32
    %1131 = torch.aten.mul.int %358, %int32_1016 : !torch.int, !torch.int -> !torch.int
    %int2_1017 = torch.constant.int 2
    %int0_1018 = torch.constant.int 0
    %int1_1019 = torch.constant.int 1
    %1132 = torch.aten.slice.Tensor %1130, %int2_1017, %int0_1018, %1131, %int1_1019 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1132, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_1020 = torch.constant.int 0
    %1133 = torch.aten.clone %1132, %int0_1020 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1133, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_1021 = torch.constant.int 1
    %1134 = torch.aten.size.int %1129, %int1_1021 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_1022 = torch.constant.int 32
    %1135 = torch.aten.mul.int %1134, %int32_1022 : !torch.int, !torch.int -> !torch.int
    %int4_1023 = torch.constant.int 4
    %int8_1024 = torch.constant.int 8
    %int128_1025 = torch.constant.int 128
    %1136 = torch.prim.ListConstruct %int4_1023, %1135, %int8_1024, %int128_1025 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1137 = torch.aten._unsafe_view %1133, %1136 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %1137, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_1026 = torch.constant.int 0
    %int0_1027 = torch.constant.int 0
    %int9223372036854775807_1028 = torch.constant.int 9223372036854775807
    %int1_1029 = torch.constant.int 1
    %1138 = torch.aten.slice.Tensor %1137, %int0_1026, %int0_1027, %int9223372036854775807_1028, %int1_1029 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %1138, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_1030 = torch.constant.int 0
    %int0_1031 = torch.constant.int 0
    %int9223372036854775807_1032 = torch.constant.int 9223372036854775807
    %int1_1033 = torch.constant.int 1
    %1139 = torch.aten.slice.Tensor %1127, %int0_1030, %int0_1031, %int9223372036854775807_1032, %int1_1033 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %1139, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_1034 = torch.constant.int 1
    %int0_1035 = torch.constant.int 0
    %int9223372036854775807_1036 = torch.constant.int 9223372036854775807
    %int1_1037 = torch.constant.int 1
    %1140 = torch.aten.slice.Tensor %1139, %int1_1034, %int0_1035, %int9223372036854775807_1036, %int1_1037 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %1140, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_1038 = torch.constant.int 2
    %int1_1039 = torch.constant.int 1
    %1141 = torch.aten.select.int %1140, %int2_1038, %int1_1039 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1141, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int2_1040 = torch.constant.int 2
    %int0_1041 = torch.constant.int 0
    %int1_1042 = torch.constant.int 1
    %1142 = torch.aten.slice.Tensor %1141, %int2_1040, %int0_1041, %1131, %int1_1042 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1142, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_1043 = torch.constant.int 0
    %1143 = torch.aten.clone %1142, %int0_1043 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1143, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_1044 = torch.constant.int 1
    %1144 = torch.aten.size.int %1140, %int1_1044 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_1045 = torch.constant.int 32
    %1145 = torch.aten.mul.int %1144, %int32_1045 : !torch.int, !torch.int -> !torch.int
    %int4_1046 = torch.constant.int 4
    %int8_1047 = torch.constant.int 8
    %int128_1048 = torch.constant.int 128
    %1146 = torch.prim.ListConstruct %int4_1046, %1145, %int8_1047, %int128_1048 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1147 = torch.aten._unsafe_view %1143, %1146 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %1147, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_1049 = torch.constant.int 0
    %int0_1050 = torch.constant.int 0
    %int9223372036854775807_1051 = torch.constant.int 9223372036854775807
    %int1_1052 = torch.constant.int 1
    %1148 = torch.aten.slice.Tensor %1147, %int0_1049, %int0_1050, %int9223372036854775807_1051, %int1_1052 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %1148, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int-2_1053 = torch.constant.int -2
    %1149 = torch.aten.unsqueeze %1138, %int-2_1053 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %1149, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_1054 = torch.constant.int 1
    %1150 = torch.aten.size.int %1137, %int1_1054 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_1055 = torch.constant.int 4
    %int8_1056 = torch.constant.int 8
    %int4_1057 = torch.constant.int 4
    %int128_1058 = torch.constant.int 128
    %1151 = torch.prim.ListConstruct %int4_1055, %1150, %int8_1056, %int4_1057, %int128_1058 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_1059 = torch.constant.bool false
    %1152 = torch.aten.expand %1149, %1151, %false_1059 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1152, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_1060 = torch.constant.int 0
    %1153 = torch.aten.clone %1152, %int0_1060 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1153, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_1061 = torch.constant.int 4
    %int32_1062 = torch.constant.int 32
    %int128_1063 = torch.constant.int 128
    %1154 = torch.prim.ListConstruct %int4_1061, %1150, %int32_1062, %int128_1063 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1155 = torch.aten._unsafe_view %1153, %1154 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %1155, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_1064 = torch.constant.int -2
    %1156 = torch.aten.unsqueeze %1148, %int-2_1064 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %1156, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_1065 = torch.constant.int 1
    %1157 = torch.aten.size.int %1147, %int1_1065 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_1066 = torch.constant.int 4
    %int8_1067 = torch.constant.int 8
    %int4_1068 = torch.constant.int 4
    %int128_1069 = torch.constant.int 128
    %1158 = torch.prim.ListConstruct %int4_1066, %1157, %int8_1067, %int4_1068, %int128_1069 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_1070 = torch.constant.bool false
    %1159 = torch.aten.expand %1156, %1158, %false_1070 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1159, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_1071 = torch.constant.int 0
    %1160 = torch.aten.clone %1159, %int0_1071 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1160, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_1072 = torch.constant.int 4
    %int32_1073 = torch.constant.int 32
    %int128_1074 = torch.constant.int 128
    %1161 = torch.prim.ListConstruct %int4_1072, %1157, %int32_1073, %int128_1074 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1162 = torch.aten._unsafe_view %1160, %1161 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %1162, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_1075 = torch.constant.int 1
    %int2_1076 = torch.constant.int 2
    %1163 = torch.aten.transpose.int %1043, %int1_1075, %int2_1076 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,128],f16>
    %int1_1077 = torch.constant.int 1
    %int2_1078 = torch.constant.int 2
    %1164 = torch.aten.transpose.int %1155, %int1_1077, %int2_1078 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %1164, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_1079 = torch.constant.int 1
    %int2_1080 = torch.constant.int 2
    %1165 = torch.aten.transpose.int %1162, %int1_1079, %int2_1080 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %1165, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_1081 = torch.constant.float 0.000000e+00
    %false_1082 = torch.constant.bool false
    %none_1083 = torch.constant.none
    %1166:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1163, %1164, %1165, %float0.000000e00_1081, %false_1082, %368, %none_1083) : (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.vtensor<[4,1,1,?],f16>, !torch.none) -> (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,1],f32>) 
    %int1_1084 = torch.constant.int 1
    %int2_1085 = torch.constant.int 2
    %1167 = torch.aten.transpose.int %1166#0, %int1_1084, %int2_1085 : !torch.vtensor<[4,32,1,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int4_1086 = torch.constant.int 4
    %int1_1087 = torch.constant.int 1
    %int4096_1088 = torch.constant.int 4096
    %1168 = torch.prim.ListConstruct %int4_1086, %int1_1087, %int4096_1088 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1169 = torch.aten.view %1167, %1168 : !torch.vtensor<[4,1,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_1089 = torch.constant.int -2
    %int-1_1090 = torch.constant.int -1
    %1170 = torch.aten.transpose.int %40, %int-2_1089, %int-1_1090 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_1091 = torch.constant.int 4
    %int4096_1092 = torch.constant.int 4096
    %1171 = torch.prim.ListConstruct %int4_1091, %int4096_1092 : (!torch.int, !torch.int) -> !torch.list<int>
    %1172 = torch.aten.view %1169, %1171 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %1173 = torch.aten.mm %1172, %1170 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_1093 = torch.constant.int 4
    %int1_1094 = torch.constant.int 1
    %int4096_1095 = torch.constant.int 4096
    %1174 = torch.prim.ListConstruct %int4_1093, %int1_1094, %int4096_1095 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1175 = torch.aten.view %1173, %1174 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_1096 = torch.constant.int 1
    %1176 = torch.aten.add.Tensor %1003, %1175, %int1_1096 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_1097 = torch.constant.int 6
    %1177 = torch.prims.convert_element_type %1176, %int6_1097 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_1098 = torch.constant.int 2
    %1178 = torch.aten.pow.Tensor_Scalar %1177, %int2_1098 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_1099 = torch.constant.int -1
    %1179 = torch.prim.ListConstruct %int-1_1099 : (!torch.int) -> !torch.list<int>
    %true_1100 = torch.constant.bool true
    %none_1101 = torch.constant.none
    %1180 = torch.aten.mean.dim %1178, %1179, %true_1100, %none_1101 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_1102 = torch.constant.float 9.9999997473787516E-6
    %int1_1103 = torch.constant.int 1
    %1181 = torch.aten.add.Scalar %1180, %float9.999990e-06_1102, %int1_1103 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %1182 = torch.aten.rsqrt %1181 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %1183 = torch.aten.mul.Tensor %1177, %1182 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_1104 = torch.constant.int 5
    %1184 = torch.prims.convert_element_type %1183, %int5_1104 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %1185 = torch.aten.mul.Tensor %41, %1184 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_1105 = torch.constant.int 5
    %1186 = torch.prims.convert_element_type %1185, %int5_1105 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_1106 = torch.constant.int -2
    %int-1_1107 = torch.constant.int -1
    %1187 = torch.aten.transpose.int %42, %int-2_1106, %int-1_1107 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_1108 = torch.constant.int 4
    %int4096_1109 = torch.constant.int 4096
    %1188 = torch.prim.ListConstruct %int4_1108, %int4096_1109 : (!torch.int, !torch.int) -> !torch.list<int>
    %1189 = torch.aten.view %1186, %1188 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %1190 = torch.aten.mm %1189, %1187 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_1110 = torch.constant.int 4
    %int1_1111 = torch.constant.int 1
    %int14336_1112 = torch.constant.int 14336
    %1191 = torch.prim.ListConstruct %int4_1110, %int1_1111, %int14336_1112 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1192 = torch.aten.view %1190, %1191 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %1193 = torch.aten.silu %1192 : !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_1113 = torch.constant.int -2
    %int-1_1114 = torch.constant.int -1
    %1194 = torch.aten.transpose.int %43, %int-2_1113, %int-1_1114 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_1115 = torch.constant.int 4
    %int4096_1116 = torch.constant.int 4096
    %1195 = torch.prim.ListConstruct %int4_1115, %int4096_1116 : (!torch.int, !torch.int) -> !torch.list<int>
    %1196 = torch.aten.view %1186, %1195 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %1197 = torch.aten.mm %1196, %1194 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_1117 = torch.constant.int 4
    %int1_1118 = torch.constant.int 1
    %int14336_1119 = torch.constant.int 14336
    %1198 = torch.prim.ListConstruct %int4_1117, %int1_1118, %int14336_1119 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1199 = torch.aten.view %1197, %1198 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %1200 = torch.aten.mul.Tensor %1193, %1199 : !torch.vtensor<[4,1,14336],f16>, !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_1120 = torch.constant.int -2
    %int-1_1121 = torch.constant.int -1
    %1201 = torch.aten.transpose.int %44, %int-2_1120, %int-1_1121 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int4_1122 = torch.constant.int 4
    %int14336_1123 = torch.constant.int 14336
    %1202 = torch.prim.ListConstruct %int4_1122, %int14336_1123 : (!torch.int, !torch.int) -> !torch.list<int>
    %1203 = torch.aten.view %1200, %1202 : !torch.vtensor<[4,1,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,14336],f16>
    %1204 = torch.aten.mm %1203, %1201 : !torch.vtensor<[4,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_1124 = torch.constant.int 4
    %int1_1125 = torch.constant.int 1
    %int4096_1126 = torch.constant.int 4096
    %1205 = torch.prim.ListConstruct %int4_1124, %int1_1125, %int4096_1126 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1206 = torch.aten.view %1204, %1205 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_1127 = torch.constant.int 1
    %1207 = torch.aten.add.Tensor %1176, %1206, %int1_1127 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_1128 = torch.constant.int 6
    %1208 = torch.prims.convert_element_type %1207, %int6_1128 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_1129 = torch.constant.int 2
    %1209 = torch.aten.pow.Tensor_Scalar %1208, %int2_1129 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_1130 = torch.constant.int -1
    %1210 = torch.prim.ListConstruct %int-1_1130 : (!torch.int) -> !torch.list<int>
    %true_1131 = torch.constant.bool true
    %none_1132 = torch.constant.none
    %1211 = torch.aten.mean.dim %1209, %1210, %true_1131, %none_1132 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_1133 = torch.constant.float 9.9999997473787516E-6
    %int1_1134 = torch.constant.int 1
    %1212 = torch.aten.add.Scalar %1211, %float9.999990e-06_1133, %int1_1134 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %1213 = torch.aten.rsqrt %1212 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %1214 = torch.aten.mul.Tensor %1208, %1213 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_1135 = torch.constant.int 5
    %1215 = torch.prims.convert_element_type %1214, %int5_1135 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %1216 = torch.aten.mul.Tensor %45, %1215 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_1136 = torch.constant.int 5
    %1217 = torch.prims.convert_element_type %1216, %int5_1136 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_1137 = torch.constant.int -2
    %int-1_1138 = torch.constant.int -1
    %1218 = torch.aten.transpose.int %46, %int-2_1137, %int-1_1138 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_1139 = torch.constant.int 4
    %int4096_1140 = torch.constant.int 4096
    %1219 = torch.prim.ListConstruct %int4_1139, %int4096_1140 : (!torch.int, !torch.int) -> !torch.list<int>
    %1220 = torch.aten.view %1217, %1219 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %1221 = torch.aten.mm %1220, %1218 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_1141 = torch.constant.int 4
    %int1_1142 = torch.constant.int 1
    %int4096_1143 = torch.constant.int 4096
    %1222 = torch.prim.ListConstruct %int4_1141, %int1_1142, %int4096_1143 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1223 = torch.aten.view %1221, %1222 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_1144 = torch.constant.int -2
    %int-1_1145 = torch.constant.int -1
    %1224 = torch.aten.transpose.int %47, %int-2_1144, %int-1_1145 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_1146 = torch.constant.int 4
    %int4096_1147 = torch.constant.int 4096
    %1225 = torch.prim.ListConstruct %int4_1146, %int4096_1147 : (!torch.int, !torch.int) -> !torch.list<int>
    %1226 = torch.aten.view %1217, %1225 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %1227 = torch.aten.mm %1226, %1224 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_1148 = torch.constant.int 4
    %int1_1149 = torch.constant.int 1
    %int1024_1150 = torch.constant.int 1024
    %1228 = torch.prim.ListConstruct %int4_1148, %int1_1149, %int1024_1150 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1229 = torch.aten.view %1227, %1228 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int-2_1151 = torch.constant.int -2
    %int-1_1152 = torch.constant.int -1
    %1230 = torch.aten.transpose.int %48, %int-2_1151, %int-1_1152 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_1153 = torch.constant.int 4
    %int4096_1154 = torch.constant.int 4096
    %1231 = torch.prim.ListConstruct %int4_1153, %int4096_1154 : (!torch.int, !torch.int) -> !torch.list<int>
    %1232 = torch.aten.view %1217, %1231 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %1233 = torch.aten.mm %1232, %1230 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_1155 = torch.constant.int 4
    %int1_1156 = torch.constant.int 1
    %int1024_1157 = torch.constant.int 1024
    %1234 = torch.prim.ListConstruct %int4_1155, %int1_1156, %int1024_1157 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1235 = torch.aten.view %1233, %1234 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int4_1158 = torch.constant.int 4
    %int1_1159 = torch.constant.int 1
    %int32_1160 = torch.constant.int 32
    %int128_1161 = torch.constant.int 128
    %1236 = torch.prim.ListConstruct %int4_1158, %int1_1159, %int32_1160, %int128_1161 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1237 = torch.aten.view %1223, %1236 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,128],f16>
    %int4_1162 = torch.constant.int 4
    %int1_1163 = torch.constant.int 1
    %int8_1164 = torch.constant.int 8
    %int128_1165 = torch.constant.int 128
    %1238 = torch.prim.ListConstruct %int4_1162, %int1_1163, %int8_1164, %int128_1165 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1239 = torch.aten.view %1229, %1238 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int4_1166 = torch.constant.int 4
    %int1_1167 = torch.constant.int 1
    %int8_1168 = torch.constant.int 8
    %int128_1169 = torch.constant.int 128
    %1240 = torch.prim.ListConstruct %int4_1166, %int1_1167, %int8_1168, %int128_1169 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1241 = torch.aten.view %1235, %1240 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int6_1170 = torch.constant.int 6
    %1242 = torch.prims.convert_element_type %1237, %int6_1170 : !torch.vtensor<[4,1,32,128],f16>, !torch.int -> !torch.vtensor<[4,1,32,128],f32>
    %1243 = torch_c.to_builtin_tensor %1242 : !torch.vtensor<[4,1,32,128],f32> -> tensor<4x1x32x128xf32>
    %1244 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %1245 = util.call @sharktank_rotary_embedding_4_1_32_128_f32(%1243, %1244) : (tensor<4x1x32x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x32x128xf32>
    %1246 = torch_c.from_builtin_tensor %1245 : tensor<4x1x32x128xf32> -> !torch.vtensor<[4,1,32,128],f32>
    %int5_1171 = torch.constant.int 5
    %1247 = torch.prims.convert_element_type %1246, %int5_1171 : !torch.vtensor<[4,1,32,128],f32>, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int6_1172 = torch.constant.int 6
    %1248 = torch.prims.convert_element_type %1239, %int6_1172 : !torch.vtensor<[4,1,8,128],f16>, !torch.int -> !torch.vtensor<[4,1,8,128],f32>
    %1249 = torch_c.to_builtin_tensor %1248 : !torch.vtensor<[4,1,8,128],f32> -> tensor<4x1x8x128xf32>
    %1250 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %1251 = util.call @sharktank_rotary_embedding_4_1_8_128_f32(%1249, %1250) : (tensor<4x1x8x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x8x128xf32>
    %1252 = torch_c.from_builtin_tensor %1251 : tensor<4x1x8x128xf32> -> !torch.vtensor<[4,1,8,128],f32>
    %int5_1173 = torch.constant.int 5
    %1253 = torch.prims.convert_element_type %1252, %int5_1173 : !torch.vtensor<[4,1,8,128],f32>, !torch.int -> !torch.vtensor<[4,1,8,128],f16>
    %int32_1174 = torch.constant.int 32
    %1254 = torch.aten.floor_divide.Scalar %arg2, %int32_1174 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_1175 = torch.constant.int 1
    %1255 = torch.aten.unsqueeze %1254, %int1_1175 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_1176 = torch.constant.int 1
    %false_1177 = torch.constant.bool false
    %1256 = torch.aten.gather %arg3, %int1_1176, %1255, %false_1177 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_1178 = torch.constant.int 32
    %1257 = torch.aten.remainder.Scalar %arg2, %int32_1178 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_1179 = torch.constant.int 1
    %1258 = torch.aten.unsqueeze %1257, %int1_1179 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_1180 = torch.constant.none
    %1259 = torch.aten.clone %49, %none_1180 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_1181 = torch.constant.int 0
    %1260 = torch.aten.unsqueeze %1259, %int0_1181 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_1182 = torch.constant.int 4
    %int1_1183 = torch.constant.int 1
    %1261 = torch.prim.ListConstruct %int4_1182, %int1_1183 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_1184 = torch.constant.int 1
    %int1_1185 = torch.constant.int 1
    %1262 = torch.prim.ListConstruct %int1_1184, %int1_1185 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_1186 = torch.constant.int 4
    %int0_1187 = torch.constant.int 0
    %cpu_1188 = torch.constant.device "cpu"
    %false_1189 = torch.constant.bool false
    %1263 = torch.aten.empty_strided %1261, %1262, %int4_1186, %int0_1187, %cpu_1188, %false_1189 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int4_1190 = torch.constant.int 4
    %1264 = torch.aten.fill.Scalar %1263, %int4_1190 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_1191 = torch.constant.int 4
    %int1_1192 = torch.constant.int 1
    %1265 = torch.prim.ListConstruct %int4_1191, %int1_1192 : (!torch.int, !torch.int) -> !torch.list<int>
    %1266 = torch.aten.repeat %1260, %1265 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_1193 = torch.constant.int 32
    %1267 = torch.aten.mul.Scalar %1256, %int32_1193 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_1194 = torch.constant.int 1
    %1268 = torch.aten.add.Tensor %1267, %1264, %int1_1194 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_1195 = torch.constant.int 2
    %1269 = torch.aten.mul.Scalar %1268, %int2_1195 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_1196 = torch.constant.int 1
    %1270 = torch.aten.add.Tensor %1269, %1266, %int1_1196 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_1197 = torch.constant.int 32
    %1271 = torch.aten.mul.Scalar %1270, %int32_1197 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_1198 = torch.constant.int 1
    %1272 = torch.aten.add.Tensor %1271, %1258, %int1_1198 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_1199 = torch.constant.int 32
    %int2_1200 = torch.constant.int 2
    %int32_1201 = torch.constant.int 32
    %int8_1202 = torch.constant.int 8
    %int128_1203 = torch.constant.int 128
    %1273 = torch.prim.ListConstruct %437, %int32_1199, %int2_1200, %int32_1201, %int8_1202, %int128_1203 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1274 = torch.aten.view %1110, %1273 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1274, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_1204 = torch.constant.int 32
    %1275 = torch.aten.mul.int %437, %int32_1204 : !torch.int, !torch.int -> !torch.int
    %int2_1205 = torch.constant.int 2
    %1276 = torch.aten.mul.int %1275, %int2_1205 : !torch.int, !torch.int -> !torch.int
    %int32_1206 = torch.constant.int 32
    %1277 = torch.aten.mul.int %1276, %int32_1206 : !torch.int, !torch.int -> !torch.int
    %int8_1207 = torch.constant.int 8
    %int128_1208 = torch.constant.int 128
    %1278 = torch.prim.ListConstruct %1277, %int8_1207, %int128_1208 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1279 = torch.aten.view %1274, %1278 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %1279, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %1280 = torch.prim.ListConstruct %1272 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_1209 = torch.constant.bool false
    %1281 = torch.aten.index_put %1279, %1280, %1253, %false_1209 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %1281, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_1210 = torch.constant.int 32
    %int2_1211 = torch.constant.int 2
    %int32_1212 = torch.constant.int 32
    %int8_1213 = torch.constant.int 8
    %int128_1214 = torch.constant.int 128
    %1282 = torch.prim.ListConstruct %437, %int32_1210, %int2_1211, %int32_1212, %int8_1213, %int128_1214 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1283 = torch.aten.view %1281, %1282 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1283, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_1215 = torch.constant.int 2097152
    %1284 = torch.prim.ListConstruct %437, %int2097152_1215 : (!torch.int, !torch.int) -> !torch.list<int>
    %1285 = torch.aten.view %1283, %1284 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %1285, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_1216 = torch.constant.int 32
    %int2_1217 = torch.constant.int 2
    %int32_1218 = torch.constant.int 32
    %int8_1219 = torch.constant.int 8
    %int128_1220 = torch.constant.int 128
    %1286 = torch.prim.ListConstruct %437, %int32_1216, %int2_1217, %int32_1218, %int8_1219, %int128_1220 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1287 = torch.aten.view %1285, %1286 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1287, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int8_1221 = torch.constant.int 8
    %int128_1222 = torch.constant.int 128
    %1288 = torch.prim.ListConstruct %1277, %int8_1221, %int128_1222 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1289 = torch.aten.view %1287, %1288 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %1289, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_1223 = torch.constant.int 32
    %1290 = torch.aten.floor_divide.Scalar %arg2, %int32_1223 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_1224 = torch.constant.int 1
    %1291 = torch.aten.unsqueeze %1290, %int1_1224 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_1225 = torch.constant.int 1
    %false_1226 = torch.constant.bool false
    %1292 = torch.aten.gather %arg3, %int1_1225, %1291, %false_1226 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_1227 = torch.constant.int 32
    %1293 = torch.aten.remainder.Scalar %arg2, %int32_1227 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_1228 = torch.constant.int 1
    %1294 = torch.aten.unsqueeze %1293, %int1_1228 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_1229 = torch.constant.none
    %1295 = torch.aten.clone %50, %none_1229 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_1230 = torch.constant.int 0
    %1296 = torch.aten.unsqueeze %1295, %int0_1230 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_1231 = torch.constant.int 4
    %int1_1232 = torch.constant.int 1
    %1297 = torch.prim.ListConstruct %int4_1231, %int1_1232 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_1233 = torch.constant.int 1
    %int1_1234 = torch.constant.int 1
    %1298 = torch.prim.ListConstruct %int1_1233, %int1_1234 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_1235 = torch.constant.int 4
    %int0_1236 = torch.constant.int 0
    %cpu_1237 = torch.constant.device "cpu"
    %false_1238 = torch.constant.bool false
    %1299 = torch.aten.empty_strided %1297, %1298, %int4_1235, %int0_1236, %cpu_1237, %false_1238 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int4_1239 = torch.constant.int 4
    %1300 = torch.aten.fill.Scalar %1299, %int4_1239 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_1240 = torch.constant.int 4
    %int1_1241 = torch.constant.int 1
    %1301 = torch.prim.ListConstruct %int4_1240, %int1_1241 : (!torch.int, !torch.int) -> !torch.list<int>
    %1302 = torch.aten.repeat %1296, %1301 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_1242 = torch.constant.int 32
    %1303 = torch.aten.mul.Scalar %1292, %int32_1242 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_1243 = torch.constant.int 1
    %1304 = torch.aten.add.Tensor %1303, %1300, %int1_1243 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_1244 = torch.constant.int 2
    %1305 = torch.aten.mul.Scalar %1304, %int2_1244 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_1245 = torch.constant.int 1
    %1306 = torch.aten.add.Tensor %1305, %1302, %int1_1245 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_1246 = torch.constant.int 32
    %1307 = torch.aten.mul.Scalar %1306, %int32_1246 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_1247 = torch.constant.int 1
    %1308 = torch.aten.add.Tensor %1307, %1294, %int1_1247 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %1309 = torch.prim.ListConstruct %1308 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_1248 = torch.constant.bool false
    %1310 = torch.aten.index_put %1289, %1309, %1241, %false_1248 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %1310, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_1249 = torch.constant.int 32
    %int2_1250 = torch.constant.int 2
    %int32_1251 = torch.constant.int 32
    %int8_1252 = torch.constant.int 8
    %int128_1253 = torch.constant.int 128
    %1311 = torch.prim.ListConstruct %437, %int32_1249, %int2_1250, %int32_1251, %int8_1252, %int128_1253 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1312 = torch.aten.view %1310, %1311 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1312, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_1254 = torch.constant.int 2097152
    %1313 = torch.prim.ListConstruct %437, %int2097152_1254 : (!torch.int, !torch.int) -> !torch.list<int>
    %1314 = torch.aten.view %1312, %1313 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %1314, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int4_1255 = torch.constant.int 4
    %1315 = torch.prim.ListConstruct %int4_1255, %358 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_1256 = torch.constant.int 1
    %1316 = torch.prim.ListConstruct %358, %int1_1256 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_1257 = torch.constant.int 4
    %int0_1258 = torch.constant.int 0
    %cpu_1259 = torch.constant.device "cpu"
    %false_1260 = torch.constant.bool false
    %1317 = torch.aten.empty_strided %1315, %1316, %int4_1257, %int0_1258, %cpu_1259, %false_1260 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1317, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_1261 = torch.constant.int 4
    %1318 = torch.aten.fill.Scalar %1317, %int4_1261 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1318, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int32_1262 = torch.constant.int 32
    %1319 = torch.aten.mul.Scalar %arg3, %int32_1262 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1319, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_1263 = torch.constant.int 1
    %1320 = torch.aten.add.Tensor %1319, %1318, %int1_1263 : !torch.vtensor<[4,?],si64>, !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1320, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_1264 = torch.constant.int 4
    %1321 = torch.aten.mul.int %int4_1264, %358 : !torch.int, !torch.int -> !torch.int
    %1322 = torch.prim.ListConstruct %1321 : (!torch.int) -> !torch.list<int>
    %1323 = torch.aten.view %1320, %1322 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1323, [%356], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_1265 = torch.constant.int 32
    %int2_1266 = torch.constant.int 2
    %int32_1267 = torch.constant.int 32
    %int8_1268 = torch.constant.int 8
    %int128_1269 = torch.constant.int 128
    %1324 = torch.prim.ListConstruct %437, %int32_1265, %int2_1266, %int32_1267, %int8_1268, %int128_1269 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1325 = torch.aten.view %1314, %1324 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1325, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_1270 = torch.constant.int 32
    %1326 = torch.aten.mul.int %437, %int32_1270 : !torch.int, !torch.int -> !torch.int
    %int2_1271 = torch.constant.int 2
    %int32_1272 = torch.constant.int 32
    %int8_1273 = torch.constant.int 8
    %int128_1274 = torch.constant.int 128
    %1327 = torch.prim.ListConstruct %1326, %int2_1271, %int32_1272, %int8_1273, %int128_1274 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1328 = torch.aten.view %1325, %1327 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %1328, [%357], affine_map<()[s0] -> (s0 * 32, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int0_1275 = torch.constant.int 0
    %1329 = torch.aten.index_select %1328, %int0_1275, %1323 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.int, !torch.vtensor<[?],si64> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %1329, [%356], affine_map<()[s0] -> (s0 * 4, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int4_1276 = torch.constant.int 4
    %int2_1277 = torch.constant.int 2
    %int32_1278 = torch.constant.int 32
    %int8_1279 = torch.constant.int 8
    %int128_1280 = torch.constant.int 128
    %1330 = torch.prim.ListConstruct %int4_1276, %358, %int2_1277, %int32_1278, %int8_1279, %int128_1280 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1331 = torch.aten.view %1329, %1330 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %1331, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int0_1281 = torch.constant.int 0
    %int0_1282 = torch.constant.int 0
    %int9223372036854775807_1283 = torch.constant.int 9223372036854775807
    %int1_1284 = torch.constant.int 1
    %1332 = torch.aten.slice.Tensor %1331, %int0_1281, %int0_1282, %int9223372036854775807_1283, %int1_1284 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %1332, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_1285 = torch.constant.int 1
    %int0_1286 = torch.constant.int 0
    %int9223372036854775807_1287 = torch.constant.int 9223372036854775807
    %int1_1288 = torch.constant.int 1
    %1333 = torch.aten.slice.Tensor %1332, %int1_1285, %int0_1286, %int9223372036854775807_1287, %int1_1288 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %1333, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_1289 = torch.constant.int 2
    %int0_1290 = torch.constant.int 0
    %1334 = torch.aten.select.int %1333, %int2_1289, %int0_1290 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1334, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int32_1291 = torch.constant.int 32
    %1335 = torch.aten.mul.int %358, %int32_1291 : !torch.int, !torch.int -> !torch.int
    %int2_1292 = torch.constant.int 2
    %int0_1293 = torch.constant.int 0
    %int1_1294 = torch.constant.int 1
    %1336 = torch.aten.slice.Tensor %1334, %int2_1292, %int0_1293, %1335, %int1_1294 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1336, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_1295 = torch.constant.int 0
    %1337 = torch.aten.clone %1336, %int0_1295 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1337, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_1296 = torch.constant.int 1
    %1338 = torch.aten.size.int %1333, %int1_1296 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_1297 = torch.constant.int 32
    %1339 = torch.aten.mul.int %1338, %int32_1297 : !torch.int, !torch.int -> !torch.int
    %int4_1298 = torch.constant.int 4
    %int8_1299 = torch.constant.int 8
    %int128_1300 = torch.constant.int 128
    %1340 = torch.prim.ListConstruct %int4_1298, %1339, %int8_1299, %int128_1300 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1341 = torch.aten._unsafe_view %1337, %1340 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %1341, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_1301 = torch.constant.int 0
    %int0_1302 = torch.constant.int 0
    %int9223372036854775807_1303 = torch.constant.int 9223372036854775807
    %int1_1304 = torch.constant.int 1
    %1342 = torch.aten.slice.Tensor %1341, %int0_1301, %int0_1302, %int9223372036854775807_1303, %int1_1304 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %1342, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_1305 = torch.constant.int 0
    %int0_1306 = torch.constant.int 0
    %int9223372036854775807_1307 = torch.constant.int 9223372036854775807
    %int1_1308 = torch.constant.int 1
    %1343 = torch.aten.slice.Tensor %1331, %int0_1305, %int0_1306, %int9223372036854775807_1307, %int1_1308 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %1343, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_1309 = torch.constant.int 1
    %int0_1310 = torch.constant.int 0
    %int9223372036854775807_1311 = torch.constant.int 9223372036854775807
    %int1_1312 = torch.constant.int 1
    %1344 = torch.aten.slice.Tensor %1343, %int1_1309, %int0_1310, %int9223372036854775807_1311, %int1_1312 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %1344, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_1313 = torch.constant.int 2
    %int1_1314 = torch.constant.int 1
    %1345 = torch.aten.select.int %1344, %int2_1313, %int1_1314 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1345, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int2_1315 = torch.constant.int 2
    %int0_1316 = torch.constant.int 0
    %int1_1317 = torch.constant.int 1
    %1346 = torch.aten.slice.Tensor %1345, %int2_1315, %int0_1316, %1335, %int1_1317 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1346, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_1318 = torch.constant.int 0
    %1347 = torch.aten.clone %1346, %int0_1318 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1347, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_1319 = torch.constant.int 1
    %1348 = torch.aten.size.int %1344, %int1_1319 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_1320 = torch.constant.int 32
    %1349 = torch.aten.mul.int %1348, %int32_1320 : !torch.int, !torch.int -> !torch.int
    %int4_1321 = torch.constant.int 4
    %int8_1322 = torch.constant.int 8
    %int128_1323 = torch.constant.int 128
    %1350 = torch.prim.ListConstruct %int4_1321, %1349, %int8_1322, %int128_1323 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1351 = torch.aten._unsafe_view %1347, %1350 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %1351, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_1324 = torch.constant.int 0
    %int0_1325 = torch.constant.int 0
    %int9223372036854775807_1326 = torch.constant.int 9223372036854775807
    %int1_1327 = torch.constant.int 1
    %1352 = torch.aten.slice.Tensor %1351, %int0_1324, %int0_1325, %int9223372036854775807_1326, %int1_1327 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %1352, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int-2_1328 = torch.constant.int -2
    %1353 = torch.aten.unsqueeze %1342, %int-2_1328 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %1353, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_1329 = torch.constant.int 1
    %1354 = torch.aten.size.int %1341, %int1_1329 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_1330 = torch.constant.int 4
    %int8_1331 = torch.constant.int 8
    %int4_1332 = torch.constant.int 4
    %int128_1333 = torch.constant.int 128
    %1355 = torch.prim.ListConstruct %int4_1330, %1354, %int8_1331, %int4_1332, %int128_1333 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_1334 = torch.constant.bool false
    %1356 = torch.aten.expand %1353, %1355, %false_1334 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1356, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_1335 = torch.constant.int 0
    %1357 = torch.aten.clone %1356, %int0_1335 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1357, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_1336 = torch.constant.int 4
    %int32_1337 = torch.constant.int 32
    %int128_1338 = torch.constant.int 128
    %1358 = torch.prim.ListConstruct %int4_1336, %1354, %int32_1337, %int128_1338 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1359 = torch.aten._unsafe_view %1357, %1358 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %1359, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_1339 = torch.constant.int -2
    %1360 = torch.aten.unsqueeze %1352, %int-2_1339 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %1360, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_1340 = torch.constant.int 1
    %1361 = torch.aten.size.int %1351, %int1_1340 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_1341 = torch.constant.int 4
    %int8_1342 = torch.constant.int 8
    %int4_1343 = torch.constant.int 4
    %int128_1344 = torch.constant.int 128
    %1362 = torch.prim.ListConstruct %int4_1341, %1361, %int8_1342, %int4_1343, %int128_1344 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_1345 = torch.constant.bool false
    %1363 = torch.aten.expand %1360, %1362, %false_1345 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1363, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_1346 = torch.constant.int 0
    %1364 = torch.aten.clone %1363, %int0_1346 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1364, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_1347 = torch.constant.int 4
    %int32_1348 = torch.constant.int 32
    %int128_1349 = torch.constant.int 128
    %1365 = torch.prim.ListConstruct %int4_1347, %1361, %int32_1348, %int128_1349 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1366 = torch.aten._unsafe_view %1364, %1365 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %1366, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_1350 = torch.constant.int 1
    %int2_1351 = torch.constant.int 2
    %1367 = torch.aten.transpose.int %1247, %int1_1350, %int2_1351 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,128],f16>
    %int1_1352 = torch.constant.int 1
    %int2_1353 = torch.constant.int 2
    %1368 = torch.aten.transpose.int %1359, %int1_1352, %int2_1353 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %1368, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_1354 = torch.constant.int 1
    %int2_1355 = torch.constant.int 2
    %1369 = torch.aten.transpose.int %1366, %int1_1354, %int2_1355 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %1369, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_1356 = torch.constant.float 0.000000e+00
    %false_1357 = torch.constant.bool false
    %none_1358 = torch.constant.none
    %1370:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1367, %1368, %1369, %float0.000000e00_1356, %false_1357, %368, %none_1358) : (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.vtensor<[4,1,1,?],f16>, !torch.none) -> (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,1],f32>) 
    %int1_1359 = torch.constant.int 1
    %int2_1360 = torch.constant.int 2
    %1371 = torch.aten.transpose.int %1370#0, %int1_1359, %int2_1360 : !torch.vtensor<[4,32,1,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int4_1361 = torch.constant.int 4
    %int1_1362 = torch.constant.int 1
    %int4096_1363 = torch.constant.int 4096
    %1372 = torch.prim.ListConstruct %int4_1361, %int1_1362, %int4096_1363 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1373 = torch.aten.view %1371, %1372 : !torch.vtensor<[4,1,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_1364 = torch.constant.int -2
    %int-1_1365 = torch.constant.int -1
    %1374 = torch.aten.transpose.int %51, %int-2_1364, %int-1_1365 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_1366 = torch.constant.int 4
    %int4096_1367 = torch.constant.int 4096
    %1375 = torch.prim.ListConstruct %int4_1366, %int4096_1367 : (!torch.int, !torch.int) -> !torch.list<int>
    %1376 = torch.aten.view %1373, %1375 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %1377 = torch.aten.mm %1376, %1374 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_1368 = torch.constant.int 4
    %int1_1369 = torch.constant.int 1
    %int4096_1370 = torch.constant.int 4096
    %1378 = torch.prim.ListConstruct %int4_1368, %int1_1369, %int4096_1370 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1379 = torch.aten.view %1377, %1378 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_1371 = torch.constant.int 1
    %1380 = torch.aten.add.Tensor %1207, %1379, %int1_1371 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_1372 = torch.constant.int 6
    %1381 = torch.prims.convert_element_type %1380, %int6_1372 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_1373 = torch.constant.int 2
    %1382 = torch.aten.pow.Tensor_Scalar %1381, %int2_1373 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_1374 = torch.constant.int -1
    %1383 = torch.prim.ListConstruct %int-1_1374 : (!torch.int) -> !torch.list<int>
    %true_1375 = torch.constant.bool true
    %none_1376 = torch.constant.none
    %1384 = torch.aten.mean.dim %1382, %1383, %true_1375, %none_1376 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_1377 = torch.constant.float 9.9999997473787516E-6
    %int1_1378 = torch.constant.int 1
    %1385 = torch.aten.add.Scalar %1384, %float9.999990e-06_1377, %int1_1378 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %1386 = torch.aten.rsqrt %1385 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %1387 = torch.aten.mul.Tensor %1381, %1386 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_1379 = torch.constant.int 5
    %1388 = torch.prims.convert_element_type %1387, %int5_1379 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %1389 = torch.aten.mul.Tensor %52, %1388 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_1380 = torch.constant.int 5
    %1390 = torch.prims.convert_element_type %1389, %int5_1380 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_1381 = torch.constant.int -2
    %int-1_1382 = torch.constant.int -1
    %1391 = torch.aten.transpose.int %53, %int-2_1381, %int-1_1382 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_1383 = torch.constant.int 4
    %int4096_1384 = torch.constant.int 4096
    %1392 = torch.prim.ListConstruct %int4_1383, %int4096_1384 : (!torch.int, !torch.int) -> !torch.list<int>
    %1393 = torch.aten.view %1390, %1392 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %1394 = torch.aten.mm %1393, %1391 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_1385 = torch.constant.int 4
    %int1_1386 = torch.constant.int 1
    %int14336_1387 = torch.constant.int 14336
    %1395 = torch.prim.ListConstruct %int4_1385, %int1_1386, %int14336_1387 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1396 = torch.aten.view %1394, %1395 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %1397 = torch.aten.silu %1396 : !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_1388 = torch.constant.int -2
    %int-1_1389 = torch.constant.int -1
    %1398 = torch.aten.transpose.int %54, %int-2_1388, %int-1_1389 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_1390 = torch.constant.int 4
    %int4096_1391 = torch.constant.int 4096
    %1399 = torch.prim.ListConstruct %int4_1390, %int4096_1391 : (!torch.int, !torch.int) -> !torch.list<int>
    %1400 = torch.aten.view %1390, %1399 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %1401 = torch.aten.mm %1400, %1398 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_1392 = torch.constant.int 4
    %int1_1393 = torch.constant.int 1
    %int14336_1394 = torch.constant.int 14336
    %1402 = torch.prim.ListConstruct %int4_1392, %int1_1393, %int14336_1394 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1403 = torch.aten.view %1401, %1402 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %1404 = torch.aten.mul.Tensor %1397, %1403 : !torch.vtensor<[4,1,14336],f16>, !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_1395 = torch.constant.int -2
    %int-1_1396 = torch.constant.int -1
    %1405 = torch.aten.transpose.int %55, %int-2_1395, %int-1_1396 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int4_1397 = torch.constant.int 4
    %int14336_1398 = torch.constant.int 14336
    %1406 = torch.prim.ListConstruct %int4_1397, %int14336_1398 : (!torch.int, !torch.int) -> !torch.list<int>
    %1407 = torch.aten.view %1404, %1406 : !torch.vtensor<[4,1,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,14336],f16>
    %1408 = torch.aten.mm %1407, %1405 : !torch.vtensor<[4,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_1399 = torch.constant.int 4
    %int1_1400 = torch.constant.int 1
    %int4096_1401 = torch.constant.int 4096
    %1409 = torch.prim.ListConstruct %int4_1399, %int1_1400, %int4096_1401 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1410 = torch.aten.view %1408, %1409 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_1402 = torch.constant.int 1
    %1411 = torch.aten.add.Tensor %1380, %1410, %int1_1402 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_1403 = torch.constant.int 6
    %1412 = torch.prims.convert_element_type %1411, %int6_1403 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_1404 = torch.constant.int 2
    %1413 = torch.aten.pow.Tensor_Scalar %1412, %int2_1404 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_1405 = torch.constant.int -1
    %1414 = torch.prim.ListConstruct %int-1_1405 : (!torch.int) -> !torch.list<int>
    %true_1406 = torch.constant.bool true
    %none_1407 = torch.constant.none
    %1415 = torch.aten.mean.dim %1413, %1414, %true_1406, %none_1407 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_1408 = torch.constant.float 9.9999997473787516E-6
    %int1_1409 = torch.constant.int 1
    %1416 = torch.aten.add.Scalar %1415, %float9.999990e-06_1408, %int1_1409 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %1417 = torch.aten.rsqrt %1416 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %1418 = torch.aten.mul.Tensor %1412, %1417 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_1410 = torch.constant.int 5
    %1419 = torch.prims.convert_element_type %1418, %int5_1410 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %1420 = torch.aten.mul.Tensor %56, %1419 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_1411 = torch.constant.int 5
    %1421 = torch.prims.convert_element_type %1420, %int5_1411 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_1412 = torch.constant.int -2
    %int-1_1413 = torch.constant.int -1
    %1422 = torch.aten.transpose.int %57, %int-2_1412, %int-1_1413 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_1414 = torch.constant.int 4
    %int4096_1415 = torch.constant.int 4096
    %1423 = torch.prim.ListConstruct %int4_1414, %int4096_1415 : (!torch.int, !torch.int) -> !torch.list<int>
    %1424 = torch.aten.view %1421, %1423 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %1425 = torch.aten.mm %1424, %1422 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_1416 = torch.constant.int 4
    %int1_1417 = torch.constant.int 1
    %int4096_1418 = torch.constant.int 4096
    %1426 = torch.prim.ListConstruct %int4_1416, %int1_1417, %int4096_1418 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1427 = torch.aten.view %1425, %1426 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_1419 = torch.constant.int -2
    %int-1_1420 = torch.constant.int -1
    %1428 = torch.aten.transpose.int %58, %int-2_1419, %int-1_1420 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_1421 = torch.constant.int 4
    %int4096_1422 = torch.constant.int 4096
    %1429 = torch.prim.ListConstruct %int4_1421, %int4096_1422 : (!torch.int, !torch.int) -> !torch.list<int>
    %1430 = torch.aten.view %1421, %1429 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %1431 = torch.aten.mm %1430, %1428 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_1423 = torch.constant.int 4
    %int1_1424 = torch.constant.int 1
    %int1024_1425 = torch.constant.int 1024
    %1432 = torch.prim.ListConstruct %int4_1423, %int1_1424, %int1024_1425 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1433 = torch.aten.view %1431, %1432 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int-2_1426 = torch.constant.int -2
    %int-1_1427 = torch.constant.int -1
    %1434 = torch.aten.transpose.int %59, %int-2_1426, %int-1_1427 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_1428 = torch.constant.int 4
    %int4096_1429 = torch.constant.int 4096
    %1435 = torch.prim.ListConstruct %int4_1428, %int4096_1429 : (!torch.int, !torch.int) -> !torch.list<int>
    %1436 = torch.aten.view %1421, %1435 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %1437 = torch.aten.mm %1436, %1434 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_1430 = torch.constant.int 4
    %int1_1431 = torch.constant.int 1
    %int1024_1432 = torch.constant.int 1024
    %1438 = torch.prim.ListConstruct %int4_1430, %int1_1431, %int1024_1432 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1439 = torch.aten.view %1437, %1438 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int4_1433 = torch.constant.int 4
    %int1_1434 = torch.constant.int 1
    %int32_1435 = torch.constant.int 32
    %int128_1436 = torch.constant.int 128
    %1440 = torch.prim.ListConstruct %int4_1433, %int1_1434, %int32_1435, %int128_1436 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1441 = torch.aten.view %1427, %1440 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,128],f16>
    %int4_1437 = torch.constant.int 4
    %int1_1438 = torch.constant.int 1
    %int8_1439 = torch.constant.int 8
    %int128_1440 = torch.constant.int 128
    %1442 = torch.prim.ListConstruct %int4_1437, %int1_1438, %int8_1439, %int128_1440 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1443 = torch.aten.view %1433, %1442 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int4_1441 = torch.constant.int 4
    %int1_1442 = torch.constant.int 1
    %int8_1443 = torch.constant.int 8
    %int128_1444 = torch.constant.int 128
    %1444 = torch.prim.ListConstruct %int4_1441, %int1_1442, %int8_1443, %int128_1444 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1445 = torch.aten.view %1439, %1444 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int6_1445 = torch.constant.int 6
    %1446 = torch.prims.convert_element_type %1441, %int6_1445 : !torch.vtensor<[4,1,32,128],f16>, !torch.int -> !torch.vtensor<[4,1,32,128],f32>
    %1447 = torch_c.to_builtin_tensor %1446 : !torch.vtensor<[4,1,32,128],f32> -> tensor<4x1x32x128xf32>
    %1448 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %1449 = util.call @sharktank_rotary_embedding_4_1_32_128_f32(%1447, %1448) : (tensor<4x1x32x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x32x128xf32>
    %1450 = torch_c.from_builtin_tensor %1449 : tensor<4x1x32x128xf32> -> !torch.vtensor<[4,1,32,128],f32>
    %int5_1446 = torch.constant.int 5
    %1451 = torch.prims.convert_element_type %1450, %int5_1446 : !torch.vtensor<[4,1,32,128],f32>, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int6_1447 = torch.constant.int 6
    %1452 = torch.prims.convert_element_type %1443, %int6_1447 : !torch.vtensor<[4,1,8,128],f16>, !torch.int -> !torch.vtensor<[4,1,8,128],f32>
    %1453 = torch_c.to_builtin_tensor %1452 : !torch.vtensor<[4,1,8,128],f32> -> tensor<4x1x8x128xf32>
    %1454 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %1455 = util.call @sharktank_rotary_embedding_4_1_8_128_f32(%1453, %1454) : (tensor<4x1x8x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x8x128xf32>
    %1456 = torch_c.from_builtin_tensor %1455 : tensor<4x1x8x128xf32> -> !torch.vtensor<[4,1,8,128],f32>
    %int5_1448 = torch.constant.int 5
    %1457 = torch.prims.convert_element_type %1456, %int5_1448 : !torch.vtensor<[4,1,8,128],f32>, !torch.int -> !torch.vtensor<[4,1,8,128],f16>
    %int32_1449 = torch.constant.int 32
    %1458 = torch.aten.floor_divide.Scalar %arg2, %int32_1449 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_1450 = torch.constant.int 1
    %1459 = torch.aten.unsqueeze %1458, %int1_1450 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_1451 = torch.constant.int 1
    %false_1452 = torch.constant.bool false
    %1460 = torch.aten.gather %arg3, %int1_1451, %1459, %false_1452 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_1453 = torch.constant.int 32
    %1461 = torch.aten.remainder.Scalar %arg2, %int32_1453 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_1454 = torch.constant.int 1
    %1462 = torch.aten.unsqueeze %1461, %int1_1454 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_1455 = torch.constant.none
    %1463 = torch.aten.clone %60, %none_1455 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_1456 = torch.constant.int 0
    %1464 = torch.aten.unsqueeze %1463, %int0_1456 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_1457 = torch.constant.int 4
    %int1_1458 = torch.constant.int 1
    %1465 = torch.prim.ListConstruct %int4_1457, %int1_1458 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_1459 = torch.constant.int 1
    %int1_1460 = torch.constant.int 1
    %1466 = torch.prim.ListConstruct %int1_1459, %int1_1460 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_1461 = torch.constant.int 4
    %int0_1462 = torch.constant.int 0
    %cpu_1463 = torch.constant.device "cpu"
    %false_1464 = torch.constant.bool false
    %1467 = torch.aten.empty_strided %1465, %1466, %int4_1461, %int0_1462, %cpu_1463, %false_1464 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int5_1465 = torch.constant.int 5
    %1468 = torch.aten.fill.Scalar %1467, %int5_1465 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_1466 = torch.constant.int 4
    %int1_1467 = torch.constant.int 1
    %1469 = torch.prim.ListConstruct %int4_1466, %int1_1467 : (!torch.int, !torch.int) -> !torch.list<int>
    %1470 = torch.aten.repeat %1464, %1469 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_1468 = torch.constant.int 32
    %1471 = torch.aten.mul.Scalar %1460, %int32_1468 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_1469 = torch.constant.int 1
    %1472 = torch.aten.add.Tensor %1471, %1468, %int1_1469 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_1470 = torch.constant.int 2
    %1473 = torch.aten.mul.Scalar %1472, %int2_1470 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_1471 = torch.constant.int 1
    %1474 = torch.aten.add.Tensor %1473, %1470, %int1_1471 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_1472 = torch.constant.int 32
    %1475 = torch.aten.mul.Scalar %1474, %int32_1472 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_1473 = torch.constant.int 1
    %1476 = torch.aten.add.Tensor %1475, %1462, %int1_1473 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_1474 = torch.constant.int 32
    %int2_1475 = torch.constant.int 2
    %int32_1476 = torch.constant.int 32
    %int8_1477 = torch.constant.int 8
    %int128_1478 = torch.constant.int 128
    %1477 = torch.prim.ListConstruct %437, %int32_1474, %int2_1475, %int32_1476, %int8_1477, %int128_1478 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1478 = torch.aten.view %1314, %1477 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1478, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_1479 = torch.constant.int 32
    %1479 = torch.aten.mul.int %437, %int32_1479 : !torch.int, !torch.int -> !torch.int
    %int2_1480 = torch.constant.int 2
    %1480 = torch.aten.mul.int %1479, %int2_1480 : !torch.int, !torch.int -> !torch.int
    %int32_1481 = torch.constant.int 32
    %1481 = torch.aten.mul.int %1480, %int32_1481 : !torch.int, !torch.int -> !torch.int
    %int8_1482 = torch.constant.int 8
    %int128_1483 = torch.constant.int 128
    %1482 = torch.prim.ListConstruct %1481, %int8_1482, %int128_1483 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1483 = torch.aten.view %1478, %1482 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %1483, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %1484 = torch.prim.ListConstruct %1476 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_1484 = torch.constant.bool false
    %1485 = torch.aten.index_put %1483, %1484, %1457, %false_1484 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %1485, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_1485 = torch.constant.int 32
    %int2_1486 = torch.constant.int 2
    %int32_1487 = torch.constant.int 32
    %int8_1488 = torch.constant.int 8
    %int128_1489 = torch.constant.int 128
    %1486 = torch.prim.ListConstruct %437, %int32_1485, %int2_1486, %int32_1487, %int8_1488, %int128_1489 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1487 = torch.aten.view %1485, %1486 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1487, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_1490 = torch.constant.int 2097152
    %1488 = torch.prim.ListConstruct %437, %int2097152_1490 : (!torch.int, !torch.int) -> !torch.list<int>
    %1489 = torch.aten.view %1487, %1488 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %1489, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_1491 = torch.constant.int 32
    %int2_1492 = torch.constant.int 2
    %int32_1493 = torch.constant.int 32
    %int8_1494 = torch.constant.int 8
    %int128_1495 = torch.constant.int 128
    %1490 = torch.prim.ListConstruct %437, %int32_1491, %int2_1492, %int32_1493, %int8_1494, %int128_1495 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1491 = torch.aten.view %1489, %1490 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1491, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int8_1496 = torch.constant.int 8
    %int128_1497 = torch.constant.int 128
    %1492 = torch.prim.ListConstruct %1481, %int8_1496, %int128_1497 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1493 = torch.aten.view %1491, %1492 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %1493, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_1498 = torch.constant.int 32
    %1494 = torch.aten.floor_divide.Scalar %arg2, %int32_1498 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_1499 = torch.constant.int 1
    %1495 = torch.aten.unsqueeze %1494, %int1_1499 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_1500 = torch.constant.int 1
    %false_1501 = torch.constant.bool false
    %1496 = torch.aten.gather %arg3, %int1_1500, %1495, %false_1501 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_1502 = torch.constant.int 32
    %1497 = torch.aten.remainder.Scalar %arg2, %int32_1502 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_1503 = torch.constant.int 1
    %1498 = torch.aten.unsqueeze %1497, %int1_1503 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_1504 = torch.constant.none
    %1499 = torch.aten.clone %61, %none_1504 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_1505 = torch.constant.int 0
    %1500 = torch.aten.unsqueeze %1499, %int0_1505 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_1506 = torch.constant.int 4
    %int1_1507 = torch.constant.int 1
    %1501 = torch.prim.ListConstruct %int4_1506, %int1_1507 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_1508 = torch.constant.int 1
    %int1_1509 = torch.constant.int 1
    %1502 = torch.prim.ListConstruct %int1_1508, %int1_1509 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_1510 = torch.constant.int 4
    %int0_1511 = torch.constant.int 0
    %cpu_1512 = torch.constant.device "cpu"
    %false_1513 = torch.constant.bool false
    %1503 = torch.aten.empty_strided %1501, %1502, %int4_1510, %int0_1511, %cpu_1512, %false_1513 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int5_1514 = torch.constant.int 5
    %1504 = torch.aten.fill.Scalar %1503, %int5_1514 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_1515 = torch.constant.int 4
    %int1_1516 = torch.constant.int 1
    %1505 = torch.prim.ListConstruct %int4_1515, %int1_1516 : (!torch.int, !torch.int) -> !torch.list<int>
    %1506 = torch.aten.repeat %1500, %1505 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_1517 = torch.constant.int 32
    %1507 = torch.aten.mul.Scalar %1496, %int32_1517 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_1518 = torch.constant.int 1
    %1508 = torch.aten.add.Tensor %1507, %1504, %int1_1518 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_1519 = torch.constant.int 2
    %1509 = torch.aten.mul.Scalar %1508, %int2_1519 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_1520 = torch.constant.int 1
    %1510 = torch.aten.add.Tensor %1509, %1506, %int1_1520 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_1521 = torch.constant.int 32
    %1511 = torch.aten.mul.Scalar %1510, %int32_1521 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_1522 = torch.constant.int 1
    %1512 = torch.aten.add.Tensor %1511, %1498, %int1_1522 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %1513 = torch.prim.ListConstruct %1512 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_1523 = torch.constant.bool false
    %1514 = torch.aten.index_put %1493, %1513, %1445, %false_1523 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %1514, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_1524 = torch.constant.int 32
    %int2_1525 = torch.constant.int 2
    %int32_1526 = torch.constant.int 32
    %int8_1527 = torch.constant.int 8
    %int128_1528 = torch.constant.int 128
    %1515 = torch.prim.ListConstruct %437, %int32_1524, %int2_1525, %int32_1526, %int8_1527, %int128_1528 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1516 = torch.aten.view %1514, %1515 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1516, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_1529 = torch.constant.int 2097152
    %1517 = torch.prim.ListConstruct %437, %int2097152_1529 : (!torch.int, !torch.int) -> !torch.list<int>
    %1518 = torch.aten.view %1516, %1517 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %1518, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int4_1530 = torch.constant.int 4
    %1519 = torch.prim.ListConstruct %int4_1530, %358 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_1531 = torch.constant.int 1
    %1520 = torch.prim.ListConstruct %358, %int1_1531 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_1532 = torch.constant.int 4
    %int0_1533 = torch.constant.int 0
    %cpu_1534 = torch.constant.device "cpu"
    %false_1535 = torch.constant.bool false
    %1521 = torch.aten.empty_strided %1519, %1520, %int4_1532, %int0_1533, %cpu_1534, %false_1535 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1521, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int5_1536 = torch.constant.int 5
    %1522 = torch.aten.fill.Scalar %1521, %int5_1536 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1522, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int32_1537 = torch.constant.int 32
    %1523 = torch.aten.mul.Scalar %arg3, %int32_1537 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1523, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_1538 = torch.constant.int 1
    %1524 = torch.aten.add.Tensor %1523, %1522, %int1_1538 : !torch.vtensor<[4,?],si64>, !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1524, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_1539 = torch.constant.int 4
    %1525 = torch.aten.mul.int %int4_1539, %358 : !torch.int, !torch.int -> !torch.int
    %1526 = torch.prim.ListConstruct %1525 : (!torch.int) -> !torch.list<int>
    %1527 = torch.aten.view %1524, %1526 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1527, [%356], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_1540 = torch.constant.int 32
    %int2_1541 = torch.constant.int 2
    %int32_1542 = torch.constant.int 32
    %int8_1543 = torch.constant.int 8
    %int128_1544 = torch.constant.int 128
    %1528 = torch.prim.ListConstruct %437, %int32_1540, %int2_1541, %int32_1542, %int8_1543, %int128_1544 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1529 = torch.aten.view %1518, %1528 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1529, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_1545 = torch.constant.int 32
    %1530 = torch.aten.mul.int %437, %int32_1545 : !torch.int, !torch.int -> !torch.int
    %int2_1546 = torch.constant.int 2
    %int32_1547 = torch.constant.int 32
    %int8_1548 = torch.constant.int 8
    %int128_1549 = torch.constant.int 128
    %1531 = torch.prim.ListConstruct %1530, %int2_1546, %int32_1547, %int8_1548, %int128_1549 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1532 = torch.aten.view %1529, %1531 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %1532, [%357], affine_map<()[s0] -> (s0 * 32, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int0_1550 = torch.constant.int 0
    %1533 = torch.aten.index_select %1532, %int0_1550, %1527 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.int, !torch.vtensor<[?],si64> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %1533, [%356], affine_map<()[s0] -> (s0 * 4, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int4_1551 = torch.constant.int 4
    %int2_1552 = torch.constant.int 2
    %int32_1553 = torch.constant.int 32
    %int8_1554 = torch.constant.int 8
    %int128_1555 = torch.constant.int 128
    %1534 = torch.prim.ListConstruct %int4_1551, %358, %int2_1552, %int32_1553, %int8_1554, %int128_1555 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1535 = torch.aten.view %1533, %1534 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %1535, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int0_1556 = torch.constant.int 0
    %int0_1557 = torch.constant.int 0
    %int9223372036854775807_1558 = torch.constant.int 9223372036854775807
    %int1_1559 = torch.constant.int 1
    %1536 = torch.aten.slice.Tensor %1535, %int0_1556, %int0_1557, %int9223372036854775807_1558, %int1_1559 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %1536, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_1560 = torch.constant.int 1
    %int0_1561 = torch.constant.int 0
    %int9223372036854775807_1562 = torch.constant.int 9223372036854775807
    %int1_1563 = torch.constant.int 1
    %1537 = torch.aten.slice.Tensor %1536, %int1_1560, %int0_1561, %int9223372036854775807_1562, %int1_1563 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %1537, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_1564 = torch.constant.int 2
    %int0_1565 = torch.constant.int 0
    %1538 = torch.aten.select.int %1537, %int2_1564, %int0_1565 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1538, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int32_1566 = torch.constant.int 32
    %1539 = torch.aten.mul.int %358, %int32_1566 : !torch.int, !torch.int -> !torch.int
    %int2_1567 = torch.constant.int 2
    %int0_1568 = torch.constant.int 0
    %int1_1569 = torch.constant.int 1
    %1540 = torch.aten.slice.Tensor %1538, %int2_1567, %int0_1568, %1539, %int1_1569 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1540, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_1570 = torch.constant.int 0
    %1541 = torch.aten.clone %1540, %int0_1570 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1541, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_1571 = torch.constant.int 1
    %1542 = torch.aten.size.int %1537, %int1_1571 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_1572 = torch.constant.int 32
    %1543 = torch.aten.mul.int %1542, %int32_1572 : !torch.int, !torch.int -> !torch.int
    %int4_1573 = torch.constant.int 4
    %int8_1574 = torch.constant.int 8
    %int128_1575 = torch.constant.int 128
    %1544 = torch.prim.ListConstruct %int4_1573, %1543, %int8_1574, %int128_1575 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1545 = torch.aten._unsafe_view %1541, %1544 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %1545, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_1576 = torch.constant.int 0
    %int0_1577 = torch.constant.int 0
    %int9223372036854775807_1578 = torch.constant.int 9223372036854775807
    %int1_1579 = torch.constant.int 1
    %1546 = torch.aten.slice.Tensor %1545, %int0_1576, %int0_1577, %int9223372036854775807_1578, %int1_1579 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %1546, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_1580 = torch.constant.int 0
    %int0_1581 = torch.constant.int 0
    %int9223372036854775807_1582 = torch.constant.int 9223372036854775807
    %int1_1583 = torch.constant.int 1
    %1547 = torch.aten.slice.Tensor %1535, %int0_1580, %int0_1581, %int9223372036854775807_1582, %int1_1583 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %1547, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_1584 = torch.constant.int 1
    %int0_1585 = torch.constant.int 0
    %int9223372036854775807_1586 = torch.constant.int 9223372036854775807
    %int1_1587 = torch.constant.int 1
    %1548 = torch.aten.slice.Tensor %1547, %int1_1584, %int0_1585, %int9223372036854775807_1586, %int1_1587 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %1548, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_1588 = torch.constant.int 2
    %int1_1589 = torch.constant.int 1
    %1549 = torch.aten.select.int %1548, %int2_1588, %int1_1589 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1549, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int2_1590 = torch.constant.int 2
    %int0_1591 = torch.constant.int 0
    %int1_1592 = torch.constant.int 1
    %1550 = torch.aten.slice.Tensor %1549, %int2_1590, %int0_1591, %1539, %int1_1592 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1550, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_1593 = torch.constant.int 0
    %1551 = torch.aten.clone %1550, %int0_1593 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1551, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_1594 = torch.constant.int 1
    %1552 = torch.aten.size.int %1548, %int1_1594 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_1595 = torch.constant.int 32
    %1553 = torch.aten.mul.int %1552, %int32_1595 : !torch.int, !torch.int -> !torch.int
    %int4_1596 = torch.constant.int 4
    %int8_1597 = torch.constant.int 8
    %int128_1598 = torch.constant.int 128
    %1554 = torch.prim.ListConstruct %int4_1596, %1553, %int8_1597, %int128_1598 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1555 = torch.aten._unsafe_view %1551, %1554 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %1555, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_1599 = torch.constant.int 0
    %int0_1600 = torch.constant.int 0
    %int9223372036854775807_1601 = torch.constant.int 9223372036854775807
    %int1_1602 = torch.constant.int 1
    %1556 = torch.aten.slice.Tensor %1555, %int0_1599, %int0_1600, %int9223372036854775807_1601, %int1_1602 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %1556, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int-2_1603 = torch.constant.int -2
    %1557 = torch.aten.unsqueeze %1546, %int-2_1603 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %1557, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_1604 = torch.constant.int 1
    %1558 = torch.aten.size.int %1545, %int1_1604 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_1605 = torch.constant.int 4
    %int8_1606 = torch.constant.int 8
    %int4_1607 = torch.constant.int 4
    %int128_1608 = torch.constant.int 128
    %1559 = torch.prim.ListConstruct %int4_1605, %1558, %int8_1606, %int4_1607, %int128_1608 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_1609 = torch.constant.bool false
    %1560 = torch.aten.expand %1557, %1559, %false_1609 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1560, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_1610 = torch.constant.int 0
    %1561 = torch.aten.clone %1560, %int0_1610 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1561, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_1611 = torch.constant.int 4
    %int32_1612 = torch.constant.int 32
    %int128_1613 = torch.constant.int 128
    %1562 = torch.prim.ListConstruct %int4_1611, %1558, %int32_1612, %int128_1613 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1563 = torch.aten._unsafe_view %1561, %1562 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %1563, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_1614 = torch.constant.int -2
    %1564 = torch.aten.unsqueeze %1556, %int-2_1614 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %1564, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_1615 = torch.constant.int 1
    %1565 = torch.aten.size.int %1555, %int1_1615 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_1616 = torch.constant.int 4
    %int8_1617 = torch.constant.int 8
    %int4_1618 = torch.constant.int 4
    %int128_1619 = torch.constant.int 128
    %1566 = torch.prim.ListConstruct %int4_1616, %1565, %int8_1617, %int4_1618, %int128_1619 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_1620 = torch.constant.bool false
    %1567 = torch.aten.expand %1564, %1566, %false_1620 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1567, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_1621 = torch.constant.int 0
    %1568 = torch.aten.clone %1567, %int0_1621 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1568, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_1622 = torch.constant.int 4
    %int32_1623 = torch.constant.int 32
    %int128_1624 = torch.constant.int 128
    %1569 = torch.prim.ListConstruct %int4_1622, %1565, %int32_1623, %int128_1624 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1570 = torch.aten._unsafe_view %1568, %1569 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %1570, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_1625 = torch.constant.int 1
    %int2_1626 = torch.constant.int 2
    %1571 = torch.aten.transpose.int %1451, %int1_1625, %int2_1626 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,128],f16>
    %int1_1627 = torch.constant.int 1
    %int2_1628 = torch.constant.int 2
    %1572 = torch.aten.transpose.int %1563, %int1_1627, %int2_1628 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %1572, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_1629 = torch.constant.int 1
    %int2_1630 = torch.constant.int 2
    %1573 = torch.aten.transpose.int %1570, %int1_1629, %int2_1630 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %1573, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_1631 = torch.constant.float 0.000000e+00
    %false_1632 = torch.constant.bool false
    %none_1633 = torch.constant.none
    %1574:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1571, %1572, %1573, %float0.000000e00_1631, %false_1632, %368, %none_1633) : (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.vtensor<[4,1,1,?],f16>, !torch.none) -> (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,1],f32>) 
    %int1_1634 = torch.constant.int 1
    %int2_1635 = torch.constant.int 2
    %1575 = torch.aten.transpose.int %1574#0, %int1_1634, %int2_1635 : !torch.vtensor<[4,32,1,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int4_1636 = torch.constant.int 4
    %int1_1637 = torch.constant.int 1
    %int4096_1638 = torch.constant.int 4096
    %1576 = torch.prim.ListConstruct %int4_1636, %int1_1637, %int4096_1638 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1577 = torch.aten.view %1575, %1576 : !torch.vtensor<[4,1,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_1639 = torch.constant.int -2
    %int-1_1640 = torch.constant.int -1
    %1578 = torch.aten.transpose.int %62, %int-2_1639, %int-1_1640 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_1641 = torch.constant.int 4
    %int4096_1642 = torch.constant.int 4096
    %1579 = torch.prim.ListConstruct %int4_1641, %int4096_1642 : (!torch.int, !torch.int) -> !torch.list<int>
    %1580 = torch.aten.view %1577, %1579 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %1581 = torch.aten.mm %1580, %1578 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_1643 = torch.constant.int 4
    %int1_1644 = torch.constant.int 1
    %int4096_1645 = torch.constant.int 4096
    %1582 = torch.prim.ListConstruct %int4_1643, %int1_1644, %int4096_1645 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1583 = torch.aten.view %1581, %1582 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_1646 = torch.constant.int 1
    %1584 = torch.aten.add.Tensor %1411, %1583, %int1_1646 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_1647 = torch.constant.int 6
    %1585 = torch.prims.convert_element_type %1584, %int6_1647 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_1648 = torch.constant.int 2
    %1586 = torch.aten.pow.Tensor_Scalar %1585, %int2_1648 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_1649 = torch.constant.int -1
    %1587 = torch.prim.ListConstruct %int-1_1649 : (!torch.int) -> !torch.list<int>
    %true_1650 = torch.constant.bool true
    %none_1651 = torch.constant.none
    %1588 = torch.aten.mean.dim %1586, %1587, %true_1650, %none_1651 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_1652 = torch.constant.float 9.9999997473787516E-6
    %int1_1653 = torch.constant.int 1
    %1589 = torch.aten.add.Scalar %1588, %float9.999990e-06_1652, %int1_1653 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %1590 = torch.aten.rsqrt %1589 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %1591 = torch.aten.mul.Tensor %1585, %1590 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_1654 = torch.constant.int 5
    %1592 = torch.prims.convert_element_type %1591, %int5_1654 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %1593 = torch.aten.mul.Tensor %63, %1592 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_1655 = torch.constant.int 5
    %1594 = torch.prims.convert_element_type %1593, %int5_1655 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_1656 = torch.constant.int -2
    %int-1_1657 = torch.constant.int -1
    %1595 = torch.aten.transpose.int %64, %int-2_1656, %int-1_1657 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_1658 = torch.constant.int 4
    %int4096_1659 = torch.constant.int 4096
    %1596 = torch.prim.ListConstruct %int4_1658, %int4096_1659 : (!torch.int, !torch.int) -> !torch.list<int>
    %1597 = torch.aten.view %1594, %1596 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %1598 = torch.aten.mm %1597, %1595 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_1660 = torch.constant.int 4
    %int1_1661 = torch.constant.int 1
    %int14336_1662 = torch.constant.int 14336
    %1599 = torch.prim.ListConstruct %int4_1660, %int1_1661, %int14336_1662 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1600 = torch.aten.view %1598, %1599 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %1601 = torch.aten.silu %1600 : !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_1663 = torch.constant.int -2
    %int-1_1664 = torch.constant.int -1
    %1602 = torch.aten.transpose.int %65, %int-2_1663, %int-1_1664 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_1665 = torch.constant.int 4
    %int4096_1666 = torch.constant.int 4096
    %1603 = torch.prim.ListConstruct %int4_1665, %int4096_1666 : (!torch.int, !torch.int) -> !torch.list<int>
    %1604 = torch.aten.view %1594, %1603 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %1605 = torch.aten.mm %1604, %1602 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_1667 = torch.constant.int 4
    %int1_1668 = torch.constant.int 1
    %int14336_1669 = torch.constant.int 14336
    %1606 = torch.prim.ListConstruct %int4_1667, %int1_1668, %int14336_1669 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1607 = torch.aten.view %1605, %1606 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %1608 = torch.aten.mul.Tensor %1601, %1607 : !torch.vtensor<[4,1,14336],f16>, !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_1670 = torch.constant.int -2
    %int-1_1671 = torch.constant.int -1
    %1609 = torch.aten.transpose.int %66, %int-2_1670, %int-1_1671 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int4_1672 = torch.constant.int 4
    %int14336_1673 = torch.constant.int 14336
    %1610 = torch.prim.ListConstruct %int4_1672, %int14336_1673 : (!torch.int, !torch.int) -> !torch.list<int>
    %1611 = torch.aten.view %1608, %1610 : !torch.vtensor<[4,1,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,14336],f16>
    %1612 = torch.aten.mm %1611, %1609 : !torch.vtensor<[4,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_1674 = torch.constant.int 4
    %int1_1675 = torch.constant.int 1
    %int4096_1676 = torch.constant.int 4096
    %1613 = torch.prim.ListConstruct %int4_1674, %int1_1675, %int4096_1676 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1614 = torch.aten.view %1612, %1613 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_1677 = torch.constant.int 1
    %1615 = torch.aten.add.Tensor %1584, %1614, %int1_1677 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_1678 = torch.constant.int 6
    %1616 = torch.prims.convert_element_type %1615, %int6_1678 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_1679 = torch.constant.int 2
    %1617 = torch.aten.pow.Tensor_Scalar %1616, %int2_1679 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_1680 = torch.constant.int -1
    %1618 = torch.prim.ListConstruct %int-1_1680 : (!torch.int) -> !torch.list<int>
    %true_1681 = torch.constant.bool true
    %none_1682 = torch.constant.none
    %1619 = torch.aten.mean.dim %1617, %1618, %true_1681, %none_1682 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_1683 = torch.constant.float 9.9999997473787516E-6
    %int1_1684 = torch.constant.int 1
    %1620 = torch.aten.add.Scalar %1619, %float9.999990e-06_1683, %int1_1684 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %1621 = torch.aten.rsqrt %1620 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %1622 = torch.aten.mul.Tensor %1616, %1621 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_1685 = torch.constant.int 5
    %1623 = torch.prims.convert_element_type %1622, %int5_1685 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %1624 = torch.aten.mul.Tensor %67, %1623 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_1686 = torch.constant.int 5
    %1625 = torch.prims.convert_element_type %1624, %int5_1686 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_1687 = torch.constant.int -2
    %int-1_1688 = torch.constant.int -1
    %1626 = torch.aten.transpose.int %68, %int-2_1687, %int-1_1688 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_1689 = torch.constant.int 4
    %int4096_1690 = torch.constant.int 4096
    %1627 = torch.prim.ListConstruct %int4_1689, %int4096_1690 : (!torch.int, !torch.int) -> !torch.list<int>
    %1628 = torch.aten.view %1625, %1627 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %1629 = torch.aten.mm %1628, %1626 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_1691 = torch.constant.int 4
    %int1_1692 = torch.constant.int 1
    %int4096_1693 = torch.constant.int 4096
    %1630 = torch.prim.ListConstruct %int4_1691, %int1_1692, %int4096_1693 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1631 = torch.aten.view %1629, %1630 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_1694 = torch.constant.int -2
    %int-1_1695 = torch.constant.int -1
    %1632 = torch.aten.transpose.int %69, %int-2_1694, %int-1_1695 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_1696 = torch.constant.int 4
    %int4096_1697 = torch.constant.int 4096
    %1633 = torch.prim.ListConstruct %int4_1696, %int4096_1697 : (!torch.int, !torch.int) -> !torch.list<int>
    %1634 = torch.aten.view %1625, %1633 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %1635 = torch.aten.mm %1634, %1632 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_1698 = torch.constant.int 4
    %int1_1699 = torch.constant.int 1
    %int1024_1700 = torch.constant.int 1024
    %1636 = torch.prim.ListConstruct %int4_1698, %int1_1699, %int1024_1700 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1637 = torch.aten.view %1635, %1636 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int-2_1701 = torch.constant.int -2
    %int-1_1702 = torch.constant.int -1
    %1638 = torch.aten.transpose.int %70, %int-2_1701, %int-1_1702 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_1703 = torch.constant.int 4
    %int4096_1704 = torch.constant.int 4096
    %1639 = torch.prim.ListConstruct %int4_1703, %int4096_1704 : (!torch.int, !torch.int) -> !torch.list<int>
    %1640 = torch.aten.view %1625, %1639 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %1641 = torch.aten.mm %1640, %1638 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_1705 = torch.constant.int 4
    %int1_1706 = torch.constant.int 1
    %int1024_1707 = torch.constant.int 1024
    %1642 = torch.prim.ListConstruct %int4_1705, %int1_1706, %int1024_1707 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1643 = torch.aten.view %1641, %1642 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int4_1708 = torch.constant.int 4
    %int1_1709 = torch.constant.int 1
    %int32_1710 = torch.constant.int 32
    %int128_1711 = torch.constant.int 128
    %1644 = torch.prim.ListConstruct %int4_1708, %int1_1709, %int32_1710, %int128_1711 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1645 = torch.aten.view %1631, %1644 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,128],f16>
    %int4_1712 = torch.constant.int 4
    %int1_1713 = torch.constant.int 1
    %int8_1714 = torch.constant.int 8
    %int128_1715 = torch.constant.int 128
    %1646 = torch.prim.ListConstruct %int4_1712, %int1_1713, %int8_1714, %int128_1715 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1647 = torch.aten.view %1637, %1646 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int4_1716 = torch.constant.int 4
    %int1_1717 = torch.constant.int 1
    %int8_1718 = torch.constant.int 8
    %int128_1719 = torch.constant.int 128
    %1648 = torch.prim.ListConstruct %int4_1716, %int1_1717, %int8_1718, %int128_1719 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1649 = torch.aten.view %1643, %1648 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int6_1720 = torch.constant.int 6
    %1650 = torch.prims.convert_element_type %1645, %int6_1720 : !torch.vtensor<[4,1,32,128],f16>, !torch.int -> !torch.vtensor<[4,1,32,128],f32>
    %1651 = torch_c.to_builtin_tensor %1650 : !torch.vtensor<[4,1,32,128],f32> -> tensor<4x1x32x128xf32>
    %1652 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %1653 = util.call @sharktank_rotary_embedding_4_1_32_128_f32(%1651, %1652) : (tensor<4x1x32x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x32x128xf32>
    %1654 = torch_c.from_builtin_tensor %1653 : tensor<4x1x32x128xf32> -> !torch.vtensor<[4,1,32,128],f32>
    %int5_1721 = torch.constant.int 5
    %1655 = torch.prims.convert_element_type %1654, %int5_1721 : !torch.vtensor<[4,1,32,128],f32>, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int6_1722 = torch.constant.int 6
    %1656 = torch.prims.convert_element_type %1647, %int6_1722 : !torch.vtensor<[4,1,8,128],f16>, !torch.int -> !torch.vtensor<[4,1,8,128],f32>
    %1657 = torch_c.to_builtin_tensor %1656 : !torch.vtensor<[4,1,8,128],f32> -> tensor<4x1x8x128xf32>
    %1658 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %1659 = util.call @sharktank_rotary_embedding_4_1_8_128_f32(%1657, %1658) : (tensor<4x1x8x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x8x128xf32>
    %1660 = torch_c.from_builtin_tensor %1659 : tensor<4x1x8x128xf32> -> !torch.vtensor<[4,1,8,128],f32>
    %int5_1723 = torch.constant.int 5
    %1661 = torch.prims.convert_element_type %1660, %int5_1723 : !torch.vtensor<[4,1,8,128],f32>, !torch.int -> !torch.vtensor<[4,1,8,128],f16>
    %int32_1724 = torch.constant.int 32
    %1662 = torch.aten.floor_divide.Scalar %arg2, %int32_1724 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_1725 = torch.constant.int 1
    %1663 = torch.aten.unsqueeze %1662, %int1_1725 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_1726 = torch.constant.int 1
    %false_1727 = torch.constant.bool false
    %1664 = torch.aten.gather %arg3, %int1_1726, %1663, %false_1727 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_1728 = torch.constant.int 32
    %1665 = torch.aten.remainder.Scalar %arg2, %int32_1728 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_1729 = torch.constant.int 1
    %1666 = torch.aten.unsqueeze %1665, %int1_1729 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_1730 = torch.constant.none
    %1667 = torch.aten.clone %71, %none_1730 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_1731 = torch.constant.int 0
    %1668 = torch.aten.unsqueeze %1667, %int0_1731 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_1732 = torch.constant.int 4
    %int1_1733 = torch.constant.int 1
    %1669 = torch.prim.ListConstruct %int4_1732, %int1_1733 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_1734 = torch.constant.int 1
    %int1_1735 = torch.constant.int 1
    %1670 = torch.prim.ListConstruct %int1_1734, %int1_1735 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_1736 = torch.constant.int 4
    %int0_1737 = torch.constant.int 0
    %cpu_1738 = torch.constant.device "cpu"
    %false_1739 = torch.constant.bool false
    %1671 = torch.aten.empty_strided %1669, %1670, %int4_1736, %int0_1737, %cpu_1738, %false_1739 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int6_1740 = torch.constant.int 6
    %1672 = torch.aten.fill.Scalar %1671, %int6_1740 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_1741 = torch.constant.int 4
    %int1_1742 = torch.constant.int 1
    %1673 = torch.prim.ListConstruct %int4_1741, %int1_1742 : (!torch.int, !torch.int) -> !torch.list<int>
    %1674 = torch.aten.repeat %1668, %1673 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_1743 = torch.constant.int 32
    %1675 = torch.aten.mul.Scalar %1664, %int32_1743 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_1744 = torch.constant.int 1
    %1676 = torch.aten.add.Tensor %1675, %1672, %int1_1744 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_1745 = torch.constant.int 2
    %1677 = torch.aten.mul.Scalar %1676, %int2_1745 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_1746 = torch.constant.int 1
    %1678 = torch.aten.add.Tensor %1677, %1674, %int1_1746 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_1747 = torch.constant.int 32
    %1679 = torch.aten.mul.Scalar %1678, %int32_1747 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_1748 = torch.constant.int 1
    %1680 = torch.aten.add.Tensor %1679, %1666, %int1_1748 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_1749 = torch.constant.int 32
    %int2_1750 = torch.constant.int 2
    %int32_1751 = torch.constant.int 32
    %int8_1752 = torch.constant.int 8
    %int128_1753 = torch.constant.int 128
    %1681 = torch.prim.ListConstruct %437, %int32_1749, %int2_1750, %int32_1751, %int8_1752, %int128_1753 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1682 = torch.aten.view %1518, %1681 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1682, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_1754 = torch.constant.int 32
    %1683 = torch.aten.mul.int %437, %int32_1754 : !torch.int, !torch.int -> !torch.int
    %int2_1755 = torch.constant.int 2
    %1684 = torch.aten.mul.int %1683, %int2_1755 : !torch.int, !torch.int -> !torch.int
    %int32_1756 = torch.constant.int 32
    %1685 = torch.aten.mul.int %1684, %int32_1756 : !torch.int, !torch.int -> !torch.int
    %int8_1757 = torch.constant.int 8
    %int128_1758 = torch.constant.int 128
    %1686 = torch.prim.ListConstruct %1685, %int8_1757, %int128_1758 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1687 = torch.aten.view %1682, %1686 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %1687, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %1688 = torch.prim.ListConstruct %1680 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_1759 = torch.constant.bool false
    %1689 = torch.aten.index_put %1687, %1688, %1661, %false_1759 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %1689, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_1760 = torch.constant.int 32
    %int2_1761 = torch.constant.int 2
    %int32_1762 = torch.constant.int 32
    %int8_1763 = torch.constant.int 8
    %int128_1764 = torch.constant.int 128
    %1690 = torch.prim.ListConstruct %437, %int32_1760, %int2_1761, %int32_1762, %int8_1763, %int128_1764 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1691 = torch.aten.view %1689, %1690 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1691, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_1765 = torch.constant.int 2097152
    %1692 = torch.prim.ListConstruct %437, %int2097152_1765 : (!torch.int, !torch.int) -> !torch.list<int>
    %1693 = torch.aten.view %1691, %1692 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %1693, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_1766 = torch.constant.int 32
    %int2_1767 = torch.constant.int 2
    %int32_1768 = torch.constant.int 32
    %int8_1769 = torch.constant.int 8
    %int128_1770 = torch.constant.int 128
    %1694 = torch.prim.ListConstruct %437, %int32_1766, %int2_1767, %int32_1768, %int8_1769, %int128_1770 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1695 = torch.aten.view %1693, %1694 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1695, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int8_1771 = torch.constant.int 8
    %int128_1772 = torch.constant.int 128
    %1696 = torch.prim.ListConstruct %1685, %int8_1771, %int128_1772 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1697 = torch.aten.view %1695, %1696 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %1697, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_1773 = torch.constant.int 32
    %1698 = torch.aten.floor_divide.Scalar %arg2, %int32_1773 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_1774 = torch.constant.int 1
    %1699 = torch.aten.unsqueeze %1698, %int1_1774 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_1775 = torch.constant.int 1
    %false_1776 = torch.constant.bool false
    %1700 = torch.aten.gather %arg3, %int1_1775, %1699, %false_1776 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_1777 = torch.constant.int 32
    %1701 = torch.aten.remainder.Scalar %arg2, %int32_1777 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_1778 = torch.constant.int 1
    %1702 = torch.aten.unsqueeze %1701, %int1_1778 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_1779 = torch.constant.none
    %1703 = torch.aten.clone %72, %none_1779 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_1780 = torch.constant.int 0
    %1704 = torch.aten.unsqueeze %1703, %int0_1780 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_1781 = torch.constant.int 4
    %int1_1782 = torch.constant.int 1
    %1705 = torch.prim.ListConstruct %int4_1781, %int1_1782 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_1783 = torch.constant.int 1
    %int1_1784 = torch.constant.int 1
    %1706 = torch.prim.ListConstruct %int1_1783, %int1_1784 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_1785 = torch.constant.int 4
    %int0_1786 = torch.constant.int 0
    %cpu_1787 = torch.constant.device "cpu"
    %false_1788 = torch.constant.bool false
    %1707 = torch.aten.empty_strided %1705, %1706, %int4_1785, %int0_1786, %cpu_1787, %false_1788 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int6_1789 = torch.constant.int 6
    %1708 = torch.aten.fill.Scalar %1707, %int6_1789 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_1790 = torch.constant.int 4
    %int1_1791 = torch.constant.int 1
    %1709 = torch.prim.ListConstruct %int4_1790, %int1_1791 : (!torch.int, !torch.int) -> !torch.list<int>
    %1710 = torch.aten.repeat %1704, %1709 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_1792 = torch.constant.int 32
    %1711 = torch.aten.mul.Scalar %1700, %int32_1792 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_1793 = torch.constant.int 1
    %1712 = torch.aten.add.Tensor %1711, %1708, %int1_1793 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_1794 = torch.constant.int 2
    %1713 = torch.aten.mul.Scalar %1712, %int2_1794 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_1795 = torch.constant.int 1
    %1714 = torch.aten.add.Tensor %1713, %1710, %int1_1795 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_1796 = torch.constant.int 32
    %1715 = torch.aten.mul.Scalar %1714, %int32_1796 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_1797 = torch.constant.int 1
    %1716 = torch.aten.add.Tensor %1715, %1702, %int1_1797 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %1717 = torch.prim.ListConstruct %1716 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_1798 = torch.constant.bool false
    %1718 = torch.aten.index_put %1697, %1717, %1649, %false_1798 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %1718, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_1799 = torch.constant.int 32
    %int2_1800 = torch.constant.int 2
    %int32_1801 = torch.constant.int 32
    %int8_1802 = torch.constant.int 8
    %int128_1803 = torch.constant.int 128
    %1719 = torch.prim.ListConstruct %437, %int32_1799, %int2_1800, %int32_1801, %int8_1802, %int128_1803 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1720 = torch.aten.view %1718, %1719 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1720, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_1804 = torch.constant.int 2097152
    %1721 = torch.prim.ListConstruct %437, %int2097152_1804 : (!torch.int, !torch.int) -> !torch.list<int>
    %1722 = torch.aten.view %1720, %1721 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %1722, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int4_1805 = torch.constant.int 4
    %1723 = torch.prim.ListConstruct %int4_1805, %358 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_1806 = torch.constant.int 1
    %1724 = torch.prim.ListConstruct %358, %int1_1806 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_1807 = torch.constant.int 4
    %int0_1808 = torch.constant.int 0
    %cpu_1809 = torch.constant.device "cpu"
    %false_1810 = torch.constant.bool false
    %1725 = torch.aten.empty_strided %1723, %1724, %int4_1807, %int0_1808, %cpu_1809, %false_1810 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1725, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int6_1811 = torch.constant.int 6
    %1726 = torch.aten.fill.Scalar %1725, %int6_1811 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1726, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int32_1812 = torch.constant.int 32
    %1727 = torch.aten.mul.Scalar %arg3, %int32_1812 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1727, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_1813 = torch.constant.int 1
    %1728 = torch.aten.add.Tensor %1727, %1726, %int1_1813 : !torch.vtensor<[4,?],si64>, !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1728, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_1814 = torch.constant.int 4
    %1729 = torch.aten.mul.int %int4_1814, %358 : !torch.int, !torch.int -> !torch.int
    %1730 = torch.prim.ListConstruct %1729 : (!torch.int) -> !torch.list<int>
    %1731 = torch.aten.view %1728, %1730 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1731, [%356], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_1815 = torch.constant.int 32
    %int2_1816 = torch.constant.int 2
    %int32_1817 = torch.constant.int 32
    %int8_1818 = torch.constant.int 8
    %int128_1819 = torch.constant.int 128
    %1732 = torch.prim.ListConstruct %437, %int32_1815, %int2_1816, %int32_1817, %int8_1818, %int128_1819 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1733 = torch.aten.view %1722, %1732 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1733, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_1820 = torch.constant.int 32
    %1734 = torch.aten.mul.int %437, %int32_1820 : !torch.int, !torch.int -> !torch.int
    %int2_1821 = torch.constant.int 2
    %int32_1822 = torch.constant.int 32
    %int8_1823 = torch.constant.int 8
    %int128_1824 = torch.constant.int 128
    %1735 = torch.prim.ListConstruct %1734, %int2_1821, %int32_1822, %int8_1823, %int128_1824 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1736 = torch.aten.view %1733, %1735 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %1736, [%357], affine_map<()[s0] -> (s0 * 32, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int0_1825 = torch.constant.int 0
    %1737 = torch.aten.index_select %1736, %int0_1825, %1731 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.int, !torch.vtensor<[?],si64> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %1737, [%356], affine_map<()[s0] -> (s0 * 4, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int4_1826 = torch.constant.int 4
    %int2_1827 = torch.constant.int 2
    %int32_1828 = torch.constant.int 32
    %int8_1829 = torch.constant.int 8
    %int128_1830 = torch.constant.int 128
    %1738 = torch.prim.ListConstruct %int4_1826, %358, %int2_1827, %int32_1828, %int8_1829, %int128_1830 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1739 = torch.aten.view %1737, %1738 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %1739, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int0_1831 = torch.constant.int 0
    %int0_1832 = torch.constant.int 0
    %int9223372036854775807_1833 = torch.constant.int 9223372036854775807
    %int1_1834 = torch.constant.int 1
    %1740 = torch.aten.slice.Tensor %1739, %int0_1831, %int0_1832, %int9223372036854775807_1833, %int1_1834 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %1740, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_1835 = torch.constant.int 1
    %int0_1836 = torch.constant.int 0
    %int9223372036854775807_1837 = torch.constant.int 9223372036854775807
    %int1_1838 = torch.constant.int 1
    %1741 = torch.aten.slice.Tensor %1740, %int1_1835, %int0_1836, %int9223372036854775807_1837, %int1_1838 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %1741, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_1839 = torch.constant.int 2
    %int0_1840 = torch.constant.int 0
    %1742 = torch.aten.select.int %1741, %int2_1839, %int0_1840 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1742, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int32_1841 = torch.constant.int 32
    %1743 = torch.aten.mul.int %358, %int32_1841 : !torch.int, !torch.int -> !torch.int
    %int2_1842 = torch.constant.int 2
    %int0_1843 = torch.constant.int 0
    %int1_1844 = torch.constant.int 1
    %1744 = torch.aten.slice.Tensor %1742, %int2_1842, %int0_1843, %1743, %int1_1844 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1744, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_1845 = torch.constant.int 0
    %1745 = torch.aten.clone %1744, %int0_1845 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1745, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_1846 = torch.constant.int 1
    %1746 = torch.aten.size.int %1741, %int1_1846 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_1847 = torch.constant.int 32
    %1747 = torch.aten.mul.int %1746, %int32_1847 : !torch.int, !torch.int -> !torch.int
    %int4_1848 = torch.constant.int 4
    %int8_1849 = torch.constant.int 8
    %int128_1850 = torch.constant.int 128
    %1748 = torch.prim.ListConstruct %int4_1848, %1747, %int8_1849, %int128_1850 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1749 = torch.aten._unsafe_view %1745, %1748 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %1749, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_1851 = torch.constant.int 0
    %int0_1852 = torch.constant.int 0
    %int9223372036854775807_1853 = torch.constant.int 9223372036854775807
    %int1_1854 = torch.constant.int 1
    %1750 = torch.aten.slice.Tensor %1749, %int0_1851, %int0_1852, %int9223372036854775807_1853, %int1_1854 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %1750, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_1855 = torch.constant.int 0
    %int0_1856 = torch.constant.int 0
    %int9223372036854775807_1857 = torch.constant.int 9223372036854775807
    %int1_1858 = torch.constant.int 1
    %1751 = torch.aten.slice.Tensor %1739, %int0_1855, %int0_1856, %int9223372036854775807_1857, %int1_1858 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %1751, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_1859 = torch.constant.int 1
    %int0_1860 = torch.constant.int 0
    %int9223372036854775807_1861 = torch.constant.int 9223372036854775807
    %int1_1862 = torch.constant.int 1
    %1752 = torch.aten.slice.Tensor %1751, %int1_1859, %int0_1860, %int9223372036854775807_1861, %int1_1862 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %1752, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_1863 = torch.constant.int 2
    %int1_1864 = torch.constant.int 1
    %1753 = torch.aten.select.int %1752, %int2_1863, %int1_1864 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1753, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int2_1865 = torch.constant.int 2
    %int0_1866 = torch.constant.int 0
    %int1_1867 = torch.constant.int 1
    %1754 = torch.aten.slice.Tensor %1753, %int2_1865, %int0_1866, %1743, %int1_1867 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1754, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_1868 = torch.constant.int 0
    %1755 = torch.aten.clone %1754, %int0_1868 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1755, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_1869 = torch.constant.int 1
    %1756 = torch.aten.size.int %1752, %int1_1869 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_1870 = torch.constant.int 32
    %1757 = torch.aten.mul.int %1756, %int32_1870 : !torch.int, !torch.int -> !torch.int
    %int4_1871 = torch.constant.int 4
    %int8_1872 = torch.constant.int 8
    %int128_1873 = torch.constant.int 128
    %1758 = torch.prim.ListConstruct %int4_1871, %1757, %int8_1872, %int128_1873 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1759 = torch.aten._unsafe_view %1755, %1758 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %1759, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_1874 = torch.constant.int 0
    %int0_1875 = torch.constant.int 0
    %int9223372036854775807_1876 = torch.constant.int 9223372036854775807
    %int1_1877 = torch.constant.int 1
    %1760 = torch.aten.slice.Tensor %1759, %int0_1874, %int0_1875, %int9223372036854775807_1876, %int1_1877 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %1760, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int-2_1878 = torch.constant.int -2
    %1761 = torch.aten.unsqueeze %1750, %int-2_1878 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %1761, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_1879 = torch.constant.int 1
    %1762 = torch.aten.size.int %1749, %int1_1879 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_1880 = torch.constant.int 4
    %int8_1881 = torch.constant.int 8
    %int4_1882 = torch.constant.int 4
    %int128_1883 = torch.constant.int 128
    %1763 = torch.prim.ListConstruct %int4_1880, %1762, %int8_1881, %int4_1882, %int128_1883 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_1884 = torch.constant.bool false
    %1764 = torch.aten.expand %1761, %1763, %false_1884 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1764, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_1885 = torch.constant.int 0
    %1765 = torch.aten.clone %1764, %int0_1885 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1765, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_1886 = torch.constant.int 4
    %int32_1887 = torch.constant.int 32
    %int128_1888 = torch.constant.int 128
    %1766 = torch.prim.ListConstruct %int4_1886, %1762, %int32_1887, %int128_1888 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1767 = torch.aten._unsafe_view %1765, %1766 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %1767, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_1889 = torch.constant.int -2
    %1768 = torch.aten.unsqueeze %1760, %int-2_1889 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %1768, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_1890 = torch.constant.int 1
    %1769 = torch.aten.size.int %1759, %int1_1890 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_1891 = torch.constant.int 4
    %int8_1892 = torch.constant.int 8
    %int4_1893 = torch.constant.int 4
    %int128_1894 = torch.constant.int 128
    %1770 = torch.prim.ListConstruct %int4_1891, %1769, %int8_1892, %int4_1893, %int128_1894 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_1895 = torch.constant.bool false
    %1771 = torch.aten.expand %1768, %1770, %false_1895 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1771, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_1896 = torch.constant.int 0
    %1772 = torch.aten.clone %1771, %int0_1896 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1772, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_1897 = torch.constant.int 4
    %int32_1898 = torch.constant.int 32
    %int128_1899 = torch.constant.int 128
    %1773 = torch.prim.ListConstruct %int4_1897, %1769, %int32_1898, %int128_1899 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1774 = torch.aten._unsafe_view %1772, %1773 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %1774, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_1900 = torch.constant.int 1
    %int2_1901 = torch.constant.int 2
    %1775 = torch.aten.transpose.int %1655, %int1_1900, %int2_1901 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,128],f16>
    %int1_1902 = torch.constant.int 1
    %int2_1903 = torch.constant.int 2
    %1776 = torch.aten.transpose.int %1767, %int1_1902, %int2_1903 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %1776, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_1904 = torch.constant.int 1
    %int2_1905 = torch.constant.int 2
    %1777 = torch.aten.transpose.int %1774, %int1_1904, %int2_1905 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %1777, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_1906 = torch.constant.float 0.000000e+00
    %false_1907 = torch.constant.bool false
    %none_1908 = torch.constant.none
    %1778:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1775, %1776, %1777, %float0.000000e00_1906, %false_1907, %368, %none_1908) : (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.vtensor<[4,1,1,?],f16>, !torch.none) -> (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,1],f32>) 
    %int1_1909 = torch.constant.int 1
    %int2_1910 = torch.constant.int 2
    %1779 = torch.aten.transpose.int %1778#0, %int1_1909, %int2_1910 : !torch.vtensor<[4,32,1,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int4_1911 = torch.constant.int 4
    %int1_1912 = torch.constant.int 1
    %int4096_1913 = torch.constant.int 4096
    %1780 = torch.prim.ListConstruct %int4_1911, %int1_1912, %int4096_1913 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1781 = torch.aten.view %1779, %1780 : !torch.vtensor<[4,1,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_1914 = torch.constant.int -2
    %int-1_1915 = torch.constant.int -1
    %1782 = torch.aten.transpose.int %73, %int-2_1914, %int-1_1915 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_1916 = torch.constant.int 4
    %int4096_1917 = torch.constant.int 4096
    %1783 = torch.prim.ListConstruct %int4_1916, %int4096_1917 : (!torch.int, !torch.int) -> !torch.list<int>
    %1784 = torch.aten.view %1781, %1783 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %1785 = torch.aten.mm %1784, %1782 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_1918 = torch.constant.int 4
    %int1_1919 = torch.constant.int 1
    %int4096_1920 = torch.constant.int 4096
    %1786 = torch.prim.ListConstruct %int4_1918, %int1_1919, %int4096_1920 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1787 = torch.aten.view %1785, %1786 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_1921 = torch.constant.int 1
    %1788 = torch.aten.add.Tensor %1615, %1787, %int1_1921 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_1922 = torch.constant.int 6
    %1789 = torch.prims.convert_element_type %1788, %int6_1922 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_1923 = torch.constant.int 2
    %1790 = torch.aten.pow.Tensor_Scalar %1789, %int2_1923 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_1924 = torch.constant.int -1
    %1791 = torch.prim.ListConstruct %int-1_1924 : (!torch.int) -> !torch.list<int>
    %true_1925 = torch.constant.bool true
    %none_1926 = torch.constant.none
    %1792 = torch.aten.mean.dim %1790, %1791, %true_1925, %none_1926 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_1927 = torch.constant.float 9.9999997473787516E-6
    %int1_1928 = torch.constant.int 1
    %1793 = torch.aten.add.Scalar %1792, %float9.999990e-06_1927, %int1_1928 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %1794 = torch.aten.rsqrt %1793 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %1795 = torch.aten.mul.Tensor %1789, %1794 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_1929 = torch.constant.int 5
    %1796 = torch.prims.convert_element_type %1795, %int5_1929 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %1797 = torch.aten.mul.Tensor %74, %1796 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_1930 = torch.constant.int 5
    %1798 = torch.prims.convert_element_type %1797, %int5_1930 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_1931 = torch.constant.int -2
    %int-1_1932 = torch.constant.int -1
    %1799 = torch.aten.transpose.int %75, %int-2_1931, %int-1_1932 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_1933 = torch.constant.int 4
    %int4096_1934 = torch.constant.int 4096
    %1800 = torch.prim.ListConstruct %int4_1933, %int4096_1934 : (!torch.int, !torch.int) -> !torch.list<int>
    %1801 = torch.aten.view %1798, %1800 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %1802 = torch.aten.mm %1801, %1799 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_1935 = torch.constant.int 4
    %int1_1936 = torch.constant.int 1
    %int14336_1937 = torch.constant.int 14336
    %1803 = torch.prim.ListConstruct %int4_1935, %int1_1936, %int14336_1937 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1804 = torch.aten.view %1802, %1803 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %1805 = torch.aten.silu %1804 : !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_1938 = torch.constant.int -2
    %int-1_1939 = torch.constant.int -1
    %1806 = torch.aten.transpose.int %76, %int-2_1938, %int-1_1939 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_1940 = torch.constant.int 4
    %int4096_1941 = torch.constant.int 4096
    %1807 = torch.prim.ListConstruct %int4_1940, %int4096_1941 : (!torch.int, !torch.int) -> !torch.list<int>
    %1808 = torch.aten.view %1798, %1807 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %1809 = torch.aten.mm %1808, %1806 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_1942 = torch.constant.int 4
    %int1_1943 = torch.constant.int 1
    %int14336_1944 = torch.constant.int 14336
    %1810 = torch.prim.ListConstruct %int4_1942, %int1_1943, %int14336_1944 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1811 = torch.aten.view %1809, %1810 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %1812 = torch.aten.mul.Tensor %1805, %1811 : !torch.vtensor<[4,1,14336],f16>, !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_1945 = torch.constant.int -2
    %int-1_1946 = torch.constant.int -1
    %1813 = torch.aten.transpose.int %77, %int-2_1945, %int-1_1946 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int4_1947 = torch.constant.int 4
    %int14336_1948 = torch.constant.int 14336
    %1814 = torch.prim.ListConstruct %int4_1947, %int14336_1948 : (!torch.int, !torch.int) -> !torch.list<int>
    %1815 = torch.aten.view %1812, %1814 : !torch.vtensor<[4,1,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,14336],f16>
    %1816 = torch.aten.mm %1815, %1813 : !torch.vtensor<[4,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_1949 = torch.constant.int 4
    %int1_1950 = torch.constant.int 1
    %int4096_1951 = torch.constant.int 4096
    %1817 = torch.prim.ListConstruct %int4_1949, %int1_1950, %int4096_1951 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1818 = torch.aten.view %1816, %1817 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_1952 = torch.constant.int 1
    %1819 = torch.aten.add.Tensor %1788, %1818, %int1_1952 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_1953 = torch.constant.int 6
    %1820 = torch.prims.convert_element_type %1819, %int6_1953 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_1954 = torch.constant.int 2
    %1821 = torch.aten.pow.Tensor_Scalar %1820, %int2_1954 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_1955 = torch.constant.int -1
    %1822 = torch.prim.ListConstruct %int-1_1955 : (!torch.int) -> !torch.list<int>
    %true_1956 = torch.constant.bool true
    %none_1957 = torch.constant.none
    %1823 = torch.aten.mean.dim %1821, %1822, %true_1956, %none_1957 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_1958 = torch.constant.float 9.9999997473787516E-6
    %int1_1959 = torch.constant.int 1
    %1824 = torch.aten.add.Scalar %1823, %float9.999990e-06_1958, %int1_1959 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %1825 = torch.aten.rsqrt %1824 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %1826 = torch.aten.mul.Tensor %1820, %1825 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_1960 = torch.constant.int 5
    %1827 = torch.prims.convert_element_type %1826, %int5_1960 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %1828 = torch.aten.mul.Tensor %78, %1827 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_1961 = torch.constant.int 5
    %1829 = torch.prims.convert_element_type %1828, %int5_1961 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_1962 = torch.constant.int -2
    %int-1_1963 = torch.constant.int -1
    %1830 = torch.aten.transpose.int %79, %int-2_1962, %int-1_1963 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_1964 = torch.constant.int 4
    %int4096_1965 = torch.constant.int 4096
    %1831 = torch.prim.ListConstruct %int4_1964, %int4096_1965 : (!torch.int, !torch.int) -> !torch.list<int>
    %1832 = torch.aten.view %1829, %1831 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %1833 = torch.aten.mm %1832, %1830 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_1966 = torch.constant.int 4
    %int1_1967 = torch.constant.int 1
    %int4096_1968 = torch.constant.int 4096
    %1834 = torch.prim.ListConstruct %int4_1966, %int1_1967, %int4096_1968 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1835 = torch.aten.view %1833, %1834 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_1969 = torch.constant.int -2
    %int-1_1970 = torch.constant.int -1
    %1836 = torch.aten.transpose.int %80, %int-2_1969, %int-1_1970 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_1971 = torch.constant.int 4
    %int4096_1972 = torch.constant.int 4096
    %1837 = torch.prim.ListConstruct %int4_1971, %int4096_1972 : (!torch.int, !torch.int) -> !torch.list<int>
    %1838 = torch.aten.view %1829, %1837 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %1839 = torch.aten.mm %1838, %1836 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_1973 = torch.constant.int 4
    %int1_1974 = torch.constant.int 1
    %int1024_1975 = torch.constant.int 1024
    %1840 = torch.prim.ListConstruct %int4_1973, %int1_1974, %int1024_1975 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1841 = torch.aten.view %1839, %1840 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int-2_1976 = torch.constant.int -2
    %int-1_1977 = torch.constant.int -1
    %1842 = torch.aten.transpose.int %81, %int-2_1976, %int-1_1977 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_1978 = torch.constant.int 4
    %int4096_1979 = torch.constant.int 4096
    %1843 = torch.prim.ListConstruct %int4_1978, %int4096_1979 : (!torch.int, !torch.int) -> !torch.list<int>
    %1844 = torch.aten.view %1829, %1843 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %1845 = torch.aten.mm %1844, %1842 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_1980 = torch.constant.int 4
    %int1_1981 = torch.constant.int 1
    %int1024_1982 = torch.constant.int 1024
    %1846 = torch.prim.ListConstruct %int4_1980, %int1_1981, %int1024_1982 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1847 = torch.aten.view %1845, %1846 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int4_1983 = torch.constant.int 4
    %int1_1984 = torch.constant.int 1
    %int32_1985 = torch.constant.int 32
    %int128_1986 = torch.constant.int 128
    %1848 = torch.prim.ListConstruct %int4_1983, %int1_1984, %int32_1985, %int128_1986 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1849 = torch.aten.view %1835, %1848 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,128],f16>
    %int4_1987 = torch.constant.int 4
    %int1_1988 = torch.constant.int 1
    %int8_1989 = torch.constant.int 8
    %int128_1990 = torch.constant.int 128
    %1850 = torch.prim.ListConstruct %int4_1987, %int1_1988, %int8_1989, %int128_1990 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1851 = torch.aten.view %1841, %1850 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int4_1991 = torch.constant.int 4
    %int1_1992 = torch.constant.int 1
    %int8_1993 = torch.constant.int 8
    %int128_1994 = torch.constant.int 128
    %1852 = torch.prim.ListConstruct %int4_1991, %int1_1992, %int8_1993, %int128_1994 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1853 = torch.aten.view %1847, %1852 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int6_1995 = torch.constant.int 6
    %1854 = torch.prims.convert_element_type %1849, %int6_1995 : !torch.vtensor<[4,1,32,128],f16>, !torch.int -> !torch.vtensor<[4,1,32,128],f32>
    %1855 = torch_c.to_builtin_tensor %1854 : !torch.vtensor<[4,1,32,128],f32> -> tensor<4x1x32x128xf32>
    %1856 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %1857 = util.call @sharktank_rotary_embedding_4_1_32_128_f32(%1855, %1856) : (tensor<4x1x32x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x32x128xf32>
    %1858 = torch_c.from_builtin_tensor %1857 : tensor<4x1x32x128xf32> -> !torch.vtensor<[4,1,32,128],f32>
    %int5_1996 = torch.constant.int 5
    %1859 = torch.prims.convert_element_type %1858, %int5_1996 : !torch.vtensor<[4,1,32,128],f32>, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int6_1997 = torch.constant.int 6
    %1860 = torch.prims.convert_element_type %1851, %int6_1997 : !torch.vtensor<[4,1,8,128],f16>, !torch.int -> !torch.vtensor<[4,1,8,128],f32>
    %1861 = torch_c.to_builtin_tensor %1860 : !torch.vtensor<[4,1,8,128],f32> -> tensor<4x1x8x128xf32>
    %1862 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %1863 = util.call @sharktank_rotary_embedding_4_1_8_128_f32(%1861, %1862) : (tensor<4x1x8x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x8x128xf32>
    %1864 = torch_c.from_builtin_tensor %1863 : tensor<4x1x8x128xf32> -> !torch.vtensor<[4,1,8,128],f32>
    %int5_1998 = torch.constant.int 5
    %1865 = torch.prims.convert_element_type %1864, %int5_1998 : !torch.vtensor<[4,1,8,128],f32>, !torch.int -> !torch.vtensor<[4,1,8,128],f16>
    %int32_1999 = torch.constant.int 32
    %1866 = torch.aten.floor_divide.Scalar %arg2, %int32_1999 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_2000 = torch.constant.int 1
    %1867 = torch.aten.unsqueeze %1866, %int1_2000 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2001 = torch.constant.int 1
    %false_2002 = torch.constant.bool false
    %1868 = torch.aten.gather %arg3, %int1_2001, %1867, %false_2002 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_2003 = torch.constant.int 32
    %1869 = torch.aten.remainder.Scalar %arg2, %int32_2003 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_2004 = torch.constant.int 1
    %1870 = torch.aten.unsqueeze %1869, %int1_2004 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_2005 = torch.constant.none
    %1871 = torch.aten.clone %82, %none_2005 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_2006 = torch.constant.int 0
    %1872 = torch.aten.unsqueeze %1871, %int0_2006 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_2007 = torch.constant.int 4
    %int1_2008 = torch.constant.int 1
    %1873 = torch.prim.ListConstruct %int4_2007, %int1_2008 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_2009 = torch.constant.int 1
    %int1_2010 = torch.constant.int 1
    %1874 = torch.prim.ListConstruct %int1_2009, %int1_2010 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_2011 = torch.constant.int 4
    %int0_2012 = torch.constant.int 0
    %cpu_2013 = torch.constant.device "cpu"
    %false_2014 = torch.constant.bool false
    %1875 = torch.aten.empty_strided %1873, %1874, %int4_2011, %int0_2012, %cpu_2013, %false_2014 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int7 = torch.constant.int 7
    %1876 = torch.aten.fill.Scalar %1875, %int7 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_2015 = torch.constant.int 4
    %int1_2016 = torch.constant.int 1
    %1877 = torch.prim.ListConstruct %int4_2015, %int1_2016 : (!torch.int, !torch.int) -> !torch.list<int>
    %1878 = torch.aten.repeat %1872, %1877 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_2017 = torch.constant.int 32
    %1879 = torch.aten.mul.Scalar %1868, %int32_2017 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2018 = torch.constant.int 1
    %1880 = torch.aten.add.Tensor %1879, %1876, %int1_2018 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_2019 = torch.constant.int 2
    %1881 = torch.aten.mul.Scalar %1880, %int2_2019 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2020 = torch.constant.int 1
    %1882 = torch.aten.add.Tensor %1881, %1878, %int1_2020 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_2021 = torch.constant.int 32
    %1883 = torch.aten.mul.Scalar %1882, %int32_2021 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2022 = torch.constant.int 1
    %1884 = torch.aten.add.Tensor %1883, %1870, %int1_2022 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_2023 = torch.constant.int 32
    %int2_2024 = torch.constant.int 2
    %int32_2025 = torch.constant.int 32
    %int8_2026 = torch.constant.int 8
    %int128_2027 = torch.constant.int 128
    %1885 = torch.prim.ListConstruct %437, %int32_2023, %int2_2024, %int32_2025, %int8_2026, %int128_2027 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1886 = torch.aten.view %1722, %1885 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1886, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_2028 = torch.constant.int 32
    %1887 = torch.aten.mul.int %437, %int32_2028 : !torch.int, !torch.int -> !torch.int
    %int2_2029 = torch.constant.int 2
    %1888 = torch.aten.mul.int %1887, %int2_2029 : !torch.int, !torch.int -> !torch.int
    %int32_2030 = torch.constant.int 32
    %1889 = torch.aten.mul.int %1888, %int32_2030 : !torch.int, !torch.int -> !torch.int
    %int8_2031 = torch.constant.int 8
    %int128_2032 = torch.constant.int 128
    %1890 = torch.prim.ListConstruct %1889, %int8_2031, %int128_2032 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1891 = torch.aten.view %1886, %1890 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %1891, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %1892 = torch.prim.ListConstruct %1884 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_2033 = torch.constant.bool false
    %1893 = torch.aten.index_put %1891, %1892, %1865, %false_2033 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %1893, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_2034 = torch.constant.int 32
    %int2_2035 = torch.constant.int 2
    %int32_2036 = torch.constant.int 32
    %int8_2037 = torch.constant.int 8
    %int128_2038 = torch.constant.int 128
    %1894 = torch.prim.ListConstruct %437, %int32_2034, %int2_2035, %int32_2036, %int8_2037, %int128_2038 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1895 = torch.aten.view %1893, %1894 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1895, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_2039 = torch.constant.int 2097152
    %1896 = torch.prim.ListConstruct %437, %int2097152_2039 : (!torch.int, !torch.int) -> !torch.list<int>
    %1897 = torch.aten.view %1895, %1896 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %1897, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_2040 = torch.constant.int 32
    %int2_2041 = torch.constant.int 2
    %int32_2042 = torch.constant.int 32
    %int8_2043 = torch.constant.int 8
    %int128_2044 = torch.constant.int 128
    %1898 = torch.prim.ListConstruct %437, %int32_2040, %int2_2041, %int32_2042, %int8_2043, %int128_2044 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1899 = torch.aten.view %1897, %1898 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1899, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int8_2045 = torch.constant.int 8
    %int128_2046 = torch.constant.int 128
    %1900 = torch.prim.ListConstruct %1889, %int8_2045, %int128_2046 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1901 = torch.aten.view %1899, %1900 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %1901, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_2047 = torch.constant.int 32
    %1902 = torch.aten.floor_divide.Scalar %arg2, %int32_2047 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_2048 = torch.constant.int 1
    %1903 = torch.aten.unsqueeze %1902, %int1_2048 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2049 = torch.constant.int 1
    %false_2050 = torch.constant.bool false
    %1904 = torch.aten.gather %arg3, %int1_2049, %1903, %false_2050 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_2051 = torch.constant.int 32
    %1905 = torch.aten.remainder.Scalar %arg2, %int32_2051 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_2052 = torch.constant.int 1
    %1906 = torch.aten.unsqueeze %1905, %int1_2052 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_2053 = torch.constant.none
    %1907 = torch.aten.clone %83, %none_2053 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_2054 = torch.constant.int 0
    %1908 = torch.aten.unsqueeze %1907, %int0_2054 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_2055 = torch.constant.int 4
    %int1_2056 = torch.constant.int 1
    %1909 = torch.prim.ListConstruct %int4_2055, %int1_2056 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_2057 = torch.constant.int 1
    %int1_2058 = torch.constant.int 1
    %1910 = torch.prim.ListConstruct %int1_2057, %int1_2058 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_2059 = torch.constant.int 4
    %int0_2060 = torch.constant.int 0
    %cpu_2061 = torch.constant.device "cpu"
    %false_2062 = torch.constant.bool false
    %1911 = torch.aten.empty_strided %1909, %1910, %int4_2059, %int0_2060, %cpu_2061, %false_2062 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int7_2063 = torch.constant.int 7
    %1912 = torch.aten.fill.Scalar %1911, %int7_2063 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_2064 = torch.constant.int 4
    %int1_2065 = torch.constant.int 1
    %1913 = torch.prim.ListConstruct %int4_2064, %int1_2065 : (!torch.int, !torch.int) -> !torch.list<int>
    %1914 = torch.aten.repeat %1908, %1913 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_2066 = torch.constant.int 32
    %1915 = torch.aten.mul.Scalar %1904, %int32_2066 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2067 = torch.constant.int 1
    %1916 = torch.aten.add.Tensor %1915, %1912, %int1_2067 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_2068 = torch.constant.int 2
    %1917 = torch.aten.mul.Scalar %1916, %int2_2068 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2069 = torch.constant.int 1
    %1918 = torch.aten.add.Tensor %1917, %1914, %int1_2069 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_2070 = torch.constant.int 32
    %1919 = torch.aten.mul.Scalar %1918, %int32_2070 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2071 = torch.constant.int 1
    %1920 = torch.aten.add.Tensor %1919, %1906, %int1_2071 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %1921 = torch.prim.ListConstruct %1920 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_2072 = torch.constant.bool false
    %1922 = torch.aten.index_put %1901, %1921, %1853, %false_2072 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %1922, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_2073 = torch.constant.int 32
    %int2_2074 = torch.constant.int 2
    %int32_2075 = torch.constant.int 32
    %int8_2076 = torch.constant.int 8
    %int128_2077 = torch.constant.int 128
    %1923 = torch.prim.ListConstruct %437, %int32_2073, %int2_2074, %int32_2075, %int8_2076, %int128_2077 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1924 = torch.aten.view %1922, %1923 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1924, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_2078 = torch.constant.int 2097152
    %1925 = torch.prim.ListConstruct %437, %int2097152_2078 : (!torch.int, !torch.int) -> !torch.list<int>
    %1926 = torch.aten.view %1924, %1925 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %1926, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int4_2079 = torch.constant.int 4
    %1927 = torch.prim.ListConstruct %int4_2079, %358 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_2080 = torch.constant.int 1
    %1928 = torch.prim.ListConstruct %358, %int1_2080 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_2081 = torch.constant.int 4
    %int0_2082 = torch.constant.int 0
    %cpu_2083 = torch.constant.device "cpu"
    %false_2084 = torch.constant.bool false
    %1929 = torch.aten.empty_strided %1927, %1928, %int4_2081, %int0_2082, %cpu_2083, %false_2084 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1929, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int7_2085 = torch.constant.int 7
    %1930 = torch.aten.fill.Scalar %1929, %int7_2085 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1930, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int32_2086 = torch.constant.int 32
    %1931 = torch.aten.mul.Scalar %arg3, %int32_2086 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1931, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_2087 = torch.constant.int 1
    %1932 = torch.aten.add.Tensor %1931, %1930, %int1_2087 : !torch.vtensor<[4,?],si64>, !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %1932, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_2088 = torch.constant.int 4
    %1933 = torch.aten.mul.int %int4_2088, %358 : !torch.int, !torch.int -> !torch.int
    %1934 = torch.prim.ListConstruct %1933 : (!torch.int) -> !torch.list<int>
    %1935 = torch.aten.view %1932, %1934 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %1935, [%356], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_2089 = torch.constant.int 32
    %int2_2090 = torch.constant.int 2
    %int32_2091 = torch.constant.int 32
    %int8_2092 = torch.constant.int 8
    %int128_2093 = torch.constant.int 128
    %1936 = torch.prim.ListConstruct %437, %int32_2089, %int2_2090, %int32_2091, %int8_2092, %int128_2093 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1937 = torch.aten.view %1926, %1936 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %1937, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_2094 = torch.constant.int 32
    %1938 = torch.aten.mul.int %437, %int32_2094 : !torch.int, !torch.int -> !torch.int
    %int2_2095 = torch.constant.int 2
    %int32_2096 = torch.constant.int 32
    %int8_2097 = torch.constant.int 8
    %int128_2098 = torch.constant.int 128
    %1939 = torch.prim.ListConstruct %1938, %int2_2095, %int32_2096, %int8_2097, %int128_2098 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1940 = torch.aten.view %1937, %1939 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %1940, [%357], affine_map<()[s0] -> (s0 * 32, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int0_2099 = torch.constant.int 0
    %1941 = torch.aten.index_select %1940, %int0_2099, %1935 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.int, !torch.vtensor<[?],si64> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %1941, [%356], affine_map<()[s0] -> (s0 * 4, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int4_2100 = torch.constant.int 4
    %int2_2101 = torch.constant.int 2
    %int32_2102 = torch.constant.int 32
    %int8_2103 = torch.constant.int 8
    %int128_2104 = torch.constant.int 128
    %1942 = torch.prim.ListConstruct %int4_2100, %358, %int2_2101, %int32_2102, %int8_2103, %int128_2104 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1943 = torch.aten.view %1941, %1942 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %1943, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int0_2105 = torch.constant.int 0
    %int0_2106 = torch.constant.int 0
    %int9223372036854775807_2107 = torch.constant.int 9223372036854775807
    %int1_2108 = torch.constant.int 1
    %1944 = torch.aten.slice.Tensor %1943, %int0_2105, %int0_2106, %int9223372036854775807_2107, %int1_2108 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %1944, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_2109 = torch.constant.int 1
    %int0_2110 = torch.constant.int 0
    %int9223372036854775807_2111 = torch.constant.int 9223372036854775807
    %int1_2112 = torch.constant.int 1
    %1945 = torch.aten.slice.Tensor %1944, %int1_2109, %int0_2110, %int9223372036854775807_2111, %int1_2112 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %1945, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_2113 = torch.constant.int 2
    %int0_2114 = torch.constant.int 0
    %1946 = torch.aten.select.int %1945, %int2_2113, %int0_2114 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1946, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int32_2115 = torch.constant.int 32
    %1947 = torch.aten.mul.int %358, %int32_2115 : !torch.int, !torch.int -> !torch.int
    %int2_2116 = torch.constant.int 2
    %int0_2117 = torch.constant.int 0
    %int1_2118 = torch.constant.int 1
    %1948 = torch.aten.slice.Tensor %1946, %int2_2116, %int0_2117, %1947, %int1_2118 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1948, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_2119 = torch.constant.int 0
    %1949 = torch.aten.clone %1948, %int0_2119 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1949, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_2120 = torch.constant.int 1
    %1950 = torch.aten.size.int %1945, %int1_2120 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_2121 = torch.constant.int 32
    %1951 = torch.aten.mul.int %1950, %int32_2121 : !torch.int, !torch.int -> !torch.int
    %int4_2122 = torch.constant.int 4
    %int8_2123 = torch.constant.int 8
    %int128_2124 = torch.constant.int 128
    %1952 = torch.prim.ListConstruct %int4_2122, %1951, %int8_2123, %int128_2124 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1953 = torch.aten._unsafe_view %1949, %1952 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %1953, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_2125 = torch.constant.int 0
    %int0_2126 = torch.constant.int 0
    %int9223372036854775807_2127 = torch.constant.int 9223372036854775807
    %int1_2128 = torch.constant.int 1
    %1954 = torch.aten.slice.Tensor %1953, %int0_2125, %int0_2126, %int9223372036854775807_2127, %int1_2128 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %1954, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_2129 = torch.constant.int 0
    %int0_2130 = torch.constant.int 0
    %int9223372036854775807_2131 = torch.constant.int 9223372036854775807
    %int1_2132 = torch.constant.int 1
    %1955 = torch.aten.slice.Tensor %1943, %int0_2129, %int0_2130, %int9223372036854775807_2131, %int1_2132 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %1955, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_2133 = torch.constant.int 1
    %int0_2134 = torch.constant.int 0
    %int9223372036854775807_2135 = torch.constant.int 9223372036854775807
    %int1_2136 = torch.constant.int 1
    %1956 = torch.aten.slice.Tensor %1955, %int1_2133, %int0_2134, %int9223372036854775807_2135, %int1_2136 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %1956, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_2137 = torch.constant.int 2
    %int1_2138 = torch.constant.int 1
    %1957 = torch.aten.select.int %1956, %int2_2137, %int1_2138 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1957, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int2_2139 = torch.constant.int 2
    %int0_2140 = torch.constant.int 0
    %int1_2141 = torch.constant.int 1
    %1958 = torch.aten.slice.Tensor %1957, %int2_2139, %int0_2140, %1947, %int1_2141 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1958, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_2142 = torch.constant.int 0
    %1959 = torch.aten.clone %1958, %int0_2142 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %1959, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_2143 = torch.constant.int 1
    %1960 = torch.aten.size.int %1956, %int1_2143 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_2144 = torch.constant.int 32
    %1961 = torch.aten.mul.int %1960, %int32_2144 : !torch.int, !torch.int -> !torch.int
    %int4_2145 = torch.constant.int 4
    %int8_2146 = torch.constant.int 8
    %int128_2147 = torch.constant.int 128
    %1962 = torch.prim.ListConstruct %int4_2145, %1961, %int8_2146, %int128_2147 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1963 = torch.aten._unsafe_view %1959, %1962 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %1963, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_2148 = torch.constant.int 0
    %int0_2149 = torch.constant.int 0
    %int9223372036854775807_2150 = torch.constant.int 9223372036854775807
    %int1_2151 = torch.constant.int 1
    %1964 = torch.aten.slice.Tensor %1963, %int0_2148, %int0_2149, %int9223372036854775807_2150, %int1_2151 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %1964, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int-2_2152 = torch.constant.int -2
    %1965 = torch.aten.unsqueeze %1954, %int-2_2152 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %1965, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_2153 = torch.constant.int 1
    %1966 = torch.aten.size.int %1953, %int1_2153 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_2154 = torch.constant.int 4
    %int8_2155 = torch.constant.int 8
    %int4_2156 = torch.constant.int 4
    %int128_2157 = torch.constant.int 128
    %1967 = torch.prim.ListConstruct %int4_2154, %1966, %int8_2155, %int4_2156, %int128_2157 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_2158 = torch.constant.bool false
    %1968 = torch.aten.expand %1965, %1967, %false_2158 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1968, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_2159 = torch.constant.int 0
    %1969 = torch.aten.clone %1968, %int0_2159 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1969, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_2160 = torch.constant.int 4
    %int32_2161 = torch.constant.int 32
    %int128_2162 = torch.constant.int 128
    %1970 = torch.prim.ListConstruct %int4_2160, %1966, %int32_2161, %int128_2162 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1971 = torch.aten._unsafe_view %1969, %1970 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %1971, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_2163 = torch.constant.int -2
    %1972 = torch.aten.unsqueeze %1964, %int-2_2163 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %1972, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_2164 = torch.constant.int 1
    %1973 = torch.aten.size.int %1963, %int1_2164 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_2165 = torch.constant.int 4
    %int8_2166 = torch.constant.int 8
    %int4_2167 = torch.constant.int 4
    %int128_2168 = torch.constant.int 128
    %1974 = torch.prim.ListConstruct %int4_2165, %1973, %int8_2166, %int4_2167, %int128_2168 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_2169 = torch.constant.bool false
    %1975 = torch.aten.expand %1972, %1974, %false_2169 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1975, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_2170 = torch.constant.int 0
    %1976 = torch.aten.clone %1975, %int0_2170 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %1976, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_2171 = torch.constant.int 4
    %int32_2172 = torch.constant.int 32
    %int128_2173 = torch.constant.int 128
    %1977 = torch.prim.ListConstruct %int4_2171, %1973, %int32_2172, %int128_2173 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1978 = torch.aten._unsafe_view %1976, %1977 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %1978, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_2174 = torch.constant.int 1
    %int2_2175 = torch.constant.int 2
    %1979 = torch.aten.transpose.int %1859, %int1_2174, %int2_2175 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,128],f16>
    %int1_2176 = torch.constant.int 1
    %int2_2177 = torch.constant.int 2
    %1980 = torch.aten.transpose.int %1971, %int1_2176, %int2_2177 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %1980, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_2178 = torch.constant.int 1
    %int2_2179 = torch.constant.int 2
    %1981 = torch.aten.transpose.int %1978, %int1_2178, %int2_2179 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %1981, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_2180 = torch.constant.float 0.000000e+00
    %false_2181 = torch.constant.bool false
    %none_2182 = torch.constant.none
    %1982:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1979, %1980, %1981, %float0.000000e00_2180, %false_2181, %368, %none_2182) : (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.vtensor<[4,1,1,?],f16>, !torch.none) -> (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,1],f32>) 
    %int1_2183 = torch.constant.int 1
    %int2_2184 = torch.constant.int 2
    %1983 = torch.aten.transpose.int %1982#0, %int1_2183, %int2_2184 : !torch.vtensor<[4,32,1,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int4_2185 = torch.constant.int 4
    %int1_2186 = torch.constant.int 1
    %int4096_2187 = torch.constant.int 4096
    %1984 = torch.prim.ListConstruct %int4_2185, %int1_2186, %int4096_2187 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1985 = torch.aten.view %1983, %1984 : !torch.vtensor<[4,1,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_2188 = torch.constant.int -2
    %int-1_2189 = torch.constant.int -1
    %1986 = torch.aten.transpose.int %84, %int-2_2188, %int-1_2189 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_2190 = torch.constant.int 4
    %int4096_2191 = torch.constant.int 4096
    %1987 = torch.prim.ListConstruct %int4_2190, %int4096_2191 : (!torch.int, !torch.int) -> !torch.list<int>
    %1988 = torch.aten.view %1985, %1987 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %1989 = torch.aten.mm %1988, %1986 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_2192 = torch.constant.int 4
    %int1_2193 = torch.constant.int 1
    %int4096_2194 = torch.constant.int 4096
    %1990 = torch.prim.ListConstruct %int4_2192, %int1_2193, %int4096_2194 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1991 = torch.aten.view %1989, %1990 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_2195 = torch.constant.int 1
    %1992 = torch.aten.add.Tensor %1819, %1991, %int1_2195 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_2196 = torch.constant.int 6
    %1993 = torch.prims.convert_element_type %1992, %int6_2196 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_2197 = torch.constant.int 2
    %1994 = torch.aten.pow.Tensor_Scalar %1993, %int2_2197 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_2198 = torch.constant.int -1
    %1995 = torch.prim.ListConstruct %int-1_2198 : (!torch.int) -> !torch.list<int>
    %true_2199 = torch.constant.bool true
    %none_2200 = torch.constant.none
    %1996 = torch.aten.mean.dim %1994, %1995, %true_2199, %none_2200 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_2201 = torch.constant.float 9.9999997473787516E-6
    %int1_2202 = torch.constant.int 1
    %1997 = torch.aten.add.Scalar %1996, %float9.999990e-06_2201, %int1_2202 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %1998 = torch.aten.rsqrt %1997 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %1999 = torch.aten.mul.Tensor %1993, %1998 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_2203 = torch.constant.int 5
    %2000 = torch.prims.convert_element_type %1999, %int5_2203 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %2001 = torch.aten.mul.Tensor %85, %2000 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_2204 = torch.constant.int 5
    %2002 = torch.prims.convert_element_type %2001, %int5_2204 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_2205 = torch.constant.int -2
    %int-1_2206 = torch.constant.int -1
    %2003 = torch.aten.transpose.int %86, %int-2_2205, %int-1_2206 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_2207 = torch.constant.int 4
    %int4096_2208 = torch.constant.int 4096
    %2004 = torch.prim.ListConstruct %int4_2207, %int4096_2208 : (!torch.int, !torch.int) -> !torch.list<int>
    %2005 = torch.aten.view %2002, %2004 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %2006 = torch.aten.mm %2005, %2003 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_2209 = torch.constant.int 4
    %int1_2210 = torch.constant.int 1
    %int14336_2211 = torch.constant.int 14336
    %2007 = torch.prim.ListConstruct %int4_2209, %int1_2210, %int14336_2211 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2008 = torch.aten.view %2006, %2007 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %2009 = torch.aten.silu %2008 : !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_2212 = torch.constant.int -2
    %int-1_2213 = torch.constant.int -1
    %2010 = torch.aten.transpose.int %87, %int-2_2212, %int-1_2213 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_2214 = torch.constant.int 4
    %int4096_2215 = torch.constant.int 4096
    %2011 = torch.prim.ListConstruct %int4_2214, %int4096_2215 : (!torch.int, !torch.int) -> !torch.list<int>
    %2012 = torch.aten.view %2002, %2011 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %2013 = torch.aten.mm %2012, %2010 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_2216 = torch.constant.int 4
    %int1_2217 = torch.constant.int 1
    %int14336_2218 = torch.constant.int 14336
    %2014 = torch.prim.ListConstruct %int4_2216, %int1_2217, %int14336_2218 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2015 = torch.aten.view %2013, %2014 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %2016 = torch.aten.mul.Tensor %2009, %2015 : !torch.vtensor<[4,1,14336],f16>, !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_2219 = torch.constant.int -2
    %int-1_2220 = torch.constant.int -1
    %2017 = torch.aten.transpose.int %88, %int-2_2219, %int-1_2220 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int4_2221 = torch.constant.int 4
    %int14336_2222 = torch.constant.int 14336
    %2018 = torch.prim.ListConstruct %int4_2221, %int14336_2222 : (!torch.int, !torch.int) -> !torch.list<int>
    %2019 = torch.aten.view %2016, %2018 : !torch.vtensor<[4,1,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,14336],f16>
    %2020 = torch.aten.mm %2019, %2017 : !torch.vtensor<[4,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_2223 = torch.constant.int 4
    %int1_2224 = torch.constant.int 1
    %int4096_2225 = torch.constant.int 4096
    %2021 = torch.prim.ListConstruct %int4_2223, %int1_2224, %int4096_2225 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2022 = torch.aten.view %2020, %2021 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_2226 = torch.constant.int 1
    %2023 = torch.aten.add.Tensor %1992, %2022, %int1_2226 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_2227 = torch.constant.int 6
    %2024 = torch.prims.convert_element_type %2023, %int6_2227 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_2228 = torch.constant.int 2
    %2025 = torch.aten.pow.Tensor_Scalar %2024, %int2_2228 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_2229 = torch.constant.int -1
    %2026 = torch.prim.ListConstruct %int-1_2229 : (!torch.int) -> !torch.list<int>
    %true_2230 = torch.constant.bool true
    %none_2231 = torch.constant.none
    %2027 = torch.aten.mean.dim %2025, %2026, %true_2230, %none_2231 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_2232 = torch.constant.float 9.9999997473787516E-6
    %int1_2233 = torch.constant.int 1
    %2028 = torch.aten.add.Scalar %2027, %float9.999990e-06_2232, %int1_2233 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %2029 = torch.aten.rsqrt %2028 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %2030 = torch.aten.mul.Tensor %2024, %2029 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_2234 = torch.constant.int 5
    %2031 = torch.prims.convert_element_type %2030, %int5_2234 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %2032 = torch.aten.mul.Tensor %89, %2031 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_2235 = torch.constant.int 5
    %2033 = torch.prims.convert_element_type %2032, %int5_2235 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_2236 = torch.constant.int -2
    %int-1_2237 = torch.constant.int -1
    %2034 = torch.aten.transpose.int %90, %int-2_2236, %int-1_2237 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_2238 = torch.constant.int 4
    %int4096_2239 = torch.constant.int 4096
    %2035 = torch.prim.ListConstruct %int4_2238, %int4096_2239 : (!torch.int, !torch.int) -> !torch.list<int>
    %2036 = torch.aten.view %2033, %2035 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %2037 = torch.aten.mm %2036, %2034 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_2240 = torch.constant.int 4
    %int1_2241 = torch.constant.int 1
    %int4096_2242 = torch.constant.int 4096
    %2038 = torch.prim.ListConstruct %int4_2240, %int1_2241, %int4096_2242 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2039 = torch.aten.view %2037, %2038 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_2243 = torch.constant.int -2
    %int-1_2244 = torch.constant.int -1
    %2040 = torch.aten.transpose.int %91, %int-2_2243, %int-1_2244 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_2245 = torch.constant.int 4
    %int4096_2246 = torch.constant.int 4096
    %2041 = torch.prim.ListConstruct %int4_2245, %int4096_2246 : (!torch.int, !torch.int) -> !torch.list<int>
    %2042 = torch.aten.view %2033, %2041 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %2043 = torch.aten.mm %2042, %2040 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_2247 = torch.constant.int 4
    %int1_2248 = torch.constant.int 1
    %int1024_2249 = torch.constant.int 1024
    %2044 = torch.prim.ListConstruct %int4_2247, %int1_2248, %int1024_2249 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2045 = torch.aten.view %2043, %2044 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int-2_2250 = torch.constant.int -2
    %int-1_2251 = torch.constant.int -1
    %2046 = torch.aten.transpose.int %92, %int-2_2250, %int-1_2251 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_2252 = torch.constant.int 4
    %int4096_2253 = torch.constant.int 4096
    %2047 = torch.prim.ListConstruct %int4_2252, %int4096_2253 : (!torch.int, !torch.int) -> !torch.list<int>
    %2048 = torch.aten.view %2033, %2047 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %2049 = torch.aten.mm %2048, %2046 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_2254 = torch.constant.int 4
    %int1_2255 = torch.constant.int 1
    %int1024_2256 = torch.constant.int 1024
    %2050 = torch.prim.ListConstruct %int4_2254, %int1_2255, %int1024_2256 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2051 = torch.aten.view %2049, %2050 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int4_2257 = torch.constant.int 4
    %int1_2258 = torch.constant.int 1
    %int32_2259 = torch.constant.int 32
    %int128_2260 = torch.constant.int 128
    %2052 = torch.prim.ListConstruct %int4_2257, %int1_2258, %int32_2259, %int128_2260 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2053 = torch.aten.view %2039, %2052 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,128],f16>
    %int4_2261 = torch.constant.int 4
    %int1_2262 = torch.constant.int 1
    %int8_2263 = torch.constant.int 8
    %int128_2264 = torch.constant.int 128
    %2054 = torch.prim.ListConstruct %int4_2261, %int1_2262, %int8_2263, %int128_2264 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2055 = torch.aten.view %2045, %2054 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int4_2265 = torch.constant.int 4
    %int1_2266 = torch.constant.int 1
    %int8_2267 = torch.constant.int 8
    %int128_2268 = torch.constant.int 128
    %2056 = torch.prim.ListConstruct %int4_2265, %int1_2266, %int8_2267, %int128_2268 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2057 = torch.aten.view %2051, %2056 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int6_2269 = torch.constant.int 6
    %2058 = torch.prims.convert_element_type %2053, %int6_2269 : !torch.vtensor<[4,1,32,128],f16>, !torch.int -> !torch.vtensor<[4,1,32,128],f32>
    %2059 = torch_c.to_builtin_tensor %2058 : !torch.vtensor<[4,1,32,128],f32> -> tensor<4x1x32x128xf32>
    %2060 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %2061 = util.call @sharktank_rotary_embedding_4_1_32_128_f32(%2059, %2060) : (tensor<4x1x32x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x32x128xf32>
    %2062 = torch_c.from_builtin_tensor %2061 : tensor<4x1x32x128xf32> -> !torch.vtensor<[4,1,32,128],f32>
    %int5_2270 = torch.constant.int 5
    %2063 = torch.prims.convert_element_type %2062, %int5_2270 : !torch.vtensor<[4,1,32,128],f32>, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int6_2271 = torch.constant.int 6
    %2064 = torch.prims.convert_element_type %2055, %int6_2271 : !torch.vtensor<[4,1,8,128],f16>, !torch.int -> !torch.vtensor<[4,1,8,128],f32>
    %2065 = torch_c.to_builtin_tensor %2064 : !torch.vtensor<[4,1,8,128],f32> -> tensor<4x1x8x128xf32>
    %2066 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %2067 = util.call @sharktank_rotary_embedding_4_1_8_128_f32(%2065, %2066) : (tensor<4x1x8x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x8x128xf32>
    %2068 = torch_c.from_builtin_tensor %2067 : tensor<4x1x8x128xf32> -> !torch.vtensor<[4,1,8,128],f32>
    %int5_2272 = torch.constant.int 5
    %2069 = torch.prims.convert_element_type %2068, %int5_2272 : !torch.vtensor<[4,1,8,128],f32>, !torch.int -> !torch.vtensor<[4,1,8,128],f16>
    %int32_2273 = torch.constant.int 32
    %2070 = torch.aten.floor_divide.Scalar %arg2, %int32_2273 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_2274 = torch.constant.int 1
    %2071 = torch.aten.unsqueeze %2070, %int1_2274 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2275 = torch.constant.int 1
    %false_2276 = torch.constant.bool false
    %2072 = torch.aten.gather %arg3, %int1_2275, %2071, %false_2276 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_2277 = torch.constant.int 32
    %2073 = torch.aten.remainder.Scalar %arg2, %int32_2277 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_2278 = torch.constant.int 1
    %2074 = torch.aten.unsqueeze %2073, %int1_2278 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_2279 = torch.constant.none
    %2075 = torch.aten.clone %93, %none_2279 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_2280 = torch.constant.int 0
    %2076 = torch.aten.unsqueeze %2075, %int0_2280 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_2281 = torch.constant.int 4
    %int1_2282 = torch.constant.int 1
    %2077 = torch.prim.ListConstruct %int4_2281, %int1_2282 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_2283 = torch.constant.int 1
    %int1_2284 = torch.constant.int 1
    %2078 = torch.prim.ListConstruct %int1_2283, %int1_2284 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_2285 = torch.constant.int 4
    %int0_2286 = torch.constant.int 0
    %cpu_2287 = torch.constant.device "cpu"
    %false_2288 = torch.constant.bool false
    %2079 = torch.aten.empty_strided %2077, %2078, %int4_2285, %int0_2286, %cpu_2287, %false_2288 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int8_2289 = torch.constant.int 8
    %2080 = torch.aten.fill.Scalar %2079, %int8_2289 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_2290 = torch.constant.int 4
    %int1_2291 = torch.constant.int 1
    %2081 = torch.prim.ListConstruct %int4_2290, %int1_2291 : (!torch.int, !torch.int) -> !torch.list<int>
    %2082 = torch.aten.repeat %2076, %2081 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_2292 = torch.constant.int 32
    %2083 = torch.aten.mul.Scalar %2072, %int32_2292 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2293 = torch.constant.int 1
    %2084 = torch.aten.add.Tensor %2083, %2080, %int1_2293 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_2294 = torch.constant.int 2
    %2085 = torch.aten.mul.Scalar %2084, %int2_2294 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2295 = torch.constant.int 1
    %2086 = torch.aten.add.Tensor %2085, %2082, %int1_2295 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_2296 = torch.constant.int 32
    %2087 = torch.aten.mul.Scalar %2086, %int32_2296 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2297 = torch.constant.int 1
    %2088 = torch.aten.add.Tensor %2087, %2074, %int1_2297 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_2298 = torch.constant.int 32
    %int2_2299 = torch.constant.int 2
    %int32_2300 = torch.constant.int 32
    %int8_2301 = torch.constant.int 8
    %int128_2302 = torch.constant.int 128
    %2089 = torch.prim.ListConstruct %437, %int32_2298, %int2_2299, %int32_2300, %int8_2301, %int128_2302 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2090 = torch.aten.view %1926, %2089 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2090, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_2303 = torch.constant.int 32
    %2091 = torch.aten.mul.int %437, %int32_2303 : !torch.int, !torch.int -> !torch.int
    %int2_2304 = torch.constant.int 2
    %2092 = torch.aten.mul.int %2091, %int2_2304 : !torch.int, !torch.int -> !torch.int
    %int32_2305 = torch.constant.int 32
    %2093 = torch.aten.mul.int %2092, %int32_2305 : !torch.int, !torch.int -> !torch.int
    %int8_2306 = torch.constant.int 8
    %int128_2307 = torch.constant.int 128
    %2094 = torch.prim.ListConstruct %2093, %int8_2306, %int128_2307 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2095 = torch.aten.view %2090, %2094 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %2095, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %2096 = torch.prim.ListConstruct %2088 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_2308 = torch.constant.bool false
    %2097 = torch.aten.index_put %2095, %2096, %2069, %false_2308 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %2097, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_2309 = torch.constant.int 32
    %int2_2310 = torch.constant.int 2
    %int32_2311 = torch.constant.int 32
    %int8_2312 = torch.constant.int 8
    %int128_2313 = torch.constant.int 128
    %2098 = torch.prim.ListConstruct %437, %int32_2309, %int2_2310, %int32_2311, %int8_2312, %int128_2313 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2099 = torch.aten.view %2097, %2098 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2099, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_2314 = torch.constant.int 2097152
    %2100 = torch.prim.ListConstruct %437, %int2097152_2314 : (!torch.int, !torch.int) -> !torch.list<int>
    %2101 = torch.aten.view %2099, %2100 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %2101, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_2315 = torch.constant.int 32
    %int2_2316 = torch.constant.int 2
    %int32_2317 = torch.constant.int 32
    %int8_2318 = torch.constant.int 8
    %int128_2319 = torch.constant.int 128
    %2102 = torch.prim.ListConstruct %437, %int32_2315, %int2_2316, %int32_2317, %int8_2318, %int128_2319 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2103 = torch.aten.view %2101, %2102 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2103, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int8_2320 = torch.constant.int 8
    %int128_2321 = torch.constant.int 128
    %2104 = torch.prim.ListConstruct %2093, %int8_2320, %int128_2321 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2105 = torch.aten.view %2103, %2104 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %2105, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_2322 = torch.constant.int 32
    %2106 = torch.aten.floor_divide.Scalar %arg2, %int32_2322 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_2323 = torch.constant.int 1
    %2107 = torch.aten.unsqueeze %2106, %int1_2323 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2324 = torch.constant.int 1
    %false_2325 = torch.constant.bool false
    %2108 = torch.aten.gather %arg3, %int1_2324, %2107, %false_2325 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_2326 = torch.constant.int 32
    %2109 = torch.aten.remainder.Scalar %arg2, %int32_2326 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_2327 = torch.constant.int 1
    %2110 = torch.aten.unsqueeze %2109, %int1_2327 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_2328 = torch.constant.none
    %2111 = torch.aten.clone %94, %none_2328 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_2329 = torch.constant.int 0
    %2112 = torch.aten.unsqueeze %2111, %int0_2329 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_2330 = torch.constant.int 4
    %int1_2331 = torch.constant.int 1
    %2113 = torch.prim.ListConstruct %int4_2330, %int1_2331 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_2332 = torch.constant.int 1
    %int1_2333 = torch.constant.int 1
    %2114 = torch.prim.ListConstruct %int1_2332, %int1_2333 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_2334 = torch.constant.int 4
    %int0_2335 = torch.constant.int 0
    %cpu_2336 = torch.constant.device "cpu"
    %false_2337 = torch.constant.bool false
    %2115 = torch.aten.empty_strided %2113, %2114, %int4_2334, %int0_2335, %cpu_2336, %false_2337 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int8_2338 = torch.constant.int 8
    %2116 = torch.aten.fill.Scalar %2115, %int8_2338 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_2339 = torch.constant.int 4
    %int1_2340 = torch.constant.int 1
    %2117 = torch.prim.ListConstruct %int4_2339, %int1_2340 : (!torch.int, !torch.int) -> !torch.list<int>
    %2118 = torch.aten.repeat %2112, %2117 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_2341 = torch.constant.int 32
    %2119 = torch.aten.mul.Scalar %2108, %int32_2341 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2342 = torch.constant.int 1
    %2120 = torch.aten.add.Tensor %2119, %2116, %int1_2342 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_2343 = torch.constant.int 2
    %2121 = torch.aten.mul.Scalar %2120, %int2_2343 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2344 = torch.constant.int 1
    %2122 = torch.aten.add.Tensor %2121, %2118, %int1_2344 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_2345 = torch.constant.int 32
    %2123 = torch.aten.mul.Scalar %2122, %int32_2345 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2346 = torch.constant.int 1
    %2124 = torch.aten.add.Tensor %2123, %2110, %int1_2346 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %2125 = torch.prim.ListConstruct %2124 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_2347 = torch.constant.bool false
    %2126 = torch.aten.index_put %2105, %2125, %2057, %false_2347 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %2126, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_2348 = torch.constant.int 32
    %int2_2349 = torch.constant.int 2
    %int32_2350 = torch.constant.int 32
    %int8_2351 = torch.constant.int 8
    %int128_2352 = torch.constant.int 128
    %2127 = torch.prim.ListConstruct %437, %int32_2348, %int2_2349, %int32_2350, %int8_2351, %int128_2352 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2128 = torch.aten.view %2126, %2127 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2128, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_2353 = torch.constant.int 2097152
    %2129 = torch.prim.ListConstruct %437, %int2097152_2353 : (!torch.int, !torch.int) -> !torch.list<int>
    %2130 = torch.aten.view %2128, %2129 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %2130, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int4_2354 = torch.constant.int 4
    %2131 = torch.prim.ListConstruct %int4_2354, %358 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_2355 = torch.constant.int 1
    %2132 = torch.prim.ListConstruct %358, %int1_2355 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_2356 = torch.constant.int 4
    %int0_2357 = torch.constant.int 0
    %cpu_2358 = torch.constant.device "cpu"
    %false_2359 = torch.constant.bool false
    %2133 = torch.aten.empty_strided %2131, %2132, %int4_2356, %int0_2357, %cpu_2358, %false_2359 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2133, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int8_2360 = torch.constant.int 8
    %2134 = torch.aten.fill.Scalar %2133, %int8_2360 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2134, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int32_2361 = torch.constant.int 32
    %2135 = torch.aten.mul.Scalar %arg3, %int32_2361 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2135, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_2362 = torch.constant.int 1
    %2136 = torch.aten.add.Tensor %2135, %2134, %int1_2362 : !torch.vtensor<[4,?],si64>, !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2136, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_2363 = torch.constant.int 4
    %2137 = torch.aten.mul.int %int4_2363, %358 : !torch.int, !torch.int -> !torch.int
    %2138 = torch.prim.ListConstruct %2137 : (!torch.int) -> !torch.list<int>
    %2139 = torch.aten.view %2136, %2138 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2139, [%356], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_2364 = torch.constant.int 32
    %int2_2365 = torch.constant.int 2
    %int32_2366 = torch.constant.int 32
    %int8_2367 = torch.constant.int 8
    %int128_2368 = torch.constant.int 128
    %2140 = torch.prim.ListConstruct %437, %int32_2364, %int2_2365, %int32_2366, %int8_2367, %int128_2368 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2141 = torch.aten.view %2130, %2140 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2141, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_2369 = torch.constant.int 32
    %2142 = torch.aten.mul.int %437, %int32_2369 : !torch.int, !torch.int -> !torch.int
    %int2_2370 = torch.constant.int 2
    %int32_2371 = torch.constant.int 32
    %int8_2372 = torch.constant.int 8
    %int128_2373 = torch.constant.int 128
    %2143 = torch.prim.ListConstruct %2142, %int2_2370, %int32_2371, %int8_2372, %int128_2373 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2144 = torch.aten.view %2141, %2143 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %2144, [%357], affine_map<()[s0] -> (s0 * 32, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int0_2374 = torch.constant.int 0
    %2145 = torch.aten.index_select %2144, %int0_2374, %2139 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.int, !torch.vtensor<[?],si64> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %2145, [%356], affine_map<()[s0] -> (s0 * 4, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int4_2375 = torch.constant.int 4
    %int2_2376 = torch.constant.int 2
    %int32_2377 = torch.constant.int 32
    %int8_2378 = torch.constant.int 8
    %int128_2379 = torch.constant.int 128
    %2146 = torch.prim.ListConstruct %int4_2375, %358, %int2_2376, %int32_2377, %int8_2378, %int128_2379 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2147 = torch.aten.view %2145, %2146 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %2147, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int0_2380 = torch.constant.int 0
    %int0_2381 = torch.constant.int 0
    %int9223372036854775807_2382 = torch.constant.int 9223372036854775807
    %int1_2383 = torch.constant.int 1
    %2148 = torch.aten.slice.Tensor %2147, %int0_2380, %int0_2381, %int9223372036854775807_2382, %int1_2383 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %2148, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_2384 = torch.constant.int 1
    %int0_2385 = torch.constant.int 0
    %int9223372036854775807_2386 = torch.constant.int 9223372036854775807
    %int1_2387 = torch.constant.int 1
    %2149 = torch.aten.slice.Tensor %2148, %int1_2384, %int0_2385, %int9223372036854775807_2386, %int1_2387 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %2149, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_2388 = torch.constant.int 2
    %int0_2389 = torch.constant.int 0
    %2150 = torch.aten.select.int %2149, %int2_2388, %int0_2389 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2150, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int32_2390 = torch.constant.int 32
    %2151 = torch.aten.mul.int %358, %int32_2390 : !torch.int, !torch.int -> !torch.int
    %int2_2391 = torch.constant.int 2
    %int0_2392 = torch.constant.int 0
    %int1_2393 = torch.constant.int 1
    %2152 = torch.aten.slice.Tensor %2150, %int2_2391, %int0_2392, %2151, %int1_2393 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2152, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_2394 = torch.constant.int 0
    %2153 = torch.aten.clone %2152, %int0_2394 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2153, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_2395 = torch.constant.int 1
    %2154 = torch.aten.size.int %2149, %int1_2395 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_2396 = torch.constant.int 32
    %2155 = torch.aten.mul.int %2154, %int32_2396 : !torch.int, !torch.int -> !torch.int
    %int4_2397 = torch.constant.int 4
    %int8_2398 = torch.constant.int 8
    %int128_2399 = torch.constant.int 128
    %2156 = torch.prim.ListConstruct %int4_2397, %2155, %int8_2398, %int128_2399 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2157 = torch.aten._unsafe_view %2153, %2156 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %2157, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_2400 = torch.constant.int 0
    %int0_2401 = torch.constant.int 0
    %int9223372036854775807_2402 = torch.constant.int 9223372036854775807
    %int1_2403 = torch.constant.int 1
    %2158 = torch.aten.slice.Tensor %2157, %int0_2400, %int0_2401, %int9223372036854775807_2402, %int1_2403 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %2158, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_2404 = torch.constant.int 0
    %int0_2405 = torch.constant.int 0
    %int9223372036854775807_2406 = torch.constant.int 9223372036854775807
    %int1_2407 = torch.constant.int 1
    %2159 = torch.aten.slice.Tensor %2147, %int0_2404, %int0_2405, %int9223372036854775807_2406, %int1_2407 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %2159, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_2408 = torch.constant.int 1
    %int0_2409 = torch.constant.int 0
    %int9223372036854775807_2410 = torch.constant.int 9223372036854775807
    %int1_2411 = torch.constant.int 1
    %2160 = torch.aten.slice.Tensor %2159, %int1_2408, %int0_2409, %int9223372036854775807_2410, %int1_2411 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %2160, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_2412 = torch.constant.int 2
    %int1_2413 = torch.constant.int 1
    %2161 = torch.aten.select.int %2160, %int2_2412, %int1_2413 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2161, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int2_2414 = torch.constant.int 2
    %int0_2415 = torch.constant.int 0
    %int1_2416 = torch.constant.int 1
    %2162 = torch.aten.slice.Tensor %2161, %int2_2414, %int0_2415, %2151, %int1_2416 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2162, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_2417 = torch.constant.int 0
    %2163 = torch.aten.clone %2162, %int0_2417 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2163, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_2418 = torch.constant.int 1
    %2164 = torch.aten.size.int %2160, %int1_2418 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_2419 = torch.constant.int 32
    %2165 = torch.aten.mul.int %2164, %int32_2419 : !torch.int, !torch.int -> !torch.int
    %int4_2420 = torch.constant.int 4
    %int8_2421 = torch.constant.int 8
    %int128_2422 = torch.constant.int 128
    %2166 = torch.prim.ListConstruct %int4_2420, %2165, %int8_2421, %int128_2422 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2167 = torch.aten._unsafe_view %2163, %2166 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %2167, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_2423 = torch.constant.int 0
    %int0_2424 = torch.constant.int 0
    %int9223372036854775807_2425 = torch.constant.int 9223372036854775807
    %int1_2426 = torch.constant.int 1
    %2168 = torch.aten.slice.Tensor %2167, %int0_2423, %int0_2424, %int9223372036854775807_2425, %int1_2426 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %2168, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int-2_2427 = torch.constant.int -2
    %2169 = torch.aten.unsqueeze %2158, %int-2_2427 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %2169, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_2428 = torch.constant.int 1
    %2170 = torch.aten.size.int %2157, %int1_2428 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_2429 = torch.constant.int 4
    %int8_2430 = torch.constant.int 8
    %int4_2431 = torch.constant.int 4
    %int128_2432 = torch.constant.int 128
    %2171 = torch.prim.ListConstruct %int4_2429, %2170, %int8_2430, %int4_2431, %int128_2432 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_2433 = torch.constant.bool false
    %2172 = torch.aten.expand %2169, %2171, %false_2433 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2172, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_2434 = torch.constant.int 0
    %2173 = torch.aten.clone %2172, %int0_2434 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2173, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_2435 = torch.constant.int 4
    %int32_2436 = torch.constant.int 32
    %int128_2437 = torch.constant.int 128
    %2174 = torch.prim.ListConstruct %int4_2435, %2170, %int32_2436, %int128_2437 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2175 = torch.aten._unsafe_view %2173, %2174 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2175, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_2438 = torch.constant.int -2
    %2176 = torch.aten.unsqueeze %2168, %int-2_2438 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %2176, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_2439 = torch.constant.int 1
    %2177 = torch.aten.size.int %2167, %int1_2439 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_2440 = torch.constant.int 4
    %int8_2441 = torch.constant.int 8
    %int4_2442 = torch.constant.int 4
    %int128_2443 = torch.constant.int 128
    %2178 = torch.prim.ListConstruct %int4_2440, %2177, %int8_2441, %int4_2442, %int128_2443 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_2444 = torch.constant.bool false
    %2179 = torch.aten.expand %2176, %2178, %false_2444 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2179, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_2445 = torch.constant.int 0
    %2180 = torch.aten.clone %2179, %int0_2445 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2180, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_2446 = torch.constant.int 4
    %int32_2447 = torch.constant.int 32
    %int128_2448 = torch.constant.int 128
    %2181 = torch.prim.ListConstruct %int4_2446, %2177, %int32_2447, %int128_2448 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2182 = torch.aten._unsafe_view %2180, %2181 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2182, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_2449 = torch.constant.int 1
    %int2_2450 = torch.constant.int 2
    %2183 = torch.aten.transpose.int %2063, %int1_2449, %int2_2450 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,128],f16>
    %int1_2451 = torch.constant.int 1
    %int2_2452 = torch.constant.int 2
    %2184 = torch.aten.transpose.int %2175, %int1_2451, %int2_2452 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %2184, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_2453 = torch.constant.int 1
    %int2_2454 = torch.constant.int 2
    %2185 = torch.aten.transpose.int %2182, %int1_2453, %int2_2454 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %2185, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_2455 = torch.constant.float 0.000000e+00
    %false_2456 = torch.constant.bool false
    %none_2457 = torch.constant.none
    %2186:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2183, %2184, %2185, %float0.000000e00_2455, %false_2456, %368, %none_2457) : (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.vtensor<[4,1,1,?],f16>, !torch.none) -> (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,1],f32>) 
    %int1_2458 = torch.constant.int 1
    %int2_2459 = torch.constant.int 2
    %2187 = torch.aten.transpose.int %2186#0, %int1_2458, %int2_2459 : !torch.vtensor<[4,32,1,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int4_2460 = torch.constant.int 4
    %int1_2461 = torch.constant.int 1
    %int4096_2462 = torch.constant.int 4096
    %2188 = torch.prim.ListConstruct %int4_2460, %int1_2461, %int4096_2462 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2189 = torch.aten.view %2187, %2188 : !torch.vtensor<[4,1,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_2463 = torch.constant.int -2
    %int-1_2464 = torch.constant.int -1
    %2190 = torch.aten.transpose.int %95, %int-2_2463, %int-1_2464 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_2465 = torch.constant.int 4
    %int4096_2466 = torch.constant.int 4096
    %2191 = torch.prim.ListConstruct %int4_2465, %int4096_2466 : (!torch.int, !torch.int) -> !torch.list<int>
    %2192 = torch.aten.view %2189, %2191 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %2193 = torch.aten.mm %2192, %2190 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_2467 = torch.constant.int 4
    %int1_2468 = torch.constant.int 1
    %int4096_2469 = torch.constant.int 4096
    %2194 = torch.prim.ListConstruct %int4_2467, %int1_2468, %int4096_2469 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2195 = torch.aten.view %2193, %2194 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_2470 = torch.constant.int 1
    %2196 = torch.aten.add.Tensor %2023, %2195, %int1_2470 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_2471 = torch.constant.int 6
    %2197 = torch.prims.convert_element_type %2196, %int6_2471 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_2472 = torch.constant.int 2
    %2198 = torch.aten.pow.Tensor_Scalar %2197, %int2_2472 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_2473 = torch.constant.int -1
    %2199 = torch.prim.ListConstruct %int-1_2473 : (!torch.int) -> !torch.list<int>
    %true_2474 = torch.constant.bool true
    %none_2475 = torch.constant.none
    %2200 = torch.aten.mean.dim %2198, %2199, %true_2474, %none_2475 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_2476 = torch.constant.float 9.9999997473787516E-6
    %int1_2477 = torch.constant.int 1
    %2201 = torch.aten.add.Scalar %2200, %float9.999990e-06_2476, %int1_2477 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %2202 = torch.aten.rsqrt %2201 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %2203 = torch.aten.mul.Tensor %2197, %2202 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_2478 = torch.constant.int 5
    %2204 = torch.prims.convert_element_type %2203, %int5_2478 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %2205 = torch.aten.mul.Tensor %96, %2204 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_2479 = torch.constant.int 5
    %2206 = torch.prims.convert_element_type %2205, %int5_2479 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_2480 = torch.constant.int -2
    %int-1_2481 = torch.constant.int -1
    %2207 = torch.aten.transpose.int %97, %int-2_2480, %int-1_2481 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_2482 = torch.constant.int 4
    %int4096_2483 = torch.constant.int 4096
    %2208 = torch.prim.ListConstruct %int4_2482, %int4096_2483 : (!torch.int, !torch.int) -> !torch.list<int>
    %2209 = torch.aten.view %2206, %2208 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %2210 = torch.aten.mm %2209, %2207 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_2484 = torch.constant.int 4
    %int1_2485 = torch.constant.int 1
    %int14336_2486 = torch.constant.int 14336
    %2211 = torch.prim.ListConstruct %int4_2484, %int1_2485, %int14336_2486 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2212 = torch.aten.view %2210, %2211 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %2213 = torch.aten.silu %2212 : !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_2487 = torch.constant.int -2
    %int-1_2488 = torch.constant.int -1
    %2214 = torch.aten.transpose.int %98, %int-2_2487, %int-1_2488 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_2489 = torch.constant.int 4
    %int4096_2490 = torch.constant.int 4096
    %2215 = torch.prim.ListConstruct %int4_2489, %int4096_2490 : (!torch.int, !torch.int) -> !torch.list<int>
    %2216 = torch.aten.view %2206, %2215 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %2217 = torch.aten.mm %2216, %2214 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_2491 = torch.constant.int 4
    %int1_2492 = torch.constant.int 1
    %int14336_2493 = torch.constant.int 14336
    %2218 = torch.prim.ListConstruct %int4_2491, %int1_2492, %int14336_2493 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2219 = torch.aten.view %2217, %2218 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %2220 = torch.aten.mul.Tensor %2213, %2219 : !torch.vtensor<[4,1,14336],f16>, !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_2494 = torch.constant.int -2
    %int-1_2495 = torch.constant.int -1
    %2221 = torch.aten.transpose.int %99, %int-2_2494, %int-1_2495 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int4_2496 = torch.constant.int 4
    %int14336_2497 = torch.constant.int 14336
    %2222 = torch.prim.ListConstruct %int4_2496, %int14336_2497 : (!torch.int, !torch.int) -> !torch.list<int>
    %2223 = torch.aten.view %2220, %2222 : !torch.vtensor<[4,1,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,14336],f16>
    %2224 = torch.aten.mm %2223, %2221 : !torch.vtensor<[4,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_2498 = torch.constant.int 4
    %int1_2499 = torch.constant.int 1
    %int4096_2500 = torch.constant.int 4096
    %2225 = torch.prim.ListConstruct %int4_2498, %int1_2499, %int4096_2500 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2226 = torch.aten.view %2224, %2225 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_2501 = torch.constant.int 1
    %2227 = torch.aten.add.Tensor %2196, %2226, %int1_2501 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_2502 = torch.constant.int 6
    %2228 = torch.prims.convert_element_type %2227, %int6_2502 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_2503 = torch.constant.int 2
    %2229 = torch.aten.pow.Tensor_Scalar %2228, %int2_2503 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_2504 = torch.constant.int -1
    %2230 = torch.prim.ListConstruct %int-1_2504 : (!torch.int) -> !torch.list<int>
    %true_2505 = torch.constant.bool true
    %none_2506 = torch.constant.none
    %2231 = torch.aten.mean.dim %2229, %2230, %true_2505, %none_2506 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_2507 = torch.constant.float 9.9999997473787516E-6
    %int1_2508 = torch.constant.int 1
    %2232 = torch.aten.add.Scalar %2231, %float9.999990e-06_2507, %int1_2508 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %2233 = torch.aten.rsqrt %2232 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %2234 = torch.aten.mul.Tensor %2228, %2233 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_2509 = torch.constant.int 5
    %2235 = torch.prims.convert_element_type %2234, %int5_2509 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %2236 = torch.aten.mul.Tensor %100, %2235 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_2510 = torch.constant.int 5
    %2237 = torch.prims.convert_element_type %2236, %int5_2510 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_2511 = torch.constant.int -2
    %int-1_2512 = torch.constant.int -1
    %2238 = torch.aten.transpose.int %101, %int-2_2511, %int-1_2512 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_2513 = torch.constant.int 4
    %int4096_2514 = torch.constant.int 4096
    %2239 = torch.prim.ListConstruct %int4_2513, %int4096_2514 : (!torch.int, !torch.int) -> !torch.list<int>
    %2240 = torch.aten.view %2237, %2239 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %2241 = torch.aten.mm %2240, %2238 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_2515 = torch.constant.int 4
    %int1_2516 = torch.constant.int 1
    %int4096_2517 = torch.constant.int 4096
    %2242 = torch.prim.ListConstruct %int4_2515, %int1_2516, %int4096_2517 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2243 = torch.aten.view %2241, %2242 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_2518 = torch.constant.int -2
    %int-1_2519 = torch.constant.int -1
    %2244 = torch.aten.transpose.int %102, %int-2_2518, %int-1_2519 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_2520 = torch.constant.int 4
    %int4096_2521 = torch.constant.int 4096
    %2245 = torch.prim.ListConstruct %int4_2520, %int4096_2521 : (!torch.int, !torch.int) -> !torch.list<int>
    %2246 = torch.aten.view %2237, %2245 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %2247 = torch.aten.mm %2246, %2244 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_2522 = torch.constant.int 4
    %int1_2523 = torch.constant.int 1
    %int1024_2524 = torch.constant.int 1024
    %2248 = torch.prim.ListConstruct %int4_2522, %int1_2523, %int1024_2524 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2249 = torch.aten.view %2247, %2248 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int-2_2525 = torch.constant.int -2
    %int-1_2526 = torch.constant.int -1
    %2250 = torch.aten.transpose.int %103, %int-2_2525, %int-1_2526 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_2527 = torch.constant.int 4
    %int4096_2528 = torch.constant.int 4096
    %2251 = torch.prim.ListConstruct %int4_2527, %int4096_2528 : (!torch.int, !torch.int) -> !torch.list<int>
    %2252 = torch.aten.view %2237, %2251 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %2253 = torch.aten.mm %2252, %2250 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_2529 = torch.constant.int 4
    %int1_2530 = torch.constant.int 1
    %int1024_2531 = torch.constant.int 1024
    %2254 = torch.prim.ListConstruct %int4_2529, %int1_2530, %int1024_2531 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2255 = torch.aten.view %2253, %2254 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int4_2532 = torch.constant.int 4
    %int1_2533 = torch.constant.int 1
    %int32_2534 = torch.constant.int 32
    %int128_2535 = torch.constant.int 128
    %2256 = torch.prim.ListConstruct %int4_2532, %int1_2533, %int32_2534, %int128_2535 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2257 = torch.aten.view %2243, %2256 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,128],f16>
    %int4_2536 = torch.constant.int 4
    %int1_2537 = torch.constant.int 1
    %int8_2538 = torch.constant.int 8
    %int128_2539 = torch.constant.int 128
    %2258 = torch.prim.ListConstruct %int4_2536, %int1_2537, %int8_2538, %int128_2539 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2259 = torch.aten.view %2249, %2258 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int4_2540 = torch.constant.int 4
    %int1_2541 = torch.constant.int 1
    %int8_2542 = torch.constant.int 8
    %int128_2543 = torch.constant.int 128
    %2260 = torch.prim.ListConstruct %int4_2540, %int1_2541, %int8_2542, %int128_2543 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2261 = torch.aten.view %2255, %2260 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int6_2544 = torch.constant.int 6
    %2262 = torch.prims.convert_element_type %2257, %int6_2544 : !torch.vtensor<[4,1,32,128],f16>, !torch.int -> !torch.vtensor<[4,1,32,128],f32>
    %2263 = torch_c.to_builtin_tensor %2262 : !torch.vtensor<[4,1,32,128],f32> -> tensor<4x1x32x128xf32>
    %2264 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %2265 = util.call @sharktank_rotary_embedding_4_1_32_128_f32(%2263, %2264) : (tensor<4x1x32x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x32x128xf32>
    %2266 = torch_c.from_builtin_tensor %2265 : tensor<4x1x32x128xf32> -> !torch.vtensor<[4,1,32,128],f32>
    %int5_2545 = torch.constant.int 5
    %2267 = torch.prims.convert_element_type %2266, %int5_2545 : !torch.vtensor<[4,1,32,128],f32>, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int6_2546 = torch.constant.int 6
    %2268 = torch.prims.convert_element_type %2259, %int6_2546 : !torch.vtensor<[4,1,8,128],f16>, !torch.int -> !torch.vtensor<[4,1,8,128],f32>
    %2269 = torch_c.to_builtin_tensor %2268 : !torch.vtensor<[4,1,8,128],f32> -> tensor<4x1x8x128xf32>
    %2270 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %2271 = util.call @sharktank_rotary_embedding_4_1_8_128_f32(%2269, %2270) : (tensor<4x1x8x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x8x128xf32>
    %2272 = torch_c.from_builtin_tensor %2271 : tensor<4x1x8x128xf32> -> !torch.vtensor<[4,1,8,128],f32>
    %int5_2547 = torch.constant.int 5
    %2273 = torch.prims.convert_element_type %2272, %int5_2547 : !torch.vtensor<[4,1,8,128],f32>, !torch.int -> !torch.vtensor<[4,1,8,128],f16>
    %int32_2548 = torch.constant.int 32
    %2274 = torch.aten.floor_divide.Scalar %arg2, %int32_2548 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_2549 = torch.constant.int 1
    %2275 = torch.aten.unsqueeze %2274, %int1_2549 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2550 = torch.constant.int 1
    %false_2551 = torch.constant.bool false
    %2276 = torch.aten.gather %arg3, %int1_2550, %2275, %false_2551 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_2552 = torch.constant.int 32
    %2277 = torch.aten.remainder.Scalar %arg2, %int32_2552 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_2553 = torch.constant.int 1
    %2278 = torch.aten.unsqueeze %2277, %int1_2553 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_2554 = torch.constant.none
    %2279 = torch.aten.clone %104, %none_2554 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_2555 = torch.constant.int 0
    %2280 = torch.aten.unsqueeze %2279, %int0_2555 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_2556 = torch.constant.int 4
    %int1_2557 = torch.constant.int 1
    %2281 = torch.prim.ListConstruct %int4_2556, %int1_2557 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_2558 = torch.constant.int 1
    %int1_2559 = torch.constant.int 1
    %2282 = torch.prim.ListConstruct %int1_2558, %int1_2559 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_2560 = torch.constant.int 4
    %int0_2561 = torch.constant.int 0
    %cpu_2562 = torch.constant.device "cpu"
    %false_2563 = torch.constant.bool false
    %2283 = torch.aten.empty_strided %2281, %2282, %int4_2560, %int0_2561, %cpu_2562, %false_2563 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int9 = torch.constant.int 9
    %2284 = torch.aten.fill.Scalar %2283, %int9 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_2564 = torch.constant.int 4
    %int1_2565 = torch.constant.int 1
    %2285 = torch.prim.ListConstruct %int4_2564, %int1_2565 : (!torch.int, !torch.int) -> !torch.list<int>
    %2286 = torch.aten.repeat %2280, %2285 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_2566 = torch.constant.int 32
    %2287 = torch.aten.mul.Scalar %2276, %int32_2566 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2567 = torch.constant.int 1
    %2288 = torch.aten.add.Tensor %2287, %2284, %int1_2567 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_2568 = torch.constant.int 2
    %2289 = torch.aten.mul.Scalar %2288, %int2_2568 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2569 = torch.constant.int 1
    %2290 = torch.aten.add.Tensor %2289, %2286, %int1_2569 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_2570 = torch.constant.int 32
    %2291 = torch.aten.mul.Scalar %2290, %int32_2570 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2571 = torch.constant.int 1
    %2292 = torch.aten.add.Tensor %2291, %2278, %int1_2571 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_2572 = torch.constant.int 32
    %int2_2573 = torch.constant.int 2
    %int32_2574 = torch.constant.int 32
    %int8_2575 = torch.constant.int 8
    %int128_2576 = torch.constant.int 128
    %2293 = torch.prim.ListConstruct %437, %int32_2572, %int2_2573, %int32_2574, %int8_2575, %int128_2576 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2294 = torch.aten.view %2130, %2293 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2294, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_2577 = torch.constant.int 32
    %2295 = torch.aten.mul.int %437, %int32_2577 : !torch.int, !torch.int -> !torch.int
    %int2_2578 = torch.constant.int 2
    %2296 = torch.aten.mul.int %2295, %int2_2578 : !torch.int, !torch.int -> !torch.int
    %int32_2579 = torch.constant.int 32
    %2297 = torch.aten.mul.int %2296, %int32_2579 : !torch.int, !torch.int -> !torch.int
    %int8_2580 = torch.constant.int 8
    %int128_2581 = torch.constant.int 128
    %2298 = torch.prim.ListConstruct %2297, %int8_2580, %int128_2581 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2299 = torch.aten.view %2294, %2298 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %2299, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %2300 = torch.prim.ListConstruct %2292 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_2582 = torch.constant.bool false
    %2301 = torch.aten.index_put %2299, %2300, %2273, %false_2582 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %2301, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_2583 = torch.constant.int 32
    %int2_2584 = torch.constant.int 2
    %int32_2585 = torch.constant.int 32
    %int8_2586 = torch.constant.int 8
    %int128_2587 = torch.constant.int 128
    %2302 = torch.prim.ListConstruct %437, %int32_2583, %int2_2584, %int32_2585, %int8_2586, %int128_2587 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2303 = torch.aten.view %2301, %2302 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2303, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_2588 = torch.constant.int 2097152
    %2304 = torch.prim.ListConstruct %437, %int2097152_2588 : (!torch.int, !torch.int) -> !torch.list<int>
    %2305 = torch.aten.view %2303, %2304 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %2305, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_2589 = torch.constant.int 32
    %int2_2590 = torch.constant.int 2
    %int32_2591 = torch.constant.int 32
    %int8_2592 = torch.constant.int 8
    %int128_2593 = torch.constant.int 128
    %2306 = torch.prim.ListConstruct %437, %int32_2589, %int2_2590, %int32_2591, %int8_2592, %int128_2593 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2307 = torch.aten.view %2305, %2306 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2307, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int8_2594 = torch.constant.int 8
    %int128_2595 = torch.constant.int 128
    %2308 = torch.prim.ListConstruct %2297, %int8_2594, %int128_2595 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2309 = torch.aten.view %2307, %2308 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %2309, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_2596 = torch.constant.int 32
    %2310 = torch.aten.floor_divide.Scalar %arg2, %int32_2596 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_2597 = torch.constant.int 1
    %2311 = torch.aten.unsqueeze %2310, %int1_2597 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2598 = torch.constant.int 1
    %false_2599 = torch.constant.bool false
    %2312 = torch.aten.gather %arg3, %int1_2598, %2311, %false_2599 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_2600 = torch.constant.int 32
    %2313 = torch.aten.remainder.Scalar %arg2, %int32_2600 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_2601 = torch.constant.int 1
    %2314 = torch.aten.unsqueeze %2313, %int1_2601 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_2602 = torch.constant.none
    %2315 = torch.aten.clone %105, %none_2602 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_2603 = torch.constant.int 0
    %2316 = torch.aten.unsqueeze %2315, %int0_2603 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_2604 = torch.constant.int 4
    %int1_2605 = torch.constant.int 1
    %2317 = torch.prim.ListConstruct %int4_2604, %int1_2605 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_2606 = torch.constant.int 1
    %int1_2607 = torch.constant.int 1
    %2318 = torch.prim.ListConstruct %int1_2606, %int1_2607 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_2608 = torch.constant.int 4
    %int0_2609 = torch.constant.int 0
    %cpu_2610 = torch.constant.device "cpu"
    %false_2611 = torch.constant.bool false
    %2319 = torch.aten.empty_strided %2317, %2318, %int4_2608, %int0_2609, %cpu_2610, %false_2611 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int9_2612 = torch.constant.int 9
    %2320 = torch.aten.fill.Scalar %2319, %int9_2612 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_2613 = torch.constant.int 4
    %int1_2614 = torch.constant.int 1
    %2321 = torch.prim.ListConstruct %int4_2613, %int1_2614 : (!torch.int, !torch.int) -> !torch.list<int>
    %2322 = torch.aten.repeat %2316, %2321 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_2615 = torch.constant.int 32
    %2323 = torch.aten.mul.Scalar %2312, %int32_2615 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2616 = torch.constant.int 1
    %2324 = torch.aten.add.Tensor %2323, %2320, %int1_2616 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_2617 = torch.constant.int 2
    %2325 = torch.aten.mul.Scalar %2324, %int2_2617 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2618 = torch.constant.int 1
    %2326 = torch.aten.add.Tensor %2325, %2322, %int1_2618 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_2619 = torch.constant.int 32
    %2327 = torch.aten.mul.Scalar %2326, %int32_2619 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2620 = torch.constant.int 1
    %2328 = torch.aten.add.Tensor %2327, %2314, %int1_2620 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %2329 = torch.prim.ListConstruct %2328 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_2621 = torch.constant.bool false
    %2330 = torch.aten.index_put %2309, %2329, %2261, %false_2621 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %2330, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_2622 = torch.constant.int 32
    %int2_2623 = torch.constant.int 2
    %int32_2624 = torch.constant.int 32
    %int8_2625 = torch.constant.int 8
    %int128_2626 = torch.constant.int 128
    %2331 = torch.prim.ListConstruct %437, %int32_2622, %int2_2623, %int32_2624, %int8_2625, %int128_2626 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2332 = torch.aten.view %2330, %2331 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2332, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_2627 = torch.constant.int 2097152
    %2333 = torch.prim.ListConstruct %437, %int2097152_2627 : (!torch.int, !torch.int) -> !torch.list<int>
    %2334 = torch.aten.view %2332, %2333 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %2334, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int4_2628 = torch.constant.int 4
    %2335 = torch.prim.ListConstruct %int4_2628, %358 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_2629 = torch.constant.int 1
    %2336 = torch.prim.ListConstruct %358, %int1_2629 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_2630 = torch.constant.int 4
    %int0_2631 = torch.constant.int 0
    %cpu_2632 = torch.constant.device "cpu"
    %false_2633 = torch.constant.bool false
    %2337 = torch.aten.empty_strided %2335, %2336, %int4_2630, %int0_2631, %cpu_2632, %false_2633 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2337, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int9_2634 = torch.constant.int 9
    %2338 = torch.aten.fill.Scalar %2337, %int9_2634 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2338, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int32_2635 = torch.constant.int 32
    %2339 = torch.aten.mul.Scalar %arg3, %int32_2635 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2339, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_2636 = torch.constant.int 1
    %2340 = torch.aten.add.Tensor %2339, %2338, %int1_2636 : !torch.vtensor<[4,?],si64>, !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2340, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_2637 = torch.constant.int 4
    %2341 = torch.aten.mul.int %int4_2637, %358 : !torch.int, !torch.int -> !torch.int
    %2342 = torch.prim.ListConstruct %2341 : (!torch.int) -> !torch.list<int>
    %2343 = torch.aten.view %2340, %2342 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2343, [%356], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_2638 = torch.constant.int 32
    %int2_2639 = torch.constant.int 2
    %int32_2640 = torch.constant.int 32
    %int8_2641 = torch.constant.int 8
    %int128_2642 = torch.constant.int 128
    %2344 = torch.prim.ListConstruct %437, %int32_2638, %int2_2639, %int32_2640, %int8_2641, %int128_2642 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2345 = torch.aten.view %2334, %2344 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2345, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_2643 = torch.constant.int 32
    %2346 = torch.aten.mul.int %437, %int32_2643 : !torch.int, !torch.int -> !torch.int
    %int2_2644 = torch.constant.int 2
    %int32_2645 = torch.constant.int 32
    %int8_2646 = torch.constant.int 8
    %int128_2647 = torch.constant.int 128
    %2347 = torch.prim.ListConstruct %2346, %int2_2644, %int32_2645, %int8_2646, %int128_2647 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2348 = torch.aten.view %2345, %2347 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %2348, [%357], affine_map<()[s0] -> (s0 * 32, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int0_2648 = torch.constant.int 0
    %2349 = torch.aten.index_select %2348, %int0_2648, %2343 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.int, !torch.vtensor<[?],si64> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %2349, [%356], affine_map<()[s0] -> (s0 * 4, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int4_2649 = torch.constant.int 4
    %int2_2650 = torch.constant.int 2
    %int32_2651 = torch.constant.int 32
    %int8_2652 = torch.constant.int 8
    %int128_2653 = torch.constant.int 128
    %2350 = torch.prim.ListConstruct %int4_2649, %358, %int2_2650, %int32_2651, %int8_2652, %int128_2653 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2351 = torch.aten.view %2349, %2350 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %2351, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int0_2654 = torch.constant.int 0
    %int0_2655 = torch.constant.int 0
    %int9223372036854775807_2656 = torch.constant.int 9223372036854775807
    %int1_2657 = torch.constant.int 1
    %2352 = torch.aten.slice.Tensor %2351, %int0_2654, %int0_2655, %int9223372036854775807_2656, %int1_2657 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %2352, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_2658 = torch.constant.int 1
    %int0_2659 = torch.constant.int 0
    %int9223372036854775807_2660 = torch.constant.int 9223372036854775807
    %int1_2661 = torch.constant.int 1
    %2353 = torch.aten.slice.Tensor %2352, %int1_2658, %int0_2659, %int9223372036854775807_2660, %int1_2661 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %2353, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_2662 = torch.constant.int 2
    %int0_2663 = torch.constant.int 0
    %2354 = torch.aten.select.int %2353, %int2_2662, %int0_2663 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2354, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int32_2664 = torch.constant.int 32
    %2355 = torch.aten.mul.int %358, %int32_2664 : !torch.int, !torch.int -> !torch.int
    %int2_2665 = torch.constant.int 2
    %int0_2666 = torch.constant.int 0
    %int1_2667 = torch.constant.int 1
    %2356 = torch.aten.slice.Tensor %2354, %int2_2665, %int0_2666, %2355, %int1_2667 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2356, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_2668 = torch.constant.int 0
    %2357 = torch.aten.clone %2356, %int0_2668 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2357, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_2669 = torch.constant.int 1
    %2358 = torch.aten.size.int %2353, %int1_2669 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_2670 = torch.constant.int 32
    %2359 = torch.aten.mul.int %2358, %int32_2670 : !torch.int, !torch.int -> !torch.int
    %int4_2671 = torch.constant.int 4
    %int8_2672 = torch.constant.int 8
    %int128_2673 = torch.constant.int 128
    %2360 = torch.prim.ListConstruct %int4_2671, %2359, %int8_2672, %int128_2673 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2361 = torch.aten._unsafe_view %2357, %2360 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %2361, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_2674 = torch.constant.int 0
    %int0_2675 = torch.constant.int 0
    %int9223372036854775807_2676 = torch.constant.int 9223372036854775807
    %int1_2677 = torch.constant.int 1
    %2362 = torch.aten.slice.Tensor %2361, %int0_2674, %int0_2675, %int9223372036854775807_2676, %int1_2677 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %2362, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_2678 = torch.constant.int 0
    %int0_2679 = torch.constant.int 0
    %int9223372036854775807_2680 = torch.constant.int 9223372036854775807
    %int1_2681 = torch.constant.int 1
    %2363 = torch.aten.slice.Tensor %2351, %int0_2678, %int0_2679, %int9223372036854775807_2680, %int1_2681 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %2363, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_2682 = torch.constant.int 1
    %int0_2683 = torch.constant.int 0
    %int9223372036854775807_2684 = torch.constant.int 9223372036854775807
    %int1_2685 = torch.constant.int 1
    %2364 = torch.aten.slice.Tensor %2363, %int1_2682, %int0_2683, %int9223372036854775807_2684, %int1_2685 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %2364, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_2686 = torch.constant.int 2
    %int1_2687 = torch.constant.int 1
    %2365 = torch.aten.select.int %2364, %int2_2686, %int1_2687 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2365, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int2_2688 = torch.constant.int 2
    %int0_2689 = torch.constant.int 0
    %int1_2690 = torch.constant.int 1
    %2366 = torch.aten.slice.Tensor %2365, %int2_2688, %int0_2689, %2355, %int1_2690 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2366, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_2691 = torch.constant.int 0
    %2367 = torch.aten.clone %2366, %int0_2691 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2367, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_2692 = torch.constant.int 1
    %2368 = torch.aten.size.int %2364, %int1_2692 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_2693 = torch.constant.int 32
    %2369 = torch.aten.mul.int %2368, %int32_2693 : !torch.int, !torch.int -> !torch.int
    %int4_2694 = torch.constant.int 4
    %int8_2695 = torch.constant.int 8
    %int128_2696 = torch.constant.int 128
    %2370 = torch.prim.ListConstruct %int4_2694, %2369, %int8_2695, %int128_2696 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2371 = torch.aten._unsafe_view %2367, %2370 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %2371, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_2697 = torch.constant.int 0
    %int0_2698 = torch.constant.int 0
    %int9223372036854775807_2699 = torch.constant.int 9223372036854775807
    %int1_2700 = torch.constant.int 1
    %2372 = torch.aten.slice.Tensor %2371, %int0_2697, %int0_2698, %int9223372036854775807_2699, %int1_2700 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %2372, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int-2_2701 = torch.constant.int -2
    %2373 = torch.aten.unsqueeze %2362, %int-2_2701 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %2373, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_2702 = torch.constant.int 1
    %2374 = torch.aten.size.int %2361, %int1_2702 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_2703 = torch.constant.int 4
    %int8_2704 = torch.constant.int 8
    %int4_2705 = torch.constant.int 4
    %int128_2706 = torch.constant.int 128
    %2375 = torch.prim.ListConstruct %int4_2703, %2374, %int8_2704, %int4_2705, %int128_2706 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_2707 = torch.constant.bool false
    %2376 = torch.aten.expand %2373, %2375, %false_2707 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2376, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_2708 = torch.constant.int 0
    %2377 = torch.aten.clone %2376, %int0_2708 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2377, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_2709 = torch.constant.int 4
    %int32_2710 = torch.constant.int 32
    %int128_2711 = torch.constant.int 128
    %2378 = torch.prim.ListConstruct %int4_2709, %2374, %int32_2710, %int128_2711 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2379 = torch.aten._unsafe_view %2377, %2378 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2379, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_2712 = torch.constant.int -2
    %2380 = torch.aten.unsqueeze %2372, %int-2_2712 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %2380, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_2713 = torch.constant.int 1
    %2381 = torch.aten.size.int %2371, %int1_2713 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_2714 = torch.constant.int 4
    %int8_2715 = torch.constant.int 8
    %int4_2716 = torch.constant.int 4
    %int128_2717 = torch.constant.int 128
    %2382 = torch.prim.ListConstruct %int4_2714, %2381, %int8_2715, %int4_2716, %int128_2717 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_2718 = torch.constant.bool false
    %2383 = torch.aten.expand %2380, %2382, %false_2718 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2383, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_2719 = torch.constant.int 0
    %2384 = torch.aten.clone %2383, %int0_2719 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2384, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_2720 = torch.constant.int 4
    %int32_2721 = torch.constant.int 32
    %int128_2722 = torch.constant.int 128
    %2385 = torch.prim.ListConstruct %int4_2720, %2381, %int32_2721, %int128_2722 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2386 = torch.aten._unsafe_view %2384, %2385 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2386, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_2723 = torch.constant.int 1
    %int2_2724 = torch.constant.int 2
    %2387 = torch.aten.transpose.int %2267, %int1_2723, %int2_2724 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,128],f16>
    %int1_2725 = torch.constant.int 1
    %int2_2726 = torch.constant.int 2
    %2388 = torch.aten.transpose.int %2379, %int1_2725, %int2_2726 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %2388, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_2727 = torch.constant.int 1
    %int2_2728 = torch.constant.int 2
    %2389 = torch.aten.transpose.int %2386, %int1_2727, %int2_2728 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %2389, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_2729 = torch.constant.float 0.000000e+00
    %false_2730 = torch.constant.bool false
    %none_2731 = torch.constant.none
    %2390:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2387, %2388, %2389, %float0.000000e00_2729, %false_2730, %368, %none_2731) : (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.vtensor<[4,1,1,?],f16>, !torch.none) -> (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,1],f32>) 
    %int1_2732 = torch.constant.int 1
    %int2_2733 = torch.constant.int 2
    %2391 = torch.aten.transpose.int %2390#0, %int1_2732, %int2_2733 : !torch.vtensor<[4,32,1,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int4_2734 = torch.constant.int 4
    %int1_2735 = torch.constant.int 1
    %int4096_2736 = torch.constant.int 4096
    %2392 = torch.prim.ListConstruct %int4_2734, %int1_2735, %int4096_2736 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2393 = torch.aten.view %2391, %2392 : !torch.vtensor<[4,1,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_2737 = torch.constant.int -2
    %int-1_2738 = torch.constant.int -1
    %2394 = torch.aten.transpose.int %106, %int-2_2737, %int-1_2738 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_2739 = torch.constant.int 4
    %int4096_2740 = torch.constant.int 4096
    %2395 = torch.prim.ListConstruct %int4_2739, %int4096_2740 : (!torch.int, !torch.int) -> !torch.list<int>
    %2396 = torch.aten.view %2393, %2395 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %2397 = torch.aten.mm %2396, %2394 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_2741 = torch.constant.int 4
    %int1_2742 = torch.constant.int 1
    %int4096_2743 = torch.constant.int 4096
    %2398 = torch.prim.ListConstruct %int4_2741, %int1_2742, %int4096_2743 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2399 = torch.aten.view %2397, %2398 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_2744 = torch.constant.int 1
    %2400 = torch.aten.add.Tensor %2227, %2399, %int1_2744 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_2745 = torch.constant.int 6
    %2401 = torch.prims.convert_element_type %2400, %int6_2745 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_2746 = torch.constant.int 2
    %2402 = torch.aten.pow.Tensor_Scalar %2401, %int2_2746 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_2747 = torch.constant.int -1
    %2403 = torch.prim.ListConstruct %int-1_2747 : (!torch.int) -> !torch.list<int>
    %true_2748 = torch.constant.bool true
    %none_2749 = torch.constant.none
    %2404 = torch.aten.mean.dim %2402, %2403, %true_2748, %none_2749 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_2750 = torch.constant.float 9.9999997473787516E-6
    %int1_2751 = torch.constant.int 1
    %2405 = torch.aten.add.Scalar %2404, %float9.999990e-06_2750, %int1_2751 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %2406 = torch.aten.rsqrt %2405 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %2407 = torch.aten.mul.Tensor %2401, %2406 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_2752 = torch.constant.int 5
    %2408 = torch.prims.convert_element_type %2407, %int5_2752 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %2409 = torch.aten.mul.Tensor %107, %2408 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_2753 = torch.constant.int 5
    %2410 = torch.prims.convert_element_type %2409, %int5_2753 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_2754 = torch.constant.int -2
    %int-1_2755 = torch.constant.int -1
    %2411 = torch.aten.transpose.int %108, %int-2_2754, %int-1_2755 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_2756 = torch.constant.int 4
    %int4096_2757 = torch.constant.int 4096
    %2412 = torch.prim.ListConstruct %int4_2756, %int4096_2757 : (!torch.int, !torch.int) -> !torch.list<int>
    %2413 = torch.aten.view %2410, %2412 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %2414 = torch.aten.mm %2413, %2411 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_2758 = torch.constant.int 4
    %int1_2759 = torch.constant.int 1
    %int14336_2760 = torch.constant.int 14336
    %2415 = torch.prim.ListConstruct %int4_2758, %int1_2759, %int14336_2760 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2416 = torch.aten.view %2414, %2415 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %2417 = torch.aten.silu %2416 : !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_2761 = torch.constant.int -2
    %int-1_2762 = torch.constant.int -1
    %2418 = torch.aten.transpose.int %109, %int-2_2761, %int-1_2762 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_2763 = torch.constant.int 4
    %int4096_2764 = torch.constant.int 4096
    %2419 = torch.prim.ListConstruct %int4_2763, %int4096_2764 : (!torch.int, !torch.int) -> !torch.list<int>
    %2420 = torch.aten.view %2410, %2419 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %2421 = torch.aten.mm %2420, %2418 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_2765 = torch.constant.int 4
    %int1_2766 = torch.constant.int 1
    %int14336_2767 = torch.constant.int 14336
    %2422 = torch.prim.ListConstruct %int4_2765, %int1_2766, %int14336_2767 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2423 = torch.aten.view %2421, %2422 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %2424 = torch.aten.mul.Tensor %2417, %2423 : !torch.vtensor<[4,1,14336],f16>, !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_2768 = torch.constant.int -2
    %int-1_2769 = torch.constant.int -1
    %2425 = torch.aten.transpose.int %110, %int-2_2768, %int-1_2769 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int4_2770 = torch.constant.int 4
    %int14336_2771 = torch.constant.int 14336
    %2426 = torch.prim.ListConstruct %int4_2770, %int14336_2771 : (!torch.int, !torch.int) -> !torch.list<int>
    %2427 = torch.aten.view %2424, %2426 : !torch.vtensor<[4,1,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,14336],f16>
    %2428 = torch.aten.mm %2427, %2425 : !torch.vtensor<[4,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_2772 = torch.constant.int 4
    %int1_2773 = torch.constant.int 1
    %int4096_2774 = torch.constant.int 4096
    %2429 = torch.prim.ListConstruct %int4_2772, %int1_2773, %int4096_2774 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2430 = torch.aten.view %2428, %2429 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_2775 = torch.constant.int 1
    %2431 = torch.aten.add.Tensor %2400, %2430, %int1_2775 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_2776 = torch.constant.int 6
    %2432 = torch.prims.convert_element_type %2431, %int6_2776 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_2777 = torch.constant.int 2
    %2433 = torch.aten.pow.Tensor_Scalar %2432, %int2_2777 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_2778 = torch.constant.int -1
    %2434 = torch.prim.ListConstruct %int-1_2778 : (!torch.int) -> !torch.list<int>
    %true_2779 = torch.constant.bool true
    %none_2780 = torch.constant.none
    %2435 = torch.aten.mean.dim %2433, %2434, %true_2779, %none_2780 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_2781 = torch.constant.float 9.9999997473787516E-6
    %int1_2782 = torch.constant.int 1
    %2436 = torch.aten.add.Scalar %2435, %float9.999990e-06_2781, %int1_2782 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %2437 = torch.aten.rsqrt %2436 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %2438 = torch.aten.mul.Tensor %2432, %2437 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_2783 = torch.constant.int 5
    %2439 = torch.prims.convert_element_type %2438, %int5_2783 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %2440 = torch.aten.mul.Tensor %111, %2439 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_2784 = torch.constant.int 5
    %2441 = torch.prims.convert_element_type %2440, %int5_2784 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_2785 = torch.constant.int -2
    %int-1_2786 = torch.constant.int -1
    %2442 = torch.aten.transpose.int %112, %int-2_2785, %int-1_2786 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_2787 = torch.constant.int 4
    %int4096_2788 = torch.constant.int 4096
    %2443 = torch.prim.ListConstruct %int4_2787, %int4096_2788 : (!torch.int, !torch.int) -> !torch.list<int>
    %2444 = torch.aten.view %2441, %2443 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %2445 = torch.aten.mm %2444, %2442 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_2789 = torch.constant.int 4
    %int1_2790 = torch.constant.int 1
    %int4096_2791 = torch.constant.int 4096
    %2446 = torch.prim.ListConstruct %int4_2789, %int1_2790, %int4096_2791 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2447 = torch.aten.view %2445, %2446 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_2792 = torch.constant.int -2
    %int-1_2793 = torch.constant.int -1
    %2448 = torch.aten.transpose.int %113, %int-2_2792, %int-1_2793 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_2794 = torch.constant.int 4
    %int4096_2795 = torch.constant.int 4096
    %2449 = torch.prim.ListConstruct %int4_2794, %int4096_2795 : (!torch.int, !torch.int) -> !torch.list<int>
    %2450 = torch.aten.view %2441, %2449 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %2451 = torch.aten.mm %2450, %2448 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_2796 = torch.constant.int 4
    %int1_2797 = torch.constant.int 1
    %int1024_2798 = torch.constant.int 1024
    %2452 = torch.prim.ListConstruct %int4_2796, %int1_2797, %int1024_2798 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2453 = torch.aten.view %2451, %2452 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int-2_2799 = torch.constant.int -2
    %int-1_2800 = torch.constant.int -1
    %2454 = torch.aten.transpose.int %114, %int-2_2799, %int-1_2800 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_2801 = torch.constant.int 4
    %int4096_2802 = torch.constant.int 4096
    %2455 = torch.prim.ListConstruct %int4_2801, %int4096_2802 : (!torch.int, !torch.int) -> !torch.list<int>
    %2456 = torch.aten.view %2441, %2455 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %2457 = torch.aten.mm %2456, %2454 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_2803 = torch.constant.int 4
    %int1_2804 = torch.constant.int 1
    %int1024_2805 = torch.constant.int 1024
    %2458 = torch.prim.ListConstruct %int4_2803, %int1_2804, %int1024_2805 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2459 = torch.aten.view %2457, %2458 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int4_2806 = torch.constant.int 4
    %int1_2807 = torch.constant.int 1
    %int32_2808 = torch.constant.int 32
    %int128_2809 = torch.constant.int 128
    %2460 = torch.prim.ListConstruct %int4_2806, %int1_2807, %int32_2808, %int128_2809 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2461 = torch.aten.view %2447, %2460 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,128],f16>
    %int4_2810 = torch.constant.int 4
    %int1_2811 = torch.constant.int 1
    %int8_2812 = torch.constant.int 8
    %int128_2813 = torch.constant.int 128
    %2462 = torch.prim.ListConstruct %int4_2810, %int1_2811, %int8_2812, %int128_2813 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2463 = torch.aten.view %2453, %2462 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int4_2814 = torch.constant.int 4
    %int1_2815 = torch.constant.int 1
    %int8_2816 = torch.constant.int 8
    %int128_2817 = torch.constant.int 128
    %2464 = torch.prim.ListConstruct %int4_2814, %int1_2815, %int8_2816, %int128_2817 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2465 = torch.aten.view %2459, %2464 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int6_2818 = torch.constant.int 6
    %2466 = torch.prims.convert_element_type %2461, %int6_2818 : !torch.vtensor<[4,1,32,128],f16>, !torch.int -> !torch.vtensor<[4,1,32,128],f32>
    %2467 = torch_c.to_builtin_tensor %2466 : !torch.vtensor<[4,1,32,128],f32> -> tensor<4x1x32x128xf32>
    %2468 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %2469 = util.call @sharktank_rotary_embedding_4_1_32_128_f32(%2467, %2468) : (tensor<4x1x32x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x32x128xf32>
    %2470 = torch_c.from_builtin_tensor %2469 : tensor<4x1x32x128xf32> -> !torch.vtensor<[4,1,32,128],f32>
    %int5_2819 = torch.constant.int 5
    %2471 = torch.prims.convert_element_type %2470, %int5_2819 : !torch.vtensor<[4,1,32,128],f32>, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int6_2820 = torch.constant.int 6
    %2472 = torch.prims.convert_element_type %2463, %int6_2820 : !torch.vtensor<[4,1,8,128],f16>, !torch.int -> !torch.vtensor<[4,1,8,128],f32>
    %2473 = torch_c.to_builtin_tensor %2472 : !torch.vtensor<[4,1,8,128],f32> -> tensor<4x1x8x128xf32>
    %2474 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %2475 = util.call @sharktank_rotary_embedding_4_1_8_128_f32(%2473, %2474) : (tensor<4x1x8x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x8x128xf32>
    %2476 = torch_c.from_builtin_tensor %2475 : tensor<4x1x8x128xf32> -> !torch.vtensor<[4,1,8,128],f32>
    %int5_2821 = torch.constant.int 5
    %2477 = torch.prims.convert_element_type %2476, %int5_2821 : !torch.vtensor<[4,1,8,128],f32>, !torch.int -> !torch.vtensor<[4,1,8,128],f16>
    %int32_2822 = torch.constant.int 32
    %2478 = torch.aten.floor_divide.Scalar %arg2, %int32_2822 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_2823 = torch.constant.int 1
    %2479 = torch.aten.unsqueeze %2478, %int1_2823 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2824 = torch.constant.int 1
    %false_2825 = torch.constant.bool false
    %2480 = torch.aten.gather %arg3, %int1_2824, %2479, %false_2825 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_2826 = torch.constant.int 32
    %2481 = torch.aten.remainder.Scalar %arg2, %int32_2826 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_2827 = torch.constant.int 1
    %2482 = torch.aten.unsqueeze %2481, %int1_2827 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_2828 = torch.constant.none
    %2483 = torch.aten.clone %115, %none_2828 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_2829 = torch.constant.int 0
    %2484 = torch.aten.unsqueeze %2483, %int0_2829 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_2830 = torch.constant.int 4
    %int1_2831 = torch.constant.int 1
    %2485 = torch.prim.ListConstruct %int4_2830, %int1_2831 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_2832 = torch.constant.int 1
    %int1_2833 = torch.constant.int 1
    %2486 = torch.prim.ListConstruct %int1_2832, %int1_2833 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_2834 = torch.constant.int 4
    %int0_2835 = torch.constant.int 0
    %cpu_2836 = torch.constant.device "cpu"
    %false_2837 = torch.constant.bool false
    %2487 = torch.aten.empty_strided %2485, %2486, %int4_2834, %int0_2835, %cpu_2836, %false_2837 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int10 = torch.constant.int 10
    %2488 = torch.aten.fill.Scalar %2487, %int10 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_2838 = torch.constant.int 4
    %int1_2839 = torch.constant.int 1
    %2489 = torch.prim.ListConstruct %int4_2838, %int1_2839 : (!torch.int, !torch.int) -> !torch.list<int>
    %2490 = torch.aten.repeat %2484, %2489 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_2840 = torch.constant.int 32
    %2491 = torch.aten.mul.Scalar %2480, %int32_2840 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2841 = torch.constant.int 1
    %2492 = torch.aten.add.Tensor %2491, %2488, %int1_2841 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_2842 = torch.constant.int 2
    %2493 = torch.aten.mul.Scalar %2492, %int2_2842 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2843 = torch.constant.int 1
    %2494 = torch.aten.add.Tensor %2493, %2490, %int1_2843 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_2844 = torch.constant.int 32
    %2495 = torch.aten.mul.Scalar %2494, %int32_2844 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2845 = torch.constant.int 1
    %2496 = torch.aten.add.Tensor %2495, %2482, %int1_2845 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_2846 = torch.constant.int 32
    %int2_2847 = torch.constant.int 2
    %int32_2848 = torch.constant.int 32
    %int8_2849 = torch.constant.int 8
    %int128_2850 = torch.constant.int 128
    %2497 = torch.prim.ListConstruct %437, %int32_2846, %int2_2847, %int32_2848, %int8_2849, %int128_2850 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2498 = torch.aten.view %2334, %2497 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2498, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_2851 = torch.constant.int 32
    %2499 = torch.aten.mul.int %437, %int32_2851 : !torch.int, !torch.int -> !torch.int
    %int2_2852 = torch.constant.int 2
    %2500 = torch.aten.mul.int %2499, %int2_2852 : !torch.int, !torch.int -> !torch.int
    %int32_2853 = torch.constant.int 32
    %2501 = torch.aten.mul.int %2500, %int32_2853 : !torch.int, !torch.int -> !torch.int
    %int8_2854 = torch.constant.int 8
    %int128_2855 = torch.constant.int 128
    %2502 = torch.prim.ListConstruct %2501, %int8_2854, %int128_2855 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2503 = torch.aten.view %2498, %2502 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %2503, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %2504 = torch.prim.ListConstruct %2496 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_2856 = torch.constant.bool false
    %2505 = torch.aten.index_put %2503, %2504, %2477, %false_2856 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %2505, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_2857 = torch.constant.int 32
    %int2_2858 = torch.constant.int 2
    %int32_2859 = torch.constant.int 32
    %int8_2860 = torch.constant.int 8
    %int128_2861 = torch.constant.int 128
    %2506 = torch.prim.ListConstruct %437, %int32_2857, %int2_2858, %int32_2859, %int8_2860, %int128_2861 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2507 = torch.aten.view %2505, %2506 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2507, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_2862 = torch.constant.int 2097152
    %2508 = torch.prim.ListConstruct %437, %int2097152_2862 : (!torch.int, !torch.int) -> !torch.list<int>
    %2509 = torch.aten.view %2507, %2508 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %2509, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_2863 = torch.constant.int 32
    %int2_2864 = torch.constant.int 2
    %int32_2865 = torch.constant.int 32
    %int8_2866 = torch.constant.int 8
    %int128_2867 = torch.constant.int 128
    %2510 = torch.prim.ListConstruct %437, %int32_2863, %int2_2864, %int32_2865, %int8_2866, %int128_2867 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2511 = torch.aten.view %2509, %2510 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2511, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int8_2868 = torch.constant.int 8
    %int128_2869 = torch.constant.int 128
    %2512 = torch.prim.ListConstruct %2501, %int8_2868, %int128_2869 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2513 = torch.aten.view %2511, %2512 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %2513, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_2870 = torch.constant.int 32
    %2514 = torch.aten.floor_divide.Scalar %arg2, %int32_2870 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_2871 = torch.constant.int 1
    %2515 = torch.aten.unsqueeze %2514, %int1_2871 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2872 = torch.constant.int 1
    %false_2873 = torch.constant.bool false
    %2516 = torch.aten.gather %arg3, %int1_2872, %2515, %false_2873 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_2874 = torch.constant.int 32
    %2517 = torch.aten.remainder.Scalar %arg2, %int32_2874 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_2875 = torch.constant.int 1
    %2518 = torch.aten.unsqueeze %2517, %int1_2875 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_2876 = torch.constant.none
    %2519 = torch.aten.clone %116, %none_2876 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_2877 = torch.constant.int 0
    %2520 = torch.aten.unsqueeze %2519, %int0_2877 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_2878 = torch.constant.int 4
    %int1_2879 = torch.constant.int 1
    %2521 = torch.prim.ListConstruct %int4_2878, %int1_2879 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_2880 = torch.constant.int 1
    %int1_2881 = torch.constant.int 1
    %2522 = torch.prim.ListConstruct %int1_2880, %int1_2881 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_2882 = torch.constant.int 4
    %int0_2883 = torch.constant.int 0
    %cpu_2884 = torch.constant.device "cpu"
    %false_2885 = torch.constant.bool false
    %2523 = torch.aten.empty_strided %2521, %2522, %int4_2882, %int0_2883, %cpu_2884, %false_2885 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int10_2886 = torch.constant.int 10
    %2524 = torch.aten.fill.Scalar %2523, %int10_2886 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_2887 = torch.constant.int 4
    %int1_2888 = torch.constant.int 1
    %2525 = torch.prim.ListConstruct %int4_2887, %int1_2888 : (!torch.int, !torch.int) -> !torch.list<int>
    %2526 = torch.aten.repeat %2520, %2525 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_2889 = torch.constant.int 32
    %2527 = torch.aten.mul.Scalar %2516, %int32_2889 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2890 = torch.constant.int 1
    %2528 = torch.aten.add.Tensor %2527, %2524, %int1_2890 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_2891 = torch.constant.int 2
    %2529 = torch.aten.mul.Scalar %2528, %int2_2891 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2892 = torch.constant.int 1
    %2530 = torch.aten.add.Tensor %2529, %2526, %int1_2892 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_2893 = torch.constant.int 32
    %2531 = torch.aten.mul.Scalar %2530, %int32_2893 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_2894 = torch.constant.int 1
    %2532 = torch.aten.add.Tensor %2531, %2518, %int1_2894 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %2533 = torch.prim.ListConstruct %2532 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_2895 = torch.constant.bool false
    %2534 = torch.aten.index_put %2513, %2533, %2465, %false_2895 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %2534, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_2896 = torch.constant.int 32
    %int2_2897 = torch.constant.int 2
    %int32_2898 = torch.constant.int 32
    %int8_2899 = torch.constant.int 8
    %int128_2900 = torch.constant.int 128
    %2535 = torch.prim.ListConstruct %437, %int32_2896, %int2_2897, %int32_2898, %int8_2899, %int128_2900 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2536 = torch.aten.view %2534, %2535 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2536, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_2901 = torch.constant.int 2097152
    %2537 = torch.prim.ListConstruct %437, %int2097152_2901 : (!torch.int, !torch.int) -> !torch.list<int>
    %2538 = torch.aten.view %2536, %2537 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %2538, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int4_2902 = torch.constant.int 4
    %2539 = torch.prim.ListConstruct %int4_2902, %358 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_2903 = torch.constant.int 1
    %2540 = torch.prim.ListConstruct %358, %int1_2903 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_2904 = torch.constant.int 4
    %int0_2905 = torch.constant.int 0
    %cpu_2906 = torch.constant.device "cpu"
    %false_2907 = torch.constant.bool false
    %2541 = torch.aten.empty_strided %2539, %2540, %int4_2904, %int0_2905, %cpu_2906, %false_2907 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2541, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int10_2908 = torch.constant.int 10
    %2542 = torch.aten.fill.Scalar %2541, %int10_2908 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2542, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int32_2909 = torch.constant.int 32
    %2543 = torch.aten.mul.Scalar %arg3, %int32_2909 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2543, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_2910 = torch.constant.int 1
    %2544 = torch.aten.add.Tensor %2543, %2542, %int1_2910 : !torch.vtensor<[4,?],si64>, !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2544, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_2911 = torch.constant.int 4
    %2545 = torch.aten.mul.int %int4_2911, %358 : !torch.int, !torch.int -> !torch.int
    %2546 = torch.prim.ListConstruct %2545 : (!torch.int) -> !torch.list<int>
    %2547 = torch.aten.view %2544, %2546 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2547, [%356], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_2912 = torch.constant.int 32
    %int2_2913 = torch.constant.int 2
    %int32_2914 = torch.constant.int 32
    %int8_2915 = torch.constant.int 8
    %int128_2916 = torch.constant.int 128
    %2548 = torch.prim.ListConstruct %437, %int32_2912, %int2_2913, %int32_2914, %int8_2915, %int128_2916 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2549 = torch.aten.view %2538, %2548 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2549, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_2917 = torch.constant.int 32
    %2550 = torch.aten.mul.int %437, %int32_2917 : !torch.int, !torch.int -> !torch.int
    %int2_2918 = torch.constant.int 2
    %int32_2919 = torch.constant.int 32
    %int8_2920 = torch.constant.int 8
    %int128_2921 = torch.constant.int 128
    %2551 = torch.prim.ListConstruct %2550, %int2_2918, %int32_2919, %int8_2920, %int128_2921 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2552 = torch.aten.view %2549, %2551 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %2552, [%357], affine_map<()[s0] -> (s0 * 32, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int0_2922 = torch.constant.int 0
    %2553 = torch.aten.index_select %2552, %int0_2922, %2547 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.int, !torch.vtensor<[?],si64> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %2553, [%356], affine_map<()[s0] -> (s0 * 4, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int4_2923 = torch.constant.int 4
    %int2_2924 = torch.constant.int 2
    %int32_2925 = torch.constant.int 32
    %int8_2926 = torch.constant.int 8
    %int128_2927 = torch.constant.int 128
    %2554 = torch.prim.ListConstruct %int4_2923, %358, %int2_2924, %int32_2925, %int8_2926, %int128_2927 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2555 = torch.aten.view %2553, %2554 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %2555, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int0_2928 = torch.constant.int 0
    %int0_2929 = torch.constant.int 0
    %int9223372036854775807_2930 = torch.constant.int 9223372036854775807
    %int1_2931 = torch.constant.int 1
    %2556 = torch.aten.slice.Tensor %2555, %int0_2928, %int0_2929, %int9223372036854775807_2930, %int1_2931 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %2556, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_2932 = torch.constant.int 1
    %int0_2933 = torch.constant.int 0
    %int9223372036854775807_2934 = torch.constant.int 9223372036854775807
    %int1_2935 = torch.constant.int 1
    %2557 = torch.aten.slice.Tensor %2556, %int1_2932, %int0_2933, %int9223372036854775807_2934, %int1_2935 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %2557, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_2936 = torch.constant.int 2
    %int0_2937 = torch.constant.int 0
    %2558 = torch.aten.select.int %2557, %int2_2936, %int0_2937 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2558, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int32_2938 = torch.constant.int 32
    %2559 = torch.aten.mul.int %358, %int32_2938 : !torch.int, !torch.int -> !torch.int
    %int2_2939 = torch.constant.int 2
    %int0_2940 = torch.constant.int 0
    %int1_2941 = torch.constant.int 1
    %2560 = torch.aten.slice.Tensor %2558, %int2_2939, %int0_2940, %2559, %int1_2941 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2560, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_2942 = torch.constant.int 0
    %2561 = torch.aten.clone %2560, %int0_2942 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2561, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_2943 = torch.constant.int 1
    %2562 = torch.aten.size.int %2557, %int1_2943 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_2944 = torch.constant.int 32
    %2563 = torch.aten.mul.int %2562, %int32_2944 : !torch.int, !torch.int -> !torch.int
    %int4_2945 = torch.constant.int 4
    %int8_2946 = torch.constant.int 8
    %int128_2947 = torch.constant.int 128
    %2564 = torch.prim.ListConstruct %int4_2945, %2563, %int8_2946, %int128_2947 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2565 = torch.aten._unsafe_view %2561, %2564 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %2565, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_2948 = torch.constant.int 0
    %int0_2949 = torch.constant.int 0
    %int9223372036854775807_2950 = torch.constant.int 9223372036854775807
    %int1_2951 = torch.constant.int 1
    %2566 = torch.aten.slice.Tensor %2565, %int0_2948, %int0_2949, %int9223372036854775807_2950, %int1_2951 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %2566, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_2952 = torch.constant.int 0
    %int0_2953 = torch.constant.int 0
    %int9223372036854775807_2954 = torch.constant.int 9223372036854775807
    %int1_2955 = torch.constant.int 1
    %2567 = torch.aten.slice.Tensor %2555, %int0_2952, %int0_2953, %int9223372036854775807_2954, %int1_2955 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %2567, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_2956 = torch.constant.int 1
    %int0_2957 = torch.constant.int 0
    %int9223372036854775807_2958 = torch.constant.int 9223372036854775807
    %int1_2959 = torch.constant.int 1
    %2568 = torch.aten.slice.Tensor %2567, %int1_2956, %int0_2957, %int9223372036854775807_2958, %int1_2959 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %2568, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_2960 = torch.constant.int 2
    %int1_2961 = torch.constant.int 1
    %2569 = torch.aten.select.int %2568, %int2_2960, %int1_2961 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2569, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int2_2962 = torch.constant.int 2
    %int0_2963 = torch.constant.int 0
    %int1_2964 = torch.constant.int 1
    %2570 = torch.aten.slice.Tensor %2569, %int2_2962, %int0_2963, %2559, %int1_2964 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2570, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_2965 = torch.constant.int 0
    %2571 = torch.aten.clone %2570, %int0_2965 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2571, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_2966 = torch.constant.int 1
    %2572 = torch.aten.size.int %2568, %int1_2966 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_2967 = torch.constant.int 32
    %2573 = torch.aten.mul.int %2572, %int32_2967 : !torch.int, !torch.int -> !torch.int
    %int4_2968 = torch.constant.int 4
    %int8_2969 = torch.constant.int 8
    %int128_2970 = torch.constant.int 128
    %2574 = torch.prim.ListConstruct %int4_2968, %2573, %int8_2969, %int128_2970 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2575 = torch.aten._unsafe_view %2571, %2574 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %2575, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_2971 = torch.constant.int 0
    %int0_2972 = torch.constant.int 0
    %int9223372036854775807_2973 = torch.constant.int 9223372036854775807
    %int1_2974 = torch.constant.int 1
    %2576 = torch.aten.slice.Tensor %2575, %int0_2971, %int0_2972, %int9223372036854775807_2973, %int1_2974 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %2576, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int-2_2975 = torch.constant.int -2
    %2577 = torch.aten.unsqueeze %2566, %int-2_2975 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %2577, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_2976 = torch.constant.int 1
    %2578 = torch.aten.size.int %2565, %int1_2976 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_2977 = torch.constant.int 4
    %int8_2978 = torch.constant.int 8
    %int4_2979 = torch.constant.int 4
    %int128_2980 = torch.constant.int 128
    %2579 = torch.prim.ListConstruct %int4_2977, %2578, %int8_2978, %int4_2979, %int128_2980 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_2981 = torch.constant.bool false
    %2580 = torch.aten.expand %2577, %2579, %false_2981 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2580, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_2982 = torch.constant.int 0
    %2581 = torch.aten.clone %2580, %int0_2982 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2581, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_2983 = torch.constant.int 4
    %int32_2984 = torch.constant.int 32
    %int128_2985 = torch.constant.int 128
    %2582 = torch.prim.ListConstruct %int4_2983, %2578, %int32_2984, %int128_2985 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2583 = torch.aten._unsafe_view %2581, %2582 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2583, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_2986 = torch.constant.int -2
    %2584 = torch.aten.unsqueeze %2576, %int-2_2986 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %2584, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_2987 = torch.constant.int 1
    %2585 = torch.aten.size.int %2575, %int1_2987 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_2988 = torch.constant.int 4
    %int8_2989 = torch.constant.int 8
    %int4_2990 = torch.constant.int 4
    %int128_2991 = torch.constant.int 128
    %2586 = torch.prim.ListConstruct %int4_2988, %2585, %int8_2989, %int4_2990, %int128_2991 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_2992 = torch.constant.bool false
    %2587 = torch.aten.expand %2584, %2586, %false_2992 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2587, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_2993 = torch.constant.int 0
    %2588 = torch.aten.clone %2587, %int0_2993 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2588, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_2994 = torch.constant.int 4
    %int32_2995 = torch.constant.int 32
    %int128_2996 = torch.constant.int 128
    %2589 = torch.prim.ListConstruct %int4_2994, %2585, %int32_2995, %int128_2996 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2590 = torch.aten._unsafe_view %2588, %2589 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2590, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_2997 = torch.constant.int 1
    %int2_2998 = torch.constant.int 2
    %2591 = torch.aten.transpose.int %2471, %int1_2997, %int2_2998 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,128],f16>
    %int1_2999 = torch.constant.int 1
    %int2_3000 = torch.constant.int 2
    %2592 = torch.aten.transpose.int %2583, %int1_2999, %int2_3000 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %2592, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_3001 = torch.constant.int 1
    %int2_3002 = torch.constant.int 2
    %2593 = torch.aten.transpose.int %2590, %int1_3001, %int2_3002 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %2593, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_3003 = torch.constant.float 0.000000e+00
    %false_3004 = torch.constant.bool false
    %none_3005 = torch.constant.none
    %2594:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2591, %2592, %2593, %float0.000000e00_3003, %false_3004, %368, %none_3005) : (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.vtensor<[4,1,1,?],f16>, !torch.none) -> (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,1],f32>) 
    %int1_3006 = torch.constant.int 1
    %int2_3007 = torch.constant.int 2
    %2595 = torch.aten.transpose.int %2594#0, %int1_3006, %int2_3007 : !torch.vtensor<[4,32,1,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int4_3008 = torch.constant.int 4
    %int1_3009 = torch.constant.int 1
    %int4096_3010 = torch.constant.int 4096
    %2596 = torch.prim.ListConstruct %int4_3008, %int1_3009, %int4096_3010 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2597 = torch.aten.view %2595, %2596 : !torch.vtensor<[4,1,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_3011 = torch.constant.int -2
    %int-1_3012 = torch.constant.int -1
    %2598 = torch.aten.transpose.int %117, %int-2_3011, %int-1_3012 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_3013 = torch.constant.int 4
    %int4096_3014 = torch.constant.int 4096
    %2599 = torch.prim.ListConstruct %int4_3013, %int4096_3014 : (!torch.int, !torch.int) -> !torch.list<int>
    %2600 = torch.aten.view %2597, %2599 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %2601 = torch.aten.mm %2600, %2598 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_3015 = torch.constant.int 4
    %int1_3016 = torch.constant.int 1
    %int4096_3017 = torch.constant.int 4096
    %2602 = torch.prim.ListConstruct %int4_3015, %int1_3016, %int4096_3017 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2603 = torch.aten.view %2601, %2602 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_3018 = torch.constant.int 1
    %2604 = torch.aten.add.Tensor %2431, %2603, %int1_3018 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_3019 = torch.constant.int 6
    %2605 = torch.prims.convert_element_type %2604, %int6_3019 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_3020 = torch.constant.int 2
    %2606 = torch.aten.pow.Tensor_Scalar %2605, %int2_3020 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_3021 = torch.constant.int -1
    %2607 = torch.prim.ListConstruct %int-1_3021 : (!torch.int) -> !torch.list<int>
    %true_3022 = torch.constant.bool true
    %none_3023 = torch.constant.none
    %2608 = torch.aten.mean.dim %2606, %2607, %true_3022, %none_3023 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_3024 = torch.constant.float 9.9999997473787516E-6
    %int1_3025 = torch.constant.int 1
    %2609 = torch.aten.add.Scalar %2608, %float9.999990e-06_3024, %int1_3025 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %2610 = torch.aten.rsqrt %2609 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %2611 = torch.aten.mul.Tensor %2605, %2610 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_3026 = torch.constant.int 5
    %2612 = torch.prims.convert_element_type %2611, %int5_3026 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %2613 = torch.aten.mul.Tensor %118, %2612 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_3027 = torch.constant.int 5
    %2614 = torch.prims.convert_element_type %2613, %int5_3027 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_3028 = torch.constant.int -2
    %int-1_3029 = torch.constant.int -1
    %2615 = torch.aten.transpose.int %119, %int-2_3028, %int-1_3029 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_3030 = torch.constant.int 4
    %int4096_3031 = torch.constant.int 4096
    %2616 = torch.prim.ListConstruct %int4_3030, %int4096_3031 : (!torch.int, !torch.int) -> !torch.list<int>
    %2617 = torch.aten.view %2614, %2616 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %2618 = torch.aten.mm %2617, %2615 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_3032 = torch.constant.int 4
    %int1_3033 = torch.constant.int 1
    %int14336_3034 = torch.constant.int 14336
    %2619 = torch.prim.ListConstruct %int4_3032, %int1_3033, %int14336_3034 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2620 = torch.aten.view %2618, %2619 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %2621 = torch.aten.silu %2620 : !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_3035 = torch.constant.int -2
    %int-1_3036 = torch.constant.int -1
    %2622 = torch.aten.transpose.int %120, %int-2_3035, %int-1_3036 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_3037 = torch.constant.int 4
    %int4096_3038 = torch.constant.int 4096
    %2623 = torch.prim.ListConstruct %int4_3037, %int4096_3038 : (!torch.int, !torch.int) -> !torch.list<int>
    %2624 = torch.aten.view %2614, %2623 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %2625 = torch.aten.mm %2624, %2622 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_3039 = torch.constant.int 4
    %int1_3040 = torch.constant.int 1
    %int14336_3041 = torch.constant.int 14336
    %2626 = torch.prim.ListConstruct %int4_3039, %int1_3040, %int14336_3041 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2627 = torch.aten.view %2625, %2626 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %2628 = torch.aten.mul.Tensor %2621, %2627 : !torch.vtensor<[4,1,14336],f16>, !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_3042 = torch.constant.int -2
    %int-1_3043 = torch.constant.int -1
    %2629 = torch.aten.transpose.int %121, %int-2_3042, %int-1_3043 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int4_3044 = torch.constant.int 4
    %int14336_3045 = torch.constant.int 14336
    %2630 = torch.prim.ListConstruct %int4_3044, %int14336_3045 : (!torch.int, !torch.int) -> !torch.list<int>
    %2631 = torch.aten.view %2628, %2630 : !torch.vtensor<[4,1,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,14336],f16>
    %2632 = torch.aten.mm %2631, %2629 : !torch.vtensor<[4,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_3046 = torch.constant.int 4
    %int1_3047 = torch.constant.int 1
    %int4096_3048 = torch.constant.int 4096
    %2633 = torch.prim.ListConstruct %int4_3046, %int1_3047, %int4096_3048 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2634 = torch.aten.view %2632, %2633 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_3049 = torch.constant.int 1
    %2635 = torch.aten.add.Tensor %2604, %2634, %int1_3049 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_3050 = torch.constant.int 6
    %2636 = torch.prims.convert_element_type %2635, %int6_3050 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_3051 = torch.constant.int 2
    %2637 = torch.aten.pow.Tensor_Scalar %2636, %int2_3051 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_3052 = torch.constant.int -1
    %2638 = torch.prim.ListConstruct %int-1_3052 : (!torch.int) -> !torch.list<int>
    %true_3053 = torch.constant.bool true
    %none_3054 = torch.constant.none
    %2639 = torch.aten.mean.dim %2637, %2638, %true_3053, %none_3054 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_3055 = torch.constant.float 9.9999997473787516E-6
    %int1_3056 = torch.constant.int 1
    %2640 = torch.aten.add.Scalar %2639, %float9.999990e-06_3055, %int1_3056 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %2641 = torch.aten.rsqrt %2640 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %2642 = torch.aten.mul.Tensor %2636, %2641 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_3057 = torch.constant.int 5
    %2643 = torch.prims.convert_element_type %2642, %int5_3057 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %2644 = torch.aten.mul.Tensor %122, %2643 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_3058 = torch.constant.int 5
    %2645 = torch.prims.convert_element_type %2644, %int5_3058 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_3059 = torch.constant.int -2
    %int-1_3060 = torch.constant.int -1
    %2646 = torch.aten.transpose.int %123, %int-2_3059, %int-1_3060 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_3061 = torch.constant.int 4
    %int4096_3062 = torch.constant.int 4096
    %2647 = torch.prim.ListConstruct %int4_3061, %int4096_3062 : (!torch.int, !torch.int) -> !torch.list<int>
    %2648 = torch.aten.view %2645, %2647 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %2649 = torch.aten.mm %2648, %2646 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_3063 = torch.constant.int 4
    %int1_3064 = torch.constant.int 1
    %int4096_3065 = torch.constant.int 4096
    %2650 = torch.prim.ListConstruct %int4_3063, %int1_3064, %int4096_3065 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2651 = torch.aten.view %2649, %2650 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_3066 = torch.constant.int -2
    %int-1_3067 = torch.constant.int -1
    %2652 = torch.aten.transpose.int %124, %int-2_3066, %int-1_3067 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_3068 = torch.constant.int 4
    %int4096_3069 = torch.constant.int 4096
    %2653 = torch.prim.ListConstruct %int4_3068, %int4096_3069 : (!torch.int, !torch.int) -> !torch.list<int>
    %2654 = torch.aten.view %2645, %2653 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %2655 = torch.aten.mm %2654, %2652 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_3070 = torch.constant.int 4
    %int1_3071 = torch.constant.int 1
    %int1024_3072 = torch.constant.int 1024
    %2656 = torch.prim.ListConstruct %int4_3070, %int1_3071, %int1024_3072 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2657 = torch.aten.view %2655, %2656 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int-2_3073 = torch.constant.int -2
    %int-1_3074 = torch.constant.int -1
    %2658 = torch.aten.transpose.int %125, %int-2_3073, %int-1_3074 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_3075 = torch.constant.int 4
    %int4096_3076 = torch.constant.int 4096
    %2659 = torch.prim.ListConstruct %int4_3075, %int4096_3076 : (!torch.int, !torch.int) -> !torch.list<int>
    %2660 = torch.aten.view %2645, %2659 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %2661 = torch.aten.mm %2660, %2658 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_3077 = torch.constant.int 4
    %int1_3078 = torch.constant.int 1
    %int1024_3079 = torch.constant.int 1024
    %2662 = torch.prim.ListConstruct %int4_3077, %int1_3078, %int1024_3079 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2663 = torch.aten.view %2661, %2662 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int4_3080 = torch.constant.int 4
    %int1_3081 = torch.constant.int 1
    %int32_3082 = torch.constant.int 32
    %int128_3083 = torch.constant.int 128
    %2664 = torch.prim.ListConstruct %int4_3080, %int1_3081, %int32_3082, %int128_3083 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2665 = torch.aten.view %2651, %2664 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,128],f16>
    %int4_3084 = torch.constant.int 4
    %int1_3085 = torch.constant.int 1
    %int8_3086 = torch.constant.int 8
    %int128_3087 = torch.constant.int 128
    %2666 = torch.prim.ListConstruct %int4_3084, %int1_3085, %int8_3086, %int128_3087 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2667 = torch.aten.view %2657, %2666 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int4_3088 = torch.constant.int 4
    %int1_3089 = torch.constant.int 1
    %int8_3090 = torch.constant.int 8
    %int128_3091 = torch.constant.int 128
    %2668 = torch.prim.ListConstruct %int4_3088, %int1_3089, %int8_3090, %int128_3091 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2669 = torch.aten.view %2663, %2668 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int6_3092 = torch.constant.int 6
    %2670 = torch.prims.convert_element_type %2665, %int6_3092 : !torch.vtensor<[4,1,32,128],f16>, !torch.int -> !torch.vtensor<[4,1,32,128],f32>
    %2671 = torch_c.to_builtin_tensor %2670 : !torch.vtensor<[4,1,32,128],f32> -> tensor<4x1x32x128xf32>
    %2672 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %2673 = util.call @sharktank_rotary_embedding_4_1_32_128_f32(%2671, %2672) : (tensor<4x1x32x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x32x128xf32>
    %2674 = torch_c.from_builtin_tensor %2673 : tensor<4x1x32x128xf32> -> !torch.vtensor<[4,1,32,128],f32>
    %int5_3093 = torch.constant.int 5
    %2675 = torch.prims.convert_element_type %2674, %int5_3093 : !torch.vtensor<[4,1,32,128],f32>, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int6_3094 = torch.constant.int 6
    %2676 = torch.prims.convert_element_type %2667, %int6_3094 : !torch.vtensor<[4,1,8,128],f16>, !torch.int -> !torch.vtensor<[4,1,8,128],f32>
    %2677 = torch_c.to_builtin_tensor %2676 : !torch.vtensor<[4,1,8,128],f32> -> tensor<4x1x8x128xf32>
    %2678 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %2679 = util.call @sharktank_rotary_embedding_4_1_8_128_f32(%2677, %2678) : (tensor<4x1x8x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x8x128xf32>
    %2680 = torch_c.from_builtin_tensor %2679 : tensor<4x1x8x128xf32> -> !torch.vtensor<[4,1,8,128],f32>
    %int5_3095 = torch.constant.int 5
    %2681 = torch.prims.convert_element_type %2680, %int5_3095 : !torch.vtensor<[4,1,8,128],f32>, !torch.int -> !torch.vtensor<[4,1,8,128],f16>
    %int32_3096 = torch.constant.int 32
    %2682 = torch.aten.floor_divide.Scalar %arg2, %int32_3096 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_3097 = torch.constant.int 1
    %2683 = torch.aten.unsqueeze %2682, %int1_3097 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3098 = torch.constant.int 1
    %false_3099 = torch.constant.bool false
    %2684 = torch.aten.gather %arg3, %int1_3098, %2683, %false_3099 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_3100 = torch.constant.int 32
    %2685 = torch.aten.remainder.Scalar %arg2, %int32_3100 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_3101 = torch.constant.int 1
    %2686 = torch.aten.unsqueeze %2685, %int1_3101 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_3102 = torch.constant.none
    %2687 = torch.aten.clone %126, %none_3102 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_3103 = torch.constant.int 0
    %2688 = torch.aten.unsqueeze %2687, %int0_3103 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_3104 = torch.constant.int 4
    %int1_3105 = torch.constant.int 1
    %2689 = torch.prim.ListConstruct %int4_3104, %int1_3105 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_3106 = torch.constant.int 1
    %int1_3107 = torch.constant.int 1
    %2690 = torch.prim.ListConstruct %int1_3106, %int1_3107 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_3108 = torch.constant.int 4
    %int0_3109 = torch.constant.int 0
    %cpu_3110 = torch.constant.device "cpu"
    %false_3111 = torch.constant.bool false
    %2691 = torch.aten.empty_strided %2689, %2690, %int4_3108, %int0_3109, %cpu_3110, %false_3111 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int11 = torch.constant.int 11
    %2692 = torch.aten.fill.Scalar %2691, %int11 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_3112 = torch.constant.int 4
    %int1_3113 = torch.constant.int 1
    %2693 = torch.prim.ListConstruct %int4_3112, %int1_3113 : (!torch.int, !torch.int) -> !torch.list<int>
    %2694 = torch.aten.repeat %2688, %2693 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_3114 = torch.constant.int 32
    %2695 = torch.aten.mul.Scalar %2684, %int32_3114 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3115 = torch.constant.int 1
    %2696 = torch.aten.add.Tensor %2695, %2692, %int1_3115 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_3116 = torch.constant.int 2
    %2697 = torch.aten.mul.Scalar %2696, %int2_3116 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3117 = torch.constant.int 1
    %2698 = torch.aten.add.Tensor %2697, %2694, %int1_3117 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_3118 = torch.constant.int 32
    %2699 = torch.aten.mul.Scalar %2698, %int32_3118 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3119 = torch.constant.int 1
    %2700 = torch.aten.add.Tensor %2699, %2686, %int1_3119 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_3120 = torch.constant.int 32
    %int2_3121 = torch.constant.int 2
    %int32_3122 = torch.constant.int 32
    %int8_3123 = torch.constant.int 8
    %int128_3124 = torch.constant.int 128
    %2701 = torch.prim.ListConstruct %437, %int32_3120, %int2_3121, %int32_3122, %int8_3123, %int128_3124 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2702 = torch.aten.view %2538, %2701 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2702, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_3125 = torch.constant.int 32
    %2703 = torch.aten.mul.int %437, %int32_3125 : !torch.int, !torch.int -> !torch.int
    %int2_3126 = torch.constant.int 2
    %2704 = torch.aten.mul.int %2703, %int2_3126 : !torch.int, !torch.int -> !torch.int
    %int32_3127 = torch.constant.int 32
    %2705 = torch.aten.mul.int %2704, %int32_3127 : !torch.int, !torch.int -> !torch.int
    %int8_3128 = torch.constant.int 8
    %int128_3129 = torch.constant.int 128
    %2706 = torch.prim.ListConstruct %2705, %int8_3128, %int128_3129 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2707 = torch.aten.view %2702, %2706 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %2707, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %2708 = torch.prim.ListConstruct %2700 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_3130 = torch.constant.bool false
    %2709 = torch.aten.index_put %2707, %2708, %2681, %false_3130 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %2709, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_3131 = torch.constant.int 32
    %int2_3132 = torch.constant.int 2
    %int32_3133 = torch.constant.int 32
    %int8_3134 = torch.constant.int 8
    %int128_3135 = torch.constant.int 128
    %2710 = torch.prim.ListConstruct %437, %int32_3131, %int2_3132, %int32_3133, %int8_3134, %int128_3135 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2711 = torch.aten.view %2709, %2710 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2711, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_3136 = torch.constant.int 2097152
    %2712 = torch.prim.ListConstruct %437, %int2097152_3136 : (!torch.int, !torch.int) -> !torch.list<int>
    %2713 = torch.aten.view %2711, %2712 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %2713, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_3137 = torch.constant.int 32
    %int2_3138 = torch.constant.int 2
    %int32_3139 = torch.constant.int 32
    %int8_3140 = torch.constant.int 8
    %int128_3141 = torch.constant.int 128
    %2714 = torch.prim.ListConstruct %437, %int32_3137, %int2_3138, %int32_3139, %int8_3140, %int128_3141 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2715 = torch.aten.view %2713, %2714 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2715, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int8_3142 = torch.constant.int 8
    %int128_3143 = torch.constant.int 128
    %2716 = torch.prim.ListConstruct %2705, %int8_3142, %int128_3143 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2717 = torch.aten.view %2715, %2716 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %2717, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_3144 = torch.constant.int 32
    %2718 = torch.aten.floor_divide.Scalar %arg2, %int32_3144 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_3145 = torch.constant.int 1
    %2719 = torch.aten.unsqueeze %2718, %int1_3145 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3146 = torch.constant.int 1
    %false_3147 = torch.constant.bool false
    %2720 = torch.aten.gather %arg3, %int1_3146, %2719, %false_3147 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_3148 = torch.constant.int 32
    %2721 = torch.aten.remainder.Scalar %arg2, %int32_3148 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_3149 = torch.constant.int 1
    %2722 = torch.aten.unsqueeze %2721, %int1_3149 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_3150 = torch.constant.none
    %2723 = torch.aten.clone %127, %none_3150 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_3151 = torch.constant.int 0
    %2724 = torch.aten.unsqueeze %2723, %int0_3151 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_3152 = torch.constant.int 4
    %int1_3153 = torch.constant.int 1
    %2725 = torch.prim.ListConstruct %int4_3152, %int1_3153 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_3154 = torch.constant.int 1
    %int1_3155 = torch.constant.int 1
    %2726 = torch.prim.ListConstruct %int1_3154, %int1_3155 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_3156 = torch.constant.int 4
    %int0_3157 = torch.constant.int 0
    %cpu_3158 = torch.constant.device "cpu"
    %false_3159 = torch.constant.bool false
    %2727 = torch.aten.empty_strided %2725, %2726, %int4_3156, %int0_3157, %cpu_3158, %false_3159 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int11_3160 = torch.constant.int 11
    %2728 = torch.aten.fill.Scalar %2727, %int11_3160 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_3161 = torch.constant.int 4
    %int1_3162 = torch.constant.int 1
    %2729 = torch.prim.ListConstruct %int4_3161, %int1_3162 : (!torch.int, !torch.int) -> !torch.list<int>
    %2730 = torch.aten.repeat %2724, %2729 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_3163 = torch.constant.int 32
    %2731 = torch.aten.mul.Scalar %2720, %int32_3163 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3164 = torch.constant.int 1
    %2732 = torch.aten.add.Tensor %2731, %2728, %int1_3164 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_3165 = torch.constant.int 2
    %2733 = torch.aten.mul.Scalar %2732, %int2_3165 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3166 = torch.constant.int 1
    %2734 = torch.aten.add.Tensor %2733, %2730, %int1_3166 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_3167 = torch.constant.int 32
    %2735 = torch.aten.mul.Scalar %2734, %int32_3167 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3168 = torch.constant.int 1
    %2736 = torch.aten.add.Tensor %2735, %2722, %int1_3168 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %2737 = torch.prim.ListConstruct %2736 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_3169 = torch.constant.bool false
    %2738 = torch.aten.index_put %2717, %2737, %2669, %false_3169 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %2738, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_3170 = torch.constant.int 32
    %int2_3171 = torch.constant.int 2
    %int32_3172 = torch.constant.int 32
    %int8_3173 = torch.constant.int 8
    %int128_3174 = torch.constant.int 128
    %2739 = torch.prim.ListConstruct %437, %int32_3170, %int2_3171, %int32_3172, %int8_3173, %int128_3174 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2740 = torch.aten.view %2738, %2739 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2740, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_3175 = torch.constant.int 2097152
    %2741 = torch.prim.ListConstruct %437, %int2097152_3175 : (!torch.int, !torch.int) -> !torch.list<int>
    %2742 = torch.aten.view %2740, %2741 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %2742, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int4_3176 = torch.constant.int 4
    %2743 = torch.prim.ListConstruct %int4_3176, %358 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_3177 = torch.constant.int 1
    %2744 = torch.prim.ListConstruct %358, %int1_3177 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_3178 = torch.constant.int 4
    %int0_3179 = torch.constant.int 0
    %cpu_3180 = torch.constant.device "cpu"
    %false_3181 = torch.constant.bool false
    %2745 = torch.aten.empty_strided %2743, %2744, %int4_3178, %int0_3179, %cpu_3180, %false_3181 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2745, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int11_3182 = torch.constant.int 11
    %2746 = torch.aten.fill.Scalar %2745, %int11_3182 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2746, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int32_3183 = torch.constant.int 32
    %2747 = torch.aten.mul.Scalar %arg3, %int32_3183 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2747, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_3184 = torch.constant.int 1
    %2748 = torch.aten.add.Tensor %2747, %2746, %int1_3184 : !torch.vtensor<[4,?],si64>, !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2748, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_3185 = torch.constant.int 4
    %2749 = torch.aten.mul.int %int4_3185, %358 : !torch.int, !torch.int -> !torch.int
    %2750 = torch.prim.ListConstruct %2749 : (!torch.int) -> !torch.list<int>
    %2751 = torch.aten.view %2748, %2750 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2751, [%356], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_3186 = torch.constant.int 32
    %int2_3187 = torch.constant.int 2
    %int32_3188 = torch.constant.int 32
    %int8_3189 = torch.constant.int 8
    %int128_3190 = torch.constant.int 128
    %2752 = torch.prim.ListConstruct %437, %int32_3186, %int2_3187, %int32_3188, %int8_3189, %int128_3190 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2753 = torch.aten.view %2742, %2752 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2753, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_3191 = torch.constant.int 32
    %2754 = torch.aten.mul.int %437, %int32_3191 : !torch.int, !torch.int -> !torch.int
    %int2_3192 = torch.constant.int 2
    %int32_3193 = torch.constant.int 32
    %int8_3194 = torch.constant.int 8
    %int128_3195 = torch.constant.int 128
    %2755 = torch.prim.ListConstruct %2754, %int2_3192, %int32_3193, %int8_3194, %int128_3195 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2756 = torch.aten.view %2753, %2755 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %2756, [%357], affine_map<()[s0] -> (s0 * 32, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int0_3196 = torch.constant.int 0
    %2757 = torch.aten.index_select %2756, %int0_3196, %2751 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.int, !torch.vtensor<[?],si64> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %2757, [%356], affine_map<()[s0] -> (s0 * 4, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int4_3197 = torch.constant.int 4
    %int2_3198 = torch.constant.int 2
    %int32_3199 = torch.constant.int 32
    %int8_3200 = torch.constant.int 8
    %int128_3201 = torch.constant.int 128
    %2758 = torch.prim.ListConstruct %int4_3197, %358, %int2_3198, %int32_3199, %int8_3200, %int128_3201 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2759 = torch.aten.view %2757, %2758 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %2759, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int0_3202 = torch.constant.int 0
    %int0_3203 = torch.constant.int 0
    %int9223372036854775807_3204 = torch.constant.int 9223372036854775807
    %int1_3205 = torch.constant.int 1
    %2760 = torch.aten.slice.Tensor %2759, %int0_3202, %int0_3203, %int9223372036854775807_3204, %int1_3205 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %2760, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_3206 = torch.constant.int 1
    %int0_3207 = torch.constant.int 0
    %int9223372036854775807_3208 = torch.constant.int 9223372036854775807
    %int1_3209 = torch.constant.int 1
    %2761 = torch.aten.slice.Tensor %2760, %int1_3206, %int0_3207, %int9223372036854775807_3208, %int1_3209 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %2761, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_3210 = torch.constant.int 2
    %int0_3211 = torch.constant.int 0
    %2762 = torch.aten.select.int %2761, %int2_3210, %int0_3211 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2762, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int32_3212 = torch.constant.int 32
    %2763 = torch.aten.mul.int %358, %int32_3212 : !torch.int, !torch.int -> !torch.int
    %int2_3213 = torch.constant.int 2
    %int0_3214 = torch.constant.int 0
    %int1_3215 = torch.constant.int 1
    %2764 = torch.aten.slice.Tensor %2762, %int2_3213, %int0_3214, %2763, %int1_3215 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2764, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_3216 = torch.constant.int 0
    %2765 = torch.aten.clone %2764, %int0_3216 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2765, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_3217 = torch.constant.int 1
    %2766 = torch.aten.size.int %2761, %int1_3217 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_3218 = torch.constant.int 32
    %2767 = torch.aten.mul.int %2766, %int32_3218 : !torch.int, !torch.int -> !torch.int
    %int4_3219 = torch.constant.int 4
    %int8_3220 = torch.constant.int 8
    %int128_3221 = torch.constant.int 128
    %2768 = torch.prim.ListConstruct %int4_3219, %2767, %int8_3220, %int128_3221 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2769 = torch.aten._unsafe_view %2765, %2768 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %2769, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_3222 = torch.constant.int 0
    %int0_3223 = torch.constant.int 0
    %int9223372036854775807_3224 = torch.constant.int 9223372036854775807
    %int1_3225 = torch.constant.int 1
    %2770 = torch.aten.slice.Tensor %2769, %int0_3222, %int0_3223, %int9223372036854775807_3224, %int1_3225 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %2770, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_3226 = torch.constant.int 0
    %int0_3227 = torch.constant.int 0
    %int9223372036854775807_3228 = torch.constant.int 9223372036854775807
    %int1_3229 = torch.constant.int 1
    %2771 = torch.aten.slice.Tensor %2759, %int0_3226, %int0_3227, %int9223372036854775807_3228, %int1_3229 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %2771, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_3230 = torch.constant.int 1
    %int0_3231 = torch.constant.int 0
    %int9223372036854775807_3232 = torch.constant.int 9223372036854775807
    %int1_3233 = torch.constant.int 1
    %2772 = torch.aten.slice.Tensor %2771, %int1_3230, %int0_3231, %int9223372036854775807_3232, %int1_3233 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %2772, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_3234 = torch.constant.int 2
    %int1_3235 = torch.constant.int 1
    %2773 = torch.aten.select.int %2772, %int2_3234, %int1_3235 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2773, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int2_3236 = torch.constant.int 2
    %int0_3237 = torch.constant.int 0
    %int1_3238 = torch.constant.int 1
    %2774 = torch.aten.slice.Tensor %2773, %int2_3236, %int0_3237, %2763, %int1_3238 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2774, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_3239 = torch.constant.int 0
    %2775 = torch.aten.clone %2774, %int0_3239 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2775, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_3240 = torch.constant.int 1
    %2776 = torch.aten.size.int %2772, %int1_3240 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_3241 = torch.constant.int 32
    %2777 = torch.aten.mul.int %2776, %int32_3241 : !torch.int, !torch.int -> !torch.int
    %int4_3242 = torch.constant.int 4
    %int8_3243 = torch.constant.int 8
    %int128_3244 = torch.constant.int 128
    %2778 = torch.prim.ListConstruct %int4_3242, %2777, %int8_3243, %int128_3244 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2779 = torch.aten._unsafe_view %2775, %2778 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %2779, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_3245 = torch.constant.int 0
    %int0_3246 = torch.constant.int 0
    %int9223372036854775807_3247 = torch.constant.int 9223372036854775807
    %int1_3248 = torch.constant.int 1
    %2780 = torch.aten.slice.Tensor %2779, %int0_3245, %int0_3246, %int9223372036854775807_3247, %int1_3248 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %2780, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int-2_3249 = torch.constant.int -2
    %2781 = torch.aten.unsqueeze %2770, %int-2_3249 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %2781, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_3250 = torch.constant.int 1
    %2782 = torch.aten.size.int %2769, %int1_3250 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_3251 = torch.constant.int 4
    %int8_3252 = torch.constant.int 8
    %int4_3253 = torch.constant.int 4
    %int128_3254 = torch.constant.int 128
    %2783 = torch.prim.ListConstruct %int4_3251, %2782, %int8_3252, %int4_3253, %int128_3254 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_3255 = torch.constant.bool false
    %2784 = torch.aten.expand %2781, %2783, %false_3255 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2784, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_3256 = torch.constant.int 0
    %2785 = torch.aten.clone %2784, %int0_3256 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2785, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_3257 = torch.constant.int 4
    %int32_3258 = torch.constant.int 32
    %int128_3259 = torch.constant.int 128
    %2786 = torch.prim.ListConstruct %int4_3257, %2782, %int32_3258, %int128_3259 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2787 = torch.aten._unsafe_view %2785, %2786 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2787, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_3260 = torch.constant.int -2
    %2788 = torch.aten.unsqueeze %2780, %int-2_3260 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %2788, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_3261 = torch.constant.int 1
    %2789 = torch.aten.size.int %2779, %int1_3261 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_3262 = torch.constant.int 4
    %int8_3263 = torch.constant.int 8
    %int4_3264 = torch.constant.int 4
    %int128_3265 = torch.constant.int 128
    %2790 = torch.prim.ListConstruct %int4_3262, %2789, %int8_3263, %int4_3264, %int128_3265 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_3266 = torch.constant.bool false
    %2791 = torch.aten.expand %2788, %2790, %false_3266 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2791, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_3267 = torch.constant.int 0
    %2792 = torch.aten.clone %2791, %int0_3267 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2792, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_3268 = torch.constant.int 4
    %int32_3269 = torch.constant.int 32
    %int128_3270 = torch.constant.int 128
    %2793 = torch.prim.ListConstruct %int4_3268, %2789, %int32_3269, %int128_3270 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2794 = torch.aten._unsafe_view %2792, %2793 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2794, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_3271 = torch.constant.int 1
    %int2_3272 = torch.constant.int 2
    %2795 = torch.aten.transpose.int %2675, %int1_3271, %int2_3272 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,128],f16>
    %int1_3273 = torch.constant.int 1
    %int2_3274 = torch.constant.int 2
    %2796 = torch.aten.transpose.int %2787, %int1_3273, %int2_3274 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %2796, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_3275 = torch.constant.int 1
    %int2_3276 = torch.constant.int 2
    %2797 = torch.aten.transpose.int %2794, %int1_3275, %int2_3276 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %2797, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_3277 = torch.constant.float 0.000000e+00
    %false_3278 = torch.constant.bool false
    %none_3279 = torch.constant.none
    %2798:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2795, %2796, %2797, %float0.000000e00_3277, %false_3278, %368, %none_3279) : (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.vtensor<[4,1,1,?],f16>, !torch.none) -> (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,1],f32>) 
    %int1_3280 = torch.constant.int 1
    %int2_3281 = torch.constant.int 2
    %2799 = torch.aten.transpose.int %2798#0, %int1_3280, %int2_3281 : !torch.vtensor<[4,32,1,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int4_3282 = torch.constant.int 4
    %int1_3283 = torch.constant.int 1
    %int4096_3284 = torch.constant.int 4096
    %2800 = torch.prim.ListConstruct %int4_3282, %int1_3283, %int4096_3284 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2801 = torch.aten.view %2799, %2800 : !torch.vtensor<[4,1,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_3285 = torch.constant.int -2
    %int-1_3286 = torch.constant.int -1
    %2802 = torch.aten.transpose.int %128, %int-2_3285, %int-1_3286 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_3287 = torch.constant.int 4
    %int4096_3288 = torch.constant.int 4096
    %2803 = torch.prim.ListConstruct %int4_3287, %int4096_3288 : (!torch.int, !torch.int) -> !torch.list<int>
    %2804 = torch.aten.view %2801, %2803 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %2805 = torch.aten.mm %2804, %2802 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_3289 = torch.constant.int 4
    %int1_3290 = torch.constant.int 1
    %int4096_3291 = torch.constant.int 4096
    %2806 = torch.prim.ListConstruct %int4_3289, %int1_3290, %int4096_3291 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2807 = torch.aten.view %2805, %2806 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_3292 = torch.constant.int 1
    %2808 = torch.aten.add.Tensor %2635, %2807, %int1_3292 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_3293 = torch.constant.int 6
    %2809 = torch.prims.convert_element_type %2808, %int6_3293 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_3294 = torch.constant.int 2
    %2810 = torch.aten.pow.Tensor_Scalar %2809, %int2_3294 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_3295 = torch.constant.int -1
    %2811 = torch.prim.ListConstruct %int-1_3295 : (!torch.int) -> !torch.list<int>
    %true_3296 = torch.constant.bool true
    %none_3297 = torch.constant.none
    %2812 = torch.aten.mean.dim %2810, %2811, %true_3296, %none_3297 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_3298 = torch.constant.float 9.9999997473787516E-6
    %int1_3299 = torch.constant.int 1
    %2813 = torch.aten.add.Scalar %2812, %float9.999990e-06_3298, %int1_3299 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %2814 = torch.aten.rsqrt %2813 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %2815 = torch.aten.mul.Tensor %2809, %2814 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_3300 = torch.constant.int 5
    %2816 = torch.prims.convert_element_type %2815, %int5_3300 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %2817 = torch.aten.mul.Tensor %129, %2816 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_3301 = torch.constant.int 5
    %2818 = torch.prims.convert_element_type %2817, %int5_3301 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_3302 = torch.constant.int -2
    %int-1_3303 = torch.constant.int -1
    %2819 = torch.aten.transpose.int %130, %int-2_3302, %int-1_3303 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_3304 = torch.constant.int 4
    %int4096_3305 = torch.constant.int 4096
    %2820 = torch.prim.ListConstruct %int4_3304, %int4096_3305 : (!torch.int, !torch.int) -> !torch.list<int>
    %2821 = torch.aten.view %2818, %2820 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %2822 = torch.aten.mm %2821, %2819 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_3306 = torch.constant.int 4
    %int1_3307 = torch.constant.int 1
    %int14336_3308 = torch.constant.int 14336
    %2823 = torch.prim.ListConstruct %int4_3306, %int1_3307, %int14336_3308 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2824 = torch.aten.view %2822, %2823 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %2825 = torch.aten.silu %2824 : !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_3309 = torch.constant.int -2
    %int-1_3310 = torch.constant.int -1
    %2826 = torch.aten.transpose.int %131, %int-2_3309, %int-1_3310 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_3311 = torch.constant.int 4
    %int4096_3312 = torch.constant.int 4096
    %2827 = torch.prim.ListConstruct %int4_3311, %int4096_3312 : (!torch.int, !torch.int) -> !torch.list<int>
    %2828 = torch.aten.view %2818, %2827 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %2829 = torch.aten.mm %2828, %2826 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_3313 = torch.constant.int 4
    %int1_3314 = torch.constant.int 1
    %int14336_3315 = torch.constant.int 14336
    %2830 = torch.prim.ListConstruct %int4_3313, %int1_3314, %int14336_3315 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2831 = torch.aten.view %2829, %2830 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %2832 = torch.aten.mul.Tensor %2825, %2831 : !torch.vtensor<[4,1,14336],f16>, !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_3316 = torch.constant.int -2
    %int-1_3317 = torch.constant.int -1
    %2833 = torch.aten.transpose.int %132, %int-2_3316, %int-1_3317 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int4_3318 = torch.constant.int 4
    %int14336_3319 = torch.constant.int 14336
    %2834 = torch.prim.ListConstruct %int4_3318, %int14336_3319 : (!torch.int, !torch.int) -> !torch.list<int>
    %2835 = torch.aten.view %2832, %2834 : !torch.vtensor<[4,1,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,14336],f16>
    %2836 = torch.aten.mm %2835, %2833 : !torch.vtensor<[4,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_3320 = torch.constant.int 4
    %int1_3321 = torch.constant.int 1
    %int4096_3322 = torch.constant.int 4096
    %2837 = torch.prim.ListConstruct %int4_3320, %int1_3321, %int4096_3322 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2838 = torch.aten.view %2836, %2837 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_3323 = torch.constant.int 1
    %2839 = torch.aten.add.Tensor %2808, %2838, %int1_3323 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_3324 = torch.constant.int 6
    %2840 = torch.prims.convert_element_type %2839, %int6_3324 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_3325 = torch.constant.int 2
    %2841 = torch.aten.pow.Tensor_Scalar %2840, %int2_3325 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_3326 = torch.constant.int -1
    %2842 = torch.prim.ListConstruct %int-1_3326 : (!torch.int) -> !torch.list<int>
    %true_3327 = torch.constant.bool true
    %none_3328 = torch.constant.none
    %2843 = torch.aten.mean.dim %2841, %2842, %true_3327, %none_3328 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_3329 = torch.constant.float 9.9999997473787516E-6
    %int1_3330 = torch.constant.int 1
    %2844 = torch.aten.add.Scalar %2843, %float9.999990e-06_3329, %int1_3330 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %2845 = torch.aten.rsqrt %2844 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %2846 = torch.aten.mul.Tensor %2840, %2845 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_3331 = torch.constant.int 5
    %2847 = torch.prims.convert_element_type %2846, %int5_3331 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %2848 = torch.aten.mul.Tensor %133, %2847 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_3332 = torch.constant.int 5
    %2849 = torch.prims.convert_element_type %2848, %int5_3332 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_3333 = torch.constant.int -2
    %int-1_3334 = torch.constant.int -1
    %2850 = torch.aten.transpose.int %134, %int-2_3333, %int-1_3334 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_3335 = torch.constant.int 4
    %int4096_3336 = torch.constant.int 4096
    %2851 = torch.prim.ListConstruct %int4_3335, %int4096_3336 : (!torch.int, !torch.int) -> !torch.list<int>
    %2852 = torch.aten.view %2849, %2851 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %2853 = torch.aten.mm %2852, %2850 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_3337 = torch.constant.int 4
    %int1_3338 = torch.constant.int 1
    %int4096_3339 = torch.constant.int 4096
    %2854 = torch.prim.ListConstruct %int4_3337, %int1_3338, %int4096_3339 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2855 = torch.aten.view %2853, %2854 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_3340 = torch.constant.int -2
    %int-1_3341 = torch.constant.int -1
    %2856 = torch.aten.transpose.int %135, %int-2_3340, %int-1_3341 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_3342 = torch.constant.int 4
    %int4096_3343 = torch.constant.int 4096
    %2857 = torch.prim.ListConstruct %int4_3342, %int4096_3343 : (!torch.int, !torch.int) -> !torch.list<int>
    %2858 = torch.aten.view %2849, %2857 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %2859 = torch.aten.mm %2858, %2856 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_3344 = torch.constant.int 4
    %int1_3345 = torch.constant.int 1
    %int1024_3346 = torch.constant.int 1024
    %2860 = torch.prim.ListConstruct %int4_3344, %int1_3345, %int1024_3346 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2861 = torch.aten.view %2859, %2860 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int-2_3347 = torch.constant.int -2
    %int-1_3348 = torch.constant.int -1
    %2862 = torch.aten.transpose.int %136, %int-2_3347, %int-1_3348 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_3349 = torch.constant.int 4
    %int4096_3350 = torch.constant.int 4096
    %2863 = torch.prim.ListConstruct %int4_3349, %int4096_3350 : (!torch.int, !torch.int) -> !torch.list<int>
    %2864 = torch.aten.view %2849, %2863 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %2865 = torch.aten.mm %2864, %2862 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_3351 = torch.constant.int 4
    %int1_3352 = torch.constant.int 1
    %int1024_3353 = torch.constant.int 1024
    %2866 = torch.prim.ListConstruct %int4_3351, %int1_3352, %int1024_3353 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2867 = torch.aten.view %2865, %2866 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int4_3354 = torch.constant.int 4
    %int1_3355 = torch.constant.int 1
    %int32_3356 = torch.constant.int 32
    %int128_3357 = torch.constant.int 128
    %2868 = torch.prim.ListConstruct %int4_3354, %int1_3355, %int32_3356, %int128_3357 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2869 = torch.aten.view %2855, %2868 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,128],f16>
    %int4_3358 = torch.constant.int 4
    %int1_3359 = torch.constant.int 1
    %int8_3360 = torch.constant.int 8
    %int128_3361 = torch.constant.int 128
    %2870 = torch.prim.ListConstruct %int4_3358, %int1_3359, %int8_3360, %int128_3361 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2871 = torch.aten.view %2861, %2870 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int4_3362 = torch.constant.int 4
    %int1_3363 = torch.constant.int 1
    %int8_3364 = torch.constant.int 8
    %int128_3365 = torch.constant.int 128
    %2872 = torch.prim.ListConstruct %int4_3362, %int1_3363, %int8_3364, %int128_3365 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2873 = torch.aten.view %2867, %2872 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int6_3366 = torch.constant.int 6
    %2874 = torch.prims.convert_element_type %2869, %int6_3366 : !torch.vtensor<[4,1,32,128],f16>, !torch.int -> !torch.vtensor<[4,1,32,128],f32>
    %2875 = torch_c.to_builtin_tensor %2874 : !torch.vtensor<[4,1,32,128],f32> -> tensor<4x1x32x128xf32>
    %2876 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %2877 = util.call @sharktank_rotary_embedding_4_1_32_128_f32(%2875, %2876) : (tensor<4x1x32x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x32x128xf32>
    %2878 = torch_c.from_builtin_tensor %2877 : tensor<4x1x32x128xf32> -> !torch.vtensor<[4,1,32,128],f32>
    %int5_3367 = torch.constant.int 5
    %2879 = torch.prims.convert_element_type %2878, %int5_3367 : !torch.vtensor<[4,1,32,128],f32>, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int6_3368 = torch.constant.int 6
    %2880 = torch.prims.convert_element_type %2871, %int6_3368 : !torch.vtensor<[4,1,8,128],f16>, !torch.int -> !torch.vtensor<[4,1,8,128],f32>
    %2881 = torch_c.to_builtin_tensor %2880 : !torch.vtensor<[4,1,8,128],f32> -> tensor<4x1x8x128xf32>
    %2882 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %2883 = util.call @sharktank_rotary_embedding_4_1_8_128_f32(%2881, %2882) : (tensor<4x1x8x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x8x128xf32>
    %2884 = torch_c.from_builtin_tensor %2883 : tensor<4x1x8x128xf32> -> !torch.vtensor<[4,1,8,128],f32>
    %int5_3369 = torch.constant.int 5
    %2885 = torch.prims.convert_element_type %2884, %int5_3369 : !torch.vtensor<[4,1,8,128],f32>, !torch.int -> !torch.vtensor<[4,1,8,128],f16>
    %int32_3370 = torch.constant.int 32
    %2886 = torch.aten.floor_divide.Scalar %arg2, %int32_3370 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_3371 = torch.constant.int 1
    %2887 = torch.aten.unsqueeze %2886, %int1_3371 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3372 = torch.constant.int 1
    %false_3373 = torch.constant.bool false
    %2888 = torch.aten.gather %arg3, %int1_3372, %2887, %false_3373 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_3374 = torch.constant.int 32
    %2889 = torch.aten.remainder.Scalar %arg2, %int32_3374 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_3375 = torch.constant.int 1
    %2890 = torch.aten.unsqueeze %2889, %int1_3375 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_3376 = torch.constant.none
    %2891 = torch.aten.clone %137, %none_3376 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_3377 = torch.constant.int 0
    %2892 = torch.aten.unsqueeze %2891, %int0_3377 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_3378 = torch.constant.int 4
    %int1_3379 = torch.constant.int 1
    %2893 = torch.prim.ListConstruct %int4_3378, %int1_3379 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_3380 = torch.constant.int 1
    %int1_3381 = torch.constant.int 1
    %2894 = torch.prim.ListConstruct %int1_3380, %int1_3381 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_3382 = torch.constant.int 4
    %int0_3383 = torch.constant.int 0
    %cpu_3384 = torch.constant.device "cpu"
    %false_3385 = torch.constant.bool false
    %2895 = torch.aten.empty_strided %2893, %2894, %int4_3382, %int0_3383, %cpu_3384, %false_3385 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int12 = torch.constant.int 12
    %2896 = torch.aten.fill.Scalar %2895, %int12 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_3386 = torch.constant.int 4
    %int1_3387 = torch.constant.int 1
    %2897 = torch.prim.ListConstruct %int4_3386, %int1_3387 : (!torch.int, !torch.int) -> !torch.list<int>
    %2898 = torch.aten.repeat %2892, %2897 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_3388 = torch.constant.int 32
    %2899 = torch.aten.mul.Scalar %2888, %int32_3388 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3389 = torch.constant.int 1
    %2900 = torch.aten.add.Tensor %2899, %2896, %int1_3389 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_3390 = torch.constant.int 2
    %2901 = torch.aten.mul.Scalar %2900, %int2_3390 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3391 = torch.constant.int 1
    %2902 = torch.aten.add.Tensor %2901, %2898, %int1_3391 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_3392 = torch.constant.int 32
    %2903 = torch.aten.mul.Scalar %2902, %int32_3392 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3393 = torch.constant.int 1
    %2904 = torch.aten.add.Tensor %2903, %2890, %int1_3393 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_3394 = torch.constant.int 32
    %int2_3395 = torch.constant.int 2
    %int32_3396 = torch.constant.int 32
    %int8_3397 = torch.constant.int 8
    %int128_3398 = torch.constant.int 128
    %2905 = torch.prim.ListConstruct %437, %int32_3394, %int2_3395, %int32_3396, %int8_3397, %int128_3398 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2906 = torch.aten.view %2742, %2905 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2906, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_3399 = torch.constant.int 32
    %2907 = torch.aten.mul.int %437, %int32_3399 : !torch.int, !torch.int -> !torch.int
    %int2_3400 = torch.constant.int 2
    %2908 = torch.aten.mul.int %2907, %int2_3400 : !torch.int, !torch.int -> !torch.int
    %int32_3401 = torch.constant.int 32
    %2909 = torch.aten.mul.int %2908, %int32_3401 : !torch.int, !torch.int -> !torch.int
    %int8_3402 = torch.constant.int 8
    %int128_3403 = torch.constant.int 128
    %2910 = torch.prim.ListConstruct %2909, %int8_3402, %int128_3403 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2911 = torch.aten.view %2906, %2910 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %2911, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %2912 = torch.prim.ListConstruct %2904 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_3404 = torch.constant.bool false
    %2913 = torch.aten.index_put %2911, %2912, %2885, %false_3404 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %2913, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_3405 = torch.constant.int 32
    %int2_3406 = torch.constant.int 2
    %int32_3407 = torch.constant.int 32
    %int8_3408 = torch.constant.int 8
    %int128_3409 = torch.constant.int 128
    %2914 = torch.prim.ListConstruct %437, %int32_3405, %int2_3406, %int32_3407, %int8_3408, %int128_3409 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2915 = torch.aten.view %2913, %2914 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2915, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_3410 = torch.constant.int 2097152
    %2916 = torch.prim.ListConstruct %437, %int2097152_3410 : (!torch.int, !torch.int) -> !torch.list<int>
    %2917 = torch.aten.view %2915, %2916 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %2917, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_3411 = torch.constant.int 32
    %int2_3412 = torch.constant.int 2
    %int32_3413 = torch.constant.int 32
    %int8_3414 = torch.constant.int 8
    %int128_3415 = torch.constant.int 128
    %2918 = torch.prim.ListConstruct %437, %int32_3411, %int2_3412, %int32_3413, %int8_3414, %int128_3415 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2919 = torch.aten.view %2917, %2918 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2919, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int8_3416 = torch.constant.int 8
    %int128_3417 = torch.constant.int 128
    %2920 = torch.prim.ListConstruct %2909, %int8_3416, %int128_3417 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2921 = torch.aten.view %2919, %2920 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %2921, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_3418 = torch.constant.int 32
    %2922 = torch.aten.floor_divide.Scalar %arg2, %int32_3418 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_3419 = torch.constant.int 1
    %2923 = torch.aten.unsqueeze %2922, %int1_3419 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3420 = torch.constant.int 1
    %false_3421 = torch.constant.bool false
    %2924 = torch.aten.gather %arg3, %int1_3420, %2923, %false_3421 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_3422 = torch.constant.int 32
    %2925 = torch.aten.remainder.Scalar %arg2, %int32_3422 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_3423 = torch.constant.int 1
    %2926 = torch.aten.unsqueeze %2925, %int1_3423 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_3424 = torch.constant.none
    %2927 = torch.aten.clone %138, %none_3424 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_3425 = torch.constant.int 0
    %2928 = torch.aten.unsqueeze %2927, %int0_3425 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_3426 = torch.constant.int 4
    %int1_3427 = torch.constant.int 1
    %2929 = torch.prim.ListConstruct %int4_3426, %int1_3427 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_3428 = torch.constant.int 1
    %int1_3429 = torch.constant.int 1
    %2930 = torch.prim.ListConstruct %int1_3428, %int1_3429 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_3430 = torch.constant.int 4
    %int0_3431 = torch.constant.int 0
    %cpu_3432 = torch.constant.device "cpu"
    %false_3433 = torch.constant.bool false
    %2931 = torch.aten.empty_strided %2929, %2930, %int4_3430, %int0_3431, %cpu_3432, %false_3433 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int12_3434 = torch.constant.int 12
    %2932 = torch.aten.fill.Scalar %2931, %int12_3434 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_3435 = torch.constant.int 4
    %int1_3436 = torch.constant.int 1
    %2933 = torch.prim.ListConstruct %int4_3435, %int1_3436 : (!torch.int, !torch.int) -> !torch.list<int>
    %2934 = torch.aten.repeat %2928, %2933 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_3437 = torch.constant.int 32
    %2935 = torch.aten.mul.Scalar %2924, %int32_3437 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3438 = torch.constant.int 1
    %2936 = torch.aten.add.Tensor %2935, %2932, %int1_3438 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_3439 = torch.constant.int 2
    %2937 = torch.aten.mul.Scalar %2936, %int2_3439 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3440 = torch.constant.int 1
    %2938 = torch.aten.add.Tensor %2937, %2934, %int1_3440 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_3441 = torch.constant.int 32
    %2939 = torch.aten.mul.Scalar %2938, %int32_3441 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3442 = torch.constant.int 1
    %2940 = torch.aten.add.Tensor %2939, %2926, %int1_3442 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %2941 = torch.prim.ListConstruct %2940 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_3443 = torch.constant.bool false
    %2942 = torch.aten.index_put %2921, %2941, %2873, %false_3443 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %2942, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_3444 = torch.constant.int 32
    %int2_3445 = torch.constant.int 2
    %int32_3446 = torch.constant.int 32
    %int8_3447 = torch.constant.int 8
    %int128_3448 = torch.constant.int 128
    %2943 = torch.prim.ListConstruct %437, %int32_3444, %int2_3445, %int32_3446, %int8_3447, %int128_3448 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2944 = torch.aten.view %2942, %2943 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2944, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_3449 = torch.constant.int 2097152
    %2945 = torch.prim.ListConstruct %437, %int2097152_3449 : (!torch.int, !torch.int) -> !torch.list<int>
    %2946 = torch.aten.view %2944, %2945 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %2946, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int4_3450 = torch.constant.int 4
    %2947 = torch.prim.ListConstruct %int4_3450, %358 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_3451 = torch.constant.int 1
    %2948 = torch.prim.ListConstruct %358, %int1_3451 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_3452 = torch.constant.int 4
    %int0_3453 = torch.constant.int 0
    %cpu_3454 = torch.constant.device "cpu"
    %false_3455 = torch.constant.bool false
    %2949 = torch.aten.empty_strided %2947, %2948, %int4_3452, %int0_3453, %cpu_3454, %false_3455 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2949, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int12_3456 = torch.constant.int 12
    %2950 = torch.aten.fill.Scalar %2949, %int12_3456 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2950, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int32_3457 = torch.constant.int 32
    %2951 = torch.aten.mul.Scalar %arg3, %int32_3457 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2951, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_3458 = torch.constant.int 1
    %2952 = torch.aten.add.Tensor %2951, %2950, %int1_3458 : !torch.vtensor<[4,?],si64>, !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %2952, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_3459 = torch.constant.int 4
    %2953 = torch.aten.mul.int %int4_3459, %358 : !torch.int, !torch.int -> !torch.int
    %2954 = torch.prim.ListConstruct %2953 : (!torch.int) -> !torch.list<int>
    %2955 = torch.aten.view %2952, %2954 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %2955, [%356], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_3460 = torch.constant.int 32
    %int2_3461 = torch.constant.int 2
    %int32_3462 = torch.constant.int 32
    %int8_3463 = torch.constant.int 8
    %int128_3464 = torch.constant.int 128
    %2956 = torch.prim.ListConstruct %437, %int32_3460, %int2_3461, %int32_3462, %int8_3463, %int128_3464 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2957 = torch.aten.view %2946, %2956 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %2957, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_3465 = torch.constant.int 32
    %2958 = torch.aten.mul.int %437, %int32_3465 : !torch.int, !torch.int -> !torch.int
    %int2_3466 = torch.constant.int 2
    %int32_3467 = torch.constant.int 32
    %int8_3468 = torch.constant.int 8
    %int128_3469 = torch.constant.int 128
    %2959 = torch.prim.ListConstruct %2958, %int2_3466, %int32_3467, %int8_3468, %int128_3469 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2960 = torch.aten.view %2957, %2959 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %2960, [%357], affine_map<()[s0] -> (s0 * 32, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int0_3470 = torch.constant.int 0
    %2961 = torch.aten.index_select %2960, %int0_3470, %2955 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.int, !torch.vtensor<[?],si64> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %2961, [%356], affine_map<()[s0] -> (s0 * 4, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int4_3471 = torch.constant.int 4
    %int2_3472 = torch.constant.int 2
    %int32_3473 = torch.constant.int 32
    %int8_3474 = torch.constant.int 8
    %int128_3475 = torch.constant.int 128
    %2962 = torch.prim.ListConstruct %int4_3471, %358, %int2_3472, %int32_3473, %int8_3474, %int128_3475 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2963 = torch.aten.view %2961, %2962 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %2963, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int0_3476 = torch.constant.int 0
    %int0_3477 = torch.constant.int 0
    %int9223372036854775807_3478 = torch.constant.int 9223372036854775807
    %int1_3479 = torch.constant.int 1
    %2964 = torch.aten.slice.Tensor %2963, %int0_3476, %int0_3477, %int9223372036854775807_3478, %int1_3479 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %2964, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_3480 = torch.constant.int 1
    %int0_3481 = torch.constant.int 0
    %int9223372036854775807_3482 = torch.constant.int 9223372036854775807
    %int1_3483 = torch.constant.int 1
    %2965 = torch.aten.slice.Tensor %2964, %int1_3480, %int0_3481, %int9223372036854775807_3482, %int1_3483 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %2965, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_3484 = torch.constant.int 2
    %int0_3485 = torch.constant.int 0
    %2966 = torch.aten.select.int %2965, %int2_3484, %int0_3485 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2966, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int32_3486 = torch.constant.int 32
    %2967 = torch.aten.mul.int %358, %int32_3486 : !torch.int, !torch.int -> !torch.int
    %int2_3487 = torch.constant.int 2
    %int0_3488 = torch.constant.int 0
    %int1_3489 = torch.constant.int 1
    %2968 = torch.aten.slice.Tensor %2966, %int2_3487, %int0_3488, %2967, %int1_3489 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2968, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_3490 = torch.constant.int 0
    %2969 = torch.aten.clone %2968, %int0_3490 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2969, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_3491 = torch.constant.int 1
    %2970 = torch.aten.size.int %2965, %int1_3491 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_3492 = torch.constant.int 32
    %2971 = torch.aten.mul.int %2970, %int32_3492 : !torch.int, !torch.int -> !torch.int
    %int4_3493 = torch.constant.int 4
    %int8_3494 = torch.constant.int 8
    %int128_3495 = torch.constant.int 128
    %2972 = torch.prim.ListConstruct %int4_3493, %2971, %int8_3494, %int128_3495 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2973 = torch.aten._unsafe_view %2969, %2972 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %2973, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_3496 = torch.constant.int 0
    %int0_3497 = torch.constant.int 0
    %int9223372036854775807_3498 = torch.constant.int 9223372036854775807
    %int1_3499 = torch.constant.int 1
    %2974 = torch.aten.slice.Tensor %2973, %int0_3496, %int0_3497, %int9223372036854775807_3498, %int1_3499 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %2974, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_3500 = torch.constant.int 0
    %int0_3501 = torch.constant.int 0
    %int9223372036854775807_3502 = torch.constant.int 9223372036854775807
    %int1_3503 = torch.constant.int 1
    %2975 = torch.aten.slice.Tensor %2963, %int0_3500, %int0_3501, %int9223372036854775807_3502, %int1_3503 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %2975, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_3504 = torch.constant.int 1
    %int0_3505 = torch.constant.int 0
    %int9223372036854775807_3506 = torch.constant.int 9223372036854775807
    %int1_3507 = torch.constant.int 1
    %2976 = torch.aten.slice.Tensor %2975, %int1_3504, %int0_3505, %int9223372036854775807_3506, %int1_3507 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %2976, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_3508 = torch.constant.int 2
    %int1_3509 = torch.constant.int 1
    %2977 = torch.aten.select.int %2976, %int2_3508, %int1_3509 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2977, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int2_3510 = torch.constant.int 2
    %int0_3511 = torch.constant.int 0
    %int1_3512 = torch.constant.int 1
    %2978 = torch.aten.slice.Tensor %2977, %int2_3510, %int0_3511, %2967, %int1_3512 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2978, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_3513 = torch.constant.int 0
    %2979 = torch.aten.clone %2978, %int0_3513 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %2979, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_3514 = torch.constant.int 1
    %2980 = torch.aten.size.int %2976, %int1_3514 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_3515 = torch.constant.int 32
    %2981 = torch.aten.mul.int %2980, %int32_3515 : !torch.int, !torch.int -> !torch.int
    %int4_3516 = torch.constant.int 4
    %int8_3517 = torch.constant.int 8
    %int128_3518 = torch.constant.int 128
    %2982 = torch.prim.ListConstruct %int4_3516, %2981, %int8_3517, %int128_3518 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2983 = torch.aten._unsafe_view %2979, %2982 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %2983, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_3519 = torch.constant.int 0
    %int0_3520 = torch.constant.int 0
    %int9223372036854775807_3521 = torch.constant.int 9223372036854775807
    %int1_3522 = torch.constant.int 1
    %2984 = torch.aten.slice.Tensor %2983, %int0_3519, %int0_3520, %int9223372036854775807_3521, %int1_3522 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %2984, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int-2_3523 = torch.constant.int -2
    %2985 = torch.aten.unsqueeze %2974, %int-2_3523 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %2985, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_3524 = torch.constant.int 1
    %2986 = torch.aten.size.int %2973, %int1_3524 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_3525 = torch.constant.int 4
    %int8_3526 = torch.constant.int 8
    %int4_3527 = torch.constant.int 4
    %int128_3528 = torch.constant.int 128
    %2987 = torch.prim.ListConstruct %int4_3525, %2986, %int8_3526, %int4_3527, %int128_3528 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_3529 = torch.constant.bool false
    %2988 = torch.aten.expand %2985, %2987, %false_3529 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2988, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_3530 = torch.constant.int 0
    %2989 = torch.aten.clone %2988, %int0_3530 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2989, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_3531 = torch.constant.int 4
    %int32_3532 = torch.constant.int 32
    %int128_3533 = torch.constant.int 128
    %2990 = torch.prim.ListConstruct %int4_3531, %2986, %int32_3532, %int128_3533 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2991 = torch.aten._unsafe_view %2989, %2990 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2991, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_3534 = torch.constant.int -2
    %2992 = torch.aten.unsqueeze %2984, %int-2_3534 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %2992, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_3535 = torch.constant.int 1
    %2993 = torch.aten.size.int %2983, %int1_3535 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_3536 = torch.constant.int 4
    %int8_3537 = torch.constant.int 8
    %int4_3538 = torch.constant.int 4
    %int128_3539 = torch.constant.int 128
    %2994 = torch.prim.ListConstruct %int4_3536, %2993, %int8_3537, %int4_3538, %int128_3539 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_3540 = torch.constant.bool false
    %2995 = torch.aten.expand %2992, %2994, %false_3540 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2995, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_3541 = torch.constant.int 0
    %2996 = torch.aten.clone %2995, %int0_3541 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %2996, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_3542 = torch.constant.int 4
    %int32_3543 = torch.constant.int 32
    %int128_3544 = torch.constant.int 128
    %2997 = torch.prim.ListConstruct %int4_3542, %2993, %int32_3543, %int128_3544 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2998 = torch.aten._unsafe_view %2996, %2997 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %2998, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_3545 = torch.constant.int 1
    %int2_3546 = torch.constant.int 2
    %2999 = torch.aten.transpose.int %2879, %int1_3545, %int2_3546 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,128],f16>
    %int1_3547 = torch.constant.int 1
    %int2_3548 = torch.constant.int 2
    %3000 = torch.aten.transpose.int %2991, %int1_3547, %int2_3548 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %3000, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_3549 = torch.constant.int 1
    %int2_3550 = torch.constant.int 2
    %3001 = torch.aten.transpose.int %2998, %int1_3549, %int2_3550 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %3001, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_3551 = torch.constant.float 0.000000e+00
    %false_3552 = torch.constant.bool false
    %none_3553 = torch.constant.none
    %3002:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2999, %3000, %3001, %float0.000000e00_3551, %false_3552, %368, %none_3553) : (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.vtensor<[4,1,1,?],f16>, !torch.none) -> (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,1],f32>) 
    %int1_3554 = torch.constant.int 1
    %int2_3555 = torch.constant.int 2
    %3003 = torch.aten.transpose.int %3002#0, %int1_3554, %int2_3555 : !torch.vtensor<[4,32,1,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int4_3556 = torch.constant.int 4
    %int1_3557 = torch.constant.int 1
    %int4096_3558 = torch.constant.int 4096
    %3004 = torch.prim.ListConstruct %int4_3556, %int1_3557, %int4096_3558 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3005 = torch.aten.view %3003, %3004 : !torch.vtensor<[4,1,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_3559 = torch.constant.int -2
    %int-1_3560 = torch.constant.int -1
    %3006 = torch.aten.transpose.int %139, %int-2_3559, %int-1_3560 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_3561 = torch.constant.int 4
    %int4096_3562 = torch.constant.int 4096
    %3007 = torch.prim.ListConstruct %int4_3561, %int4096_3562 : (!torch.int, !torch.int) -> !torch.list<int>
    %3008 = torch.aten.view %3005, %3007 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %3009 = torch.aten.mm %3008, %3006 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_3563 = torch.constant.int 4
    %int1_3564 = torch.constant.int 1
    %int4096_3565 = torch.constant.int 4096
    %3010 = torch.prim.ListConstruct %int4_3563, %int1_3564, %int4096_3565 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3011 = torch.aten.view %3009, %3010 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_3566 = torch.constant.int 1
    %3012 = torch.aten.add.Tensor %2839, %3011, %int1_3566 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_3567 = torch.constant.int 6
    %3013 = torch.prims.convert_element_type %3012, %int6_3567 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_3568 = torch.constant.int 2
    %3014 = torch.aten.pow.Tensor_Scalar %3013, %int2_3568 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_3569 = torch.constant.int -1
    %3015 = torch.prim.ListConstruct %int-1_3569 : (!torch.int) -> !torch.list<int>
    %true_3570 = torch.constant.bool true
    %none_3571 = torch.constant.none
    %3016 = torch.aten.mean.dim %3014, %3015, %true_3570, %none_3571 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_3572 = torch.constant.float 9.9999997473787516E-6
    %int1_3573 = torch.constant.int 1
    %3017 = torch.aten.add.Scalar %3016, %float9.999990e-06_3572, %int1_3573 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %3018 = torch.aten.rsqrt %3017 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %3019 = torch.aten.mul.Tensor %3013, %3018 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_3574 = torch.constant.int 5
    %3020 = torch.prims.convert_element_type %3019, %int5_3574 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %3021 = torch.aten.mul.Tensor %140, %3020 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_3575 = torch.constant.int 5
    %3022 = torch.prims.convert_element_type %3021, %int5_3575 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_3576 = torch.constant.int -2
    %int-1_3577 = torch.constant.int -1
    %3023 = torch.aten.transpose.int %141, %int-2_3576, %int-1_3577 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_3578 = torch.constant.int 4
    %int4096_3579 = torch.constant.int 4096
    %3024 = torch.prim.ListConstruct %int4_3578, %int4096_3579 : (!torch.int, !torch.int) -> !torch.list<int>
    %3025 = torch.aten.view %3022, %3024 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %3026 = torch.aten.mm %3025, %3023 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_3580 = torch.constant.int 4
    %int1_3581 = torch.constant.int 1
    %int14336_3582 = torch.constant.int 14336
    %3027 = torch.prim.ListConstruct %int4_3580, %int1_3581, %int14336_3582 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3028 = torch.aten.view %3026, %3027 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %3029 = torch.aten.silu %3028 : !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_3583 = torch.constant.int -2
    %int-1_3584 = torch.constant.int -1
    %3030 = torch.aten.transpose.int %142, %int-2_3583, %int-1_3584 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_3585 = torch.constant.int 4
    %int4096_3586 = torch.constant.int 4096
    %3031 = torch.prim.ListConstruct %int4_3585, %int4096_3586 : (!torch.int, !torch.int) -> !torch.list<int>
    %3032 = torch.aten.view %3022, %3031 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %3033 = torch.aten.mm %3032, %3030 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_3587 = torch.constant.int 4
    %int1_3588 = torch.constant.int 1
    %int14336_3589 = torch.constant.int 14336
    %3034 = torch.prim.ListConstruct %int4_3587, %int1_3588, %int14336_3589 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3035 = torch.aten.view %3033, %3034 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %3036 = torch.aten.mul.Tensor %3029, %3035 : !torch.vtensor<[4,1,14336],f16>, !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_3590 = torch.constant.int -2
    %int-1_3591 = torch.constant.int -1
    %3037 = torch.aten.transpose.int %143, %int-2_3590, %int-1_3591 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int4_3592 = torch.constant.int 4
    %int14336_3593 = torch.constant.int 14336
    %3038 = torch.prim.ListConstruct %int4_3592, %int14336_3593 : (!torch.int, !torch.int) -> !torch.list<int>
    %3039 = torch.aten.view %3036, %3038 : !torch.vtensor<[4,1,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,14336],f16>
    %3040 = torch.aten.mm %3039, %3037 : !torch.vtensor<[4,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_3594 = torch.constant.int 4
    %int1_3595 = torch.constant.int 1
    %int4096_3596 = torch.constant.int 4096
    %3041 = torch.prim.ListConstruct %int4_3594, %int1_3595, %int4096_3596 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3042 = torch.aten.view %3040, %3041 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_3597 = torch.constant.int 1
    %3043 = torch.aten.add.Tensor %3012, %3042, %int1_3597 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_3598 = torch.constant.int 6
    %3044 = torch.prims.convert_element_type %3043, %int6_3598 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_3599 = torch.constant.int 2
    %3045 = torch.aten.pow.Tensor_Scalar %3044, %int2_3599 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_3600 = torch.constant.int -1
    %3046 = torch.prim.ListConstruct %int-1_3600 : (!torch.int) -> !torch.list<int>
    %true_3601 = torch.constant.bool true
    %none_3602 = torch.constant.none
    %3047 = torch.aten.mean.dim %3045, %3046, %true_3601, %none_3602 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_3603 = torch.constant.float 9.9999997473787516E-6
    %int1_3604 = torch.constant.int 1
    %3048 = torch.aten.add.Scalar %3047, %float9.999990e-06_3603, %int1_3604 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %3049 = torch.aten.rsqrt %3048 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %3050 = torch.aten.mul.Tensor %3044, %3049 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_3605 = torch.constant.int 5
    %3051 = torch.prims.convert_element_type %3050, %int5_3605 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %3052 = torch.aten.mul.Tensor %144, %3051 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_3606 = torch.constant.int 5
    %3053 = torch.prims.convert_element_type %3052, %int5_3606 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_3607 = torch.constant.int -2
    %int-1_3608 = torch.constant.int -1
    %3054 = torch.aten.transpose.int %145, %int-2_3607, %int-1_3608 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_3609 = torch.constant.int 4
    %int4096_3610 = torch.constant.int 4096
    %3055 = torch.prim.ListConstruct %int4_3609, %int4096_3610 : (!torch.int, !torch.int) -> !torch.list<int>
    %3056 = torch.aten.view %3053, %3055 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %3057 = torch.aten.mm %3056, %3054 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_3611 = torch.constant.int 4
    %int1_3612 = torch.constant.int 1
    %int4096_3613 = torch.constant.int 4096
    %3058 = torch.prim.ListConstruct %int4_3611, %int1_3612, %int4096_3613 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3059 = torch.aten.view %3057, %3058 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_3614 = torch.constant.int -2
    %int-1_3615 = torch.constant.int -1
    %3060 = torch.aten.transpose.int %146, %int-2_3614, %int-1_3615 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_3616 = torch.constant.int 4
    %int4096_3617 = torch.constant.int 4096
    %3061 = torch.prim.ListConstruct %int4_3616, %int4096_3617 : (!torch.int, !torch.int) -> !torch.list<int>
    %3062 = torch.aten.view %3053, %3061 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %3063 = torch.aten.mm %3062, %3060 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_3618 = torch.constant.int 4
    %int1_3619 = torch.constant.int 1
    %int1024_3620 = torch.constant.int 1024
    %3064 = torch.prim.ListConstruct %int4_3618, %int1_3619, %int1024_3620 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3065 = torch.aten.view %3063, %3064 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int-2_3621 = torch.constant.int -2
    %int-1_3622 = torch.constant.int -1
    %3066 = torch.aten.transpose.int %147, %int-2_3621, %int-1_3622 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_3623 = torch.constant.int 4
    %int4096_3624 = torch.constant.int 4096
    %3067 = torch.prim.ListConstruct %int4_3623, %int4096_3624 : (!torch.int, !torch.int) -> !torch.list<int>
    %3068 = torch.aten.view %3053, %3067 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %3069 = torch.aten.mm %3068, %3066 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_3625 = torch.constant.int 4
    %int1_3626 = torch.constant.int 1
    %int1024_3627 = torch.constant.int 1024
    %3070 = torch.prim.ListConstruct %int4_3625, %int1_3626, %int1024_3627 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3071 = torch.aten.view %3069, %3070 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int4_3628 = torch.constant.int 4
    %int1_3629 = torch.constant.int 1
    %int32_3630 = torch.constant.int 32
    %int128_3631 = torch.constant.int 128
    %3072 = torch.prim.ListConstruct %int4_3628, %int1_3629, %int32_3630, %int128_3631 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3073 = torch.aten.view %3059, %3072 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,128],f16>
    %int4_3632 = torch.constant.int 4
    %int1_3633 = torch.constant.int 1
    %int8_3634 = torch.constant.int 8
    %int128_3635 = torch.constant.int 128
    %3074 = torch.prim.ListConstruct %int4_3632, %int1_3633, %int8_3634, %int128_3635 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3075 = torch.aten.view %3065, %3074 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int4_3636 = torch.constant.int 4
    %int1_3637 = torch.constant.int 1
    %int8_3638 = torch.constant.int 8
    %int128_3639 = torch.constant.int 128
    %3076 = torch.prim.ListConstruct %int4_3636, %int1_3637, %int8_3638, %int128_3639 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3077 = torch.aten.view %3071, %3076 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int6_3640 = torch.constant.int 6
    %3078 = torch.prims.convert_element_type %3073, %int6_3640 : !torch.vtensor<[4,1,32,128],f16>, !torch.int -> !torch.vtensor<[4,1,32,128],f32>
    %3079 = torch_c.to_builtin_tensor %3078 : !torch.vtensor<[4,1,32,128],f32> -> tensor<4x1x32x128xf32>
    %3080 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %3081 = util.call @sharktank_rotary_embedding_4_1_32_128_f32(%3079, %3080) : (tensor<4x1x32x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x32x128xf32>
    %3082 = torch_c.from_builtin_tensor %3081 : tensor<4x1x32x128xf32> -> !torch.vtensor<[4,1,32,128],f32>
    %int5_3641 = torch.constant.int 5
    %3083 = torch.prims.convert_element_type %3082, %int5_3641 : !torch.vtensor<[4,1,32,128],f32>, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int6_3642 = torch.constant.int 6
    %3084 = torch.prims.convert_element_type %3075, %int6_3642 : !torch.vtensor<[4,1,8,128],f16>, !torch.int -> !torch.vtensor<[4,1,8,128],f32>
    %3085 = torch_c.to_builtin_tensor %3084 : !torch.vtensor<[4,1,8,128],f32> -> tensor<4x1x8x128xf32>
    %3086 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %3087 = util.call @sharktank_rotary_embedding_4_1_8_128_f32(%3085, %3086) : (tensor<4x1x8x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x8x128xf32>
    %3088 = torch_c.from_builtin_tensor %3087 : tensor<4x1x8x128xf32> -> !torch.vtensor<[4,1,8,128],f32>
    %int5_3643 = torch.constant.int 5
    %3089 = torch.prims.convert_element_type %3088, %int5_3643 : !torch.vtensor<[4,1,8,128],f32>, !torch.int -> !torch.vtensor<[4,1,8,128],f16>
    %int32_3644 = torch.constant.int 32
    %3090 = torch.aten.floor_divide.Scalar %arg2, %int32_3644 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_3645 = torch.constant.int 1
    %3091 = torch.aten.unsqueeze %3090, %int1_3645 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3646 = torch.constant.int 1
    %false_3647 = torch.constant.bool false
    %3092 = torch.aten.gather %arg3, %int1_3646, %3091, %false_3647 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_3648 = torch.constant.int 32
    %3093 = torch.aten.remainder.Scalar %arg2, %int32_3648 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_3649 = torch.constant.int 1
    %3094 = torch.aten.unsqueeze %3093, %int1_3649 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_3650 = torch.constant.none
    %3095 = torch.aten.clone %148, %none_3650 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_3651 = torch.constant.int 0
    %3096 = torch.aten.unsqueeze %3095, %int0_3651 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_3652 = torch.constant.int 4
    %int1_3653 = torch.constant.int 1
    %3097 = torch.prim.ListConstruct %int4_3652, %int1_3653 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_3654 = torch.constant.int 1
    %int1_3655 = torch.constant.int 1
    %3098 = torch.prim.ListConstruct %int1_3654, %int1_3655 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_3656 = torch.constant.int 4
    %int0_3657 = torch.constant.int 0
    %cpu_3658 = torch.constant.device "cpu"
    %false_3659 = torch.constant.bool false
    %3099 = torch.aten.empty_strided %3097, %3098, %int4_3656, %int0_3657, %cpu_3658, %false_3659 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int13 = torch.constant.int 13
    %3100 = torch.aten.fill.Scalar %3099, %int13 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_3660 = torch.constant.int 4
    %int1_3661 = torch.constant.int 1
    %3101 = torch.prim.ListConstruct %int4_3660, %int1_3661 : (!torch.int, !torch.int) -> !torch.list<int>
    %3102 = torch.aten.repeat %3096, %3101 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_3662 = torch.constant.int 32
    %3103 = torch.aten.mul.Scalar %3092, %int32_3662 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3663 = torch.constant.int 1
    %3104 = torch.aten.add.Tensor %3103, %3100, %int1_3663 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_3664 = torch.constant.int 2
    %3105 = torch.aten.mul.Scalar %3104, %int2_3664 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3665 = torch.constant.int 1
    %3106 = torch.aten.add.Tensor %3105, %3102, %int1_3665 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_3666 = torch.constant.int 32
    %3107 = torch.aten.mul.Scalar %3106, %int32_3666 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3667 = torch.constant.int 1
    %3108 = torch.aten.add.Tensor %3107, %3094, %int1_3667 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_3668 = torch.constant.int 32
    %int2_3669 = torch.constant.int 2
    %int32_3670 = torch.constant.int 32
    %int8_3671 = torch.constant.int 8
    %int128_3672 = torch.constant.int 128
    %3109 = torch.prim.ListConstruct %437, %int32_3668, %int2_3669, %int32_3670, %int8_3671, %int128_3672 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3110 = torch.aten.view %2946, %3109 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3110, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_3673 = torch.constant.int 32
    %3111 = torch.aten.mul.int %437, %int32_3673 : !torch.int, !torch.int -> !torch.int
    %int2_3674 = torch.constant.int 2
    %3112 = torch.aten.mul.int %3111, %int2_3674 : !torch.int, !torch.int -> !torch.int
    %int32_3675 = torch.constant.int 32
    %3113 = torch.aten.mul.int %3112, %int32_3675 : !torch.int, !torch.int -> !torch.int
    %int8_3676 = torch.constant.int 8
    %int128_3677 = torch.constant.int 128
    %3114 = torch.prim.ListConstruct %3113, %int8_3676, %int128_3677 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3115 = torch.aten.view %3110, %3114 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %3115, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %3116 = torch.prim.ListConstruct %3108 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_3678 = torch.constant.bool false
    %3117 = torch.aten.index_put %3115, %3116, %3089, %false_3678 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %3117, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_3679 = torch.constant.int 32
    %int2_3680 = torch.constant.int 2
    %int32_3681 = torch.constant.int 32
    %int8_3682 = torch.constant.int 8
    %int128_3683 = torch.constant.int 128
    %3118 = torch.prim.ListConstruct %437, %int32_3679, %int2_3680, %int32_3681, %int8_3682, %int128_3683 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3119 = torch.aten.view %3117, %3118 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3119, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_3684 = torch.constant.int 2097152
    %3120 = torch.prim.ListConstruct %437, %int2097152_3684 : (!torch.int, !torch.int) -> !torch.list<int>
    %3121 = torch.aten.view %3119, %3120 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %3121, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_3685 = torch.constant.int 32
    %int2_3686 = torch.constant.int 2
    %int32_3687 = torch.constant.int 32
    %int8_3688 = torch.constant.int 8
    %int128_3689 = torch.constant.int 128
    %3122 = torch.prim.ListConstruct %437, %int32_3685, %int2_3686, %int32_3687, %int8_3688, %int128_3689 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3123 = torch.aten.view %3121, %3122 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3123, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int8_3690 = torch.constant.int 8
    %int128_3691 = torch.constant.int 128
    %3124 = torch.prim.ListConstruct %3113, %int8_3690, %int128_3691 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3125 = torch.aten.view %3123, %3124 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %3125, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_3692 = torch.constant.int 32
    %3126 = torch.aten.floor_divide.Scalar %arg2, %int32_3692 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_3693 = torch.constant.int 1
    %3127 = torch.aten.unsqueeze %3126, %int1_3693 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3694 = torch.constant.int 1
    %false_3695 = torch.constant.bool false
    %3128 = torch.aten.gather %arg3, %int1_3694, %3127, %false_3695 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_3696 = torch.constant.int 32
    %3129 = torch.aten.remainder.Scalar %arg2, %int32_3696 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_3697 = torch.constant.int 1
    %3130 = torch.aten.unsqueeze %3129, %int1_3697 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_3698 = torch.constant.none
    %3131 = torch.aten.clone %149, %none_3698 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_3699 = torch.constant.int 0
    %3132 = torch.aten.unsqueeze %3131, %int0_3699 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_3700 = torch.constant.int 4
    %int1_3701 = torch.constant.int 1
    %3133 = torch.prim.ListConstruct %int4_3700, %int1_3701 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_3702 = torch.constant.int 1
    %int1_3703 = torch.constant.int 1
    %3134 = torch.prim.ListConstruct %int1_3702, %int1_3703 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_3704 = torch.constant.int 4
    %int0_3705 = torch.constant.int 0
    %cpu_3706 = torch.constant.device "cpu"
    %false_3707 = torch.constant.bool false
    %3135 = torch.aten.empty_strided %3133, %3134, %int4_3704, %int0_3705, %cpu_3706, %false_3707 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int13_3708 = torch.constant.int 13
    %3136 = torch.aten.fill.Scalar %3135, %int13_3708 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_3709 = torch.constant.int 4
    %int1_3710 = torch.constant.int 1
    %3137 = torch.prim.ListConstruct %int4_3709, %int1_3710 : (!torch.int, !torch.int) -> !torch.list<int>
    %3138 = torch.aten.repeat %3132, %3137 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_3711 = torch.constant.int 32
    %3139 = torch.aten.mul.Scalar %3128, %int32_3711 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3712 = torch.constant.int 1
    %3140 = torch.aten.add.Tensor %3139, %3136, %int1_3712 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_3713 = torch.constant.int 2
    %3141 = torch.aten.mul.Scalar %3140, %int2_3713 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3714 = torch.constant.int 1
    %3142 = torch.aten.add.Tensor %3141, %3138, %int1_3714 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_3715 = torch.constant.int 32
    %3143 = torch.aten.mul.Scalar %3142, %int32_3715 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3716 = torch.constant.int 1
    %3144 = torch.aten.add.Tensor %3143, %3130, %int1_3716 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %3145 = torch.prim.ListConstruct %3144 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_3717 = torch.constant.bool false
    %3146 = torch.aten.index_put %3125, %3145, %3077, %false_3717 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %3146, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_3718 = torch.constant.int 32
    %int2_3719 = torch.constant.int 2
    %int32_3720 = torch.constant.int 32
    %int8_3721 = torch.constant.int 8
    %int128_3722 = torch.constant.int 128
    %3147 = torch.prim.ListConstruct %437, %int32_3718, %int2_3719, %int32_3720, %int8_3721, %int128_3722 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3148 = torch.aten.view %3146, %3147 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3148, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_3723 = torch.constant.int 2097152
    %3149 = torch.prim.ListConstruct %437, %int2097152_3723 : (!torch.int, !torch.int) -> !torch.list<int>
    %3150 = torch.aten.view %3148, %3149 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %3150, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int4_3724 = torch.constant.int 4
    %3151 = torch.prim.ListConstruct %int4_3724, %358 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_3725 = torch.constant.int 1
    %3152 = torch.prim.ListConstruct %358, %int1_3725 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_3726 = torch.constant.int 4
    %int0_3727 = torch.constant.int 0
    %cpu_3728 = torch.constant.device "cpu"
    %false_3729 = torch.constant.bool false
    %3153 = torch.aten.empty_strided %3151, %3152, %int4_3726, %int0_3727, %cpu_3728, %false_3729 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3153, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int13_3730 = torch.constant.int 13
    %3154 = torch.aten.fill.Scalar %3153, %int13_3730 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3154, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int32_3731 = torch.constant.int 32
    %3155 = torch.aten.mul.Scalar %arg3, %int32_3731 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3155, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_3732 = torch.constant.int 1
    %3156 = torch.aten.add.Tensor %3155, %3154, %int1_3732 : !torch.vtensor<[4,?],si64>, !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3156, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_3733 = torch.constant.int 4
    %3157 = torch.aten.mul.int %int4_3733, %358 : !torch.int, !torch.int -> !torch.int
    %3158 = torch.prim.ListConstruct %3157 : (!torch.int) -> !torch.list<int>
    %3159 = torch.aten.view %3156, %3158 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3159, [%356], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_3734 = torch.constant.int 32
    %int2_3735 = torch.constant.int 2
    %int32_3736 = torch.constant.int 32
    %int8_3737 = torch.constant.int 8
    %int128_3738 = torch.constant.int 128
    %3160 = torch.prim.ListConstruct %437, %int32_3734, %int2_3735, %int32_3736, %int8_3737, %int128_3738 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3161 = torch.aten.view %3150, %3160 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3161, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_3739 = torch.constant.int 32
    %3162 = torch.aten.mul.int %437, %int32_3739 : !torch.int, !torch.int -> !torch.int
    %int2_3740 = torch.constant.int 2
    %int32_3741 = torch.constant.int 32
    %int8_3742 = torch.constant.int 8
    %int128_3743 = torch.constant.int 128
    %3163 = torch.prim.ListConstruct %3162, %int2_3740, %int32_3741, %int8_3742, %int128_3743 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3164 = torch.aten.view %3161, %3163 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %3164, [%357], affine_map<()[s0] -> (s0 * 32, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int0_3744 = torch.constant.int 0
    %3165 = torch.aten.index_select %3164, %int0_3744, %3159 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.int, !torch.vtensor<[?],si64> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %3165, [%356], affine_map<()[s0] -> (s0 * 4, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int4_3745 = torch.constant.int 4
    %int2_3746 = torch.constant.int 2
    %int32_3747 = torch.constant.int 32
    %int8_3748 = torch.constant.int 8
    %int128_3749 = torch.constant.int 128
    %3166 = torch.prim.ListConstruct %int4_3745, %358, %int2_3746, %int32_3747, %int8_3748, %int128_3749 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3167 = torch.aten.view %3165, %3166 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %3167, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int0_3750 = torch.constant.int 0
    %int0_3751 = torch.constant.int 0
    %int9223372036854775807_3752 = torch.constant.int 9223372036854775807
    %int1_3753 = torch.constant.int 1
    %3168 = torch.aten.slice.Tensor %3167, %int0_3750, %int0_3751, %int9223372036854775807_3752, %int1_3753 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %3168, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_3754 = torch.constant.int 1
    %int0_3755 = torch.constant.int 0
    %int9223372036854775807_3756 = torch.constant.int 9223372036854775807
    %int1_3757 = torch.constant.int 1
    %3169 = torch.aten.slice.Tensor %3168, %int1_3754, %int0_3755, %int9223372036854775807_3756, %int1_3757 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %3169, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_3758 = torch.constant.int 2
    %int0_3759 = torch.constant.int 0
    %3170 = torch.aten.select.int %3169, %int2_3758, %int0_3759 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3170, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int32_3760 = torch.constant.int 32
    %3171 = torch.aten.mul.int %358, %int32_3760 : !torch.int, !torch.int -> !torch.int
    %int2_3761 = torch.constant.int 2
    %int0_3762 = torch.constant.int 0
    %int1_3763 = torch.constant.int 1
    %3172 = torch.aten.slice.Tensor %3170, %int2_3761, %int0_3762, %3171, %int1_3763 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3172, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_3764 = torch.constant.int 0
    %3173 = torch.aten.clone %3172, %int0_3764 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3173, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_3765 = torch.constant.int 1
    %3174 = torch.aten.size.int %3169, %int1_3765 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_3766 = torch.constant.int 32
    %3175 = torch.aten.mul.int %3174, %int32_3766 : !torch.int, !torch.int -> !torch.int
    %int4_3767 = torch.constant.int 4
    %int8_3768 = torch.constant.int 8
    %int128_3769 = torch.constant.int 128
    %3176 = torch.prim.ListConstruct %int4_3767, %3175, %int8_3768, %int128_3769 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3177 = torch.aten._unsafe_view %3173, %3176 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %3177, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_3770 = torch.constant.int 0
    %int0_3771 = torch.constant.int 0
    %int9223372036854775807_3772 = torch.constant.int 9223372036854775807
    %int1_3773 = torch.constant.int 1
    %3178 = torch.aten.slice.Tensor %3177, %int0_3770, %int0_3771, %int9223372036854775807_3772, %int1_3773 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %3178, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_3774 = torch.constant.int 0
    %int0_3775 = torch.constant.int 0
    %int9223372036854775807_3776 = torch.constant.int 9223372036854775807
    %int1_3777 = torch.constant.int 1
    %3179 = torch.aten.slice.Tensor %3167, %int0_3774, %int0_3775, %int9223372036854775807_3776, %int1_3777 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %3179, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_3778 = torch.constant.int 1
    %int0_3779 = torch.constant.int 0
    %int9223372036854775807_3780 = torch.constant.int 9223372036854775807
    %int1_3781 = torch.constant.int 1
    %3180 = torch.aten.slice.Tensor %3179, %int1_3778, %int0_3779, %int9223372036854775807_3780, %int1_3781 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %3180, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_3782 = torch.constant.int 2
    %int1_3783 = torch.constant.int 1
    %3181 = torch.aten.select.int %3180, %int2_3782, %int1_3783 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3181, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int2_3784 = torch.constant.int 2
    %int0_3785 = torch.constant.int 0
    %int1_3786 = torch.constant.int 1
    %3182 = torch.aten.slice.Tensor %3181, %int2_3784, %int0_3785, %3171, %int1_3786 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3182, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_3787 = torch.constant.int 0
    %3183 = torch.aten.clone %3182, %int0_3787 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3183, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_3788 = torch.constant.int 1
    %3184 = torch.aten.size.int %3180, %int1_3788 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_3789 = torch.constant.int 32
    %3185 = torch.aten.mul.int %3184, %int32_3789 : !torch.int, !torch.int -> !torch.int
    %int4_3790 = torch.constant.int 4
    %int8_3791 = torch.constant.int 8
    %int128_3792 = torch.constant.int 128
    %3186 = torch.prim.ListConstruct %int4_3790, %3185, %int8_3791, %int128_3792 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3187 = torch.aten._unsafe_view %3183, %3186 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %3187, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_3793 = torch.constant.int 0
    %int0_3794 = torch.constant.int 0
    %int9223372036854775807_3795 = torch.constant.int 9223372036854775807
    %int1_3796 = torch.constant.int 1
    %3188 = torch.aten.slice.Tensor %3187, %int0_3793, %int0_3794, %int9223372036854775807_3795, %int1_3796 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %3188, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int-2_3797 = torch.constant.int -2
    %3189 = torch.aten.unsqueeze %3178, %int-2_3797 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %3189, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_3798 = torch.constant.int 1
    %3190 = torch.aten.size.int %3177, %int1_3798 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_3799 = torch.constant.int 4
    %int8_3800 = torch.constant.int 8
    %int4_3801 = torch.constant.int 4
    %int128_3802 = torch.constant.int 128
    %3191 = torch.prim.ListConstruct %int4_3799, %3190, %int8_3800, %int4_3801, %int128_3802 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_3803 = torch.constant.bool false
    %3192 = torch.aten.expand %3189, %3191, %false_3803 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3192, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_3804 = torch.constant.int 0
    %3193 = torch.aten.clone %3192, %int0_3804 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3193, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_3805 = torch.constant.int 4
    %int32_3806 = torch.constant.int 32
    %int128_3807 = torch.constant.int 128
    %3194 = torch.prim.ListConstruct %int4_3805, %3190, %int32_3806, %int128_3807 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3195 = torch.aten._unsafe_view %3193, %3194 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %3195, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_3808 = torch.constant.int -2
    %3196 = torch.aten.unsqueeze %3188, %int-2_3808 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %3196, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_3809 = torch.constant.int 1
    %3197 = torch.aten.size.int %3187, %int1_3809 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_3810 = torch.constant.int 4
    %int8_3811 = torch.constant.int 8
    %int4_3812 = torch.constant.int 4
    %int128_3813 = torch.constant.int 128
    %3198 = torch.prim.ListConstruct %int4_3810, %3197, %int8_3811, %int4_3812, %int128_3813 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_3814 = torch.constant.bool false
    %3199 = torch.aten.expand %3196, %3198, %false_3814 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3199, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_3815 = torch.constant.int 0
    %3200 = torch.aten.clone %3199, %int0_3815 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3200, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_3816 = torch.constant.int 4
    %int32_3817 = torch.constant.int 32
    %int128_3818 = torch.constant.int 128
    %3201 = torch.prim.ListConstruct %int4_3816, %3197, %int32_3817, %int128_3818 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3202 = torch.aten._unsafe_view %3200, %3201 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %3202, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_3819 = torch.constant.int 1
    %int2_3820 = torch.constant.int 2
    %3203 = torch.aten.transpose.int %3083, %int1_3819, %int2_3820 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,128],f16>
    %int1_3821 = torch.constant.int 1
    %int2_3822 = torch.constant.int 2
    %3204 = torch.aten.transpose.int %3195, %int1_3821, %int2_3822 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %3204, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_3823 = torch.constant.int 1
    %int2_3824 = torch.constant.int 2
    %3205 = torch.aten.transpose.int %3202, %int1_3823, %int2_3824 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %3205, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_3825 = torch.constant.float 0.000000e+00
    %false_3826 = torch.constant.bool false
    %none_3827 = torch.constant.none
    %3206:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3203, %3204, %3205, %float0.000000e00_3825, %false_3826, %368, %none_3827) : (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.vtensor<[4,1,1,?],f16>, !torch.none) -> (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,1],f32>) 
    %int1_3828 = torch.constant.int 1
    %int2_3829 = torch.constant.int 2
    %3207 = torch.aten.transpose.int %3206#0, %int1_3828, %int2_3829 : !torch.vtensor<[4,32,1,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int4_3830 = torch.constant.int 4
    %int1_3831 = torch.constant.int 1
    %int4096_3832 = torch.constant.int 4096
    %3208 = torch.prim.ListConstruct %int4_3830, %int1_3831, %int4096_3832 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3209 = torch.aten.view %3207, %3208 : !torch.vtensor<[4,1,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_3833 = torch.constant.int -2
    %int-1_3834 = torch.constant.int -1
    %3210 = torch.aten.transpose.int %150, %int-2_3833, %int-1_3834 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_3835 = torch.constant.int 4
    %int4096_3836 = torch.constant.int 4096
    %3211 = torch.prim.ListConstruct %int4_3835, %int4096_3836 : (!torch.int, !torch.int) -> !torch.list<int>
    %3212 = torch.aten.view %3209, %3211 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %3213 = torch.aten.mm %3212, %3210 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_3837 = torch.constant.int 4
    %int1_3838 = torch.constant.int 1
    %int4096_3839 = torch.constant.int 4096
    %3214 = torch.prim.ListConstruct %int4_3837, %int1_3838, %int4096_3839 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3215 = torch.aten.view %3213, %3214 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_3840 = torch.constant.int 1
    %3216 = torch.aten.add.Tensor %3043, %3215, %int1_3840 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_3841 = torch.constant.int 6
    %3217 = torch.prims.convert_element_type %3216, %int6_3841 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_3842 = torch.constant.int 2
    %3218 = torch.aten.pow.Tensor_Scalar %3217, %int2_3842 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_3843 = torch.constant.int -1
    %3219 = torch.prim.ListConstruct %int-1_3843 : (!torch.int) -> !torch.list<int>
    %true_3844 = torch.constant.bool true
    %none_3845 = torch.constant.none
    %3220 = torch.aten.mean.dim %3218, %3219, %true_3844, %none_3845 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_3846 = torch.constant.float 9.9999997473787516E-6
    %int1_3847 = torch.constant.int 1
    %3221 = torch.aten.add.Scalar %3220, %float9.999990e-06_3846, %int1_3847 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %3222 = torch.aten.rsqrt %3221 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %3223 = torch.aten.mul.Tensor %3217, %3222 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_3848 = torch.constant.int 5
    %3224 = torch.prims.convert_element_type %3223, %int5_3848 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %3225 = torch.aten.mul.Tensor %151, %3224 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_3849 = torch.constant.int 5
    %3226 = torch.prims.convert_element_type %3225, %int5_3849 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_3850 = torch.constant.int -2
    %int-1_3851 = torch.constant.int -1
    %3227 = torch.aten.transpose.int %152, %int-2_3850, %int-1_3851 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_3852 = torch.constant.int 4
    %int4096_3853 = torch.constant.int 4096
    %3228 = torch.prim.ListConstruct %int4_3852, %int4096_3853 : (!torch.int, !torch.int) -> !torch.list<int>
    %3229 = torch.aten.view %3226, %3228 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %3230 = torch.aten.mm %3229, %3227 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_3854 = torch.constant.int 4
    %int1_3855 = torch.constant.int 1
    %int14336_3856 = torch.constant.int 14336
    %3231 = torch.prim.ListConstruct %int4_3854, %int1_3855, %int14336_3856 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3232 = torch.aten.view %3230, %3231 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %3233 = torch.aten.silu %3232 : !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_3857 = torch.constant.int -2
    %int-1_3858 = torch.constant.int -1
    %3234 = torch.aten.transpose.int %153, %int-2_3857, %int-1_3858 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_3859 = torch.constant.int 4
    %int4096_3860 = torch.constant.int 4096
    %3235 = torch.prim.ListConstruct %int4_3859, %int4096_3860 : (!torch.int, !torch.int) -> !torch.list<int>
    %3236 = torch.aten.view %3226, %3235 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %3237 = torch.aten.mm %3236, %3234 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_3861 = torch.constant.int 4
    %int1_3862 = torch.constant.int 1
    %int14336_3863 = torch.constant.int 14336
    %3238 = torch.prim.ListConstruct %int4_3861, %int1_3862, %int14336_3863 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3239 = torch.aten.view %3237, %3238 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %3240 = torch.aten.mul.Tensor %3233, %3239 : !torch.vtensor<[4,1,14336],f16>, !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_3864 = torch.constant.int -2
    %int-1_3865 = torch.constant.int -1
    %3241 = torch.aten.transpose.int %154, %int-2_3864, %int-1_3865 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int4_3866 = torch.constant.int 4
    %int14336_3867 = torch.constant.int 14336
    %3242 = torch.prim.ListConstruct %int4_3866, %int14336_3867 : (!torch.int, !torch.int) -> !torch.list<int>
    %3243 = torch.aten.view %3240, %3242 : !torch.vtensor<[4,1,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,14336],f16>
    %3244 = torch.aten.mm %3243, %3241 : !torch.vtensor<[4,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_3868 = torch.constant.int 4
    %int1_3869 = torch.constant.int 1
    %int4096_3870 = torch.constant.int 4096
    %3245 = torch.prim.ListConstruct %int4_3868, %int1_3869, %int4096_3870 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3246 = torch.aten.view %3244, %3245 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_3871 = torch.constant.int 1
    %3247 = torch.aten.add.Tensor %3216, %3246, %int1_3871 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_3872 = torch.constant.int 6
    %3248 = torch.prims.convert_element_type %3247, %int6_3872 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_3873 = torch.constant.int 2
    %3249 = torch.aten.pow.Tensor_Scalar %3248, %int2_3873 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_3874 = torch.constant.int -1
    %3250 = torch.prim.ListConstruct %int-1_3874 : (!torch.int) -> !torch.list<int>
    %true_3875 = torch.constant.bool true
    %none_3876 = torch.constant.none
    %3251 = torch.aten.mean.dim %3249, %3250, %true_3875, %none_3876 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_3877 = torch.constant.float 9.9999997473787516E-6
    %int1_3878 = torch.constant.int 1
    %3252 = torch.aten.add.Scalar %3251, %float9.999990e-06_3877, %int1_3878 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %3253 = torch.aten.rsqrt %3252 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %3254 = torch.aten.mul.Tensor %3248, %3253 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_3879 = torch.constant.int 5
    %3255 = torch.prims.convert_element_type %3254, %int5_3879 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %3256 = torch.aten.mul.Tensor %155, %3255 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_3880 = torch.constant.int 5
    %3257 = torch.prims.convert_element_type %3256, %int5_3880 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_3881 = torch.constant.int -2
    %int-1_3882 = torch.constant.int -1
    %3258 = torch.aten.transpose.int %156, %int-2_3881, %int-1_3882 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_3883 = torch.constant.int 4
    %int4096_3884 = torch.constant.int 4096
    %3259 = torch.prim.ListConstruct %int4_3883, %int4096_3884 : (!torch.int, !torch.int) -> !torch.list<int>
    %3260 = torch.aten.view %3257, %3259 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %3261 = torch.aten.mm %3260, %3258 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_3885 = torch.constant.int 4
    %int1_3886 = torch.constant.int 1
    %int4096_3887 = torch.constant.int 4096
    %3262 = torch.prim.ListConstruct %int4_3885, %int1_3886, %int4096_3887 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3263 = torch.aten.view %3261, %3262 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_3888 = torch.constant.int -2
    %int-1_3889 = torch.constant.int -1
    %3264 = torch.aten.transpose.int %157, %int-2_3888, %int-1_3889 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_3890 = torch.constant.int 4
    %int4096_3891 = torch.constant.int 4096
    %3265 = torch.prim.ListConstruct %int4_3890, %int4096_3891 : (!torch.int, !torch.int) -> !torch.list<int>
    %3266 = torch.aten.view %3257, %3265 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %3267 = torch.aten.mm %3266, %3264 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_3892 = torch.constant.int 4
    %int1_3893 = torch.constant.int 1
    %int1024_3894 = torch.constant.int 1024
    %3268 = torch.prim.ListConstruct %int4_3892, %int1_3893, %int1024_3894 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3269 = torch.aten.view %3267, %3268 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int-2_3895 = torch.constant.int -2
    %int-1_3896 = torch.constant.int -1
    %3270 = torch.aten.transpose.int %158, %int-2_3895, %int-1_3896 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_3897 = torch.constant.int 4
    %int4096_3898 = torch.constant.int 4096
    %3271 = torch.prim.ListConstruct %int4_3897, %int4096_3898 : (!torch.int, !torch.int) -> !torch.list<int>
    %3272 = torch.aten.view %3257, %3271 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %3273 = torch.aten.mm %3272, %3270 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_3899 = torch.constant.int 4
    %int1_3900 = torch.constant.int 1
    %int1024_3901 = torch.constant.int 1024
    %3274 = torch.prim.ListConstruct %int4_3899, %int1_3900, %int1024_3901 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3275 = torch.aten.view %3273, %3274 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int4_3902 = torch.constant.int 4
    %int1_3903 = torch.constant.int 1
    %int32_3904 = torch.constant.int 32
    %int128_3905 = torch.constant.int 128
    %3276 = torch.prim.ListConstruct %int4_3902, %int1_3903, %int32_3904, %int128_3905 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3277 = torch.aten.view %3263, %3276 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,128],f16>
    %int4_3906 = torch.constant.int 4
    %int1_3907 = torch.constant.int 1
    %int8_3908 = torch.constant.int 8
    %int128_3909 = torch.constant.int 128
    %3278 = torch.prim.ListConstruct %int4_3906, %int1_3907, %int8_3908, %int128_3909 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3279 = torch.aten.view %3269, %3278 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int4_3910 = torch.constant.int 4
    %int1_3911 = torch.constant.int 1
    %int8_3912 = torch.constant.int 8
    %int128_3913 = torch.constant.int 128
    %3280 = torch.prim.ListConstruct %int4_3910, %int1_3911, %int8_3912, %int128_3913 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3281 = torch.aten.view %3275, %3280 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int6_3914 = torch.constant.int 6
    %3282 = torch.prims.convert_element_type %3277, %int6_3914 : !torch.vtensor<[4,1,32,128],f16>, !torch.int -> !torch.vtensor<[4,1,32,128],f32>
    %3283 = torch_c.to_builtin_tensor %3282 : !torch.vtensor<[4,1,32,128],f32> -> tensor<4x1x32x128xf32>
    %3284 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %3285 = util.call @sharktank_rotary_embedding_4_1_32_128_f32(%3283, %3284) : (tensor<4x1x32x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x32x128xf32>
    %3286 = torch_c.from_builtin_tensor %3285 : tensor<4x1x32x128xf32> -> !torch.vtensor<[4,1,32,128],f32>
    %int5_3915 = torch.constant.int 5
    %3287 = torch.prims.convert_element_type %3286, %int5_3915 : !torch.vtensor<[4,1,32,128],f32>, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int6_3916 = torch.constant.int 6
    %3288 = torch.prims.convert_element_type %3279, %int6_3916 : !torch.vtensor<[4,1,8,128],f16>, !torch.int -> !torch.vtensor<[4,1,8,128],f32>
    %3289 = torch_c.to_builtin_tensor %3288 : !torch.vtensor<[4,1,8,128],f32> -> tensor<4x1x8x128xf32>
    %3290 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %3291 = util.call @sharktank_rotary_embedding_4_1_8_128_f32(%3289, %3290) : (tensor<4x1x8x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x8x128xf32>
    %3292 = torch_c.from_builtin_tensor %3291 : tensor<4x1x8x128xf32> -> !torch.vtensor<[4,1,8,128],f32>
    %int5_3917 = torch.constant.int 5
    %3293 = torch.prims.convert_element_type %3292, %int5_3917 : !torch.vtensor<[4,1,8,128],f32>, !torch.int -> !torch.vtensor<[4,1,8,128],f16>
    %int32_3918 = torch.constant.int 32
    %3294 = torch.aten.floor_divide.Scalar %arg2, %int32_3918 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_3919 = torch.constant.int 1
    %3295 = torch.aten.unsqueeze %3294, %int1_3919 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3920 = torch.constant.int 1
    %false_3921 = torch.constant.bool false
    %3296 = torch.aten.gather %arg3, %int1_3920, %3295, %false_3921 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_3922 = torch.constant.int 32
    %3297 = torch.aten.remainder.Scalar %arg2, %int32_3922 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_3923 = torch.constant.int 1
    %3298 = torch.aten.unsqueeze %3297, %int1_3923 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_3924 = torch.constant.none
    %3299 = torch.aten.clone %159, %none_3924 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_3925 = torch.constant.int 0
    %3300 = torch.aten.unsqueeze %3299, %int0_3925 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_3926 = torch.constant.int 4
    %int1_3927 = torch.constant.int 1
    %3301 = torch.prim.ListConstruct %int4_3926, %int1_3927 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_3928 = torch.constant.int 1
    %int1_3929 = torch.constant.int 1
    %3302 = torch.prim.ListConstruct %int1_3928, %int1_3929 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_3930 = torch.constant.int 4
    %int0_3931 = torch.constant.int 0
    %cpu_3932 = torch.constant.device "cpu"
    %false_3933 = torch.constant.bool false
    %3303 = torch.aten.empty_strided %3301, %3302, %int4_3930, %int0_3931, %cpu_3932, %false_3933 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int14 = torch.constant.int 14
    %3304 = torch.aten.fill.Scalar %3303, %int14 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_3934 = torch.constant.int 4
    %int1_3935 = torch.constant.int 1
    %3305 = torch.prim.ListConstruct %int4_3934, %int1_3935 : (!torch.int, !torch.int) -> !torch.list<int>
    %3306 = torch.aten.repeat %3300, %3305 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_3936 = torch.constant.int 32
    %3307 = torch.aten.mul.Scalar %3296, %int32_3936 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3937 = torch.constant.int 1
    %3308 = torch.aten.add.Tensor %3307, %3304, %int1_3937 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_3938 = torch.constant.int 2
    %3309 = torch.aten.mul.Scalar %3308, %int2_3938 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3939 = torch.constant.int 1
    %3310 = torch.aten.add.Tensor %3309, %3306, %int1_3939 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_3940 = torch.constant.int 32
    %3311 = torch.aten.mul.Scalar %3310, %int32_3940 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3941 = torch.constant.int 1
    %3312 = torch.aten.add.Tensor %3311, %3298, %int1_3941 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_3942 = torch.constant.int 32
    %int2_3943 = torch.constant.int 2
    %int32_3944 = torch.constant.int 32
    %int8_3945 = torch.constant.int 8
    %int128_3946 = torch.constant.int 128
    %3313 = torch.prim.ListConstruct %437, %int32_3942, %int2_3943, %int32_3944, %int8_3945, %int128_3946 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3314 = torch.aten.view %3150, %3313 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3314, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_3947 = torch.constant.int 32
    %3315 = torch.aten.mul.int %437, %int32_3947 : !torch.int, !torch.int -> !torch.int
    %int2_3948 = torch.constant.int 2
    %3316 = torch.aten.mul.int %3315, %int2_3948 : !torch.int, !torch.int -> !torch.int
    %int32_3949 = torch.constant.int 32
    %3317 = torch.aten.mul.int %3316, %int32_3949 : !torch.int, !torch.int -> !torch.int
    %int8_3950 = torch.constant.int 8
    %int128_3951 = torch.constant.int 128
    %3318 = torch.prim.ListConstruct %3317, %int8_3950, %int128_3951 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3319 = torch.aten.view %3314, %3318 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %3319, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %3320 = torch.prim.ListConstruct %3312 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_3952 = torch.constant.bool false
    %3321 = torch.aten.index_put %3319, %3320, %3293, %false_3952 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %3321, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_3953 = torch.constant.int 32
    %int2_3954 = torch.constant.int 2
    %int32_3955 = torch.constant.int 32
    %int8_3956 = torch.constant.int 8
    %int128_3957 = torch.constant.int 128
    %3322 = torch.prim.ListConstruct %437, %int32_3953, %int2_3954, %int32_3955, %int8_3956, %int128_3957 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3323 = torch.aten.view %3321, %3322 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3323, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_3958 = torch.constant.int 2097152
    %3324 = torch.prim.ListConstruct %437, %int2097152_3958 : (!torch.int, !torch.int) -> !torch.list<int>
    %3325 = torch.aten.view %3323, %3324 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %3325, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_3959 = torch.constant.int 32
    %int2_3960 = torch.constant.int 2
    %int32_3961 = torch.constant.int 32
    %int8_3962 = torch.constant.int 8
    %int128_3963 = torch.constant.int 128
    %3326 = torch.prim.ListConstruct %437, %int32_3959, %int2_3960, %int32_3961, %int8_3962, %int128_3963 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3327 = torch.aten.view %3325, %3326 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3327, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int8_3964 = torch.constant.int 8
    %int128_3965 = torch.constant.int 128
    %3328 = torch.prim.ListConstruct %3317, %int8_3964, %int128_3965 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3329 = torch.aten.view %3327, %3328 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %3329, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_3966 = torch.constant.int 32
    %3330 = torch.aten.floor_divide.Scalar %arg2, %int32_3966 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_3967 = torch.constant.int 1
    %3331 = torch.aten.unsqueeze %3330, %int1_3967 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3968 = torch.constant.int 1
    %false_3969 = torch.constant.bool false
    %3332 = torch.aten.gather %arg3, %int1_3968, %3331, %false_3969 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_3970 = torch.constant.int 32
    %3333 = torch.aten.remainder.Scalar %arg2, %int32_3970 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_3971 = torch.constant.int 1
    %3334 = torch.aten.unsqueeze %3333, %int1_3971 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_3972 = torch.constant.none
    %3335 = torch.aten.clone %160, %none_3972 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_3973 = torch.constant.int 0
    %3336 = torch.aten.unsqueeze %3335, %int0_3973 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_3974 = torch.constant.int 4
    %int1_3975 = torch.constant.int 1
    %3337 = torch.prim.ListConstruct %int4_3974, %int1_3975 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_3976 = torch.constant.int 1
    %int1_3977 = torch.constant.int 1
    %3338 = torch.prim.ListConstruct %int1_3976, %int1_3977 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_3978 = torch.constant.int 4
    %int0_3979 = torch.constant.int 0
    %cpu_3980 = torch.constant.device "cpu"
    %false_3981 = torch.constant.bool false
    %3339 = torch.aten.empty_strided %3337, %3338, %int4_3978, %int0_3979, %cpu_3980, %false_3981 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int14_3982 = torch.constant.int 14
    %3340 = torch.aten.fill.Scalar %3339, %int14_3982 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_3983 = torch.constant.int 4
    %int1_3984 = torch.constant.int 1
    %3341 = torch.prim.ListConstruct %int4_3983, %int1_3984 : (!torch.int, !torch.int) -> !torch.list<int>
    %3342 = torch.aten.repeat %3336, %3341 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_3985 = torch.constant.int 32
    %3343 = torch.aten.mul.Scalar %3332, %int32_3985 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3986 = torch.constant.int 1
    %3344 = torch.aten.add.Tensor %3343, %3340, %int1_3986 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_3987 = torch.constant.int 2
    %3345 = torch.aten.mul.Scalar %3344, %int2_3987 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3988 = torch.constant.int 1
    %3346 = torch.aten.add.Tensor %3345, %3342, %int1_3988 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_3989 = torch.constant.int 32
    %3347 = torch.aten.mul.Scalar %3346, %int32_3989 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_3990 = torch.constant.int 1
    %3348 = torch.aten.add.Tensor %3347, %3334, %int1_3990 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %3349 = torch.prim.ListConstruct %3348 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_3991 = torch.constant.bool false
    %3350 = torch.aten.index_put %3329, %3349, %3281, %false_3991 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %3350, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_3992 = torch.constant.int 32
    %int2_3993 = torch.constant.int 2
    %int32_3994 = torch.constant.int 32
    %int8_3995 = torch.constant.int 8
    %int128_3996 = torch.constant.int 128
    %3351 = torch.prim.ListConstruct %437, %int32_3992, %int2_3993, %int32_3994, %int8_3995, %int128_3996 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3352 = torch.aten.view %3350, %3351 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3352, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_3997 = torch.constant.int 2097152
    %3353 = torch.prim.ListConstruct %437, %int2097152_3997 : (!torch.int, !torch.int) -> !torch.list<int>
    %3354 = torch.aten.view %3352, %3353 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %3354, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int4_3998 = torch.constant.int 4
    %3355 = torch.prim.ListConstruct %int4_3998, %358 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_3999 = torch.constant.int 1
    %3356 = torch.prim.ListConstruct %358, %int1_3999 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_4000 = torch.constant.int 4
    %int0_4001 = torch.constant.int 0
    %cpu_4002 = torch.constant.device "cpu"
    %false_4003 = torch.constant.bool false
    %3357 = torch.aten.empty_strided %3355, %3356, %int4_4000, %int0_4001, %cpu_4002, %false_4003 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3357, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int14_4004 = torch.constant.int 14
    %3358 = torch.aten.fill.Scalar %3357, %int14_4004 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3358, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int32_4005 = torch.constant.int 32
    %3359 = torch.aten.mul.Scalar %arg3, %int32_4005 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3359, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_4006 = torch.constant.int 1
    %3360 = torch.aten.add.Tensor %3359, %3358, %int1_4006 : !torch.vtensor<[4,?],si64>, !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3360, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_4007 = torch.constant.int 4
    %3361 = torch.aten.mul.int %int4_4007, %358 : !torch.int, !torch.int -> !torch.int
    %3362 = torch.prim.ListConstruct %3361 : (!torch.int) -> !torch.list<int>
    %3363 = torch.aten.view %3360, %3362 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3363, [%356], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_4008 = torch.constant.int 32
    %int2_4009 = torch.constant.int 2
    %int32_4010 = torch.constant.int 32
    %int8_4011 = torch.constant.int 8
    %int128_4012 = torch.constant.int 128
    %3364 = torch.prim.ListConstruct %437, %int32_4008, %int2_4009, %int32_4010, %int8_4011, %int128_4012 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3365 = torch.aten.view %3354, %3364 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3365, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_4013 = torch.constant.int 32
    %3366 = torch.aten.mul.int %437, %int32_4013 : !torch.int, !torch.int -> !torch.int
    %int2_4014 = torch.constant.int 2
    %int32_4015 = torch.constant.int 32
    %int8_4016 = torch.constant.int 8
    %int128_4017 = torch.constant.int 128
    %3367 = torch.prim.ListConstruct %3366, %int2_4014, %int32_4015, %int8_4016, %int128_4017 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3368 = torch.aten.view %3365, %3367 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %3368, [%357], affine_map<()[s0] -> (s0 * 32, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int0_4018 = torch.constant.int 0
    %3369 = torch.aten.index_select %3368, %int0_4018, %3363 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.int, !torch.vtensor<[?],si64> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %3369, [%356], affine_map<()[s0] -> (s0 * 4, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int4_4019 = torch.constant.int 4
    %int2_4020 = torch.constant.int 2
    %int32_4021 = torch.constant.int 32
    %int8_4022 = torch.constant.int 8
    %int128_4023 = torch.constant.int 128
    %3370 = torch.prim.ListConstruct %int4_4019, %358, %int2_4020, %int32_4021, %int8_4022, %int128_4023 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3371 = torch.aten.view %3369, %3370 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %3371, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int0_4024 = torch.constant.int 0
    %int0_4025 = torch.constant.int 0
    %int9223372036854775807_4026 = torch.constant.int 9223372036854775807
    %int1_4027 = torch.constant.int 1
    %3372 = torch.aten.slice.Tensor %3371, %int0_4024, %int0_4025, %int9223372036854775807_4026, %int1_4027 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %3372, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_4028 = torch.constant.int 1
    %int0_4029 = torch.constant.int 0
    %int9223372036854775807_4030 = torch.constant.int 9223372036854775807
    %int1_4031 = torch.constant.int 1
    %3373 = torch.aten.slice.Tensor %3372, %int1_4028, %int0_4029, %int9223372036854775807_4030, %int1_4031 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %3373, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_4032 = torch.constant.int 2
    %int0_4033 = torch.constant.int 0
    %3374 = torch.aten.select.int %3373, %int2_4032, %int0_4033 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3374, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int32_4034 = torch.constant.int 32
    %3375 = torch.aten.mul.int %358, %int32_4034 : !torch.int, !torch.int -> !torch.int
    %int2_4035 = torch.constant.int 2
    %int0_4036 = torch.constant.int 0
    %int1_4037 = torch.constant.int 1
    %3376 = torch.aten.slice.Tensor %3374, %int2_4035, %int0_4036, %3375, %int1_4037 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3376, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_4038 = torch.constant.int 0
    %3377 = torch.aten.clone %3376, %int0_4038 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3377, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_4039 = torch.constant.int 1
    %3378 = torch.aten.size.int %3373, %int1_4039 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_4040 = torch.constant.int 32
    %3379 = torch.aten.mul.int %3378, %int32_4040 : !torch.int, !torch.int -> !torch.int
    %int4_4041 = torch.constant.int 4
    %int8_4042 = torch.constant.int 8
    %int128_4043 = torch.constant.int 128
    %3380 = torch.prim.ListConstruct %int4_4041, %3379, %int8_4042, %int128_4043 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3381 = torch.aten._unsafe_view %3377, %3380 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %3381, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_4044 = torch.constant.int 0
    %int0_4045 = torch.constant.int 0
    %int9223372036854775807_4046 = torch.constant.int 9223372036854775807
    %int1_4047 = torch.constant.int 1
    %3382 = torch.aten.slice.Tensor %3381, %int0_4044, %int0_4045, %int9223372036854775807_4046, %int1_4047 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %3382, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_4048 = torch.constant.int 0
    %int0_4049 = torch.constant.int 0
    %int9223372036854775807_4050 = torch.constant.int 9223372036854775807
    %int1_4051 = torch.constant.int 1
    %3383 = torch.aten.slice.Tensor %3371, %int0_4048, %int0_4049, %int9223372036854775807_4050, %int1_4051 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %3383, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_4052 = torch.constant.int 1
    %int0_4053 = torch.constant.int 0
    %int9223372036854775807_4054 = torch.constant.int 9223372036854775807
    %int1_4055 = torch.constant.int 1
    %3384 = torch.aten.slice.Tensor %3383, %int1_4052, %int0_4053, %int9223372036854775807_4054, %int1_4055 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %3384, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_4056 = torch.constant.int 2
    %int1_4057 = torch.constant.int 1
    %3385 = torch.aten.select.int %3384, %int2_4056, %int1_4057 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3385, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int2_4058 = torch.constant.int 2
    %int0_4059 = torch.constant.int 0
    %int1_4060 = torch.constant.int 1
    %3386 = torch.aten.slice.Tensor %3385, %int2_4058, %int0_4059, %3375, %int1_4060 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3386, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_4061 = torch.constant.int 0
    %3387 = torch.aten.clone %3386, %int0_4061 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3387, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_4062 = torch.constant.int 1
    %3388 = torch.aten.size.int %3384, %int1_4062 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_4063 = torch.constant.int 32
    %3389 = torch.aten.mul.int %3388, %int32_4063 : !torch.int, !torch.int -> !torch.int
    %int4_4064 = torch.constant.int 4
    %int8_4065 = torch.constant.int 8
    %int128_4066 = torch.constant.int 128
    %3390 = torch.prim.ListConstruct %int4_4064, %3389, %int8_4065, %int128_4066 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3391 = torch.aten._unsafe_view %3387, %3390 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %3391, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_4067 = torch.constant.int 0
    %int0_4068 = torch.constant.int 0
    %int9223372036854775807_4069 = torch.constant.int 9223372036854775807
    %int1_4070 = torch.constant.int 1
    %3392 = torch.aten.slice.Tensor %3391, %int0_4067, %int0_4068, %int9223372036854775807_4069, %int1_4070 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %3392, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int-2_4071 = torch.constant.int -2
    %3393 = torch.aten.unsqueeze %3382, %int-2_4071 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %3393, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_4072 = torch.constant.int 1
    %3394 = torch.aten.size.int %3381, %int1_4072 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_4073 = torch.constant.int 4
    %int8_4074 = torch.constant.int 8
    %int4_4075 = torch.constant.int 4
    %int128_4076 = torch.constant.int 128
    %3395 = torch.prim.ListConstruct %int4_4073, %3394, %int8_4074, %int4_4075, %int128_4076 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_4077 = torch.constant.bool false
    %3396 = torch.aten.expand %3393, %3395, %false_4077 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3396, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_4078 = torch.constant.int 0
    %3397 = torch.aten.clone %3396, %int0_4078 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3397, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_4079 = torch.constant.int 4
    %int32_4080 = torch.constant.int 32
    %int128_4081 = torch.constant.int 128
    %3398 = torch.prim.ListConstruct %int4_4079, %3394, %int32_4080, %int128_4081 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3399 = torch.aten._unsafe_view %3397, %3398 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %3399, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_4082 = torch.constant.int -2
    %3400 = torch.aten.unsqueeze %3392, %int-2_4082 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %3400, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_4083 = torch.constant.int 1
    %3401 = torch.aten.size.int %3391, %int1_4083 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_4084 = torch.constant.int 4
    %int8_4085 = torch.constant.int 8
    %int4_4086 = torch.constant.int 4
    %int128_4087 = torch.constant.int 128
    %3402 = torch.prim.ListConstruct %int4_4084, %3401, %int8_4085, %int4_4086, %int128_4087 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_4088 = torch.constant.bool false
    %3403 = torch.aten.expand %3400, %3402, %false_4088 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3403, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_4089 = torch.constant.int 0
    %3404 = torch.aten.clone %3403, %int0_4089 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3404, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_4090 = torch.constant.int 4
    %int32_4091 = torch.constant.int 32
    %int128_4092 = torch.constant.int 128
    %3405 = torch.prim.ListConstruct %int4_4090, %3401, %int32_4091, %int128_4092 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3406 = torch.aten._unsafe_view %3404, %3405 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %3406, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_4093 = torch.constant.int 1
    %int2_4094 = torch.constant.int 2
    %3407 = torch.aten.transpose.int %3287, %int1_4093, %int2_4094 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,128],f16>
    %int1_4095 = torch.constant.int 1
    %int2_4096 = torch.constant.int 2
    %3408 = torch.aten.transpose.int %3399, %int1_4095, %int2_4096 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %3408, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_4097 = torch.constant.int 1
    %int2_4098 = torch.constant.int 2
    %3409 = torch.aten.transpose.int %3406, %int1_4097, %int2_4098 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %3409, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_4099 = torch.constant.float 0.000000e+00
    %false_4100 = torch.constant.bool false
    %none_4101 = torch.constant.none
    %3410:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3407, %3408, %3409, %float0.000000e00_4099, %false_4100, %368, %none_4101) : (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.vtensor<[4,1,1,?],f16>, !torch.none) -> (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,1],f32>) 
    %int1_4102 = torch.constant.int 1
    %int2_4103 = torch.constant.int 2
    %3411 = torch.aten.transpose.int %3410#0, %int1_4102, %int2_4103 : !torch.vtensor<[4,32,1,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int4_4104 = torch.constant.int 4
    %int1_4105 = torch.constant.int 1
    %int4096_4106 = torch.constant.int 4096
    %3412 = torch.prim.ListConstruct %int4_4104, %int1_4105, %int4096_4106 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3413 = torch.aten.view %3411, %3412 : !torch.vtensor<[4,1,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_4107 = torch.constant.int -2
    %int-1_4108 = torch.constant.int -1
    %3414 = torch.aten.transpose.int %161, %int-2_4107, %int-1_4108 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_4109 = torch.constant.int 4
    %int4096_4110 = torch.constant.int 4096
    %3415 = torch.prim.ListConstruct %int4_4109, %int4096_4110 : (!torch.int, !torch.int) -> !torch.list<int>
    %3416 = torch.aten.view %3413, %3415 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %3417 = torch.aten.mm %3416, %3414 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_4111 = torch.constant.int 4
    %int1_4112 = torch.constant.int 1
    %int4096_4113 = torch.constant.int 4096
    %3418 = torch.prim.ListConstruct %int4_4111, %int1_4112, %int4096_4113 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3419 = torch.aten.view %3417, %3418 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_4114 = torch.constant.int 1
    %3420 = torch.aten.add.Tensor %3247, %3419, %int1_4114 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_4115 = torch.constant.int 6
    %3421 = torch.prims.convert_element_type %3420, %int6_4115 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_4116 = torch.constant.int 2
    %3422 = torch.aten.pow.Tensor_Scalar %3421, %int2_4116 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_4117 = torch.constant.int -1
    %3423 = torch.prim.ListConstruct %int-1_4117 : (!torch.int) -> !torch.list<int>
    %true_4118 = torch.constant.bool true
    %none_4119 = torch.constant.none
    %3424 = torch.aten.mean.dim %3422, %3423, %true_4118, %none_4119 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_4120 = torch.constant.float 9.9999997473787516E-6
    %int1_4121 = torch.constant.int 1
    %3425 = torch.aten.add.Scalar %3424, %float9.999990e-06_4120, %int1_4121 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %3426 = torch.aten.rsqrt %3425 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %3427 = torch.aten.mul.Tensor %3421, %3426 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_4122 = torch.constant.int 5
    %3428 = torch.prims.convert_element_type %3427, %int5_4122 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %3429 = torch.aten.mul.Tensor %162, %3428 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_4123 = torch.constant.int 5
    %3430 = torch.prims.convert_element_type %3429, %int5_4123 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_4124 = torch.constant.int -2
    %int-1_4125 = torch.constant.int -1
    %3431 = torch.aten.transpose.int %163, %int-2_4124, %int-1_4125 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_4126 = torch.constant.int 4
    %int4096_4127 = torch.constant.int 4096
    %3432 = torch.prim.ListConstruct %int4_4126, %int4096_4127 : (!torch.int, !torch.int) -> !torch.list<int>
    %3433 = torch.aten.view %3430, %3432 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %3434 = torch.aten.mm %3433, %3431 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_4128 = torch.constant.int 4
    %int1_4129 = torch.constant.int 1
    %int14336_4130 = torch.constant.int 14336
    %3435 = torch.prim.ListConstruct %int4_4128, %int1_4129, %int14336_4130 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3436 = torch.aten.view %3434, %3435 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %3437 = torch.aten.silu %3436 : !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_4131 = torch.constant.int -2
    %int-1_4132 = torch.constant.int -1
    %3438 = torch.aten.transpose.int %164, %int-2_4131, %int-1_4132 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_4133 = torch.constant.int 4
    %int4096_4134 = torch.constant.int 4096
    %3439 = torch.prim.ListConstruct %int4_4133, %int4096_4134 : (!torch.int, !torch.int) -> !torch.list<int>
    %3440 = torch.aten.view %3430, %3439 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %3441 = torch.aten.mm %3440, %3438 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_4135 = torch.constant.int 4
    %int1_4136 = torch.constant.int 1
    %int14336_4137 = torch.constant.int 14336
    %3442 = torch.prim.ListConstruct %int4_4135, %int1_4136, %int14336_4137 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3443 = torch.aten.view %3441, %3442 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %3444 = torch.aten.mul.Tensor %3437, %3443 : !torch.vtensor<[4,1,14336],f16>, !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_4138 = torch.constant.int -2
    %int-1_4139 = torch.constant.int -1
    %3445 = torch.aten.transpose.int %165, %int-2_4138, %int-1_4139 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int4_4140 = torch.constant.int 4
    %int14336_4141 = torch.constant.int 14336
    %3446 = torch.prim.ListConstruct %int4_4140, %int14336_4141 : (!torch.int, !torch.int) -> !torch.list<int>
    %3447 = torch.aten.view %3444, %3446 : !torch.vtensor<[4,1,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,14336],f16>
    %3448 = torch.aten.mm %3447, %3445 : !torch.vtensor<[4,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_4142 = torch.constant.int 4
    %int1_4143 = torch.constant.int 1
    %int4096_4144 = torch.constant.int 4096
    %3449 = torch.prim.ListConstruct %int4_4142, %int1_4143, %int4096_4144 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3450 = torch.aten.view %3448, %3449 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_4145 = torch.constant.int 1
    %3451 = torch.aten.add.Tensor %3420, %3450, %int1_4145 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_4146 = torch.constant.int 6
    %3452 = torch.prims.convert_element_type %3451, %int6_4146 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_4147 = torch.constant.int 2
    %3453 = torch.aten.pow.Tensor_Scalar %3452, %int2_4147 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_4148 = torch.constant.int -1
    %3454 = torch.prim.ListConstruct %int-1_4148 : (!torch.int) -> !torch.list<int>
    %true_4149 = torch.constant.bool true
    %none_4150 = torch.constant.none
    %3455 = torch.aten.mean.dim %3453, %3454, %true_4149, %none_4150 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_4151 = torch.constant.float 9.9999997473787516E-6
    %int1_4152 = torch.constant.int 1
    %3456 = torch.aten.add.Scalar %3455, %float9.999990e-06_4151, %int1_4152 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %3457 = torch.aten.rsqrt %3456 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %3458 = torch.aten.mul.Tensor %3452, %3457 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_4153 = torch.constant.int 5
    %3459 = torch.prims.convert_element_type %3458, %int5_4153 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %3460 = torch.aten.mul.Tensor %166, %3459 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_4154 = torch.constant.int 5
    %3461 = torch.prims.convert_element_type %3460, %int5_4154 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_4155 = torch.constant.int -2
    %int-1_4156 = torch.constant.int -1
    %3462 = torch.aten.transpose.int %167, %int-2_4155, %int-1_4156 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_4157 = torch.constant.int 4
    %int4096_4158 = torch.constant.int 4096
    %3463 = torch.prim.ListConstruct %int4_4157, %int4096_4158 : (!torch.int, !torch.int) -> !torch.list<int>
    %3464 = torch.aten.view %3461, %3463 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %3465 = torch.aten.mm %3464, %3462 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_4159 = torch.constant.int 4
    %int1_4160 = torch.constant.int 1
    %int4096_4161 = torch.constant.int 4096
    %3466 = torch.prim.ListConstruct %int4_4159, %int1_4160, %int4096_4161 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3467 = torch.aten.view %3465, %3466 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_4162 = torch.constant.int -2
    %int-1_4163 = torch.constant.int -1
    %3468 = torch.aten.transpose.int %168, %int-2_4162, %int-1_4163 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_4164 = torch.constant.int 4
    %int4096_4165 = torch.constant.int 4096
    %3469 = torch.prim.ListConstruct %int4_4164, %int4096_4165 : (!torch.int, !torch.int) -> !torch.list<int>
    %3470 = torch.aten.view %3461, %3469 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %3471 = torch.aten.mm %3470, %3468 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_4166 = torch.constant.int 4
    %int1_4167 = torch.constant.int 1
    %int1024_4168 = torch.constant.int 1024
    %3472 = torch.prim.ListConstruct %int4_4166, %int1_4167, %int1024_4168 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3473 = torch.aten.view %3471, %3472 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int-2_4169 = torch.constant.int -2
    %int-1_4170 = torch.constant.int -1
    %3474 = torch.aten.transpose.int %169, %int-2_4169, %int-1_4170 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_4171 = torch.constant.int 4
    %int4096_4172 = torch.constant.int 4096
    %3475 = torch.prim.ListConstruct %int4_4171, %int4096_4172 : (!torch.int, !torch.int) -> !torch.list<int>
    %3476 = torch.aten.view %3461, %3475 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %3477 = torch.aten.mm %3476, %3474 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_4173 = torch.constant.int 4
    %int1_4174 = torch.constant.int 1
    %int1024_4175 = torch.constant.int 1024
    %3478 = torch.prim.ListConstruct %int4_4173, %int1_4174, %int1024_4175 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3479 = torch.aten.view %3477, %3478 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int4_4176 = torch.constant.int 4
    %int1_4177 = torch.constant.int 1
    %int32_4178 = torch.constant.int 32
    %int128_4179 = torch.constant.int 128
    %3480 = torch.prim.ListConstruct %int4_4176, %int1_4177, %int32_4178, %int128_4179 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3481 = torch.aten.view %3467, %3480 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,128],f16>
    %int4_4180 = torch.constant.int 4
    %int1_4181 = torch.constant.int 1
    %int8_4182 = torch.constant.int 8
    %int128_4183 = torch.constant.int 128
    %3482 = torch.prim.ListConstruct %int4_4180, %int1_4181, %int8_4182, %int128_4183 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3483 = torch.aten.view %3473, %3482 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int4_4184 = torch.constant.int 4
    %int1_4185 = torch.constant.int 1
    %int8_4186 = torch.constant.int 8
    %int128_4187 = torch.constant.int 128
    %3484 = torch.prim.ListConstruct %int4_4184, %int1_4185, %int8_4186, %int128_4187 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3485 = torch.aten.view %3479, %3484 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int6_4188 = torch.constant.int 6
    %3486 = torch.prims.convert_element_type %3481, %int6_4188 : !torch.vtensor<[4,1,32,128],f16>, !torch.int -> !torch.vtensor<[4,1,32,128],f32>
    %3487 = torch_c.to_builtin_tensor %3486 : !torch.vtensor<[4,1,32,128],f32> -> tensor<4x1x32x128xf32>
    %3488 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %3489 = util.call @sharktank_rotary_embedding_4_1_32_128_f32(%3487, %3488) : (tensor<4x1x32x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x32x128xf32>
    %3490 = torch_c.from_builtin_tensor %3489 : tensor<4x1x32x128xf32> -> !torch.vtensor<[4,1,32,128],f32>
    %int5_4189 = torch.constant.int 5
    %3491 = torch.prims.convert_element_type %3490, %int5_4189 : !torch.vtensor<[4,1,32,128],f32>, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int6_4190 = torch.constant.int 6
    %3492 = torch.prims.convert_element_type %3483, %int6_4190 : !torch.vtensor<[4,1,8,128],f16>, !torch.int -> !torch.vtensor<[4,1,8,128],f32>
    %3493 = torch_c.to_builtin_tensor %3492 : !torch.vtensor<[4,1,8,128],f32> -> tensor<4x1x8x128xf32>
    %3494 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %3495 = util.call @sharktank_rotary_embedding_4_1_8_128_f32(%3493, %3494) : (tensor<4x1x8x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x8x128xf32>
    %3496 = torch_c.from_builtin_tensor %3495 : tensor<4x1x8x128xf32> -> !torch.vtensor<[4,1,8,128],f32>
    %int5_4191 = torch.constant.int 5
    %3497 = torch.prims.convert_element_type %3496, %int5_4191 : !torch.vtensor<[4,1,8,128],f32>, !torch.int -> !torch.vtensor<[4,1,8,128],f16>
    %int32_4192 = torch.constant.int 32
    %3498 = torch.aten.floor_divide.Scalar %arg2, %int32_4192 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_4193 = torch.constant.int 1
    %3499 = torch.aten.unsqueeze %3498, %int1_4193 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_4194 = torch.constant.int 1
    %false_4195 = torch.constant.bool false
    %3500 = torch.aten.gather %arg3, %int1_4194, %3499, %false_4195 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_4196 = torch.constant.int 32
    %3501 = torch.aten.remainder.Scalar %arg2, %int32_4196 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_4197 = torch.constant.int 1
    %3502 = torch.aten.unsqueeze %3501, %int1_4197 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_4198 = torch.constant.none
    %3503 = torch.aten.clone %170, %none_4198 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_4199 = torch.constant.int 0
    %3504 = torch.aten.unsqueeze %3503, %int0_4199 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_4200 = torch.constant.int 4
    %int1_4201 = torch.constant.int 1
    %3505 = torch.prim.ListConstruct %int4_4200, %int1_4201 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_4202 = torch.constant.int 1
    %int1_4203 = torch.constant.int 1
    %3506 = torch.prim.ListConstruct %int1_4202, %int1_4203 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_4204 = torch.constant.int 4
    %int0_4205 = torch.constant.int 0
    %cpu_4206 = torch.constant.device "cpu"
    %false_4207 = torch.constant.bool false
    %3507 = torch.aten.empty_strided %3505, %3506, %int4_4204, %int0_4205, %cpu_4206, %false_4207 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int15 = torch.constant.int 15
    %3508 = torch.aten.fill.Scalar %3507, %int15 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_4208 = torch.constant.int 4
    %int1_4209 = torch.constant.int 1
    %3509 = torch.prim.ListConstruct %int4_4208, %int1_4209 : (!torch.int, !torch.int) -> !torch.list<int>
    %3510 = torch.aten.repeat %3504, %3509 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_4210 = torch.constant.int 32
    %3511 = torch.aten.mul.Scalar %3500, %int32_4210 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_4211 = torch.constant.int 1
    %3512 = torch.aten.add.Tensor %3511, %3508, %int1_4211 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_4212 = torch.constant.int 2
    %3513 = torch.aten.mul.Scalar %3512, %int2_4212 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_4213 = torch.constant.int 1
    %3514 = torch.aten.add.Tensor %3513, %3510, %int1_4213 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_4214 = torch.constant.int 32
    %3515 = torch.aten.mul.Scalar %3514, %int32_4214 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_4215 = torch.constant.int 1
    %3516 = torch.aten.add.Tensor %3515, %3502, %int1_4215 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_4216 = torch.constant.int 32
    %int2_4217 = torch.constant.int 2
    %int32_4218 = torch.constant.int 32
    %int8_4219 = torch.constant.int 8
    %int128_4220 = torch.constant.int 128
    %3517 = torch.prim.ListConstruct %437, %int32_4216, %int2_4217, %int32_4218, %int8_4219, %int128_4220 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3518 = torch.aten.view %3354, %3517 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3518, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_4221 = torch.constant.int 32
    %3519 = torch.aten.mul.int %437, %int32_4221 : !torch.int, !torch.int -> !torch.int
    %int2_4222 = torch.constant.int 2
    %3520 = torch.aten.mul.int %3519, %int2_4222 : !torch.int, !torch.int -> !torch.int
    %int32_4223 = torch.constant.int 32
    %3521 = torch.aten.mul.int %3520, %int32_4223 : !torch.int, !torch.int -> !torch.int
    %int8_4224 = torch.constant.int 8
    %int128_4225 = torch.constant.int 128
    %3522 = torch.prim.ListConstruct %3521, %int8_4224, %int128_4225 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3523 = torch.aten.view %3518, %3522 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %3523, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %3524 = torch.prim.ListConstruct %3516 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_4226 = torch.constant.bool false
    %3525 = torch.aten.index_put %3523, %3524, %3497, %false_4226 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %3525, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_4227 = torch.constant.int 32
    %int2_4228 = torch.constant.int 2
    %int32_4229 = torch.constant.int 32
    %int8_4230 = torch.constant.int 8
    %int128_4231 = torch.constant.int 128
    %3526 = torch.prim.ListConstruct %437, %int32_4227, %int2_4228, %int32_4229, %int8_4230, %int128_4231 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3527 = torch.aten.view %3525, %3526 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3527, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_4232 = torch.constant.int 2097152
    %3528 = torch.prim.ListConstruct %437, %int2097152_4232 : (!torch.int, !torch.int) -> !torch.list<int>
    %3529 = torch.aten.view %3527, %3528 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %3529, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_4233 = torch.constant.int 32
    %int2_4234 = torch.constant.int 2
    %int32_4235 = torch.constant.int 32
    %int8_4236 = torch.constant.int 8
    %int128_4237 = torch.constant.int 128
    %3530 = torch.prim.ListConstruct %437, %int32_4233, %int2_4234, %int32_4235, %int8_4236, %int128_4237 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3531 = torch.aten.view %3529, %3530 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3531, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int8_4238 = torch.constant.int 8
    %int128_4239 = torch.constant.int 128
    %3532 = torch.prim.ListConstruct %3521, %int8_4238, %int128_4239 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3533 = torch.aten.view %3531, %3532 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %3533, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_4240 = torch.constant.int 32
    %3534 = torch.aten.floor_divide.Scalar %arg2, %int32_4240 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_4241 = torch.constant.int 1
    %3535 = torch.aten.unsqueeze %3534, %int1_4241 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_4242 = torch.constant.int 1
    %false_4243 = torch.constant.bool false
    %3536 = torch.aten.gather %arg3, %int1_4242, %3535, %false_4243 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_4244 = torch.constant.int 32
    %3537 = torch.aten.remainder.Scalar %arg2, %int32_4244 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_4245 = torch.constant.int 1
    %3538 = torch.aten.unsqueeze %3537, %int1_4245 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_4246 = torch.constant.none
    %3539 = torch.aten.clone %171, %none_4246 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_4247 = torch.constant.int 0
    %3540 = torch.aten.unsqueeze %3539, %int0_4247 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_4248 = torch.constant.int 4
    %int1_4249 = torch.constant.int 1
    %3541 = torch.prim.ListConstruct %int4_4248, %int1_4249 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_4250 = torch.constant.int 1
    %int1_4251 = torch.constant.int 1
    %3542 = torch.prim.ListConstruct %int1_4250, %int1_4251 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_4252 = torch.constant.int 4
    %int0_4253 = torch.constant.int 0
    %cpu_4254 = torch.constant.device "cpu"
    %false_4255 = torch.constant.bool false
    %3543 = torch.aten.empty_strided %3541, %3542, %int4_4252, %int0_4253, %cpu_4254, %false_4255 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int15_4256 = torch.constant.int 15
    %3544 = torch.aten.fill.Scalar %3543, %int15_4256 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_4257 = torch.constant.int 4
    %int1_4258 = torch.constant.int 1
    %3545 = torch.prim.ListConstruct %int4_4257, %int1_4258 : (!torch.int, !torch.int) -> !torch.list<int>
    %3546 = torch.aten.repeat %3540, %3545 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_4259 = torch.constant.int 32
    %3547 = torch.aten.mul.Scalar %3536, %int32_4259 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_4260 = torch.constant.int 1
    %3548 = torch.aten.add.Tensor %3547, %3544, %int1_4260 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_4261 = torch.constant.int 2
    %3549 = torch.aten.mul.Scalar %3548, %int2_4261 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_4262 = torch.constant.int 1
    %3550 = torch.aten.add.Tensor %3549, %3546, %int1_4262 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_4263 = torch.constant.int 32
    %3551 = torch.aten.mul.Scalar %3550, %int32_4263 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_4264 = torch.constant.int 1
    %3552 = torch.aten.add.Tensor %3551, %3538, %int1_4264 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %3553 = torch.prim.ListConstruct %3552 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_4265 = torch.constant.bool false
    %3554 = torch.aten.index_put %3533, %3553, %3485, %false_4265 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %3554, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_4266 = torch.constant.int 32
    %int2_4267 = torch.constant.int 2
    %int32_4268 = torch.constant.int 32
    %int8_4269 = torch.constant.int 8
    %int128_4270 = torch.constant.int 128
    %3555 = torch.prim.ListConstruct %437, %int32_4266, %int2_4267, %int32_4268, %int8_4269, %int128_4270 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3556 = torch.aten.view %3554, %3555 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3556, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_4271 = torch.constant.int 2097152
    %3557 = torch.prim.ListConstruct %437, %int2097152_4271 : (!torch.int, !torch.int) -> !torch.list<int>
    %3558 = torch.aten.view %3556, %3557 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %3558, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int4_4272 = torch.constant.int 4
    %3559 = torch.prim.ListConstruct %int4_4272, %358 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_4273 = torch.constant.int 1
    %3560 = torch.prim.ListConstruct %358, %int1_4273 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_4274 = torch.constant.int 4
    %int0_4275 = torch.constant.int 0
    %cpu_4276 = torch.constant.device "cpu"
    %false_4277 = torch.constant.bool false
    %3561 = torch.aten.empty_strided %3559, %3560, %int4_4274, %int0_4275, %cpu_4276, %false_4277 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3561, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int15_4278 = torch.constant.int 15
    %3562 = torch.aten.fill.Scalar %3561, %int15_4278 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3562, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int32_4279 = torch.constant.int 32
    %3563 = torch.aten.mul.Scalar %arg3, %int32_4279 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3563, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_4280 = torch.constant.int 1
    %3564 = torch.aten.add.Tensor %3563, %3562, %int1_4280 : !torch.vtensor<[4,?],si64>, !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3564, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_4281 = torch.constant.int 4
    %3565 = torch.aten.mul.int %int4_4281, %358 : !torch.int, !torch.int -> !torch.int
    %3566 = torch.prim.ListConstruct %3565 : (!torch.int) -> !torch.list<int>
    %3567 = torch.aten.view %3564, %3566 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3567, [%356], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_4282 = torch.constant.int 32
    %int2_4283 = torch.constant.int 2
    %int32_4284 = torch.constant.int 32
    %int8_4285 = torch.constant.int 8
    %int128_4286 = torch.constant.int 128
    %3568 = torch.prim.ListConstruct %437, %int32_4282, %int2_4283, %int32_4284, %int8_4285, %int128_4286 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3569 = torch.aten.view %3558, %3568 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3569, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_4287 = torch.constant.int 32
    %3570 = torch.aten.mul.int %437, %int32_4287 : !torch.int, !torch.int -> !torch.int
    %int2_4288 = torch.constant.int 2
    %int32_4289 = torch.constant.int 32
    %int8_4290 = torch.constant.int 8
    %int128_4291 = torch.constant.int 128
    %3571 = torch.prim.ListConstruct %3570, %int2_4288, %int32_4289, %int8_4290, %int128_4291 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3572 = torch.aten.view %3569, %3571 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %3572, [%357], affine_map<()[s0] -> (s0 * 32, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int0_4292 = torch.constant.int 0
    %3573 = torch.aten.index_select %3572, %int0_4292, %3567 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.int, !torch.vtensor<[?],si64> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %3573, [%356], affine_map<()[s0] -> (s0 * 4, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int4_4293 = torch.constant.int 4
    %int2_4294 = torch.constant.int 2
    %int32_4295 = torch.constant.int 32
    %int8_4296 = torch.constant.int 8
    %int128_4297 = torch.constant.int 128
    %3574 = torch.prim.ListConstruct %int4_4293, %358, %int2_4294, %int32_4295, %int8_4296, %int128_4297 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3575 = torch.aten.view %3573, %3574 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %3575, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int0_4298 = torch.constant.int 0
    %int0_4299 = torch.constant.int 0
    %int9223372036854775807_4300 = torch.constant.int 9223372036854775807
    %int1_4301 = torch.constant.int 1
    %3576 = torch.aten.slice.Tensor %3575, %int0_4298, %int0_4299, %int9223372036854775807_4300, %int1_4301 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %3576, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_4302 = torch.constant.int 1
    %int0_4303 = torch.constant.int 0
    %int9223372036854775807_4304 = torch.constant.int 9223372036854775807
    %int1_4305 = torch.constant.int 1
    %3577 = torch.aten.slice.Tensor %3576, %int1_4302, %int0_4303, %int9223372036854775807_4304, %int1_4305 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %3577, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_4306 = torch.constant.int 2
    %int0_4307 = torch.constant.int 0
    %3578 = torch.aten.select.int %3577, %int2_4306, %int0_4307 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3578, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int32_4308 = torch.constant.int 32
    %3579 = torch.aten.mul.int %358, %int32_4308 : !torch.int, !torch.int -> !torch.int
    %int2_4309 = torch.constant.int 2
    %int0_4310 = torch.constant.int 0
    %int1_4311 = torch.constant.int 1
    %3580 = torch.aten.slice.Tensor %3578, %int2_4309, %int0_4310, %3579, %int1_4311 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3580, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_4312 = torch.constant.int 0
    %3581 = torch.aten.clone %3580, %int0_4312 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3581, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_4313 = torch.constant.int 1
    %3582 = torch.aten.size.int %3577, %int1_4313 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_4314 = torch.constant.int 32
    %3583 = torch.aten.mul.int %3582, %int32_4314 : !torch.int, !torch.int -> !torch.int
    %int4_4315 = torch.constant.int 4
    %int8_4316 = torch.constant.int 8
    %int128_4317 = torch.constant.int 128
    %3584 = torch.prim.ListConstruct %int4_4315, %3583, %int8_4316, %int128_4317 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3585 = torch.aten._unsafe_view %3581, %3584 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %3585, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_4318 = torch.constant.int 0
    %int0_4319 = torch.constant.int 0
    %int9223372036854775807_4320 = torch.constant.int 9223372036854775807
    %int1_4321 = torch.constant.int 1
    %3586 = torch.aten.slice.Tensor %3585, %int0_4318, %int0_4319, %int9223372036854775807_4320, %int1_4321 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %3586, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_4322 = torch.constant.int 0
    %int0_4323 = torch.constant.int 0
    %int9223372036854775807_4324 = torch.constant.int 9223372036854775807
    %int1_4325 = torch.constant.int 1
    %3587 = torch.aten.slice.Tensor %3575, %int0_4322, %int0_4323, %int9223372036854775807_4324, %int1_4325 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %3587, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_4326 = torch.constant.int 1
    %int0_4327 = torch.constant.int 0
    %int9223372036854775807_4328 = torch.constant.int 9223372036854775807
    %int1_4329 = torch.constant.int 1
    %3588 = torch.aten.slice.Tensor %3587, %int1_4326, %int0_4327, %int9223372036854775807_4328, %int1_4329 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %3588, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_4330 = torch.constant.int 2
    %int1_4331 = torch.constant.int 1
    %3589 = torch.aten.select.int %3588, %int2_4330, %int1_4331 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3589, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int2_4332 = torch.constant.int 2
    %int0_4333 = torch.constant.int 0
    %int1_4334 = torch.constant.int 1
    %3590 = torch.aten.slice.Tensor %3589, %int2_4332, %int0_4333, %3579, %int1_4334 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3590, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_4335 = torch.constant.int 0
    %3591 = torch.aten.clone %3590, %int0_4335 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3591, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_4336 = torch.constant.int 1
    %3592 = torch.aten.size.int %3588, %int1_4336 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_4337 = torch.constant.int 32
    %3593 = torch.aten.mul.int %3592, %int32_4337 : !torch.int, !torch.int -> !torch.int
    %int4_4338 = torch.constant.int 4
    %int8_4339 = torch.constant.int 8
    %int128_4340 = torch.constant.int 128
    %3594 = torch.prim.ListConstruct %int4_4338, %3593, %int8_4339, %int128_4340 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3595 = torch.aten._unsafe_view %3591, %3594 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %3595, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_4341 = torch.constant.int 0
    %int0_4342 = torch.constant.int 0
    %int9223372036854775807_4343 = torch.constant.int 9223372036854775807
    %int1_4344 = torch.constant.int 1
    %3596 = torch.aten.slice.Tensor %3595, %int0_4341, %int0_4342, %int9223372036854775807_4343, %int1_4344 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %3596, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int-2_4345 = torch.constant.int -2
    %3597 = torch.aten.unsqueeze %3586, %int-2_4345 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %3597, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_4346 = torch.constant.int 1
    %3598 = torch.aten.size.int %3585, %int1_4346 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_4347 = torch.constant.int 4
    %int8_4348 = torch.constant.int 8
    %int4_4349 = torch.constant.int 4
    %int128_4350 = torch.constant.int 128
    %3599 = torch.prim.ListConstruct %int4_4347, %3598, %int8_4348, %int4_4349, %int128_4350 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_4351 = torch.constant.bool false
    %3600 = torch.aten.expand %3597, %3599, %false_4351 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3600, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_4352 = torch.constant.int 0
    %3601 = torch.aten.clone %3600, %int0_4352 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3601, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_4353 = torch.constant.int 4
    %int32_4354 = torch.constant.int 32
    %int128_4355 = torch.constant.int 128
    %3602 = torch.prim.ListConstruct %int4_4353, %3598, %int32_4354, %int128_4355 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3603 = torch.aten._unsafe_view %3601, %3602 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %3603, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_4356 = torch.constant.int -2
    %3604 = torch.aten.unsqueeze %3596, %int-2_4356 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %3604, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_4357 = torch.constant.int 1
    %3605 = torch.aten.size.int %3595, %int1_4357 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_4358 = torch.constant.int 4
    %int8_4359 = torch.constant.int 8
    %int4_4360 = torch.constant.int 4
    %int128_4361 = torch.constant.int 128
    %3606 = torch.prim.ListConstruct %int4_4358, %3605, %int8_4359, %int4_4360, %int128_4361 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_4362 = torch.constant.bool false
    %3607 = torch.aten.expand %3604, %3606, %false_4362 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3607, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_4363 = torch.constant.int 0
    %3608 = torch.aten.clone %3607, %int0_4363 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3608, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_4364 = torch.constant.int 4
    %int32_4365 = torch.constant.int 32
    %int128_4366 = torch.constant.int 128
    %3609 = torch.prim.ListConstruct %int4_4364, %3605, %int32_4365, %int128_4366 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3610 = torch.aten._unsafe_view %3608, %3609 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %3610, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_4367 = torch.constant.int 1
    %int2_4368 = torch.constant.int 2
    %3611 = torch.aten.transpose.int %3491, %int1_4367, %int2_4368 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,128],f16>
    %int1_4369 = torch.constant.int 1
    %int2_4370 = torch.constant.int 2
    %3612 = torch.aten.transpose.int %3603, %int1_4369, %int2_4370 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %3612, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_4371 = torch.constant.int 1
    %int2_4372 = torch.constant.int 2
    %3613 = torch.aten.transpose.int %3610, %int1_4371, %int2_4372 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %3613, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_4373 = torch.constant.float 0.000000e+00
    %false_4374 = torch.constant.bool false
    %none_4375 = torch.constant.none
    %3614:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3611, %3612, %3613, %float0.000000e00_4373, %false_4374, %368, %none_4375) : (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.vtensor<[4,1,1,?],f16>, !torch.none) -> (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,1],f32>) 
    %int1_4376 = torch.constant.int 1
    %int2_4377 = torch.constant.int 2
    %3615 = torch.aten.transpose.int %3614#0, %int1_4376, %int2_4377 : !torch.vtensor<[4,32,1,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int4_4378 = torch.constant.int 4
    %int1_4379 = torch.constant.int 1
    %int4096_4380 = torch.constant.int 4096
    %3616 = torch.prim.ListConstruct %int4_4378, %int1_4379, %int4096_4380 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3617 = torch.aten.view %3615, %3616 : !torch.vtensor<[4,1,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_4381 = torch.constant.int -2
    %int-1_4382 = torch.constant.int -1
    %3618 = torch.aten.transpose.int %172, %int-2_4381, %int-1_4382 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_4383 = torch.constant.int 4
    %int4096_4384 = torch.constant.int 4096
    %3619 = torch.prim.ListConstruct %int4_4383, %int4096_4384 : (!torch.int, !torch.int) -> !torch.list<int>
    %3620 = torch.aten.view %3617, %3619 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %3621 = torch.aten.mm %3620, %3618 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_4385 = torch.constant.int 4
    %int1_4386 = torch.constant.int 1
    %int4096_4387 = torch.constant.int 4096
    %3622 = torch.prim.ListConstruct %int4_4385, %int1_4386, %int4096_4387 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3623 = torch.aten.view %3621, %3622 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_4388 = torch.constant.int 1
    %3624 = torch.aten.add.Tensor %3451, %3623, %int1_4388 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_4389 = torch.constant.int 6
    %3625 = torch.prims.convert_element_type %3624, %int6_4389 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_4390 = torch.constant.int 2
    %3626 = torch.aten.pow.Tensor_Scalar %3625, %int2_4390 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_4391 = torch.constant.int -1
    %3627 = torch.prim.ListConstruct %int-1_4391 : (!torch.int) -> !torch.list<int>
    %true_4392 = torch.constant.bool true
    %none_4393 = torch.constant.none
    %3628 = torch.aten.mean.dim %3626, %3627, %true_4392, %none_4393 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_4394 = torch.constant.float 9.9999997473787516E-6
    %int1_4395 = torch.constant.int 1
    %3629 = torch.aten.add.Scalar %3628, %float9.999990e-06_4394, %int1_4395 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %3630 = torch.aten.rsqrt %3629 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %3631 = torch.aten.mul.Tensor %3625, %3630 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_4396 = torch.constant.int 5
    %3632 = torch.prims.convert_element_type %3631, %int5_4396 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %3633 = torch.aten.mul.Tensor %173, %3632 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_4397 = torch.constant.int 5
    %3634 = torch.prims.convert_element_type %3633, %int5_4397 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_4398 = torch.constant.int -2
    %int-1_4399 = torch.constant.int -1
    %3635 = torch.aten.transpose.int %174, %int-2_4398, %int-1_4399 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_4400 = torch.constant.int 4
    %int4096_4401 = torch.constant.int 4096
    %3636 = torch.prim.ListConstruct %int4_4400, %int4096_4401 : (!torch.int, !torch.int) -> !torch.list<int>
    %3637 = torch.aten.view %3634, %3636 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %3638 = torch.aten.mm %3637, %3635 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_4402 = torch.constant.int 4
    %int1_4403 = torch.constant.int 1
    %int14336_4404 = torch.constant.int 14336
    %3639 = torch.prim.ListConstruct %int4_4402, %int1_4403, %int14336_4404 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3640 = torch.aten.view %3638, %3639 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %3641 = torch.aten.silu %3640 : !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_4405 = torch.constant.int -2
    %int-1_4406 = torch.constant.int -1
    %3642 = torch.aten.transpose.int %175, %int-2_4405, %int-1_4406 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_4407 = torch.constant.int 4
    %int4096_4408 = torch.constant.int 4096
    %3643 = torch.prim.ListConstruct %int4_4407, %int4096_4408 : (!torch.int, !torch.int) -> !torch.list<int>
    %3644 = torch.aten.view %3634, %3643 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %3645 = torch.aten.mm %3644, %3642 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_4409 = torch.constant.int 4
    %int1_4410 = torch.constant.int 1
    %int14336_4411 = torch.constant.int 14336
    %3646 = torch.prim.ListConstruct %int4_4409, %int1_4410, %int14336_4411 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3647 = torch.aten.view %3645, %3646 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %3648 = torch.aten.mul.Tensor %3641, %3647 : !torch.vtensor<[4,1,14336],f16>, !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_4412 = torch.constant.int -2
    %int-1_4413 = torch.constant.int -1
    %3649 = torch.aten.transpose.int %176, %int-2_4412, %int-1_4413 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int4_4414 = torch.constant.int 4
    %int14336_4415 = torch.constant.int 14336
    %3650 = torch.prim.ListConstruct %int4_4414, %int14336_4415 : (!torch.int, !torch.int) -> !torch.list<int>
    %3651 = torch.aten.view %3648, %3650 : !torch.vtensor<[4,1,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,14336],f16>
    %3652 = torch.aten.mm %3651, %3649 : !torch.vtensor<[4,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_4416 = torch.constant.int 4
    %int1_4417 = torch.constant.int 1
    %int4096_4418 = torch.constant.int 4096
    %3653 = torch.prim.ListConstruct %int4_4416, %int1_4417, %int4096_4418 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3654 = torch.aten.view %3652, %3653 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_4419 = torch.constant.int 1
    %3655 = torch.aten.add.Tensor %3624, %3654, %int1_4419 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_4420 = torch.constant.int 6
    %3656 = torch.prims.convert_element_type %3655, %int6_4420 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_4421 = torch.constant.int 2
    %3657 = torch.aten.pow.Tensor_Scalar %3656, %int2_4421 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_4422 = torch.constant.int -1
    %3658 = torch.prim.ListConstruct %int-1_4422 : (!torch.int) -> !torch.list<int>
    %true_4423 = torch.constant.bool true
    %none_4424 = torch.constant.none
    %3659 = torch.aten.mean.dim %3657, %3658, %true_4423, %none_4424 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_4425 = torch.constant.float 9.9999997473787516E-6
    %int1_4426 = torch.constant.int 1
    %3660 = torch.aten.add.Scalar %3659, %float9.999990e-06_4425, %int1_4426 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %3661 = torch.aten.rsqrt %3660 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %3662 = torch.aten.mul.Tensor %3656, %3661 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_4427 = torch.constant.int 5
    %3663 = torch.prims.convert_element_type %3662, %int5_4427 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %3664 = torch.aten.mul.Tensor %177, %3663 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_4428 = torch.constant.int 5
    %3665 = torch.prims.convert_element_type %3664, %int5_4428 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_4429 = torch.constant.int -2
    %int-1_4430 = torch.constant.int -1
    %3666 = torch.aten.transpose.int %178, %int-2_4429, %int-1_4430 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_4431 = torch.constant.int 4
    %int4096_4432 = torch.constant.int 4096
    %3667 = torch.prim.ListConstruct %int4_4431, %int4096_4432 : (!torch.int, !torch.int) -> !torch.list<int>
    %3668 = torch.aten.view %3665, %3667 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %3669 = torch.aten.mm %3668, %3666 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_4433 = torch.constant.int 4
    %int1_4434 = torch.constant.int 1
    %int4096_4435 = torch.constant.int 4096
    %3670 = torch.prim.ListConstruct %int4_4433, %int1_4434, %int4096_4435 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3671 = torch.aten.view %3669, %3670 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_4436 = torch.constant.int -2
    %int-1_4437 = torch.constant.int -1
    %3672 = torch.aten.transpose.int %179, %int-2_4436, %int-1_4437 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_4438 = torch.constant.int 4
    %int4096_4439 = torch.constant.int 4096
    %3673 = torch.prim.ListConstruct %int4_4438, %int4096_4439 : (!torch.int, !torch.int) -> !torch.list<int>
    %3674 = torch.aten.view %3665, %3673 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %3675 = torch.aten.mm %3674, %3672 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_4440 = torch.constant.int 4
    %int1_4441 = torch.constant.int 1
    %int1024_4442 = torch.constant.int 1024
    %3676 = torch.prim.ListConstruct %int4_4440, %int1_4441, %int1024_4442 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3677 = torch.aten.view %3675, %3676 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int-2_4443 = torch.constant.int -2
    %int-1_4444 = torch.constant.int -1
    %3678 = torch.aten.transpose.int %180, %int-2_4443, %int-1_4444 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_4445 = torch.constant.int 4
    %int4096_4446 = torch.constant.int 4096
    %3679 = torch.prim.ListConstruct %int4_4445, %int4096_4446 : (!torch.int, !torch.int) -> !torch.list<int>
    %3680 = torch.aten.view %3665, %3679 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %3681 = torch.aten.mm %3680, %3678 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_4447 = torch.constant.int 4
    %int1_4448 = torch.constant.int 1
    %int1024_4449 = torch.constant.int 1024
    %3682 = torch.prim.ListConstruct %int4_4447, %int1_4448, %int1024_4449 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3683 = torch.aten.view %3681, %3682 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int4_4450 = torch.constant.int 4
    %int1_4451 = torch.constant.int 1
    %int32_4452 = torch.constant.int 32
    %int128_4453 = torch.constant.int 128
    %3684 = torch.prim.ListConstruct %int4_4450, %int1_4451, %int32_4452, %int128_4453 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3685 = torch.aten.view %3671, %3684 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,128],f16>
    %int4_4454 = torch.constant.int 4
    %int1_4455 = torch.constant.int 1
    %int8_4456 = torch.constant.int 8
    %int128_4457 = torch.constant.int 128
    %3686 = torch.prim.ListConstruct %int4_4454, %int1_4455, %int8_4456, %int128_4457 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3687 = torch.aten.view %3677, %3686 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int4_4458 = torch.constant.int 4
    %int1_4459 = torch.constant.int 1
    %int8_4460 = torch.constant.int 8
    %int128_4461 = torch.constant.int 128
    %3688 = torch.prim.ListConstruct %int4_4458, %int1_4459, %int8_4460, %int128_4461 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3689 = torch.aten.view %3683, %3688 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int6_4462 = torch.constant.int 6
    %3690 = torch.prims.convert_element_type %3685, %int6_4462 : !torch.vtensor<[4,1,32,128],f16>, !torch.int -> !torch.vtensor<[4,1,32,128],f32>
    %3691 = torch_c.to_builtin_tensor %3690 : !torch.vtensor<[4,1,32,128],f32> -> tensor<4x1x32x128xf32>
    %3692 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %3693 = util.call @sharktank_rotary_embedding_4_1_32_128_f32(%3691, %3692) : (tensor<4x1x32x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x32x128xf32>
    %3694 = torch_c.from_builtin_tensor %3693 : tensor<4x1x32x128xf32> -> !torch.vtensor<[4,1,32,128],f32>
    %int5_4463 = torch.constant.int 5
    %3695 = torch.prims.convert_element_type %3694, %int5_4463 : !torch.vtensor<[4,1,32,128],f32>, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int6_4464 = torch.constant.int 6
    %3696 = torch.prims.convert_element_type %3687, %int6_4464 : !torch.vtensor<[4,1,8,128],f16>, !torch.int -> !torch.vtensor<[4,1,8,128],f32>
    %3697 = torch_c.to_builtin_tensor %3696 : !torch.vtensor<[4,1,8,128],f32> -> tensor<4x1x8x128xf32>
    %3698 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %3699 = util.call @sharktank_rotary_embedding_4_1_8_128_f32(%3697, %3698) : (tensor<4x1x8x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x8x128xf32>
    %3700 = torch_c.from_builtin_tensor %3699 : tensor<4x1x8x128xf32> -> !torch.vtensor<[4,1,8,128],f32>
    %int5_4465 = torch.constant.int 5
    %3701 = torch.prims.convert_element_type %3700, %int5_4465 : !torch.vtensor<[4,1,8,128],f32>, !torch.int -> !torch.vtensor<[4,1,8,128],f16>
    %int32_4466 = torch.constant.int 32
    %3702 = torch.aten.floor_divide.Scalar %arg2, %int32_4466 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_4467 = torch.constant.int 1
    %3703 = torch.aten.unsqueeze %3702, %int1_4467 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_4468 = torch.constant.int 1
    %false_4469 = torch.constant.bool false
    %3704 = torch.aten.gather %arg3, %int1_4468, %3703, %false_4469 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_4470 = torch.constant.int 32
    %3705 = torch.aten.remainder.Scalar %arg2, %int32_4470 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_4471 = torch.constant.int 1
    %3706 = torch.aten.unsqueeze %3705, %int1_4471 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_4472 = torch.constant.none
    %3707 = torch.aten.clone %181, %none_4472 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_4473 = torch.constant.int 0
    %3708 = torch.aten.unsqueeze %3707, %int0_4473 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_4474 = torch.constant.int 4
    %int1_4475 = torch.constant.int 1
    %3709 = torch.prim.ListConstruct %int4_4474, %int1_4475 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_4476 = torch.constant.int 1
    %int1_4477 = torch.constant.int 1
    %3710 = torch.prim.ListConstruct %int1_4476, %int1_4477 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_4478 = torch.constant.int 4
    %int0_4479 = torch.constant.int 0
    %cpu_4480 = torch.constant.device "cpu"
    %false_4481 = torch.constant.bool false
    %3711 = torch.aten.empty_strided %3709, %3710, %int4_4478, %int0_4479, %cpu_4480, %false_4481 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int16 = torch.constant.int 16
    %3712 = torch.aten.fill.Scalar %3711, %int16 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_4482 = torch.constant.int 4
    %int1_4483 = torch.constant.int 1
    %3713 = torch.prim.ListConstruct %int4_4482, %int1_4483 : (!torch.int, !torch.int) -> !torch.list<int>
    %3714 = torch.aten.repeat %3708, %3713 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_4484 = torch.constant.int 32
    %3715 = torch.aten.mul.Scalar %3704, %int32_4484 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_4485 = torch.constant.int 1
    %3716 = torch.aten.add.Tensor %3715, %3712, %int1_4485 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_4486 = torch.constant.int 2
    %3717 = torch.aten.mul.Scalar %3716, %int2_4486 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_4487 = torch.constant.int 1
    %3718 = torch.aten.add.Tensor %3717, %3714, %int1_4487 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_4488 = torch.constant.int 32
    %3719 = torch.aten.mul.Scalar %3718, %int32_4488 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_4489 = torch.constant.int 1
    %3720 = torch.aten.add.Tensor %3719, %3706, %int1_4489 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_4490 = torch.constant.int 32
    %int2_4491 = torch.constant.int 2
    %int32_4492 = torch.constant.int 32
    %int8_4493 = torch.constant.int 8
    %int128_4494 = torch.constant.int 128
    %3721 = torch.prim.ListConstruct %437, %int32_4490, %int2_4491, %int32_4492, %int8_4493, %int128_4494 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3722 = torch.aten.view %3558, %3721 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3722, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_4495 = torch.constant.int 32
    %3723 = torch.aten.mul.int %437, %int32_4495 : !torch.int, !torch.int -> !torch.int
    %int2_4496 = torch.constant.int 2
    %3724 = torch.aten.mul.int %3723, %int2_4496 : !torch.int, !torch.int -> !torch.int
    %int32_4497 = torch.constant.int 32
    %3725 = torch.aten.mul.int %3724, %int32_4497 : !torch.int, !torch.int -> !torch.int
    %int8_4498 = torch.constant.int 8
    %int128_4499 = torch.constant.int 128
    %3726 = torch.prim.ListConstruct %3725, %int8_4498, %int128_4499 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3727 = torch.aten.view %3722, %3726 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %3727, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %3728 = torch.prim.ListConstruct %3720 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_4500 = torch.constant.bool false
    %3729 = torch.aten.index_put %3727, %3728, %3701, %false_4500 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %3729, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_4501 = torch.constant.int 32
    %int2_4502 = torch.constant.int 2
    %int32_4503 = torch.constant.int 32
    %int8_4504 = torch.constant.int 8
    %int128_4505 = torch.constant.int 128
    %3730 = torch.prim.ListConstruct %437, %int32_4501, %int2_4502, %int32_4503, %int8_4504, %int128_4505 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3731 = torch.aten.view %3729, %3730 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3731, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_4506 = torch.constant.int 2097152
    %3732 = torch.prim.ListConstruct %437, %int2097152_4506 : (!torch.int, !torch.int) -> !torch.list<int>
    %3733 = torch.aten.view %3731, %3732 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %3733, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_4507 = torch.constant.int 32
    %int2_4508 = torch.constant.int 2
    %int32_4509 = torch.constant.int 32
    %int8_4510 = torch.constant.int 8
    %int128_4511 = torch.constant.int 128
    %3734 = torch.prim.ListConstruct %437, %int32_4507, %int2_4508, %int32_4509, %int8_4510, %int128_4511 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3735 = torch.aten.view %3733, %3734 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3735, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int8_4512 = torch.constant.int 8
    %int128_4513 = torch.constant.int 128
    %3736 = torch.prim.ListConstruct %3725, %int8_4512, %int128_4513 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3737 = torch.aten.view %3735, %3736 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %3737, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_4514 = torch.constant.int 32
    %3738 = torch.aten.floor_divide.Scalar %arg2, %int32_4514 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_4515 = torch.constant.int 1
    %3739 = torch.aten.unsqueeze %3738, %int1_4515 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_4516 = torch.constant.int 1
    %false_4517 = torch.constant.bool false
    %3740 = torch.aten.gather %arg3, %int1_4516, %3739, %false_4517 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_4518 = torch.constant.int 32
    %3741 = torch.aten.remainder.Scalar %arg2, %int32_4518 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_4519 = torch.constant.int 1
    %3742 = torch.aten.unsqueeze %3741, %int1_4519 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_4520 = torch.constant.none
    %3743 = torch.aten.clone %182, %none_4520 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_4521 = torch.constant.int 0
    %3744 = torch.aten.unsqueeze %3743, %int0_4521 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_4522 = torch.constant.int 4
    %int1_4523 = torch.constant.int 1
    %3745 = torch.prim.ListConstruct %int4_4522, %int1_4523 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_4524 = torch.constant.int 1
    %int1_4525 = torch.constant.int 1
    %3746 = torch.prim.ListConstruct %int1_4524, %int1_4525 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_4526 = torch.constant.int 4
    %int0_4527 = torch.constant.int 0
    %cpu_4528 = torch.constant.device "cpu"
    %false_4529 = torch.constant.bool false
    %3747 = torch.aten.empty_strided %3745, %3746, %int4_4526, %int0_4527, %cpu_4528, %false_4529 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int16_4530 = torch.constant.int 16
    %3748 = torch.aten.fill.Scalar %3747, %int16_4530 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_4531 = torch.constant.int 4
    %int1_4532 = torch.constant.int 1
    %3749 = torch.prim.ListConstruct %int4_4531, %int1_4532 : (!torch.int, !torch.int) -> !torch.list<int>
    %3750 = torch.aten.repeat %3744, %3749 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_4533 = torch.constant.int 32
    %3751 = torch.aten.mul.Scalar %3740, %int32_4533 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_4534 = torch.constant.int 1
    %3752 = torch.aten.add.Tensor %3751, %3748, %int1_4534 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_4535 = torch.constant.int 2
    %3753 = torch.aten.mul.Scalar %3752, %int2_4535 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_4536 = torch.constant.int 1
    %3754 = torch.aten.add.Tensor %3753, %3750, %int1_4536 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_4537 = torch.constant.int 32
    %3755 = torch.aten.mul.Scalar %3754, %int32_4537 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_4538 = torch.constant.int 1
    %3756 = torch.aten.add.Tensor %3755, %3742, %int1_4538 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %3757 = torch.prim.ListConstruct %3756 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_4539 = torch.constant.bool false
    %3758 = torch.aten.index_put %3737, %3757, %3689, %false_4539 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %3758, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_4540 = torch.constant.int 32
    %int2_4541 = torch.constant.int 2
    %int32_4542 = torch.constant.int 32
    %int8_4543 = torch.constant.int 8
    %int128_4544 = torch.constant.int 128
    %3759 = torch.prim.ListConstruct %437, %int32_4540, %int2_4541, %int32_4542, %int8_4543, %int128_4544 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3760 = torch.aten.view %3758, %3759 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3760, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_4545 = torch.constant.int 2097152
    %3761 = torch.prim.ListConstruct %437, %int2097152_4545 : (!torch.int, !torch.int) -> !torch.list<int>
    %3762 = torch.aten.view %3760, %3761 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %3762, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int4_4546 = torch.constant.int 4
    %3763 = torch.prim.ListConstruct %int4_4546, %358 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_4547 = torch.constant.int 1
    %3764 = torch.prim.ListConstruct %358, %int1_4547 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_4548 = torch.constant.int 4
    %int0_4549 = torch.constant.int 0
    %cpu_4550 = torch.constant.device "cpu"
    %false_4551 = torch.constant.bool false
    %3765 = torch.aten.empty_strided %3763, %3764, %int4_4548, %int0_4549, %cpu_4550, %false_4551 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3765, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int16_4552 = torch.constant.int 16
    %3766 = torch.aten.fill.Scalar %3765, %int16_4552 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3766, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int32_4553 = torch.constant.int 32
    %3767 = torch.aten.mul.Scalar %arg3, %int32_4553 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3767, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_4554 = torch.constant.int 1
    %3768 = torch.aten.add.Tensor %3767, %3766, %int1_4554 : !torch.vtensor<[4,?],si64>, !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3768, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_4555 = torch.constant.int 4
    %3769 = torch.aten.mul.int %int4_4555, %358 : !torch.int, !torch.int -> !torch.int
    %3770 = torch.prim.ListConstruct %3769 : (!torch.int) -> !torch.list<int>
    %3771 = torch.aten.view %3768, %3770 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3771, [%356], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_4556 = torch.constant.int 32
    %int2_4557 = torch.constant.int 2
    %int32_4558 = torch.constant.int 32
    %int8_4559 = torch.constant.int 8
    %int128_4560 = torch.constant.int 128
    %3772 = torch.prim.ListConstruct %437, %int32_4556, %int2_4557, %int32_4558, %int8_4559, %int128_4560 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3773 = torch.aten.view %3762, %3772 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3773, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_4561 = torch.constant.int 32
    %3774 = torch.aten.mul.int %437, %int32_4561 : !torch.int, !torch.int -> !torch.int
    %int2_4562 = torch.constant.int 2
    %int32_4563 = torch.constant.int 32
    %int8_4564 = torch.constant.int 8
    %int128_4565 = torch.constant.int 128
    %3775 = torch.prim.ListConstruct %3774, %int2_4562, %int32_4563, %int8_4564, %int128_4565 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3776 = torch.aten.view %3773, %3775 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %3776, [%357], affine_map<()[s0] -> (s0 * 32, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int0_4566 = torch.constant.int 0
    %3777 = torch.aten.index_select %3776, %int0_4566, %3771 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.int, !torch.vtensor<[?],si64> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %3777, [%356], affine_map<()[s0] -> (s0 * 4, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int4_4567 = torch.constant.int 4
    %int2_4568 = torch.constant.int 2
    %int32_4569 = torch.constant.int 32
    %int8_4570 = torch.constant.int 8
    %int128_4571 = torch.constant.int 128
    %3778 = torch.prim.ListConstruct %int4_4567, %358, %int2_4568, %int32_4569, %int8_4570, %int128_4571 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3779 = torch.aten.view %3777, %3778 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %3779, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int0_4572 = torch.constant.int 0
    %int0_4573 = torch.constant.int 0
    %int9223372036854775807_4574 = torch.constant.int 9223372036854775807
    %int1_4575 = torch.constant.int 1
    %3780 = torch.aten.slice.Tensor %3779, %int0_4572, %int0_4573, %int9223372036854775807_4574, %int1_4575 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %3780, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_4576 = torch.constant.int 1
    %int0_4577 = torch.constant.int 0
    %int9223372036854775807_4578 = torch.constant.int 9223372036854775807
    %int1_4579 = torch.constant.int 1
    %3781 = torch.aten.slice.Tensor %3780, %int1_4576, %int0_4577, %int9223372036854775807_4578, %int1_4579 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %3781, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_4580 = torch.constant.int 2
    %int0_4581 = torch.constant.int 0
    %3782 = torch.aten.select.int %3781, %int2_4580, %int0_4581 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3782, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int32_4582 = torch.constant.int 32
    %3783 = torch.aten.mul.int %358, %int32_4582 : !torch.int, !torch.int -> !torch.int
    %int2_4583 = torch.constant.int 2
    %int0_4584 = torch.constant.int 0
    %int1_4585 = torch.constant.int 1
    %3784 = torch.aten.slice.Tensor %3782, %int2_4583, %int0_4584, %3783, %int1_4585 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3784, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_4586 = torch.constant.int 0
    %3785 = torch.aten.clone %3784, %int0_4586 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3785, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_4587 = torch.constant.int 1
    %3786 = torch.aten.size.int %3781, %int1_4587 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_4588 = torch.constant.int 32
    %3787 = torch.aten.mul.int %3786, %int32_4588 : !torch.int, !torch.int -> !torch.int
    %int4_4589 = torch.constant.int 4
    %int8_4590 = torch.constant.int 8
    %int128_4591 = torch.constant.int 128
    %3788 = torch.prim.ListConstruct %int4_4589, %3787, %int8_4590, %int128_4591 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3789 = torch.aten._unsafe_view %3785, %3788 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %3789, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_4592 = torch.constant.int 0
    %int0_4593 = torch.constant.int 0
    %int9223372036854775807_4594 = torch.constant.int 9223372036854775807
    %int1_4595 = torch.constant.int 1
    %3790 = torch.aten.slice.Tensor %3789, %int0_4592, %int0_4593, %int9223372036854775807_4594, %int1_4595 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %3790, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_4596 = torch.constant.int 0
    %int0_4597 = torch.constant.int 0
    %int9223372036854775807_4598 = torch.constant.int 9223372036854775807
    %int1_4599 = torch.constant.int 1
    %3791 = torch.aten.slice.Tensor %3779, %int0_4596, %int0_4597, %int9223372036854775807_4598, %int1_4599 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %3791, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_4600 = torch.constant.int 1
    %int0_4601 = torch.constant.int 0
    %int9223372036854775807_4602 = torch.constant.int 9223372036854775807
    %int1_4603 = torch.constant.int 1
    %3792 = torch.aten.slice.Tensor %3791, %int1_4600, %int0_4601, %int9223372036854775807_4602, %int1_4603 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %3792, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_4604 = torch.constant.int 2
    %int1_4605 = torch.constant.int 1
    %3793 = torch.aten.select.int %3792, %int2_4604, %int1_4605 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3793, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int2_4606 = torch.constant.int 2
    %int0_4607 = torch.constant.int 0
    %int1_4608 = torch.constant.int 1
    %3794 = torch.aten.slice.Tensor %3793, %int2_4606, %int0_4607, %3783, %int1_4608 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3794, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_4609 = torch.constant.int 0
    %3795 = torch.aten.clone %3794, %int0_4609 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3795, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_4610 = torch.constant.int 1
    %3796 = torch.aten.size.int %3792, %int1_4610 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_4611 = torch.constant.int 32
    %3797 = torch.aten.mul.int %3796, %int32_4611 : !torch.int, !torch.int -> !torch.int
    %int4_4612 = torch.constant.int 4
    %int8_4613 = torch.constant.int 8
    %int128_4614 = torch.constant.int 128
    %3798 = torch.prim.ListConstruct %int4_4612, %3797, %int8_4613, %int128_4614 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3799 = torch.aten._unsafe_view %3795, %3798 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %3799, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_4615 = torch.constant.int 0
    %int0_4616 = torch.constant.int 0
    %int9223372036854775807_4617 = torch.constant.int 9223372036854775807
    %int1_4618 = torch.constant.int 1
    %3800 = torch.aten.slice.Tensor %3799, %int0_4615, %int0_4616, %int9223372036854775807_4617, %int1_4618 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %3800, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int-2_4619 = torch.constant.int -2
    %3801 = torch.aten.unsqueeze %3790, %int-2_4619 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %3801, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_4620 = torch.constant.int 1
    %3802 = torch.aten.size.int %3789, %int1_4620 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_4621 = torch.constant.int 4
    %int8_4622 = torch.constant.int 8
    %int4_4623 = torch.constant.int 4
    %int128_4624 = torch.constant.int 128
    %3803 = torch.prim.ListConstruct %int4_4621, %3802, %int8_4622, %int4_4623, %int128_4624 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_4625 = torch.constant.bool false
    %3804 = torch.aten.expand %3801, %3803, %false_4625 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3804, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_4626 = torch.constant.int 0
    %3805 = torch.aten.clone %3804, %int0_4626 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3805, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_4627 = torch.constant.int 4
    %int32_4628 = torch.constant.int 32
    %int128_4629 = torch.constant.int 128
    %3806 = torch.prim.ListConstruct %int4_4627, %3802, %int32_4628, %int128_4629 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3807 = torch.aten._unsafe_view %3805, %3806 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %3807, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_4630 = torch.constant.int -2
    %3808 = torch.aten.unsqueeze %3800, %int-2_4630 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %3808, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_4631 = torch.constant.int 1
    %3809 = torch.aten.size.int %3799, %int1_4631 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_4632 = torch.constant.int 4
    %int8_4633 = torch.constant.int 8
    %int4_4634 = torch.constant.int 4
    %int128_4635 = torch.constant.int 128
    %3810 = torch.prim.ListConstruct %int4_4632, %3809, %int8_4633, %int4_4634, %int128_4635 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_4636 = torch.constant.bool false
    %3811 = torch.aten.expand %3808, %3810, %false_4636 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3811, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_4637 = torch.constant.int 0
    %3812 = torch.aten.clone %3811, %int0_4637 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %3812, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_4638 = torch.constant.int 4
    %int32_4639 = torch.constant.int 32
    %int128_4640 = torch.constant.int 128
    %3813 = torch.prim.ListConstruct %int4_4638, %3809, %int32_4639, %int128_4640 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3814 = torch.aten._unsafe_view %3812, %3813 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %3814, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_4641 = torch.constant.int 1
    %int2_4642 = torch.constant.int 2
    %3815 = torch.aten.transpose.int %3695, %int1_4641, %int2_4642 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,128],f16>
    %int1_4643 = torch.constant.int 1
    %int2_4644 = torch.constant.int 2
    %3816 = torch.aten.transpose.int %3807, %int1_4643, %int2_4644 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %3816, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_4645 = torch.constant.int 1
    %int2_4646 = torch.constant.int 2
    %3817 = torch.aten.transpose.int %3814, %int1_4645, %int2_4646 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %3817, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_4647 = torch.constant.float 0.000000e+00
    %false_4648 = torch.constant.bool false
    %none_4649 = torch.constant.none
    %3818:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3815, %3816, %3817, %float0.000000e00_4647, %false_4648, %368, %none_4649) : (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.vtensor<[4,1,1,?],f16>, !torch.none) -> (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,1],f32>) 
    %int1_4650 = torch.constant.int 1
    %int2_4651 = torch.constant.int 2
    %3819 = torch.aten.transpose.int %3818#0, %int1_4650, %int2_4651 : !torch.vtensor<[4,32,1,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int4_4652 = torch.constant.int 4
    %int1_4653 = torch.constant.int 1
    %int4096_4654 = torch.constant.int 4096
    %3820 = torch.prim.ListConstruct %int4_4652, %int1_4653, %int4096_4654 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3821 = torch.aten.view %3819, %3820 : !torch.vtensor<[4,1,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_4655 = torch.constant.int -2
    %int-1_4656 = torch.constant.int -1
    %3822 = torch.aten.transpose.int %183, %int-2_4655, %int-1_4656 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_4657 = torch.constant.int 4
    %int4096_4658 = torch.constant.int 4096
    %3823 = torch.prim.ListConstruct %int4_4657, %int4096_4658 : (!torch.int, !torch.int) -> !torch.list<int>
    %3824 = torch.aten.view %3821, %3823 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %3825 = torch.aten.mm %3824, %3822 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_4659 = torch.constant.int 4
    %int1_4660 = torch.constant.int 1
    %int4096_4661 = torch.constant.int 4096
    %3826 = torch.prim.ListConstruct %int4_4659, %int1_4660, %int4096_4661 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3827 = torch.aten.view %3825, %3826 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_4662 = torch.constant.int 1
    %3828 = torch.aten.add.Tensor %3655, %3827, %int1_4662 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_4663 = torch.constant.int 6
    %3829 = torch.prims.convert_element_type %3828, %int6_4663 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_4664 = torch.constant.int 2
    %3830 = torch.aten.pow.Tensor_Scalar %3829, %int2_4664 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_4665 = torch.constant.int -1
    %3831 = torch.prim.ListConstruct %int-1_4665 : (!torch.int) -> !torch.list<int>
    %true_4666 = torch.constant.bool true
    %none_4667 = torch.constant.none
    %3832 = torch.aten.mean.dim %3830, %3831, %true_4666, %none_4667 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_4668 = torch.constant.float 9.9999997473787516E-6
    %int1_4669 = torch.constant.int 1
    %3833 = torch.aten.add.Scalar %3832, %float9.999990e-06_4668, %int1_4669 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %3834 = torch.aten.rsqrt %3833 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %3835 = torch.aten.mul.Tensor %3829, %3834 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_4670 = torch.constant.int 5
    %3836 = torch.prims.convert_element_type %3835, %int5_4670 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %3837 = torch.aten.mul.Tensor %184, %3836 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_4671 = torch.constant.int 5
    %3838 = torch.prims.convert_element_type %3837, %int5_4671 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_4672 = torch.constant.int -2
    %int-1_4673 = torch.constant.int -1
    %3839 = torch.aten.transpose.int %185, %int-2_4672, %int-1_4673 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_4674 = torch.constant.int 4
    %int4096_4675 = torch.constant.int 4096
    %3840 = torch.prim.ListConstruct %int4_4674, %int4096_4675 : (!torch.int, !torch.int) -> !torch.list<int>
    %3841 = torch.aten.view %3838, %3840 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %3842 = torch.aten.mm %3841, %3839 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_4676 = torch.constant.int 4
    %int1_4677 = torch.constant.int 1
    %int14336_4678 = torch.constant.int 14336
    %3843 = torch.prim.ListConstruct %int4_4676, %int1_4677, %int14336_4678 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3844 = torch.aten.view %3842, %3843 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %3845 = torch.aten.silu %3844 : !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_4679 = torch.constant.int -2
    %int-1_4680 = torch.constant.int -1
    %3846 = torch.aten.transpose.int %186, %int-2_4679, %int-1_4680 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_4681 = torch.constant.int 4
    %int4096_4682 = torch.constant.int 4096
    %3847 = torch.prim.ListConstruct %int4_4681, %int4096_4682 : (!torch.int, !torch.int) -> !torch.list<int>
    %3848 = torch.aten.view %3838, %3847 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %3849 = torch.aten.mm %3848, %3846 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_4683 = torch.constant.int 4
    %int1_4684 = torch.constant.int 1
    %int14336_4685 = torch.constant.int 14336
    %3850 = torch.prim.ListConstruct %int4_4683, %int1_4684, %int14336_4685 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3851 = torch.aten.view %3849, %3850 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %3852 = torch.aten.mul.Tensor %3845, %3851 : !torch.vtensor<[4,1,14336],f16>, !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_4686 = torch.constant.int -2
    %int-1_4687 = torch.constant.int -1
    %3853 = torch.aten.transpose.int %187, %int-2_4686, %int-1_4687 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int4_4688 = torch.constant.int 4
    %int14336_4689 = torch.constant.int 14336
    %3854 = torch.prim.ListConstruct %int4_4688, %int14336_4689 : (!torch.int, !torch.int) -> !torch.list<int>
    %3855 = torch.aten.view %3852, %3854 : !torch.vtensor<[4,1,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,14336],f16>
    %3856 = torch.aten.mm %3855, %3853 : !torch.vtensor<[4,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_4690 = torch.constant.int 4
    %int1_4691 = torch.constant.int 1
    %int4096_4692 = torch.constant.int 4096
    %3857 = torch.prim.ListConstruct %int4_4690, %int1_4691, %int4096_4692 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3858 = torch.aten.view %3856, %3857 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_4693 = torch.constant.int 1
    %3859 = torch.aten.add.Tensor %3828, %3858, %int1_4693 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_4694 = torch.constant.int 6
    %3860 = torch.prims.convert_element_type %3859, %int6_4694 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_4695 = torch.constant.int 2
    %3861 = torch.aten.pow.Tensor_Scalar %3860, %int2_4695 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_4696 = torch.constant.int -1
    %3862 = torch.prim.ListConstruct %int-1_4696 : (!torch.int) -> !torch.list<int>
    %true_4697 = torch.constant.bool true
    %none_4698 = torch.constant.none
    %3863 = torch.aten.mean.dim %3861, %3862, %true_4697, %none_4698 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_4699 = torch.constant.float 9.9999997473787516E-6
    %int1_4700 = torch.constant.int 1
    %3864 = torch.aten.add.Scalar %3863, %float9.999990e-06_4699, %int1_4700 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %3865 = torch.aten.rsqrt %3864 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %3866 = torch.aten.mul.Tensor %3860, %3865 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_4701 = torch.constant.int 5
    %3867 = torch.prims.convert_element_type %3866, %int5_4701 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %3868 = torch.aten.mul.Tensor %188, %3867 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_4702 = torch.constant.int 5
    %3869 = torch.prims.convert_element_type %3868, %int5_4702 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_4703 = torch.constant.int -2
    %int-1_4704 = torch.constant.int -1
    %3870 = torch.aten.transpose.int %189, %int-2_4703, %int-1_4704 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_4705 = torch.constant.int 4
    %int4096_4706 = torch.constant.int 4096
    %3871 = torch.prim.ListConstruct %int4_4705, %int4096_4706 : (!torch.int, !torch.int) -> !torch.list<int>
    %3872 = torch.aten.view %3869, %3871 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %3873 = torch.aten.mm %3872, %3870 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_4707 = torch.constant.int 4
    %int1_4708 = torch.constant.int 1
    %int4096_4709 = torch.constant.int 4096
    %3874 = torch.prim.ListConstruct %int4_4707, %int1_4708, %int4096_4709 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3875 = torch.aten.view %3873, %3874 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_4710 = torch.constant.int -2
    %int-1_4711 = torch.constant.int -1
    %3876 = torch.aten.transpose.int %190, %int-2_4710, %int-1_4711 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_4712 = torch.constant.int 4
    %int4096_4713 = torch.constant.int 4096
    %3877 = torch.prim.ListConstruct %int4_4712, %int4096_4713 : (!torch.int, !torch.int) -> !torch.list<int>
    %3878 = torch.aten.view %3869, %3877 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %3879 = torch.aten.mm %3878, %3876 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_4714 = torch.constant.int 4
    %int1_4715 = torch.constant.int 1
    %int1024_4716 = torch.constant.int 1024
    %3880 = torch.prim.ListConstruct %int4_4714, %int1_4715, %int1024_4716 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3881 = torch.aten.view %3879, %3880 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int-2_4717 = torch.constant.int -2
    %int-1_4718 = torch.constant.int -1
    %3882 = torch.aten.transpose.int %191, %int-2_4717, %int-1_4718 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_4719 = torch.constant.int 4
    %int4096_4720 = torch.constant.int 4096
    %3883 = torch.prim.ListConstruct %int4_4719, %int4096_4720 : (!torch.int, !torch.int) -> !torch.list<int>
    %3884 = torch.aten.view %3869, %3883 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %3885 = torch.aten.mm %3884, %3882 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_4721 = torch.constant.int 4
    %int1_4722 = torch.constant.int 1
    %int1024_4723 = torch.constant.int 1024
    %3886 = torch.prim.ListConstruct %int4_4721, %int1_4722, %int1024_4723 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3887 = torch.aten.view %3885, %3886 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int4_4724 = torch.constant.int 4
    %int1_4725 = torch.constant.int 1
    %int32_4726 = torch.constant.int 32
    %int128_4727 = torch.constant.int 128
    %3888 = torch.prim.ListConstruct %int4_4724, %int1_4725, %int32_4726, %int128_4727 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3889 = torch.aten.view %3875, %3888 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,128],f16>
    %int4_4728 = torch.constant.int 4
    %int1_4729 = torch.constant.int 1
    %int8_4730 = torch.constant.int 8
    %int128_4731 = torch.constant.int 128
    %3890 = torch.prim.ListConstruct %int4_4728, %int1_4729, %int8_4730, %int128_4731 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3891 = torch.aten.view %3881, %3890 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int4_4732 = torch.constant.int 4
    %int1_4733 = torch.constant.int 1
    %int8_4734 = torch.constant.int 8
    %int128_4735 = torch.constant.int 128
    %3892 = torch.prim.ListConstruct %int4_4732, %int1_4733, %int8_4734, %int128_4735 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3893 = torch.aten.view %3887, %3892 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int6_4736 = torch.constant.int 6
    %3894 = torch.prims.convert_element_type %3889, %int6_4736 : !torch.vtensor<[4,1,32,128],f16>, !torch.int -> !torch.vtensor<[4,1,32,128],f32>
    %3895 = torch_c.to_builtin_tensor %3894 : !torch.vtensor<[4,1,32,128],f32> -> tensor<4x1x32x128xf32>
    %3896 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %3897 = util.call @sharktank_rotary_embedding_4_1_32_128_f32(%3895, %3896) : (tensor<4x1x32x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x32x128xf32>
    %3898 = torch_c.from_builtin_tensor %3897 : tensor<4x1x32x128xf32> -> !torch.vtensor<[4,1,32,128],f32>
    %int5_4737 = torch.constant.int 5
    %3899 = torch.prims.convert_element_type %3898, %int5_4737 : !torch.vtensor<[4,1,32,128],f32>, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int6_4738 = torch.constant.int 6
    %3900 = torch.prims.convert_element_type %3891, %int6_4738 : !torch.vtensor<[4,1,8,128],f16>, !torch.int -> !torch.vtensor<[4,1,8,128],f32>
    %3901 = torch_c.to_builtin_tensor %3900 : !torch.vtensor<[4,1,8,128],f32> -> tensor<4x1x8x128xf32>
    %3902 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %3903 = util.call @sharktank_rotary_embedding_4_1_8_128_f32(%3901, %3902) : (tensor<4x1x8x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x8x128xf32>
    %3904 = torch_c.from_builtin_tensor %3903 : tensor<4x1x8x128xf32> -> !torch.vtensor<[4,1,8,128],f32>
    %int5_4739 = torch.constant.int 5
    %3905 = torch.prims.convert_element_type %3904, %int5_4739 : !torch.vtensor<[4,1,8,128],f32>, !torch.int -> !torch.vtensor<[4,1,8,128],f16>
    %int32_4740 = torch.constant.int 32
    %3906 = torch.aten.floor_divide.Scalar %arg2, %int32_4740 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_4741 = torch.constant.int 1
    %3907 = torch.aten.unsqueeze %3906, %int1_4741 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_4742 = torch.constant.int 1
    %false_4743 = torch.constant.bool false
    %3908 = torch.aten.gather %arg3, %int1_4742, %3907, %false_4743 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_4744 = torch.constant.int 32
    %3909 = torch.aten.remainder.Scalar %arg2, %int32_4744 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_4745 = torch.constant.int 1
    %3910 = torch.aten.unsqueeze %3909, %int1_4745 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_4746 = torch.constant.none
    %3911 = torch.aten.clone %192, %none_4746 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_4747 = torch.constant.int 0
    %3912 = torch.aten.unsqueeze %3911, %int0_4747 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_4748 = torch.constant.int 4
    %int1_4749 = torch.constant.int 1
    %3913 = torch.prim.ListConstruct %int4_4748, %int1_4749 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_4750 = torch.constant.int 1
    %int1_4751 = torch.constant.int 1
    %3914 = torch.prim.ListConstruct %int1_4750, %int1_4751 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_4752 = torch.constant.int 4
    %int0_4753 = torch.constant.int 0
    %cpu_4754 = torch.constant.device "cpu"
    %false_4755 = torch.constant.bool false
    %3915 = torch.aten.empty_strided %3913, %3914, %int4_4752, %int0_4753, %cpu_4754, %false_4755 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int17 = torch.constant.int 17
    %3916 = torch.aten.fill.Scalar %3915, %int17 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_4756 = torch.constant.int 4
    %int1_4757 = torch.constant.int 1
    %3917 = torch.prim.ListConstruct %int4_4756, %int1_4757 : (!torch.int, !torch.int) -> !torch.list<int>
    %3918 = torch.aten.repeat %3912, %3917 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_4758 = torch.constant.int 32
    %3919 = torch.aten.mul.Scalar %3908, %int32_4758 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_4759 = torch.constant.int 1
    %3920 = torch.aten.add.Tensor %3919, %3916, %int1_4759 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_4760 = torch.constant.int 2
    %3921 = torch.aten.mul.Scalar %3920, %int2_4760 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_4761 = torch.constant.int 1
    %3922 = torch.aten.add.Tensor %3921, %3918, %int1_4761 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_4762 = torch.constant.int 32
    %3923 = torch.aten.mul.Scalar %3922, %int32_4762 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_4763 = torch.constant.int 1
    %3924 = torch.aten.add.Tensor %3923, %3910, %int1_4763 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_4764 = torch.constant.int 32
    %int2_4765 = torch.constant.int 2
    %int32_4766 = torch.constant.int 32
    %int8_4767 = torch.constant.int 8
    %int128_4768 = torch.constant.int 128
    %3925 = torch.prim.ListConstruct %437, %int32_4764, %int2_4765, %int32_4766, %int8_4767, %int128_4768 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3926 = torch.aten.view %3762, %3925 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3926, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_4769 = torch.constant.int 32
    %3927 = torch.aten.mul.int %437, %int32_4769 : !torch.int, !torch.int -> !torch.int
    %int2_4770 = torch.constant.int 2
    %3928 = torch.aten.mul.int %3927, %int2_4770 : !torch.int, !torch.int -> !torch.int
    %int32_4771 = torch.constant.int 32
    %3929 = torch.aten.mul.int %3928, %int32_4771 : !torch.int, !torch.int -> !torch.int
    %int8_4772 = torch.constant.int 8
    %int128_4773 = torch.constant.int 128
    %3930 = torch.prim.ListConstruct %3929, %int8_4772, %int128_4773 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3931 = torch.aten.view %3926, %3930 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %3931, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %3932 = torch.prim.ListConstruct %3924 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_4774 = torch.constant.bool false
    %3933 = torch.aten.index_put %3931, %3932, %3905, %false_4774 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %3933, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_4775 = torch.constant.int 32
    %int2_4776 = torch.constant.int 2
    %int32_4777 = torch.constant.int 32
    %int8_4778 = torch.constant.int 8
    %int128_4779 = torch.constant.int 128
    %3934 = torch.prim.ListConstruct %437, %int32_4775, %int2_4776, %int32_4777, %int8_4778, %int128_4779 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3935 = torch.aten.view %3933, %3934 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3935, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_4780 = torch.constant.int 2097152
    %3936 = torch.prim.ListConstruct %437, %int2097152_4780 : (!torch.int, !torch.int) -> !torch.list<int>
    %3937 = torch.aten.view %3935, %3936 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %3937, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_4781 = torch.constant.int 32
    %int2_4782 = torch.constant.int 2
    %int32_4783 = torch.constant.int 32
    %int8_4784 = torch.constant.int 8
    %int128_4785 = torch.constant.int 128
    %3938 = torch.prim.ListConstruct %437, %int32_4781, %int2_4782, %int32_4783, %int8_4784, %int128_4785 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3939 = torch.aten.view %3937, %3938 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3939, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int8_4786 = torch.constant.int 8
    %int128_4787 = torch.constant.int 128
    %3940 = torch.prim.ListConstruct %3929, %int8_4786, %int128_4787 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3941 = torch.aten.view %3939, %3940 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %3941, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_4788 = torch.constant.int 32
    %3942 = torch.aten.floor_divide.Scalar %arg2, %int32_4788 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_4789 = torch.constant.int 1
    %3943 = torch.aten.unsqueeze %3942, %int1_4789 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_4790 = torch.constant.int 1
    %false_4791 = torch.constant.bool false
    %3944 = torch.aten.gather %arg3, %int1_4790, %3943, %false_4791 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_4792 = torch.constant.int 32
    %3945 = torch.aten.remainder.Scalar %arg2, %int32_4792 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_4793 = torch.constant.int 1
    %3946 = torch.aten.unsqueeze %3945, %int1_4793 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_4794 = torch.constant.none
    %3947 = torch.aten.clone %193, %none_4794 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_4795 = torch.constant.int 0
    %3948 = torch.aten.unsqueeze %3947, %int0_4795 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_4796 = torch.constant.int 4
    %int1_4797 = torch.constant.int 1
    %3949 = torch.prim.ListConstruct %int4_4796, %int1_4797 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_4798 = torch.constant.int 1
    %int1_4799 = torch.constant.int 1
    %3950 = torch.prim.ListConstruct %int1_4798, %int1_4799 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_4800 = torch.constant.int 4
    %int0_4801 = torch.constant.int 0
    %cpu_4802 = torch.constant.device "cpu"
    %false_4803 = torch.constant.bool false
    %3951 = torch.aten.empty_strided %3949, %3950, %int4_4800, %int0_4801, %cpu_4802, %false_4803 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int17_4804 = torch.constant.int 17
    %3952 = torch.aten.fill.Scalar %3951, %int17_4804 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_4805 = torch.constant.int 4
    %int1_4806 = torch.constant.int 1
    %3953 = torch.prim.ListConstruct %int4_4805, %int1_4806 : (!torch.int, !torch.int) -> !torch.list<int>
    %3954 = torch.aten.repeat %3948, %3953 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_4807 = torch.constant.int 32
    %3955 = torch.aten.mul.Scalar %3944, %int32_4807 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_4808 = torch.constant.int 1
    %3956 = torch.aten.add.Tensor %3955, %3952, %int1_4808 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_4809 = torch.constant.int 2
    %3957 = torch.aten.mul.Scalar %3956, %int2_4809 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_4810 = torch.constant.int 1
    %3958 = torch.aten.add.Tensor %3957, %3954, %int1_4810 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_4811 = torch.constant.int 32
    %3959 = torch.aten.mul.Scalar %3958, %int32_4811 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_4812 = torch.constant.int 1
    %3960 = torch.aten.add.Tensor %3959, %3946, %int1_4812 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %3961 = torch.prim.ListConstruct %3960 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_4813 = torch.constant.bool false
    %3962 = torch.aten.index_put %3941, %3961, %3893, %false_4813 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %3962, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_4814 = torch.constant.int 32
    %int2_4815 = torch.constant.int 2
    %int32_4816 = torch.constant.int 32
    %int8_4817 = torch.constant.int 8
    %int128_4818 = torch.constant.int 128
    %3963 = torch.prim.ListConstruct %437, %int32_4814, %int2_4815, %int32_4816, %int8_4817, %int128_4818 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3964 = torch.aten.view %3962, %3963 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3964, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_4819 = torch.constant.int 2097152
    %3965 = torch.prim.ListConstruct %437, %int2097152_4819 : (!torch.int, !torch.int) -> !torch.list<int>
    %3966 = torch.aten.view %3964, %3965 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %3966, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int4_4820 = torch.constant.int 4
    %3967 = torch.prim.ListConstruct %int4_4820, %358 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_4821 = torch.constant.int 1
    %3968 = torch.prim.ListConstruct %358, %int1_4821 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_4822 = torch.constant.int 4
    %int0_4823 = torch.constant.int 0
    %cpu_4824 = torch.constant.device "cpu"
    %false_4825 = torch.constant.bool false
    %3969 = torch.aten.empty_strided %3967, %3968, %int4_4822, %int0_4823, %cpu_4824, %false_4825 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3969, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int17_4826 = torch.constant.int 17
    %3970 = torch.aten.fill.Scalar %3969, %int17_4826 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3970, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int32_4827 = torch.constant.int 32
    %3971 = torch.aten.mul.Scalar %arg3, %int32_4827 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3971, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_4828 = torch.constant.int 1
    %3972 = torch.aten.add.Tensor %3971, %3970, %int1_4828 : !torch.vtensor<[4,?],si64>, !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %3972, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_4829 = torch.constant.int 4
    %3973 = torch.aten.mul.int %int4_4829, %358 : !torch.int, !torch.int -> !torch.int
    %3974 = torch.prim.ListConstruct %3973 : (!torch.int) -> !torch.list<int>
    %3975 = torch.aten.view %3972, %3974 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %3975, [%356], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_4830 = torch.constant.int 32
    %int2_4831 = torch.constant.int 2
    %int32_4832 = torch.constant.int 32
    %int8_4833 = torch.constant.int 8
    %int128_4834 = torch.constant.int 128
    %3976 = torch.prim.ListConstruct %437, %int32_4830, %int2_4831, %int32_4832, %int8_4833, %int128_4834 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3977 = torch.aten.view %3966, %3976 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %3977, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_4835 = torch.constant.int 32
    %3978 = torch.aten.mul.int %437, %int32_4835 : !torch.int, !torch.int -> !torch.int
    %int2_4836 = torch.constant.int 2
    %int32_4837 = torch.constant.int 32
    %int8_4838 = torch.constant.int 8
    %int128_4839 = torch.constant.int 128
    %3979 = torch.prim.ListConstruct %3978, %int2_4836, %int32_4837, %int8_4838, %int128_4839 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3980 = torch.aten.view %3977, %3979 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %3980, [%357], affine_map<()[s0] -> (s0 * 32, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int0_4840 = torch.constant.int 0
    %3981 = torch.aten.index_select %3980, %int0_4840, %3975 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.int, !torch.vtensor<[?],si64> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %3981, [%356], affine_map<()[s0] -> (s0 * 4, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int4_4841 = torch.constant.int 4
    %int2_4842 = torch.constant.int 2
    %int32_4843 = torch.constant.int 32
    %int8_4844 = torch.constant.int 8
    %int128_4845 = torch.constant.int 128
    %3982 = torch.prim.ListConstruct %int4_4841, %358, %int2_4842, %int32_4843, %int8_4844, %int128_4845 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3983 = torch.aten.view %3981, %3982 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %3983, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int0_4846 = torch.constant.int 0
    %int0_4847 = torch.constant.int 0
    %int9223372036854775807_4848 = torch.constant.int 9223372036854775807
    %int1_4849 = torch.constant.int 1
    %3984 = torch.aten.slice.Tensor %3983, %int0_4846, %int0_4847, %int9223372036854775807_4848, %int1_4849 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %3984, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_4850 = torch.constant.int 1
    %int0_4851 = torch.constant.int 0
    %int9223372036854775807_4852 = torch.constant.int 9223372036854775807
    %int1_4853 = torch.constant.int 1
    %3985 = torch.aten.slice.Tensor %3984, %int1_4850, %int0_4851, %int9223372036854775807_4852, %int1_4853 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %3985, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_4854 = torch.constant.int 2
    %int0_4855 = torch.constant.int 0
    %3986 = torch.aten.select.int %3985, %int2_4854, %int0_4855 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3986, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int32_4856 = torch.constant.int 32
    %3987 = torch.aten.mul.int %358, %int32_4856 : !torch.int, !torch.int -> !torch.int
    %int2_4857 = torch.constant.int 2
    %int0_4858 = torch.constant.int 0
    %int1_4859 = torch.constant.int 1
    %3988 = torch.aten.slice.Tensor %3986, %int2_4857, %int0_4858, %3987, %int1_4859 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3988, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_4860 = torch.constant.int 0
    %3989 = torch.aten.clone %3988, %int0_4860 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3989, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_4861 = torch.constant.int 1
    %3990 = torch.aten.size.int %3985, %int1_4861 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_4862 = torch.constant.int 32
    %3991 = torch.aten.mul.int %3990, %int32_4862 : !torch.int, !torch.int -> !torch.int
    %int4_4863 = torch.constant.int 4
    %int8_4864 = torch.constant.int 8
    %int128_4865 = torch.constant.int 128
    %3992 = torch.prim.ListConstruct %int4_4863, %3991, %int8_4864, %int128_4865 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3993 = torch.aten._unsafe_view %3989, %3992 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %3993, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_4866 = torch.constant.int 0
    %int0_4867 = torch.constant.int 0
    %int9223372036854775807_4868 = torch.constant.int 9223372036854775807
    %int1_4869 = torch.constant.int 1
    %3994 = torch.aten.slice.Tensor %3993, %int0_4866, %int0_4867, %int9223372036854775807_4868, %int1_4869 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %3994, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_4870 = torch.constant.int 0
    %int0_4871 = torch.constant.int 0
    %int9223372036854775807_4872 = torch.constant.int 9223372036854775807
    %int1_4873 = torch.constant.int 1
    %3995 = torch.aten.slice.Tensor %3983, %int0_4870, %int0_4871, %int9223372036854775807_4872, %int1_4873 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %3995, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_4874 = torch.constant.int 1
    %int0_4875 = torch.constant.int 0
    %int9223372036854775807_4876 = torch.constant.int 9223372036854775807
    %int1_4877 = torch.constant.int 1
    %3996 = torch.aten.slice.Tensor %3995, %int1_4874, %int0_4875, %int9223372036854775807_4876, %int1_4877 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %3996, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_4878 = torch.constant.int 2
    %int1_4879 = torch.constant.int 1
    %3997 = torch.aten.select.int %3996, %int2_4878, %int1_4879 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3997, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int2_4880 = torch.constant.int 2
    %int0_4881 = torch.constant.int 0
    %int1_4882 = torch.constant.int 1
    %3998 = torch.aten.slice.Tensor %3997, %int2_4880, %int0_4881, %3987, %int1_4882 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3998, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_4883 = torch.constant.int 0
    %3999 = torch.aten.clone %3998, %int0_4883 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %3999, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_4884 = torch.constant.int 1
    %4000 = torch.aten.size.int %3996, %int1_4884 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_4885 = torch.constant.int 32
    %4001 = torch.aten.mul.int %4000, %int32_4885 : !torch.int, !torch.int -> !torch.int
    %int4_4886 = torch.constant.int 4
    %int8_4887 = torch.constant.int 8
    %int128_4888 = torch.constant.int 128
    %4002 = torch.prim.ListConstruct %int4_4886, %4001, %int8_4887, %int128_4888 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4003 = torch.aten._unsafe_view %3999, %4002 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %4003, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_4889 = torch.constant.int 0
    %int0_4890 = torch.constant.int 0
    %int9223372036854775807_4891 = torch.constant.int 9223372036854775807
    %int1_4892 = torch.constant.int 1
    %4004 = torch.aten.slice.Tensor %4003, %int0_4889, %int0_4890, %int9223372036854775807_4891, %int1_4892 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %4004, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int-2_4893 = torch.constant.int -2
    %4005 = torch.aten.unsqueeze %3994, %int-2_4893 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %4005, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_4894 = torch.constant.int 1
    %4006 = torch.aten.size.int %3993, %int1_4894 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_4895 = torch.constant.int 4
    %int8_4896 = torch.constant.int 8
    %int4_4897 = torch.constant.int 4
    %int128_4898 = torch.constant.int 128
    %4007 = torch.prim.ListConstruct %int4_4895, %4006, %int8_4896, %int4_4897, %int128_4898 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_4899 = torch.constant.bool false
    %4008 = torch.aten.expand %4005, %4007, %false_4899 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4008, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_4900 = torch.constant.int 0
    %4009 = torch.aten.clone %4008, %int0_4900 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4009, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_4901 = torch.constant.int 4
    %int32_4902 = torch.constant.int 32
    %int128_4903 = torch.constant.int 128
    %4010 = torch.prim.ListConstruct %int4_4901, %4006, %int32_4902, %int128_4903 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4011 = torch.aten._unsafe_view %4009, %4010 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %4011, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_4904 = torch.constant.int -2
    %4012 = torch.aten.unsqueeze %4004, %int-2_4904 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %4012, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_4905 = torch.constant.int 1
    %4013 = torch.aten.size.int %4003, %int1_4905 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_4906 = torch.constant.int 4
    %int8_4907 = torch.constant.int 8
    %int4_4908 = torch.constant.int 4
    %int128_4909 = torch.constant.int 128
    %4014 = torch.prim.ListConstruct %int4_4906, %4013, %int8_4907, %int4_4908, %int128_4909 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_4910 = torch.constant.bool false
    %4015 = torch.aten.expand %4012, %4014, %false_4910 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4015, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_4911 = torch.constant.int 0
    %4016 = torch.aten.clone %4015, %int0_4911 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4016, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_4912 = torch.constant.int 4
    %int32_4913 = torch.constant.int 32
    %int128_4914 = torch.constant.int 128
    %4017 = torch.prim.ListConstruct %int4_4912, %4013, %int32_4913, %int128_4914 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4018 = torch.aten._unsafe_view %4016, %4017 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %4018, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_4915 = torch.constant.int 1
    %int2_4916 = torch.constant.int 2
    %4019 = torch.aten.transpose.int %3899, %int1_4915, %int2_4916 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,128],f16>
    %int1_4917 = torch.constant.int 1
    %int2_4918 = torch.constant.int 2
    %4020 = torch.aten.transpose.int %4011, %int1_4917, %int2_4918 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %4020, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_4919 = torch.constant.int 1
    %int2_4920 = torch.constant.int 2
    %4021 = torch.aten.transpose.int %4018, %int1_4919, %int2_4920 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %4021, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_4921 = torch.constant.float 0.000000e+00
    %false_4922 = torch.constant.bool false
    %none_4923 = torch.constant.none
    %4022:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4019, %4020, %4021, %float0.000000e00_4921, %false_4922, %368, %none_4923) : (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.vtensor<[4,1,1,?],f16>, !torch.none) -> (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,1],f32>) 
    %int1_4924 = torch.constant.int 1
    %int2_4925 = torch.constant.int 2
    %4023 = torch.aten.transpose.int %4022#0, %int1_4924, %int2_4925 : !torch.vtensor<[4,32,1,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int4_4926 = torch.constant.int 4
    %int1_4927 = torch.constant.int 1
    %int4096_4928 = torch.constant.int 4096
    %4024 = torch.prim.ListConstruct %int4_4926, %int1_4927, %int4096_4928 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4025 = torch.aten.view %4023, %4024 : !torch.vtensor<[4,1,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_4929 = torch.constant.int -2
    %int-1_4930 = torch.constant.int -1
    %4026 = torch.aten.transpose.int %194, %int-2_4929, %int-1_4930 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_4931 = torch.constant.int 4
    %int4096_4932 = torch.constant.int 4096
    %4027 = torch.prim.ListConstruct %int4_4931, %int4096_4932 : (!torch.int, !torch.int) -> !torch.list<int>
    %4028 = torch.aten.view %4025, %4027 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %4029 = torch.aten.mm %4028, %4026 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_4933 = torch.constant.int 4
    %int1_4934 = torch.constant.int 1
    %int4096_4935 = torch.constant.int 4096
    %4030 = torch.prim.ListConstruct %int4_4933, %int1_4934, %int4096_4935 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4031 = torch.aten.view %4029, %4030 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_4936 = torch.constant.int 1
    %4032 = torch.aten.add.Tensor %3859, %4031, %int1_4936 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_4937 = torch.constant.int 6
    %4033 = torch.prims.convert_element_type %4032, %int6_4937 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_4938 = torch.constant.int 2
    %4034 = torch.aten.pow.Tensor_Scalar %4033, %int2_4938 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_4939 = torch.constant.int -1
    %4035 = torch.prim.ListConstruct %int-1_4939 : (!torch.int) -> !torch.list<int>
    %true_4940 = torch.constant.bool true
    %none_4941 = torch.constant.none
    %4036 = torch.aten.mean.dim %4034, %4035, %true_4940, %none_4941 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_4942 = torch.constant.float 9.9999997473787516E-6
    %int1_4943 = torch.constant.int 1
    %4037 = torch.aten.add.Scalar %4036, %float9.999990e-06_4942, %int1_4943 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %4038 = torch.aten.rsqrt %4037 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %4039 = torch.aten.mul.Tensor %4033, %4038 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_4944 = torch.constant.int 5
    %4040 = torch.prims.convert_element_type %4039, %int5_4944 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %4041 = torch.aten.mul.Tensor %195, %4040 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_4945 = torch.constant.int 5
    %4042 = torch.prims.convert_element_type %4041, %int5_4945 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_4946 = torch.constant.int -2
    %int-1_4947 = torch.constant.int -1
    %4043 = torch.aten.transpose.int %196, %int-2_4946, %int-1_4947 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_4948 = torch.constant.int 4
    %int4096_4949 = torch.constant.int 4096
    %4044 = torch.prim.ListConstruct %int4_4948, %int4096_4949 : (!torch.int, !torch.int) -> !torch.list<int>
    %4045 = torch.aten.view %4042, %4044 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %4046 = torch.aten.mm %4045, %4043 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_4950 = torch.constant.int 4
    %int1_4951 = torch.constant.int 1
    %int14336_4952 = torch.constant.int 14336
    %4047 = torch.prim.ListConstruct %int4_4950, %int1_4951, %int14336_4952 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4048 = torch.aten.view %4046, %4047 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %4049 = torch.aten.silu %4048 : !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_4953 = torch.constant.int -2
    %int-1_4954 = torch.constant.int -1
    %4050 = torch.aten.transpose.int %197, %int-2_4953, %int-1_4954 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_4955 = torch.constant.int 4
    %int4096_4956 = torch.constant.int 4096
    %4051 = torch.prim.ListConstruct %int4_4955, %int4096_4956 : (!torch.int, !torch.int) -> !torch.list<int>
    %4052 = torch.aten.view %4042, %4051 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %4053 = torch.aten.mm %4052, %4050 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_4957 = torch.constant.int 4
    %int1_4958 = torch.constant.int 1
    %int14336_4959 = torch.constant.int 14336
    %4054 = torch.prim.ListConstruct %int4_4957, %int1_4958, %int14336_4959 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4055 = torch.aten.view %4053, %4054 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %4056 = torch.aten.mul.Tensor %4049, %4055 : !torch.vtensor<[4,1,14336],f16>, !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_4960 = torch.constant.int -2
    %int-1_4961 = torch.constant.int -1
    %4057 = torch.aten.transpose.int %198, %int-2_4960, %int-1_4961 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int4_4962 = torch.constant.int 4
    %int14336_4963 = torch.constant.int 14336
    %4058 = torch.prim.ListConstruct %int4_4962, %int14336_4963 : (!torch.int, !torch.int) -> !torch.list<int>
    %4059 = torch.aten.view %4056, %4058 : !torch.vtensor<[4,1,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,14336],f16>
    %4060 = torch.aten.mm %4059, %4057 : !torch.vtensor<[4,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_4964 = torch.constant.int 4
    %int1_4965 = torch.constant.int 1
    %int4096_4966 = torch.constant.int 4096
    %4061 = torch.prim.ListConstruct %int4_4964, %int1_4965, %int4096_4966 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4062 = torch.aten.view %4060, %4061 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_4967 = torch.constant.int 1
    %4063 = torch.aten.add.Tensor %4032, %4062, %int1_4967 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_4968 = torch.constant.int 6
    %4064 = torch.prims.convert_element_type %4063, %int6_4968 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_4969 = torch.constant.int 2
    %4065 = torch.aten.pow.Tensor_Scalar %4064, %int2_4969 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_4970 = torch.constant.int -1
    %4066 = torch.prim.ListConstruct %int-1_4970 : (!torch.int) -> !torch.list<int>
    %true_4971 = torch.constant.bool true
    %none_4972 = torch.constant.none
    %4067 = torch.aten.mean.dim %4065, %4066, %true_4971, %none_4972 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_4973 = torch.constant.float 9.9999997473787516E-6
    %int1_4974 = torch.constant.int 1
    %4068 = torch.aten.add.Scalar %4067, %float9.999990e-06_4973, %int1_4974 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %4069 = torch.aten.rsqrt %4068 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %4070 = torch.aten.mul.Tensor %4064, %4069 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_4975 = torch.constant.int 5
    %4071 = torch.prims.convert_element_type %4070, %int5_4975 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %4072 = torch.aten.mul.Tensor %199, %4071 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_4976 = torch.constant.int 5
    %4073 = torch.prims.convert_element_type %4072, %int5_4976 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_4977 = torch.constant.int -2
    %int-1_4978 = torch.constant.int -1
    %4074 = torch.aten.transpose.int %200, %int-2_4977, %int-1_4978 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_4979 = torch.constant.int 4
    %int4096_4980 = torch.constant.int 4096
    %4075 = torch.prim.ListConstruct %int4_4979, %int4096_4980 : (!torch.int, !torch.int) -> !torch.list<int>
    %4076 = torch.aten.view %4073, %4075 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %4077 = torch.aten.mm %4076, %4074 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_4981 = torch.constant.int 4
    %int1_4982 = torch.constant.int 1
    %int4096_4983 = torch.constant.int 4096
    %4078 = torch.prim.ListConstruct %int4_4981, %int1_4982, %int4096_4983 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4079 = torch.aten.view %4077, %4078 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_4984 = torch.constant.int -2
    %int-1_4985 = torch.constant.int -1
    %4080 = torch.aten.transpose.int %201, %int-2_4984, %int-1_4985 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_4986 = torch.constant.int 4
    %int4096_4987 = torch.constant.int 4096
    %4081 = torch.prim.ListConstruct %int4_4986, %int4096_4987 : (!torch.int, !torch.int) -> !torch.list<int>
    %4082 = torch.aten.view %4073, %4081 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %4083 = torch.aten.mm %4082, %4080 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_4988 = torch.constant.int 4
    %int1_4989 = torch.constant.int 1
    %int1024_4990 = torch.constant.int 1024
    %4084 = torch.prim.ListConstruct %int4_4988, %int1_4989, %int1024_4990 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4085 = torch.aten.view %4083, %4084 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int-2_4991 = torch.constant.int -2
    %int-1_4992 = torch.constant.int -1
    %4086 = torch.aten.transpose.int %202, %int-2_4991, %int-1_4992 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_4993 = torch.constant.int 4
    %int4096_4994 = torch.constant.int 4096
    %4087 = torch.prim.ListConstruct %int4_4993, %int4096_4994 : (!torch.int, !torch.int) -> !torch.list<int>
    %4088 = torch.aten.view %4073, %4087 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %4089 = torch.aten.mm %4088, %4086 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_4995 = torch.constant.int 4
    %int1_4996 = torch.constant.int 1
    %int1024_4997 = torch.constant.int 1024
    %4090 = torch.prim.ListConstruct %int4_4995, %int1_4996, %int1024_4997 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4091 = torch.aten.view %4089, %4090 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int4_4998 = torch.constant.int 4
    %int1_4999 = torch.constant.int 1
    %int32_5000 = torch.constant.int 32
    %int128_5001 = torch.constant.int 128
    %4092 = torch.prim.ListConstruct %int4_4998, %int1_4999, %int32_5000, %int128_5001 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4093 = torch.aten.view %4079, %4092 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,128],f16>
    %int4_5002 = torch.constant.int 4
    %int1_5003 = torch.constant.int 1
    %int8_5004 = torch.constant.int 8
    %int128_5005 = torch.constant.int 128
    %4094 = torch.prim.ListConstruct %int4_5002, %int1_5003, %int8_5004, %int128_5005 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4095 = torch.aten.view %4085, %4094 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int4_5006 = torch.constant.int 4
    %int1_5007 = torch.constant.int 1
    %int8_5008 = torch.constant.int 8
    %int128_5009 = torch.constant.int 128
    %4096 = torch.prim.ListConstruct %int4_5006, %int1_5007, %int8_5008, %int128_5009 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4097 = torch.aten.view %4091, %4096 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int6_5010 = torch.constant.int 6
    %4098 = torch.prims.convert_element_type %4093, %int6_5010 : !torch.vtensor<[4,1,32,128],f16>, !torch.int -> !torch.vtensor<[4,1,32,128],f32>
    %4099 = torch_c.to_builtin_tensor %4098 : !torch.vtensor<[4,1,32,128],f32> -> tensor<4x1x32x128xf32>
    %4100 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %4101 = util.call @sharktank_rotary_embedding_4_1_32_128_f32(%4099, %4100) : (tensor<4x1x32x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x32x128xf32>
    %4102 = torch_c.from_builtin_tensor %4101 : tensor<4x1x32x128xf32> -> !torch.vtensor<[4,1,32,128],f32>
    %int5_5011 = torch.constant.int 5
    %4103 = torch.prims.convert_element_type %4102, %int5_5011 : !torch.vtensor<[4,1,32,128],f32>, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int6_5012 = torch.constant.int 6
    %4104 = torch.prims.convert_element_type %4095, %int6_5012 : !torch.vtensor<[4,1,8,128],f16>, !torch.int -> !torch.vtensor<[4,1,8,128],f32>
    %4105 = torch_c.to_builtin_tensor %4104 : !torch.vtensor<[4,1,8,128],f32> -> tensor<4x1x8x128xf32>
    %4106 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %4107 = util.call @sharktank_rotary_embedding_4_1_8_128_f32(%4105, %4106) : (tensor<4x1x8x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x8x128xf32>
    %4108 = torch_c.from_builtin_tensor %4107 : tensor<4x1x8x128xf32> -> !torch.vtensor<[4,1,8,128],f32>
    %int5_5013 = torch.constant.int 5
    %4109 = torch.prims.convert_element_type %4108, %int5_5013 : !torch.vtensor<[4,1,8,128],f32>, !torch.int -> !torch.vtensor<[4,1,8,128],f16>
    %int32_5014 = torch.constant.int 32
    %4110 = torch.aten.floor_divide.Scalar %arg2, %int32_5014 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_5015 = torch.constant.int 1
    %4111 = torch.aten.unsqueeze %4110, %int1_5015 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5016 = torch.constant.int 1
    %false_5017 = torch.constant.bool false
    %4112 = torch.aten.gather %arg3, %int1_5016, %4111, %false_5017 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_5018 = torch.constant.int 32
    %4113 = torch.aten.remainder.Scalar %arg2, %int32_5018 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_5019 = torch.constant.int 1
    %4114 = torch.aten.unsqueeze %4113, %int1_5019 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_5020 = torch.constant.none
    %4115 = torch.aten.clone %203, %none_5020 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_5021 = torch.constant.int 0
    %4116 = torch.aten.unsqueeze %4115, %int0_5021 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_5022 = torch.constant.int 4
    %int1_5023 = torch.constant.int 1
    %4117 = torch.prim.ListConstruct %int4_5022, %int1_5023 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_5024 = torch.constant.int 1
    %int1_5025 = torch.constant.int 1
    %4118 = torch.prim.ListConstruct %int1_5024, %int1_5025 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_5026 = torch.constant.int 4
    %int0_5027 = torch.constant.int 0
    %cpu_5028 = torch.constant.device "cpu"
    %false_5029 = torch.constant.bool false
    %4119 = torch.aten.empty_strided %4117, %4118, %int4_5026, %int0_5027, %cpu_5028, %false_5029 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int18 = torch.constant.int 18
    %4120 = torch.aten.fill.Scalar %4119, %int18 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_5030 = torch.constant.int 4
    %int1_5031 = torch.constant.int 1
    %4121 = torch.prim.ListConstruct %int4_5030, %int1_5031 : (!torch.int, !torch.int) -> !torch.list<int>
    %4122 = torch.aten.repeat %4116, %4121 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_5032 = torch.constant.int 32
    %4123 = torch.aten.mul.Scalar %4112, %int32_5032 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5033 = torch.constant.int 1
    %4124 = torch.aten.add.Tensor %4123, %4120, %int1_5033 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_5034 = torch.constant.int 2
    %4125 = torch.aten.mul.Scalar %4124, %int2_5034 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5035 = torch.constant.int 1
    %4126 = torch.aten.add.Tensor %4125, %4122, %int1_5035 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_5036 = torch.constant.int 32
    %4127 = torch.aten.mul.Scalar %4126, %int32_5036 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5037 = torch.constant.int 1
    %4128 = torch.aten.add.Tensor %4127, %4114, %int1_5037 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_5038 = torch.constant.int 32
    %int2_5039 = torch.constant.int 2
    %int32_5040 = torch.constant.int 32
    %int8_5041 = torch.constant.int 8
    %int128_5042 = torch.constant.int 128
    %4129 = torch.prim.ListConstruct %437, %int32_5038, %int2_5039, %int32_5040, %int8_5041, %int128_5042 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4130 = torch.aten.view %3966, %4129 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4130, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_5043 = torch.constant.int 32
    %4131 = torch.aten.mul.int %437, %int32_5043 : !torch.int, !torch.int -> !torch.int
    %int2_5044 = torch.constant.int 2
    %4132 = torch.aten.mul.int %4131, %int2_5044 : !torch.int, !torch.int -> !torch.int
    %int32_5045 = torch.constant.int 32
    %4133 = torch.aten.mul.int %4132, %int32_5045 : !torch.int, !torch.int -> !torch.int
    %int8_5046 = torch.constant.int 8
    %int128_5047 = torch.constant.int 128
    %4134 = torch.prim.ListConstruct %4133, %int8_5046, %int128_5047 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4135 = torch.aten.view %4130, %4134 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %4135, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %4136 = torch.prim.ListConstruct %4128 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_5048 = torch.constant.bool false
    %4137 = torch.aten.index_put %4135, %4136, %4109, %false_5048 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %4137, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_5049 = torch.constant.int 32
    %int2_5050 = torch.constant.int 2
    %int32_5051 = torch.constant.int 32
    %int8_5052 = torch.constant.int 8
    %int128_5053 = torch.constant.int 128
    %4138 = torch.prim.ListConstruct %437, %int32_5049, %int2_5050, %int32_5051, %int8_5052, %int128_5053 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4139 = torch.aten.view %4137, %4138 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4139, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_5054 = torch.constant.int 2097152
    %4140 = torch.prim.ListConstruct %437, %int2097152_5054 : (!torch.int, !torch.int) -> !torch.list<int>
    %4141 = torch.aten.view %4139, %4140 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %4141, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_5055 = torch.constant.int 32
    %int2_5056 = torch.constant.int 2
    %int32_5057 = torch.constant.int 32
    %int8_5058 = torch.constant.int 8
    %int128_5059 = torch.constant.int 128
    %4142 = torch.prim.ListConstruct %437, %int32_5055, %int2_5056, %int32_5057, %int8_5058, %int128_5059 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4143 = torch.aten.view %4141, %4142 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4143, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int8_5060 = torch.constant.int 8
    %int128_5061 = torch.constant.int 128
    %4144 = torch.prim.ListConstruct %4133, %int8_5060, %int128_5061 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4145 = torch.aten.view %4143, %4144 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %4145, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_5062 = torch.constant.int 32
    %4146 = torch.aten.floor_divide.Scalar %arg2, %int32_5062 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_5063 = torch.constant.int 1
    %4147 = torch.aten.unsqueeze %4146, %int1_5063 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5064 = torch.constant.int 1
    %false_5065 = torch.constant.bool false
    %4148 = torch.aten.gather %arg3, %int1_5064, %4147, %false_5065 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_5066 = torch.constant.int 32
    %4149 = torch.aten.remainder.Scalar %arg2, %int32_5066 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_5067 = torch.constant.int 1
    %4150 = torch.aten.unsqueeze %4149, %int1_5067 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_5068 = torch.constant.none
    %4151 = torch.aten.clone %204, %none_5068 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_5069 = torch.constant.int 0
    %4152 = torch.aten.unsqueeze %4151, %int0_5069 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_5070 = torch.constant.int 4
    %int1_5071 = torch.constant.int 1
    %4153 = torch.prim.ListConstruct %int4_5070, %int1_5071 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_5072 = torch.constant.int 1
    %int1_5073 = torch.constant.int 1
    %4154 = torch.prim.ListConstruct %int1_5072, %int1_5073 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_5074 = torch.constant.int 4
    %int0_5075 = torch.constant.int 0
    %cpu_5076 = torch.constant.device "cpu"
    %false_5077 = torch.constant.bool false
    %4155 = torch.aten.empty_strided %4153, %4154, %int4_5074, %int0_5075, %cpu_5076, %false_5077 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int18_5078 = torch.constant.int 18
    %4156 = torch.aten.fill.Scalar %4155, %int18_5078 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_5079 = torch.constant.int 4
    %int1_5080 = torch.constant.int 1
    %4157 = torch.prim.ListConstruct %int4_5079, %int1_5080 : (!torch.int, !torch.int) -> !torch.list<int>
    %4158 = torch.aten.repeat %4152, %4157 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_5081 = torch.constant.int 32
    %4159 = torch.aten.mul.Scalar %4148, %int32_5081 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5082 = torch.constant.int 1
    %4160 = torch.aten.add.Tensor %4159, %4156, %int1_5082 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_5083 = torch.constant.int 2
    %4161 = torch.aten.mul.Scalar %4160, %int2_5083 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5084 = torch.constant.int 1
    %4162 = torch.aten.add.Tensor %4161, %4158, %int1_5084 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_5085 = torch.constant.int 32
    %4163 = torch.aten.mul.Scalar %4162, %int32_5085 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5086 = torch.constant.int 1
    %4164 = torch.aten.add.Tensor %4163, %4150, %int1_5086 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %4165 = torch.prim.ListConstruct %4164 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_5087 = torch.constant.bool false
    %4166 = torch.aten.index_put %4145, %4165, %4097, %false_5087 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %4166, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_5088 = torch.constant.int 32
    %int2_5089 = torch.constant.int 2
    %int32_5090 = torch.constant.int 32
    %int8_5091 = torch.constant.int 8
    %int128_5092 = torch.constant.int 128
    %4167 = torch.prim.ListConstruct %437, %int32_5088, %int2_5089, %int32_5090, %int8_5091, %int128_5092 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4168 = torch.aten.view %4166, %4167 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4168, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_5093 = torch.constant.int 2097152
    %4169 = torch.prim.ListConstruct %437, %int2097152_5093 : (!torch.int, !torch.int) -> !torch.list<int>
    %4170 = torch.aten.view %4168, %4169 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %4170, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int4_5094 = torch.constant.int 4
    %4171 = torch.prim.ListConstruct %int4_5094, %358 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_5095 = torch.constant.int 1
    %4172 = torch.prim.ListConstruct %358, %int1_5095 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_5096 = torch.constant.int 4
    %int0_5097 = torch.constant.int 0
    %cpu_5098 = torch.constant.device "cpu"
    %false_5099 = torch.constant.bool false
    %4173 = torch.aten.empty_strided %4171, %4172, %int4_5096, %int0_5097, %cpu_5098, %false_5099 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4173, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int18_5100 = torch.constant.int 18
    %4174 = torch.aten.fill.Scalar %4173, %int18_5100 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4174, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int32_5101 = torch.constant.int 32
    %4175 = torch.aten.mul.Scalar %arg3, %int32_5101 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4175, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_5102 = torch.constant.int 1
    %4176 = torch.aten.add.Tensor %4175, %4174, %int1_5102 : !torch.vtensor<[4,?],si64>, !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4176, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_5103 = torch.constant.int 4
    %4177 = torch.aten.mul.int %int4_5103, %358 : !torch.int, !torch.int -> !torch.int
    %4178 = torch.prim.ListConstruct %4177 : (!torch.int) -> !torch.list<int>
    %4179 = torch.aten.view %4176, %4178 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4179, [%356], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_5104 = torch.constant.int 32
    %int2_5105 = torch.constant.int 2
    %int32_5106 = torch.constant.int 32
    %int8_5107 = torch.constant.int 8
    %int128_5108 = torch.constant.int 128
    %4180 = torch.prim.ListConstruct %437, %int32_5104, %int2_5105, %int32_5106, %int8_5107, %int128_5108 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4181 = torch.aten.view %4170, %4180 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4181, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_5109 = torch.constant.int 32
    %4182 = torch.aten.mul.int %437, %int32_5109 : !torch.int, !torch.int -> !torch.int
    %int2_5110 = torch.constant.int 2
    %int32_5111 = torch.constant.int 32
    %int8_5112 = torch.constant.int 8
    %int128_5113 = torch.constant.int 128
    %4183 = torch.prim.ListConstruct %4182, %int2_5110, %int32_5111, %int8_5112, %int128_5113 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4184 = torch.aten.view %4181, %4183 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %4184, [%357], affine_map<()[s0] -> (s0 * 32, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int0_5114 = torch.constant.int 0
    %4185 = torch.aten.index_select %4184, %int0_5114, %4179 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.int, !torch.vtensor<[?],si64> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %4185, [%356], affine_map<()[s0] -> (s0 * 4, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int4_5115 = torch.constant.int 4
    %int2_5116 = torch.constant.int 2
    %int32_5117 = torch.constant.int 32
    %int8_5118 = torch.constant.int 8
    %int128_5119 = torch.constant.int 128
    %4186 = torch.prim.ListConstruct %int4_5115, %358, %int2_5116, %int32_5117, %int8_5118, %int128_5119 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4187 = torch.aten.view %4185, %4186 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %4187, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int0_5120 = torch.constant.int 0
    %int0_5121 = torch.constant.int 0
    %int9223372036854775807_5122 = torch.constant.int 9223372036854775807
    %int1_5123 = torch.constant.int 1
    %4188 = torch.aten.slice.Tensor %4187, %int0_5120, %int0_5121, %int9223372036854775807_5122, %int1_5123 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %4188, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_5124 = torch.constant.int 1
    %int0_5125 = torch.constant.int 0
    %int9223372036854775807_5126 = torch.constant.int 9223372036854775807
    %int1_5127 = torch.constant.int 1
    %4189 = torch.aten.slice.Tensor %4188, %int1_5124, %int0_5125, %int9223372036854775807_5126, %int1_5127 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %4189, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_5128 = torch.constant.int 2
    %int0_5129 = torch.constant.int 0
    %4190 = torch.aten.select.int %4189, %int2_5128, %int0_5129 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %4190, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int32_5130 = torch.constant.int 32
    %4191 = torch.aten.mul.int %358, %int32_5130 : !torch.int, !torch.int -> !torch.int
    %int2_5131 = torch.constant.int 2
    %int0_5132 = torch.constant.int 0
    %int1_5133 = torch.constant.int 1
    %4192 = torch.aten.slice.Tensor %4190, %int2_5131, %int0_5132, %4191, %int1_5133 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %4192, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_5134 = torch.constant.int 0
    %4193 = torch.aten.clone %4192, %int0_5134 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %4193, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_5135 = torch.constant.int 1
    %4194 = torch.aten.size.int %4189, %int1_5135 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_5136 = torch.constant.int 32
    %4195 = torch.aten.mul.int %4194, %int32_5136 : !torch.int, !torch.int -> !torch.int
    %int4_5137 = torch.constant.int 4
    %int8_5138 = torch.constant.int 8
    %int128_5139 = torch.constant.int 128
    %4196 = torch.prim.ListConstruct %int4_5137, %4195, %int8_5138, %int128_5139 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4197 = torch.aten._unsafe_view %4193, %4196 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %4197, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_5140 = torch.constant.int 0
    %int0_5141 = torch.constant.int 0
    %int9223372036854775807_5142 = torch.constant.int 9223372036854775807
    %int1_5143 = torch.constant.int 1
    %4198 = torch.aten.slice.Tensor %4197, %int0_5140, %int0_5141, %int9223372036854775807_5142, %int1_5143 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %4198, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_5144 = torch.constant.int 0
    %int0_5145 = torch.constant.int 0
    %int9223372036854775807_5146 = torch.constant.int 9223372036854775807
    %int1_5147 = torch.constant.int 1
    %4199 = torch.aten.slice.Tensor %4187, %int0_5144, %int0_5145, %int9223372036854775807_5146, %int1_5147 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %4199, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_5148 = torch.constant.int 1
    %int0_5149 = torch.constant.int 0
    %int9223372036854775807_5150 = torch.constant.int 9223372036854775807
    %int1_5151 = torch.constant.int 1
    %4200 = torch.aten.slice.Tensor %4199, %int1_5148, %int0_5149, %int9223372036854775807_5150, %int1_5151 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %4200, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_5152 = torch.constant.int 2
    %int1_5153 = torch.constant.int 1
    %4201 = torch.aten.select.int %4200, %int2_5152, %int1_5153 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %4201, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int2_5154 = torch.constant.int 2
    %int0_5155 = torch.constant.int 0
    %int1_5156 = torch.constant.int 1
    %4202 = torch.aten.slice.Tensor %4201, %int2_5154, %int0_5155, %4191, %int1_5156 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %4202, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_5157 = torch.constant.int 0
    %4203 = torch.aten.clone %4202, %int0_5157 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %4203, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_5158 = torch.constant.int 1
    %4204 = torch.aten.size.int %4200, %int1_5158 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_5159 = torch.constant.int 32
    %4205 = torch.aten.mul.int %4204, %int32_5159 : !torch.int, !torch.int -> !torch.int
    %int4_5160 = torch.constant.int 4
    %int8_5161 = torch.constant.int 8
    %int128_5162 = torch.constant.int 128
    %4206 = torch.prim.ListConstruct %int4_5160, %4205, %int8_5161, %int128_5162 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4207 = torch.aten._unsafe_view %4203, %4206 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %4207, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_5163 = torch.constant.int 0
    %int0_5164 = torch.constant.int 0
    %int9223372036854775807_5165 = torch.constant.int 9223372036854775807
    %int1_5166 = torch.constant.int 1
    %4208 = torch.aten.slice.Tensor %4207, %int0_5163, %int0_5164, %int9223372036854775807_5165, %int1_5166 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %4208, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int-2_5167 = torch.constant.int -2
    %4209 = torch.aten.unsqueeze %4198, %int-2_5167 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %4209, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_5168 = torch.constant.int 1
    %4210 = torch.aten.size.int %4197, %int1_5168 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_5169 = torch.constant.int 4
    %int8_5170 = torch.constant.int 8
    %int4_5171 = torch.constant.int 4
    %int128_5172 = torch.constant.int 128
    %4211 = torch.prim.ListConstruct %int4_5169, %4210, %int8_5170, %int4_5171, %int128_5172 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_5173 = torch.constant.bool false
    %4212 = torch.aten.expand %4209, %4211, %false_5173 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4212, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_5174 = torch.constant.int 0
    %4213 = torch.aten.clone %4212, %int0_5174 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4213, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_5175 = torch.constant.int 4
    %int32_5176 = torch.constant.int 32
    %int128_5177 = torch.constant.int 128
    %4214 = torch.prim.ListConstruct %int4_5175, %4210, %int32_5176, %int128_5177 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4215 = torch.aten._unsafe_view %4213, %4214 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %4215, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_5178 = torch.constant.int -2
    %4216 = torch.aten.unsqueeze %4208, %int-2_5178 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %4216, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_5179 = torch.constant.int 1
    %4217 = torch.aten.size.int %4207, %int1_5179 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_5180 = torch.constant.int 4
    %int8_5181 = torch.constant.int 8
    %int4_5182 = torch.constant.int 4
    %int128_5183 = torch.constant.int 128
    %4218 = torch.prim.ListConstruct %int4_5180, %4217, %int8_5181, %int4_5182, %int128_5183 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_5184 = torch.constant.bool false
    %4219 = torch.aten.expand %4216, %4218, %false_5184 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4219, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_5185 = torch.constant.int 0
    %4220 = torch.aten.clone %4219, %int0_5185 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4220, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_5186 = torch.constant.int 4
    %int32_5187 = torch.constant.int 32
    %int128_5188 = torch.constant.int 128
    %4221 = torch.prim.ListConstruct %int4_5186, %4217, %int32_5187, %int128_5188 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4222 = torch.aten._unsafe_view %4220, %4221 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %4222, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_5189 = torch.constant.int 1
    %int2_5190 = torch.constant.int 2
    %4223 = torch.aten.transpose.int %4103, %int1_5189, %int2_5190 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,128],f16>
    %int1_5191 = torch.constant.int 1
    %int2_5192 = torch.constant.int 2
    %4224 = torch.aten.transpose.int %4215, %int1_5191, %int2_5192 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %4224, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_5193 = torch.constant.int 1
    %int2_5194 = torch.constant.int 2
    %4225 = torch.aten.transpose.int %4222, %int1_5193, %int2_5194 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %4225, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_5195 = torch.constant.float 0.000000e+00
    %false_5196 = torch.constant.bool false
    %none_5197 = torch.constant.none
    %4226:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4223, %4224, %4225, %float0.000000e00_5195, %false_5196, %368, %none_5197) : (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.vtensor<[4,1,1,?],f16>, !torch.none) -> (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,1],f32>) 
    %int1_5198 = torch.constant.int 1
    %int2_5199 = torch.constant.int 2
    %4227 = torch.aten.transpose.int %4226#0, %int1_5198, %int2_5199 : !torch.vtensor<[4,32,1,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int4_5200 = torch.constant.int 4
    %int1_5201 = torch.constant.int 1
    %int4096_5202 = torch.constant.int 4096
    %4228 = torch.prim.ListConstruct %int4_5200, %int1_5201, %int4096_5202 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4229 = torch.aten.view %4227, %4228 : !torch.vtensor<[4,1,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_5203 = torch.constant.int -2
    %int-1_5204 = torch.constant.int -1
    %4230 = torch.aten.transpose.int %205, %int-2_5203, %int-1_5204 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_5205 = torch.constant.int 4
    %int4096_5206 = torch.constant.int 4096
    %4231 = torch.prim.ListConstruct %int4_5205, %int4096_5206 : (!torch.int, !torch.int) -> !torch.list<int>
    %4232 = torch.aten.view %4229, %4231 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %4233 = torch.aten.mm %4232, %4230 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_5207 = torch.constant.int 4
    %int1_5208 = torch.constant.int 1
    %int4096_5209 = torch.constant.int 4096
    %4234 = torch.prim.ListConstruct %int4_5207, %int1_5208, %int4096_5209 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4235 = torch.aten.view %4233, %4234 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_5210 = torch.constant.int 1
    %4236 = torch.aten.add.Tensor %4063, %4235, %int1_5210 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_5211 = torch.constant.int 6
    %4237 = torch.prims.convert_element_type %4236, %int6_5211 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_5212 = torch.constant.int 2
    %4238 = torch.aten.pow.Tensor_Scalar %4237, %int2_5212 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_5213 = torch.constant.int -1
    %4239 = torch.prim.ListConstruct %int-1_5213 : (!torch.int) -> !torch.list<int>
    %true_5214 = torch.constant.bool true
    %none_5215 = torch.constant.none
    %4240 = torch.aten.mean.dim %4238, %4239, %true_5214, %none_5215 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_5216 = torch.constant.float 9.9999997473787516E-6
    %int1_5217 = torch.constant.int 1
    %4241 = torch.aten.add.Scalar %4240, %float9.999990e-06_5216, %int1_5217 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %4242 = torch.aten.rsqrt %4241 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %4243 = torch.aten.mul.Tensor %4237, %4242 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_5218 = torch.constant.int 5
    %4244 = torch.prims.convert_element_type %4243, %int5_5218 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %4245 = torch.aten.mul.Tensor %206, %4244 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_5219 = torch.constant.int 5
    %4246 = torch.prims.convert_element_type %4245, %int5_5219 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_5220 = torch.constant.int -2
    %int-1_5221 = torch.constant.int -1
    %4247 = torch.aten.transpose.int %207, %int-2_5220, %int-1_5221 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_5222 = torch.constant.int 4
    %int4096_5223 = torch.constant.int 4096
    %4248 = torch.prim.ListConstruct %int4_5222, %int4096_5223 : (!torch.int, !torch.int) -> !torch.list<int>
    %4249 = torch.aten.view %4246, %4248 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %4250 = torch.aten.mm %4249, %4247 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_5224 = torch.constant.int 4
    %int1_5225 = torch.constant.int 1
    %int14336_5226 = torch.constant.int 14336
    %4251 = torch.prim.ListConstruct %int4_5224, %int1_5225, %int14336_5226 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4252 = torch.aten.view %4250, %4251 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %4253 = torch.aten.silu %4252 : !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_5227 = torch.constant.int -2
    %int-1_5228 = torch.constant.int -1
    %4254 = torch.aten.transpose.int %208, %int-2_5227, %int-1_5228 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_5229 = torch.constant.int 4
    %int4096_5230 = torch.constant.int 4096
    %4255 = torch.prim.ListConstruct %int4_5229, %int4096_5230 : (!torch.int, !torch.int) -> !torch.list<int>
    %4256 = torch.aten.view %4246, %4255 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %4257 = torch.aten.mm %4256, %4254 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_5231 = torch.constant.int 4
    %int1_5232 = torch.constant.int 1
    %int14336_5233 = torch.constant.int 14336
    %4258 = torch.prim.ListConstruct %int4_5231, %int1_5232, %int14336_5233 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4259 = torch.aten.view %4257, %4258 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %4260 = torch.aten.mul.Tensor %4253, %4259 : !torch.vtensor<[4,1,14336],f16>, !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_5234 = torch.constant.int -2
    %int-1_5235 = torch.constant.int -1
    %4261 = torch.aten.transpose.int %209, %int-2_5234, %int-1_5235 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int4_5236 = torch.constant.int 4
    %int14336_5237 = torch.constant.int 14336
    %4262 = torch.prim.ListConstruct %int4_5236, %int14336_5237 : (!torch.int, !torch.int) -> !torch.list<int>
    %4263 = torch.aten.view %4260, %4262 : !torch.vtensor<[4,1,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,14336],f16>
    %4264 = torch.aten.mm %4263, %4261 : !torch.vtensor<[4,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_5238 = torch.constant.int 4
    %int1_5239 = torch.constant.int 1
    %int4096_5240 = torch.constant.int 4096
    %4265 = torch.prim.ListConstruct %int4_5238, %int1_5239, %int4096_5240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4266 = torch.aten.view %4264, %4265 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_5241 = torch.constant.int 1
    %4267 = torch.aten.add.Tensor %4236, %4266, %int1_5241 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_5242 = torch.constant.int 6
    %4268 = torch.prims.convert_element_type %4267, %int6_5242 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_5243 = torch.constant.int 2
    %4269 = torch.aten.pow.Tensor_Scalar %4268, %int2_5243 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_5244 = torch.constant.int -1
    %4270 = torch.prim.ListConstruct %int-1_5244 : (!torch.int) -> !torch.list<int>
    %true_5245 = torch.constant.bool true
    %none_5246 = torch.constant.none
    %4271 = torch.aten.mean.dim %4269, %4270, %true_5245, %none_5246 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_5247 = torch.constant.float 9.9999997473787516E-6
    %int1_5248 = torch.constant.int 1
    %4272 = torch.aten.add.Scalar %4271, %float9.999990e-06_5247, %int1_5248 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %4273 = torch.aten.rsqrt %4272 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %4274 = torch.aten.mul.Tensor %4268, %4273 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_5249 = torch.constant.int 5
    %4275 = torch.prims.convert_element_type %4274, %int5_5249 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %4276 = torch.aten.mul.Tensor %210, %4275 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_5250 = torch.constant.int 5
    %4277 = torch.prims.convert_element_type %4276, %int5_5250 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_5251 = torch.constant.int -2
    %int-1_5252 = torch.constant.int -1
    %4278 = torch.aten.transpose.int %211, %int-2_5251, %int-1_5252 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_5253 = torch.constant.int 4
    %int4096_5254 = torch.constant.int 4096
    %4279 = torch.prim.ListConstruct %int4_5253, %int4096_5254 : (!torch.int, !torch.int) -> !torch.list<int>
    %4280 = torch.aten.view %4277, %4279 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %4281 = torch.aten.mm %4280, %4278 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_5255 = torch.constant.int 4
    %int1_5256 = torch.constant.int 1
    %int4096_5257 = torch.constant.int 4096
    %4282 = torch.prim.ListConstruct %int4_5255, %int1_5256, %int4096_5257 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4283 = torch.aten.view %4281, %4282 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_5258 = torch.constant.int -2
    %int-1_5259 = torch.constant.int -1
    %4284 = torch.aten.transpose.int %212, %int-2_5258, %int-1_5259 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_5260 = torch.constant.int 4
    %int4096_5261 = torch.constant.int 4096
    %4285 = torch.prim.ListConstruct %int4_5260, %int4096_5261 : (!torch.int, !torch.int) -> !torch.list<int>
    %4286 = torch.aten.view %4277, %4285 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %4287 = torch.aten.mm %4286, %4284 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_5262 = torch.constant.int 4
    %int1_5263 = torch.constant.int 1
    %int1024_5264 = torch.constant.int 1024
    %4288 = torch.prim.ListConstruct %int4_5262, %int1_5263, %int1024_5264 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4289 = torch.aten.view %4287, %4288 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int-2_5265 = torch.constant.int -2
    %int-1_5266 = torch.constant.int -1
    %4290 = torch.aten.transpose.int %213, %int-2_5265, %int-1_5266 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_5267 = torch.constant.int 4
    %int4096_5268 = torch.constant.int 4096
    %4291 = torch.prim.ListConstruct %int4_5267, %int4096_5268 : (!torch.int, !torch.int) -> !torch.list<int>
    %4292 = torch.aten.view %4277, %4291 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %4293 = torch.aten.mm %4292, %4290 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_5269 = torch.constant.int 4
    %int1_5270 = torch.constant.int 1
    %int1024_5271 = torch.constant.int 1024
    %4294 = torch.prim.ListConstruct %int4_5269, %int1_5270, %int1024_5271 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4295 = torch.aten.view %4293, %4294 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int4_5272 = torch.constant.int 4
    %int1_5273 = torch.constant.int 1
    %int32_5274 = torch.constant.int 32
    %int128_5275 = torch.constant.int 128
    %4296 = torch.prim.ListConstruct %int4_5272, %int1_5273, %int32_5274, %int128_5275 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4297 = torch.aten.view %4283, %4296 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,128],f16>
    %int4_5276 = torch.constant.int 4
    %int1_5277 = torch.constant.int 1
    %int8_5278 = torch.constant.int 8
    %int128_5279 = torch.constant.int 128
    %4298 = torch.prim.ListConstruct %int4_5276, %int1_5277, %int8_5278, %int128_5279 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4299 = torch.aten.view %4289, %4298 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int4_5280 = torch.constant.int 4
    %int1_5281 = torch.constant.int 1
    %int8_5282 = torch.constant.int 8
    %int128_5283 = torch.constant.int 128
    %4300 = torch.prim.ListConstruct %int4_5280, %int1_5281, %int8_5282, %int128_5283 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4301 = torch.aten.view %4295, %4300 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int6_5284 = torch.constant.int 6
    %4302 = torch.prims.convert_element_type %4297, %int6_5284 : !torch.vtensor<[4,1,32,128],f16>, !torch.int -> !torch.vtensor<[4,1,32,128],f32>
    %4303 = torch_c.to_builtin_tensor %4302 : !torch.vtensor<[4,1,32,128],f32> -> tensor<4x1x32x128xf32>
    %4304 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %4305 = util.call @sharktank_rotary_embedding_4_1_32_128_f32(%4303, %4304) : (tensor<4x1x32x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x32x128xf32>
    %4306 = torch_c.from_builtin_tensor %4305 : tensor<4x1x32x128xf32> -> !torch.vtensor<[4,1,32,128],f32>
    %int5_5285 = torch.constant.int 5
    %4307 = torch.prims.convert_element_type %4306, %int5_5285 : !torch.vtensor<[4,1,32,128],f32>, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int6_5286 = torch.constant.int 6
    %4308 = torch.prims.convert_element_type %4299, %int6_5286 : !torch.vtensor<[4,1,8,128],f16>, !torch.int -> !torch.vtensor<[4,1,8,128],f32>
    %4309 = torch_c.to_builtin_tensor %4308 : !torch.vtensor<[4,1,8,128],f32> -> tensor<4x1x8x128xf32>
    %4310 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %4311 = util.call @sharktank_rotary_embedding_4_1_8_128_f32(%4309, %4310) : (tensor<4x1x8x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x8x128xf32>
    %4312 = torch_c.from_builtin_tensor %4311 : tensor<4x1x8x128xf32> -> !torch.vtensor<[4,1,8,128],f32>
    %int5_5287 = torch.constant.int 5
    %4313 = torch.prims.convert_element_type %4312, %int5_5287 : !torch.vtensor<[4,1,8,128],f32>, !torch.int -> !torch.vtensor<[4,1,8,128],f16>
    %int32_5288 = torch.constant.int 32
    %4314 = torch.aten.floor_divide.Scalar %arg2, %int32_5288 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_5289 = torch.constant.int 1
    %4315 = torch.aten.unsqueeze %4314, %int1_5289 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5290 = torch.constant.int 1
    %false_5291 = torch.constant.bool false
    %4316 = torch.aten.gather %arg3, %int1_5290, %4315, %false_5291 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_5292 = torch.constant.int 32
    %4317 = torch.aten.remainder.Scalar %arg2, %int32_5292 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_5293 = torch.constant.int 1
    %4318 = torch.aten.unsqueeze %4317, %int1_5293 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_5294 = torch.constant.none
    %4319 = torch.aten.clone %214, %none_5294 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_5295 = torch.constant.int 0
    %4320 = torch.aten.unsqueeze %4319, %int0_5295 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_5296 = torch.constant.int 4
    %int1_5297 = torch.constant.int 1
    %4321 = torch.prim.ListConstruct %int4_5296, %int1_5297 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_5298 = torch.constant.int 1
    %int1_5299 = torch.constant.int 1
    %4322 = torch.prim.ListConstruct %int1_5298, %int1_5299 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_5300 = torch.constant.int 4
    %int0_5301 = torch.constant.int 0
    %cpu_5302 = torch.constant.device "cpu"
    %false_5303 = torch.constant.bool false
    %4323 = torch.aten.empty_strided %4321, %4322, %int4_5300, %int0_5301, %cpu_5302, %false_5303 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int19 = torch.constant.int 19
    %4324 = torch.aten.fill.Scalar %4323, %int19 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_5304 = torch.constant.int 4
    %int1_5305 = torch.constant.int 1
    %4325 = torch.prim.ListConstruct %int4_5304, %int1_5305 : (!torch.int, !torch.int) -> !torch.list<int>
    %4326 = torch.aten.repeat %4320, %4325 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_5306 = torch.constant.int 32
    %4327 = torch.aten.mul.Scalar %4316, %int32_5306 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5307 = torch.constant.int 1
    %4328 = torch.aten.add.Tensor %4327, %4324, %int1_5307 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_5308 = torch.constant.int 2
    %4329 = torch.aten.mul.Scalar %4328, %int2_5308 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5309 = torch.constant.int 1
    %4330 = torch.aten.add.Tensor %4329, %4326, %int1_5309 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_5310 = torch.constant.int 32
    %4331 = torch.aten.mul.Scalar %4330, %int32_5310 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5311 = torch.constant.int 1
    %4332 = torch.aten.add.Tensor %4331, %4318, %int1_5311 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_5312 = torch.constant.int 32
    %int2_5313 = torch.constant.int 2
    %int32_5314 = torch.constant.int 32
    %int8_5315 = torch.constant.int 8
    %int128_5316 = torch.constant.int 128
    %4333 = torch.prim.ListConstruct %437, %int32_5312, %int2_5313, %int32_5314, %int8_5315, %int128_5316 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4334 = torch.aten.view %4170, %4333 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4334, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_5317 = torch.constant.int 32
    %4335 = torch.aten.mul.int %437, %int32_5317 : !torch.int, !torch.int -> !torch.int
    %int2_5318 = torch.constant.int 2
    %4336 = torch.aten.mul.int %4335, %int2_5318 : !torch.int, !torch.int -> !torch.int
    %int32_5319 = torch.constant.int 32
    %4337 = torch.aten.mul.int %4336, %int32_5319 : !torch.int, !torch.int -> !torch.int
    %int8_5320 = torch.constant.int 8
    %int128_5321 = torch.constant.int 128
    %4338 = torch.prim.ListConstruct %4337, %int8_5320, %int128_5321 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4339 = torch.aten.view %4334, %4338 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %4339, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %4340 = torch.prim.ListConstruct %4332 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_5322 = torch.constant.bool false
    %4341 = torch.aten.index_put %4339, %4340, %4313, %false_5322 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %4341, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_5323 = torch.constant.int 32
    %int2_5324 = torch.constant.int 2
    %int32_5325 = torch.constant.int 32
    %int8_5326 = torch.constant.int 8
    %int128_5327 = torch.constant.int 128
    %4342 = torch.prim.ListConstruct %437, %int32_5323, %int2_5324, %int32_5325, %int8_5326, %int128_5327 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4343 = torch.aten.view %4341, %4342 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4343, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_5328 = torch.constant.int 2097152
    %4344 = torch.prim.ListConstruct %437, %int2097152_5328 : (!torch.int, !torch.int) -> !torch.list<int>
    %4345 = torch.aten.view %4343, %4344 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %4345, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_5329 = torch.constant.int 32
    %int2_5330 = torch.constant.int 2
    %int32_5331 = torch.constant.int 32
    %int8_5332 = torch.constant.int 8
    %int128_5333 = torch.constant.int 128
    %4346 = torch.prim.ListConstruct %437, %int32_5329, %int2_5330, %int32_5331, %int8_5332, %int128_5333 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4347 = torch.aten.view %4345, %4346 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4347, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int8_5334 = torch.constant.int 8
    %int128_5335 = torch.constant.int 128
    %4348 = torch.prim.ListConstruct %4337, %int8_5334, %int128_5335 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4349 = torch.aten.view %4347, %4348 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %4349, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_5336 = torch.constant.int 32
    %4350 = torch.aten.floor_divide.Scalar %arg2, %int32_5336 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_5337 = torch.constant.int 1
    %4351 = torch.aten.unsqueeze %4350, %int1_5337 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5338 = torch.constant.int 1
    %false_5339 = torch.constant.bool false
    %4352 = torch.aten.gather %arg3, %int1_5338, %4351, %false_5339 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_5340 = torch.constant.int 32
    %4353 = torch.aten.remainder.Scalar %arg2, %int32_5340 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_5341 = torch.constant.int 1
    %4354 = torch.aten.unsqueeze %4353, %int1_5341 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_5342 = torch.constant.none
    %4355 = torch.aten.clone %215, %none_5342 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_5343 = torch.constant.int 0
    %4356 = torch.aten.unsqueeze %4355, %int0_5343 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_5344 = torch.constant.int 4
    %int1_5345 = torch.constant.int 1
    %4357 = torch.prim.ListConstruct %int4_5344, %int1_5345 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_5346 = torch.constant.int 1
    %int1_5347 = torch.constant.int 1
    %4358 = torch.prim.ListConstruct %int1_5346, %int1_5347 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_5348 = torch.constant.int 4
    %int0_5349 = torch.constant.int 0
    %cpu_5350 = torch.constant.device "cpu"
    %false_5351 = torch.constant.bool false
    %4359 = torch.aten.empty_strided %4357, %4358, %int4_5348, %int0_5349, %cpu_5350, %false_5351 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int19_5352 = torch.constant.int 19
    %4360 = torch.aten.fill.Scalar %4359, %int19_5352 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_5353 = torch.constant.int 4
    %int1_5354 = torch.constant.int 1
    %4361 = torch.prim.ListConstruct %int4_5353, %int1_5354 : (!torch.int, !torch.int) -> !torch.list<int>
    %4362 = torch.aten.repeat %4356, %4361 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_5355 = torch.constant.int 32
    %4363 = torch.aten.mul.Scalar %4352, %int32_5355 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5356 = torch.constant.int 1
    %4364 = torch.aten.add.Tensor %4363, %4360, %int1_5356 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_5357 = torch.constant.int 2
    %4365 = torch.aten.mul.Scalar %4364, %int2_5357 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5358 = torch.constant.int 1
    %4366 = torch.aten.add.Tensor %4365, %4362, %int1_5358 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_5359 = torch.constant.int 32
    %4367 = torch.aten.mul.Scalar %4366, %int32_5359 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5360 = torch.constant.int 1
    %4368 = torch.aten.add.Tensor %4367, %4354, %int1_5360 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %4369 = torch.prim.ListConstruct %4368 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_5361 = torch.constant.bool false
    %4370 = torch.aten.index_put %4349, %4369, %4301, %false_5361 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %4370, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_5362 = torch.constant.int 32
    %int2_5363 = torch.constant.int 2
    %int32_5364 = torch.constant.int 32
    %int8_5365 = torch.constant.int 8
    %int128_5366 = torch.constant.int 128
    %4371 = torch.prim.ListConstruct %437, %int32_5362, %int2_5363, %int32_5364, %int8_5365, %int128_5366 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4372 = torch.aten.view %4370, %4371 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4372, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_5367 = torch.constant.int 2097152
    %4373 = torch.prim.ListConstruct %437, %int2097152_5367 : (!torch.int, !torch.int) -> !torch.list<int>
    %4374 = torch.aten.view %4372, %4373 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %4374, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int4_5368 = torch.constant.int 4
    %4375 = torch.prim.ListConstruct %int4_5368, %358 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_5369 = torch.constant.int 1
    %4376 = torch.prim.ListConstruct %358, %int1_5369 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_5370 = torch.constant.int 4
    %int0_5371 = torch.constant.int 0
    %cpu_5372 = torch.constant.device "cpu"
    %false_5373 = torch.constant.bool false
    %4377 = torch.aten.empty_strided %4375, %4376, %int4_5370, %int0_5371, %cpu_5372, %false_5373 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4377, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int19_5374 = torch.constant.int 19
    %4378 = torch.aten.fill.Scalar %4377, %int19_5374 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4378, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int32_5375 = torch.constant.int 32
    %4379 = torch.aten.mul.Scalar %arg3, %int32_5375 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4379, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_5376 = torch.constant.int 1
    %4380 = torch.aten.add.Tensor %4379, %4378, %int1_5376 : !torch.vtensor<[4,?],si64>, !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4380, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_5377 = torch.constant.int 4
    %4381 = torch.aten.mul.int %int4_5377, %358 : !torch.int, !torch.int -> !torch.int
    %4382 = torch.prim.ListConstruct %4381 : (!torch.int) -> !torch.list<int>
    %4383 = torch.aten.view %4380, %4382 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4383, [%356], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_5378 = torch.constant.int 32
    %int2_5379 = torch.constant.int 2
    %int32_5380 = torch.constant.int 32
    %int8_5381 = torch.constant.int 8
    %int128_5382 = torch.constant.int 128
    %4384 = torch.prim.ListConstruct %437, %int32_5378, %int2_5379, %int32_5380, %int8_5381, %int128_5382 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4385 = torch.aten.view %4374, %4384 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4385, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_5383 = torch.constant.int 32
    %4386 = torch.aten.mul.int %437, %int32_5383 : !torch.int, !torch.int -> !torch.int
    %int2_5384 = torch.constant.int 2
    %int32_5385 = torch.constant.int 32
    %int8_5386 = torch.constant.int 8
    %int128_5387 = torch.constant.int 128
    %4387 = torch.prim.ListConstruct %4386, %int2_5384, %int32_5385, %int8_5386, %int128_5387 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4388 = torch.aten.view %4385, %4387 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %4388, [%357], affine_map<()[s0] -> (s0 * 32, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int0_5388 = torch.constant.int 0
    %4389 = torch.aten.index_select %4388, %int0_5388, %4383 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.int, !torch.vtensor<[?],si64> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %4389, [%356], affine_map<()[s0] -> (s0 * 4, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int4_5389 = torch.constant.int 4
    %int2_5390 = torch.constant.int 2
    %int32_5391 = torch.constant.int 32
    %int8_5392 = torch.constant.int 8
    %int128_5393 = torch.constant.int 128
    %4390 = torch.prim.ListConstruct %int4_5389, %358, %int2_5390, %int32_5391, %int8_5392, %int128_5393 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4391 = torch.aten.view %4389, %4390 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %4391, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int0_5394 = torch.constant.int 0
    %int0_5395 = torch.constant.int 0
    %int9223372036854775807_5396 = torch.constant.int 9223372036854775807
    %int1_5397 = torch.constant.int 1
    %4392 = torch.aten.slice.Tensor %4391, %int0_5394, %int0_5395, %int9223372036854775807_5396, %int1_5397 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %4392, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_5398 = torch.constant.int 1
    %int0_5399 = torch.constant.int 0
    %int9223372036854775807_5400 = torch.constant.int 9223372036854775807
    %int1_5401 = torch.constant.int 1
    %4393 = torch.aten.slice.Tensor %4392, %int1_5398, %int0_5399, %int9223372036854775807_5400, %int1_5401 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %4393, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_5402 = torch.constant.int 2
    %int0_5403 = torch.constant.int 0
    %4394 = torch.aten.select.int %4393, %int2_5402, %int0_5403 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %4394, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int32_5404 = torch.constant.int 32
    %4395 = torch.aten.mul.int %358, %int32_5404 : !torch.int, !torch.int -> !torch.int
    %int2_5405 = torch.constant.int 2
    %int0_5406 = torch.constant.int 0
    %int1_5407 = torch.constant.int 1
    %4396 = torch.aten.slice.Tensor %4394, %int2_5405, %int0_5406, %4395, %int1_5407 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %4396, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_5408 = torch.constant.int 0
    %4397 = torch.aten.clone %4396, %int0_5408 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %4397, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_5409 = torch.constant.int 1
    %4398 = torch.aten.size.int %4393, %int1_5409 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_5410 = torch.constant.int 32
    %4399 = torch.aten.mul.int %4398, %int32_5410 : !torch.int, !torch.int -> !torch.int
    %int4_5411 = torch.constant.int 4
    %int8_5412 = torch.constant.int 8
    %int128_5413 = torch.constant.int 128
    %4400 = torch.prim.ListConstruct %int4_5411, %4399, %int8_5412, %int128_5413 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4401 = torch.aten._unsafe_view %4397, %4400 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %4401, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_5414 = torch.constant.int 0
    %int0_5415 = torch.constant.int 0
    %int9223372036854775807_5416 = torch.constant.int 9223372036854775807
    %int1_5417 = torch.constant.int 1
    %4402 = torch.aten.slice.Tensor %4401, %int0_5414, %int0_5415, %int9223372036854775807_5416, %int1_5417 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %4402, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_5418 = torch.constant.int 0
    %int0_5419 = torch.constant.int 0
    %int9223372036854775807_5420 = torch.constant.int 9223372036854775807
    %int1_5421 = torch.constant.int 1
    %4403 = torch.aten.slice.Tensor %4391, %int0_5418, %int0_5419, %int9223372036854775807_5420, %int1_5421 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %4403, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_5422 = torch.constant.int 1
    %int0_5423 = torch.constant.int 0
    %int9223372036854775807_5424 = torch.constant.int 9223372036854775807
    %int1_5425 = torch.constant.int 1
    %4404 = torch.aten.slice.Tensor %4403, %int1_5422, %int0_5423, %int9223372036854775807_5424, %int1_5425 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %4404, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_5426 = torch.constant.int 2
    %int1_5427 = torch.constant.int 1
    %4405 = torch.aten.select.int %4404, %int2_5426, %int1_5427 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %4405, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int2_5428 = torch.constant.int 2
    %int0_5429 = torch.constant.int 0
    %int1_5430 = torch.constant.int 1
    %4406 = torch.aten.slice.Tensor %4405, %int2_5428, %int0_5429, %4395, %int1_5430 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %4406, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_5431 = torch.constant.int 0
    %4407 = torch.aten.clone %4406, %int0_5431 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %4407, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_5432 = torch.constant.int 1
    %4408 = torch.aten.size.int %4404, %int1_5432 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_5433 = torch.constant.int 32
    %4409 = torch.aten.mul.int %4408, %int32_5433 : !torch.int, !torch.int -> !torch.int
    %int4_5434 = torch.constant.int 4
    %int8_5435 = torch.constant.int 8
    %int128_5436 = torch.constant.int 128
    %4410 = torch.prim.ListConstruct %int4_5434, %4409, %int8_5435, %int128_5436 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4411 = torch.aten._unsafe_view %4407, %4410 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %4411, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_5437 = torch.constant.int 0
    %int0_5438 = torch.constant.int 0
    %int9223372036854775807_5439 = torch.constant.int 9223372036854775807
    %int1_5440 = torch.constant.int 1
    %4412 = torch.aten.slice.Tensor %4411, %int0_5437, %int0_5438, %int9223372036854775807_5439, %int1_5440 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %4412, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int-2_5441 = torch.constant.int -2
    %4413 = torch.aten.unsqueeze %4402, %int-2_5441 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %4413, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_5442 = torch.constant.int 1
    %4414 = torch.aten.size.int %4401, %int1_5442 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_5443 = torch.constant.int 4
    %int8_5444 = torch.constant.int 8
    %int4_5445 = torch.constant.int 4
    %int128_5446 = torch.constant.int 128
    %4415 = torch.prim.ListConstruct %int4_5443, %4414, %int8_5444, %int4_5445, %int128_5446 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_5447 = torch.constant.bool false
    %4416 = torch.aten.expand %4413, %4415, %false_5447 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4416, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_5448 = torch.constant.int 0
    %4417 = torch.aten.clone %4416, %int0_5448 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4417, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_5449 = torch.constant.int 4
    %int32_5450 = torch.constant.int 32
    %int128_5451 = torch.constant.int 128
    %4418 = torch.prim.ListConstruct %int4_5449, %4414, %int32_5450, %int128_5451 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4419 = torch.aten._unsafe_view %4417, %4418 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %4419, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_5452 = torch.constant.int -2
    %4420 = torch.aten.unsqueeze %4412, %int-2_5452 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %4420, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_5453 = torch.constant.int 1
    %4421 = torch.aten.size.int %4411, %int1_5453 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_5454 = torch.constant.int 4
    %int8_5455 = torch.constant.int 8
    %int4_5456 = torch.constant.int 4
    %int128_5457 = torch.constant.int 128
    %4422 = torch.prim.ListConstruct %int4_5454, %4421, %int8_5455, %int4_5456, %int128_5457 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_5458 = torch.constant.bool false
    %4423 = torch.aten.expand %4420, %4422, %false_5458 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4423, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_5459 = torch.constant.int 0
    %4424 = torch.aten.clone %4423, %int0_5459 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4424, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_5460 = torch.constant.int 4
    %int32_5461 = torch.constant.int 32
    %int128_5462 = torch.constant.int 128
    %4425 = torch.prim.ListConstruct %int4_5460, %4421, %int32_5461, %int128_5462 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4426 = torch.aten._unsafe_view %4424, %4425 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %4426, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_5463 = torch.constant.int 1
    %int2_5464 = torch.constant.int 2
    %4427 = torch.aten.transpose.int %4307, %int1_5463, %int2_5464 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,128],f16>
    %int1_5465 = torch.constant.int 1
    %int2_5466 = torch.constant.int 2
    %4428 = torch.aten.transpose.int %4419, %int1_5465, %int2_5466 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %4428, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_5467 = torch.constant.int 1
    %int2_5468 = torch.constant.int 2
    %4429 = torch.aten.transpose.int %4426, %int1_5467, %int2_5468 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %4429, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_5469 = torch.constant.float 0.000000e+00
    %false_5470 = torch.constant.bool false
    %none_5471 = torch.constant.none
    %4430:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4427, %4428, %4429, %float0.000000e00_5469, %false_5470, %368, %none_5471) : (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.vtensor<[4,1,1,?],f16>, !torch.none) -> (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,1],f32>) 
    %int1_5472 = torch.constant.int 1
    %int2_5473 = torch.constant.int 2
    %4431 = torch.aten.transpose.int %4430#0, %int1_5472, %int2_5473 : !torch.vtensor<[4,32,1,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int4_5474 = torch.constant.int 4
    %int1_5475 = torch.constant.int 1
    %int4096_5476 = torch.constant.int 4096
    %4432 = torch.prim.ListConstruct %int4_5474, %int1_5475, %int4096_5476 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4433 = torch.aten.view %4431, %4432 : !torch.vtensor<[4,1,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_5477 = torch.constant.int -2
    %int-1_5478 = torch.constant.int -1
    %4434 = torch.aten.transpose.int %216, %int-2_5477, %int-1_5478 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_5479 = torch.constant.int 4
    %int4096_5480 = torch.constant.int 4096
    %4435 = torch.prim.ListConstruct %int4_5479, %int4096_5480 : (!torch.int, !torch.int) -> !torch.list<int>
    %4436 = torch.aten.view %4433, %4435 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %4437 = torch.aten.mm %4436, %4434 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_5481 = torch.constant.int 4
    %int1_5482 = torch.constant.int 1
    %int4096_5483 = torch.constant.int 4096
    %4438 = torch.prim.ListConstruct %int4_5481, %int1_5482, %int4096_5483 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4439 = torch.aten.view %4437, %4438 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_5484 = torch.constant.int 1
    %4440 = torch.aten.add.Tensor %4267, %4439, %int1_5484 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_5485 = torch.constant.int 6
    %4441 = torch.prims.convert_element_type %4440, %int6_5485 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_5486 = torch.constant.int 2
    %4442 = torch.aten.pow.Tensor_Scalar %4441, %int2_5486 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_5487 = torch.constant.int -1
    %4443 = torch.prim.ListConstruct %int-1_5487 : (!torch.int) -> !torch.list<int>
    %true_5488 = torch.constant.bool true
    %none_5489 = torch.constant.none
    %4444 = torch.aten.mean.dim %4442, %4443, %true_5488, %none_5489 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_5490 = torch.constant.float 9.9999997473787516E-6
    %int1_5491 = torch.constant.int 1
    %4445 = torch.aten.add.Scalar %4444, %float9.999990e-06_5490, %int1_5491 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %4446 = torch.aten.rsqrt %4445 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %4447 = torch.aten.mul.Tensor %4441, %4446 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_5492 = torch.constant.int 5
    %4448 = torch.prims.convert_element_type %4447, %int5_5492 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %4449 = torch.aten.mul.Tensor %217, %4448 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_5493 = torch.constant.int 5
    %4450 = torch.prims.convert_element_type %4449, %int5_5493 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_5494 = torch.constant.int -2
    %int-1_5495 = torch.constant.int -1
    %4451 = torch.aten.transpose.int %218, %int-2_5494, %int-1_5495 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_5496 = torch.constant.int 4
    %int4096_5497 = torch.constant.int 4096
    %4452 = torch.prim.ListConstruct %int4_5496, %int4096_5497 : (!torch.int, !torch.int) -> !torch.list<int>
    %4453 = torch.aten.view %4450, %4452 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %4454 = torch.aten.mm %4453, %4451 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_5498 = torch.constant.int 4
    %int1_5499 = torch.constant.int 1
    %int14336_5500 = torch.constant.int 14336
    %4455 = torch.prim.ListConstruct %int4_5498, %int1_5499, %int14336_5500 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4456 = torch.aten.view %4454, %4455 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %4457 = torch.aten.silu %4456 : !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_5501 = torch.constant.int -2
    %int-1_5502 = torch.constant.int -1
    %4458 = torch.aten.transpose.int %219, %int-2_5501, %int-1_5502 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_5503 = torch.constant.int 4
    %int4096_5504 = torch.constant.int 4096
    %4459 = torch.prim.ListConstruct %int4_5503, %int4096_5504 : (!torch.int, !torch.int) -> !torch.list<int>
    %4460 = torch.aten.view %4450, %4459 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %4461 = torch.aten.mm %4460, %4458 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_5505 = torch.constant.int 4
    %int1_5506 = torch.constant.int 1
    %int14336_5507 = torch.constant.int 14336
    %4462 = torch.prim.ListConstruct %int4_5505, %int1_5506, %int14336_5507 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4463 = torch.aten.view %4461, %4462 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %4464 = torch.aten.mul.Tensor %4457, %4463 : !torch.vtensor<[4,1,14336],f16>, !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_5508 = torch.constant.int -2
    %int-1_5509 = torch.constant.int -1
    %4465 = torch.aten.transpose.int %220, %int-2_5508, %int-1_5509 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int4_5510 = torch.constant.int 4
    %int14336_5511 = torch.constant.int 14336
    %4466 = torch.prim.ListConstruct %int4_5510, %int14336_5511 : (!torch.int, !torch.int) -> !torch.list<int>
    %4467 = torch.aten.view %4464, %4466 : !torch.vtensor<[4,1,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,14336],f16>
    %4468 = torch.aten.mm %4467, %4465 : !torch.vtensor<[4,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_5512 = torch.constant.int 4
    %int1_5513 = torch.constant.int 1
    %int4096_5514 = torch.constant.int 4096
    %4469 = torch.prim.ListConstruct %int4_5512, %int1_5513, %int4096_5514 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4470 = torch.aten.view %4468, %4469 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_5515 = torch.constant.int 1
    %4471 = torch.aten.add.Tensor %4440, %4470, %int1_5515 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_5516 = torch.constant.int 6
    %4472 = torch.prims.convert_element_type %4471, %int6_5516 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_5517 = torch.constant.int 2
    %4473 = torch.aten.pow.Tensor_Scalar %4472, %int2_5517 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_5518 = torch.constant.int -1
    %4474 = torch.prim.ListConstruct %int-1_5518 : (!torch.int) -> !torch.list<int>
    %true_5519 = torch.constant.bool true
    %none_5520 = torch.constant.none
    %4475 = torch.aten.mean.dim %4473, %4474, %true_5519, %none_5520 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_5521 = torch.constant.float 9.9999997473787516E-6
    %int1_5522 = torch.constant.int 1
    %4476 = torch.aten.add.Scalar %4475, %float9.999990e-06_5521, %int1_5522 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %4477 = torch.aten.rsqrt %4476 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %4478 = torch.aten.mul.Tensor %4472, %4477 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_5523 = torch.constant.int 5
    %4479 = torch.prims.convert_element_type %4478, %int5_5523 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %4480 = torch.aten.mul.Tensor %221, %4479 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_5524 = torch.constant.int 5
    %4481 = torch.prims.convert_element_type %4480, %int5_5524 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_5525 = torch.constant.int -2
    %int-1_5526 = torch.constant.int -1
    %4482 = torch.aten.transpose.int %222, %int-2_5525, %int-1_5526 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_5527 = torch.constant.int 4
    %int4096_5528 = torch.constant.int 4096
    %4483 = torch.prim.ListConstruct %int4_5527, %int4096_5528 : (!torch.int, !torch.int) -> !torch.list<int>
    %4484 = torch.aten.view %4481, %4483 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %4485 = torch.aten.mm %4484, %4482 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_5529 = torch.constant.int 4
    %int1_5530 = torch.constant.int 1
    %int4096_5531 = torch.constant.int 4096
    %4486 = torch.prim.ListConstruct %int4_5529, %int1_5530, %int4096_5531 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4487 = torch.aten.view %4485, %4486 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_5532 = torch.constant.int -2
    %int-1_5533 = torch.constant.int -1
    %4488 = torch.aten.transpose.int %223, %int-2_5532, %int-1_5533 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_5534 = torch.constant.int 4
    %int4096_5535 = torch.constant.int 4096
    %4489 = torch.prim.ListConstruct %int4_5534, %int4096_5535 : (!torch.int, !torch.int) -> !torch.list<int>
    %4490 = torch.aten.view %4481, %4489 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %4491 = torch.aten.mm %4490, %4488 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_5536 = torch.constant.int 4
    %int1_5537 = torch.constant.int 1
    %int1024_5538 = torch.constant.int 1024
    %4492 = torch.prim.ListConstruct %int4_5536, %int1_5537, %int1024_5538 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4493 = torch.aten.view %4491, %4492 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int-2_5539 = torch.constant.int -2
    %int-1_5540 = torch.constant.int -1
    %4494 = torch.aten.transpose.int %224, %int-2_5539, %int-1_5540 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_5541 = torch.constant.int 4
    %int4096_5542 = torch.constant.int 4096
    %4495 = torch.prim.ListConstruct %int4_5541, %int4096_5542 : (!torch.int, !torch.int) -> !torch.list<int>
    %4496 = torch.aten.view %4481, %4495 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %4497 = torch.aten.mm %4496, %4494 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_5543 = torch.constant.int 4
    %int1_5544 = torch.constant.int 1
    %int1024_5545 = torch.constant.int 1024
    %4498 = torch.prim.ListConstruct %int4_5543, %int1_5544, %int1024_5545 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4499 = torch.aten.view %4497, %4498 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int4_5546 = torch.constant.int 4
    %int1_5547 = torch.constant.int 1
    %int32_5548 = torch.constant.int 32
    %int128_5549 = torch.constant.int 128
    %4500 = torch.prim.ListConstruct %int4_5546, %int1_5547, %int32_5548, %int128_5549 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4501 = torch.aten.view %4487, %4500 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,128],f16>
    %int4_5550 = torch.constant.int 4
    %int1_5551 = torch.constant.int 1
    %int8_5552 = torch.constant.int 8
    %int128_5553 = torch.constant.int 128
    %4502 = torch.prim.ListConstruct %int4_5550, %int1_5551, %int8_5552, %int128_5553 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4503 = torch.aten.view %4493, %4502 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int4_5554 = torch.constant.int 4
    %int1_5555 = torch.constant.int 1
    %int8_5556 = torch.constant.int 8
    %int128_5557 = torch.constant.int 128
    %4504 = torch.prim.ListConstruct %int4_5554, %int1_5555, %int8_5556, %int128_5557 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4505 = torch.aten.view %4499, %4504 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int6_5558 = torch.constant.int 6
    %4506 = torch.prims.convert_element_type %4501, %int6_5558 : !torch.vtensor<[4,1,32,128],f16>, !torch.int -> !torch.vtensor<[4,1,32,128],f32>
    %4507 = torch_c.to_builtin_tensor %4506 : !torch.vtensor<[4,1,32,128],f32> -> tensor<4x1x32x128xf32>
    %4508 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %4509 = util.call @sharktank_rotary_embedding_4_1_32_128_f32(%4507, %4508) : (tensor<4x1x32x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x32x128xf32>
    %4510 = torch_c.from_builtin_tensor %4509 : tensor<4x1x32x128xf32> -> !torch.vtensor<[4,1,32,128],f32>
    %int5_5559 = torch.constant.int 5
    %4511 = torch.prims.convert_element_type %4510, %int5_5559 : !torch.vtensor<[4,1,32,128],f32>, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int6_5560 = torch.constant.int 6
    %4512 = torch.prims.convert_element_type %4503, %int6_5560 : !torch.vtensor<[4,1,8,128],f16>, !torch.int -> !torch.vtensor<[4,1,8,128],f32>
    %4513 = torch_c.to_builtin_tensor %4512 : !torch.vtensor<[4,1,8,128],f32> -> tensor<4x1x8x128xf32>
    %4514 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %4515 = util.call @sharktank_rotary_embedding_4_1_8_128_f32(%4513, %4514) : (tensor<4x1x8x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x8x128xf32>
    %4516 = torch_c.from_builtin_tensor %4515 : tensor<4x1x8x128xf32> -> !torch.vtensor<[4,1,8,128],f32>
    %int5_5561 = torch.constant.int 5
    %4517 = torch.prims.convert_element_type %4516, %int5_5561 : !torch.vtensor<[4,1,8,128],f32>, !torch.int -> !torch.vtensor<[4,1,8,128],f16>
    %int32_5562 = torch.constant.int 32
    %4518 = torch.aten.floor_divide.Scalar %arg2, %int32_5562 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_5563 = torch.constant.int 1
    %4519 = torch.aten.unsqueeze %4518, %int1_5563 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5564 = torch.constant.int 1
    %false_5565 = torch.constant.bool false
    %4520 = torch.aten.gather %arg3, %int1_5564, %4519, %false_5565 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_5566 = torch.constant.int 32
    %4521 = torch.aten.remainder.Scalar %arg2, %int32_5566 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_5567 = torch.constant.int 1
    %4522 = torch.aten.unsqueeze %4521, %int1_5567 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_5568 = torch.constant.none
    %4523 = torch.aten.clone %225, %none_5568 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_5569 = torch.constant.int 0
    %4524 = torch.aten.unsqueeze %4523, %int0_5569 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_5570 = torch.constant.int 4
    %int1_5571 = torch.constant.int 1
    %4525 = torch.prim.ListConstruct %int4_5570, %int1_5571 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_5572 = torch.constant.int 1
    %int1_5573 = torch.constant.int 1
    %4526 = torch.prim.ListConstruct %int1_5572, %int1_5573 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_5574 = torch.constant.int 4
    %int0_5575 = torch.constant.int 0
    %cpu_5576 = torch.constant.device "cpu"
    %false_5577 = torch.constant.bool false
    %4527 = torch.aten.empty_strided %4525, %4526, %int4_5574, %int0_5575, %cpu_5576, %false_5577 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int20 = torch.constant.int 20
    %4528 = torch.aten.fill.Scalar %4527, %int20 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_5578 = torch.constant.int 4
    %int1_5579 = torch.constant.int 1
    %4529 = torch.prim.ListConstruct %int4_5578, %int1_5579 : (!torch.int, !torch.int) -> !torch.list<int>
    %4530 = torch.aten.repeat %4524, %4529 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_5580 = torch.constant.int 32
    %4531 = torch.aten.mul.Scalar %4520, %int32_5580 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5581 = torch.constant.int 1
    %4532 = torch.aten.add.Tensor %4531, %4528, %int1_5581 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_5582 = torch.constant.int 2
    %4533 = torch.aten.mul.Scalar %4532, %int2_5582 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5583 = torch.constant.int 1
    %4534 = torch.aten.add.Tensor %4533, %4530, %int1_5583 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_5584 = torch.constant.int 32
    %4535 = torch.aten.mul.Scalar %4534, %int32_5584 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5585 = torch.constant.int 1
    %4536 = torch.aten.add.Tensor %4535, %4522, %int1_5585 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_5586 = torch.constant.int 32
    %int2_5587 = torch.constant.int 2
    %int32_5588 = torch.constant.int 32
    %int8_5589 = torch.constant.int 8
    %int128_5590 = torch.constant.int 128
    %4537 = torch.prim.ListConstruct %437, %int32_5586, %int2_5587, %int32_5588, %int8_5589, %int128_5590 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4538 = torch.aten.view %4374, %4537 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4538, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_5591 = torch.constant.int 32
    %4539 = torch.aten.mul.int %437, %int32_5591 : !torch.int, !torch.int -> !torch.int
    %int2_5592 = torch.constant.int 2
    %4540 = torch.aten.mul.int %4539, %int2_5592 : !torch.int, !torch.int -> !torch.int
    %int32_5593 = torch.constant.int 32
    %4541 = torch.aten.mul.int %4540, %int32_5593 : !torch.int, !torch.int -> !torch.int
    %int8_5594 = torch.constant.int 8
    %int128_5595 = torch.constant.int 128
    %4542 = torch.prim.ListConstruct %4541, %int8_5594, %int128_5595 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4543 = torch.aten.view %4538, %4542 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %4543, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %4544 = torch.prim.ListConstruct %4536 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_5596 = torch.constant.bool false
    %4545 = torch.aten.index_put %4543, %4544, %4517, %false_5596 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %4545, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_5597 = torch.constant.int 32
    %int2_5598 = torch.constant.int 2
    %int32_5599 = torch.constant.int 32
    %int8_5600 = torch.constant.int 8
    %int128_5601 = torch.constant.int 128
    %4546 = torch.prim.ListConstruct %437, %int32_5597, %int2_5598, %int32_5599, %int8_5600, %int128_5601 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4547 = torch.aten.view %4545, %4546 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4547, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_5602 = torch.constant.int 2097152
    %4548 = torch.prim.ListConstruct %437, %int2097152_5602 : (!torch.int, !torch.int) -> !torch.list<int>
    %4549 = torch.aten.view %4547, %4548 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %4549, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_5603 = torch.constant.int 32
    %int2_5604 = torch.constant.int 2
    %int32_5605 = torch.constant.int 32
    %int8_5606 = torch.constant.int 8
    %int128_5607 = torch.constant.int 128
    %4550 = torch.prim.ListConstruct %437, %int32_5603, %int2_5604, %int32_5605, %int8_5606, %int128_5607 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4551 = torch.aten.view %4549, %4550 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4551, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int8_5608 = torch.constant.int 8
    %int128_5609 = torch.constant.int 128
    %4552 = torch.prim.ListConstruct %4541, %int8_5608, %int128_5609 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4553 = torch.aten.view %4551, %4552 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %4553, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_5610 = torch.constant.int 32
    %4554 = torch.aten.floor_divide.Scalar %arg2, %int32_5610 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_5611 = torch.constant.int 1
    %4555 = torch.aten.unsqueeze %4554, %int1_5611 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5612 = torch.constant.int 1
    %false_5613 = torch.constant.bool false
    %4556 = torch.aten.gather %arg3, %int1_5612, %4555, %false_5613 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_5614 = torch.constant.int 32
    %4557 = torch.aten.remainder.Scalar %arg2, %int32_5614 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_5615 = torch.constant.int 1
    %4558 = torch.aten.unsqueeze %4557, %int1_5615 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_5616 = torch.constant.none
    %4559 = torch.aten.clone %226, %none_5616 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_5617 = torch.constant.int 0
    %4560 = torch.aten.unsqueeze %4559, %int0_5617 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_5618 = torch.constant.int 4
    %int1_5619 = torch.constant.int 1
    %4561 = torch.prim.ListConstruct %int4_5618, %int1_5619 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_5620 = torch.constant.int 1
    %int1_5621 = torch.constant.int 1
    %4562 = torch.prim.ListConstruct %int1_5620, %int1_5621 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_5622 = torch.constant.int 4
    %int0_5623 = torch.constant.int 0
    %cpu_5624 = torch.constant.device "cpu"
    %false_5625 = torch.constant.bool false
    %4563 = torch.aten.empty_strided %4561, %4562, %int4_5622, %int0_5623, %cpu_5624, %false_5625 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int20_5626 = torch.constant.int 20
    %4564 = torch.aten.fill.Scalar %4563, %int20_5626 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_5627 = torch.constant.int 4
    %int1_5628 = torch.constant.int 1
    %4565 = torch.prim.ListConstruct %int4_5627, %int1_5628 : (!torch.int, !torch.int) -> !torch.list<int>
    %4566 = torch.aten.repeat %4560, %4565 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_5629 = torch.constant.int 32
    %4567 = torch.aten.mul.Scalar %4556, %int32_5629 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5630 = torch.constant.int 1
    %4568 = torch.aten.add.Tensor %4567, %4564, %int1_5630 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_5631 = torch.constant.int 2
    %4569 = torch.aten.mul.Scalar %4568, %int2_5631 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5632 = torch.constant.int 1
    %4570 = torch.aten.add.Tensor %4569, %4566, %int1_5632 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_5633 = torch.constant.int 32
    %4571 = torch.aten.mul.Scalar %4570, %int32_5633 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5634 = torch.constant.int 1
    %4572 = torch.aten.add.Tensor %4571, %4558, %int1_5634 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %4573 = torch.prim.ListConstruct %4572 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_5635 = torch.constant.bool false
    %4574 = torch.aten.index_put %4553, %4573, %4505, %false_5635 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %4574, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_5636 = torch.constant.int 32
    %int2_5637 = torch.constant.int 2
    %int32_5638 = torch.constant.int 32
    %int8_5639 = torch.constant.int 8
    %int128_5640 = torch.constant.int 128
    %4575 = torch.prim.ListConstruct %437, %int32_5636, %int2_5637, %int32_5638, %int8_5639, %int128_5640 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4576 = torch.aten.view %4574, %4575 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4576, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_5641 = torch.constant.int 2097152
    %4577 = torch.prim.ListConstruct %437, %int2097152_5641 : (!torch.int, !torch.int) -> !torch.list<int>
    %4578 = torch.aten.view %4576, %4577 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %4578, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int4_5642 = torch.constant.int 4
    %4579 = torch.prim.ListConstruct %int4_5642, %358 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_5643 = torch.constant.int 1
    %4580 = torch.prim.ListConstruct %358, %int1_5643 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_5644 = torch.constant.int 4
    %int0_5645 = torch.constant.int 0
    %cpu_5646 = torch.constant.device "cpu"
    %false_5647 = torch.constant.bool false
    %4581 = torch.aten.empty_strided %4579, %4580, %int4_5644, %int0_5645, %cpu_5646, %false_5647 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4581, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int20_5648 = torch.constant.int 20
    %4582 = torch.aten.fill.Scalar %4581, %int20_5648 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4582, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int32_5649 = torch.constant.int 32
    %4583 = torch.aten.mul.Scalar %arg3, %int32_5649 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4583, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_5650 = torch.constant.int 1
    %4584 = torch.aten.add.Tensor %4583, %4582, %int1_5650 : !torch.vtensor<[4,?],si64>, !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4584, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_5651 = torch.constant.int 4
    %4585 = torch.aten.mul.int %int4_5651, %358 : !torch.int, !torch.int -> !torch.int
    %4586 = torch.prim.ListConstruct %4585 : (!torch.int) -> !torch.list<int>
    %4587 = torch.aten.view %4584, %4586 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4587, [%356], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_5652 = torch.constant.int 32
    %int2_5653 = torch.constant.int 2
    %int32_5654 = torch.constant.int 32
    %int8_5655 = torch.constant.int 8
    %int128_5656 = torch.constant.int 128
    %4588 = torch.prim.ListConstruct %437, %int32_5652, %int2_5653, %int32_5654, %int8_5655, %int128_5656 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4589 = torch.aten.view %4578, %4588 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4589, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_5657 = torch.constant.int 32
    %4590 = torch.aten.mul.int %437, %int32_5657 : !torch.int, !torch.int -> !torch.int
    %int2_5658 = torch.constant.int 2
    %int32_5659 = torch.constant.int 32
    %int8_5660 = torch.constant.int 8
    %int128_5661 = torch.constant.int 128
    %4591 = torch.prim.ListConstruct %4590, %int2_5658, %int32_5659, %int8_5660, %int128_5661 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4592 = torch.aten.view %4589, %4591 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %4592, [%357], affine_map<()[s0] -> (s0 * 32, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int0_5662 = torch.constant.int 0
    %4593 = torch.aten.index_select %4592, %int0_5662, %4587 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.int, !torch.vtensor<[?],si64> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %4593, [%356], affine_map<()[s0] -> (s0 * 4, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int4_5663 = torch.constant.int 4
    %int2_5664 = torch.constant.int 2
    %int32_5665 = torch.constant.int 32
    %int8_5666 = torch.constant.int 8
    %int128_5667 = torch.constant.int 128
    %4594 = torch.prim.ListConstruct %int4_5663, %358, %int2_5664, %int32_5665, %int8_5666, %int128_5667 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4595 = torch.aten.view %4593, %4594 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %4595, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int0_5668 = torch.constant.int 0
    %int0_5669 = torch.constant.int 0
    %int9223372036854775807_5670 = torch.constant.int 9223372036854775807
    %int1_5671 = torch.constant.int 1
    %4596 = torch.aten.slice.Tensor %4595, %int0_5668, %int0_5669, %int9223372036854775807_5670, %int1_5671 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %4596, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_5672 = torch.constant.int 1
    %int0_5673 = torch.constant.int 0
    %int9223372036854775807_5674 = torch.constant.int 9223372036854775807
    %int1_5675 = torch.constant.int 1
    %4597 = torch.aten.slice.Tensor %4596, %int1_5672, %int0_5673, %int9223372036854775807_5674, %int1_5675 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %4597, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_5676 = torch.constant.int 2
    %int0_5677 = torch.constant.int 0
    %4598 = torch.aten.select.int %4597, %int2_5676, %int0_5677 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %4598, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int32_5678 = torch.constant.int 32
    %4599 = torch.aten.mul.int %358, %int32_5678 : !torch.int, !torch.int -> !torch.int
    %int2_5679 = torch.constant.int 2
    %int0_5680 = torch.constant.int 0
    %int1_5681 = torch.constant.int 1
    %4600 = torch.aten.slice.Tensor %4598, %int2_5679, %int0_5680, %4599, %int1_5681 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %4600, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_5682 = torch.constant.int 0
    %4601 = torch.aten.clone %4600, %int0_5682 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %4601, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_5683 = torch.constant.int 1
    %4602 = torch.aten.size.int %4597, %int1_5683 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_5684 = torch.constant.int 32
    %4603 = torch.aten.mul.int %4602, %int32_5684 : !torch.int, !torch.int -> !torch.int
    %int4_5685 = torch.constant.int 4
    %int8_5686 = torch.constant.int 8
    %int128_5687 = torch.constant.int 128
    %4604 = torch.prim.ListConstruct %int4_5685, %4603, %int8_5686, %int128_5687 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4605 = torch.aten._unsafe_view %4601, %4604 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %4605, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_5688 = torch.constant.int 0
    %int0_5689 = torch.constant.int 0
    %int9223372036854775807_5690 = torch.constant.int 9223372036854775807
    %int1_5691 = torch.constant.int 1
    %4606 = torch.aten.slice.Tensor %4605, %int0_5688, %int0_5689, %int9223372036854775807_5690, %int1_5691 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %4606, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_5692 = torch.constant.int 0
    %int0_5693 = torch.constant.int 0
    %int9223372036854775807_5694 = torch.constant.int 9223372036854775807
    %int1_5695 = torch.constant.int 1
    %4607 = torch.aten.slice.Tensor %4595, %int0_5692, %int0_5693, %int9223372036854775807_5694, %int1_5695 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %4607, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_5696 = torch.constant.int 1
    %int0_5697 = torch.constant.int 0
    %int9223372036854775807_5698 = torch.constant.int 9223372036854775807
    %int1_5699 = torch.constant.int 1
    %4608 = torch.aten.slice.Tensor %4607, %int1_5696, %int0_5697, %int9223372036854775807_5698, %int1_5699 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %4608, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_5700 = torch.constant.int 2
    %int1_5701 = torch.constant.int 1
    %4609 = torch.aten.select.int %4608, %int2_5700, %int1_5701 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %4609, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int2_5702 = torch.constant.int 2
    %int0_5703 = torch.constant.int 0
    %int1_5704 = torch.constant.int 1
    %4610 = torch.aten.slice.Tensor %4609, %int2_5702, %int0_5703, %4599, %int1_5704 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %4610, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_5705 = torch.constant.int 0
    %4611 = torch.aten.clone %4610, %int0_5705 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %4611, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_5706 = torch.constant.int 1
    %4612 = torch.aten.size.int %4608, %int1_5706 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_5707 = torch.constant.int 32
    %4613 = torch.aten.mul.int %4612, %int32_5707 : !torch.int, !torch.int -> !torch.int
    %int4_5708 = torch.constant.int 4
    %int8_5709 = torch.constant.int 8
    %int128_5710 = torch.constant.int 128
    %4614 = torch.prim.ListConstruct %int4_5708, %4613, %int8_5709, %int128_5710 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4615 = torch.aten._unsafe_view %4611, %4614 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %4615, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_5711 = torch.constant.int 0
    %int0_5712 = torch.constant.int 0
    %int9223372036854775807_5713 = torch.constant.int 9223372036854775807
    %int1_5714 = torch.constant.int 1
    %4616 = torch.aten.slice.Tensor %4615, %int0_5711, %int0_5712, %int9223372036854775807_5713, %int1_5714 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %4616, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int-2_5715 = torch.constant.int -2
    %4617 = torch.aten.unsqueeze %4606, %int-2_5715 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %4617, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_5716 = torch.constant.int 1
    %4618 = torch.aten.size.int %4605, %int1_5716 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_5717 = torch.constant.int 4
    %int8_5718 = torch.constant.int 8
    %int4_5719 = torch.constant.int 4
    %int128_5720 = torch.constant.int 128
    %4619 = torch.prim.ListConstruct %int4_5717, %4618, %int8_5718, %int4_5719, %int128_5720 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_5721 = torch.constant.bool false
    %4620 = torch.aten.expand %4617, %4619, %false_5721 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4620, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_5722 = torch.constant.int 0
    %4621 = torch.aten.clone %4620, %int0_5722 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4621, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_5723 = torch.constant.int 4
    %int32_5724 = torch.constant.int 32
    %int128_5725 = torch.constant.int 128
    %4622 = torch.prim.ListConstruct %int4_5723, %4618, %int32_5724, %int128_5725 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4623 = torch.aten._unsafe_view %4621, %4622 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %4623, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_5726 = torch.constant.int -2
    %4624 = torch.aten.unsqueeze %4616, %int-2_5726 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %4624, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_5727 = torch.constant.int 1
    %4625 = torch.aten.size.int %4615, %int1_5727 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_5728 = torch.constant.int 4
    %int8_5729 = torch.constant.int 8
    %int4_5730 = torch.constant.int 4
    %int128_5731 = torch.constant.int 128
    %4626 = torch.prim.ListConstruct %int4_5728, %4625, %int8_5729, %int4_5730, %int128_5731 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_5732 = torch.constant.bool false
    %4627 = torch.aten.expand %4624, %4626, %false_5732 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4627, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_5733 = torch.constant.int 0
    %4628 = torch.aten.clone %4627, %int0_5733 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4628, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_5734 = torch.constant.int 4
    %int32_5735 = torch.constant.int 32
    %int128_5736 = torch.constant.int 128
    %4629 = torch.prim.ListConstruct %int4_5734, %4625, %int32_5735, %int128_5736 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4630 = torch.aten._unsafe_view %4628, %4629 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %4630, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_5737 = torch.constant.int 1
    %int2_5738 = torch.constant.int 2
    %4631 = torch.aten.transpose.int %4511, %int1_5737, %int2_5738 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,128],f16>
    %int1_5739 = torch.constant.int 1
    %int2_5740 = torch.constant.int 2
    %4632 = torch.aten.transpose.int %4623, %int1_5739, %int2_5740 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %4632, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_5741 = torch.constant.int 1
    %int2_5742 = torch.constant.int 2
    %4633 = torch.aten.transpose.int %4630, %int1_5741, %int2_5742 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %4633, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_5743 = torch.constant.float 0.000000e+00
    %false_5744 = torch.constant.bool false
    %none_5745 = torch.constant.none
    %4634:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4631, %4632, %4633, %float0.000000e00_5743, %false_5744, %368, %none_5745) : (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.vtensor<[4,1,1,?],f16>, !torch.none) -> (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,1],f32>) 
    %int1_5746 = torch.constant.int 1
    %int2_5747 = torch.constant.int 2
    %4635 = torch.aten.transpose.int %4634#0, %int1_5746, %int2_5747 : !torch.vtensor<[4,32,1,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int4_5748 = torch.constant.int 4
    %int1_5749 = torch.constant.int 1
    %int4096_5750 = torch.constant.int 4096
    %4636 = torch.prim.ListConstruct %int4_5748, %int1_5749, %int4096_5750 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4637 = torch.aten.view %4635, %4636 : !torch.vtensor<[4,1,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_5751 = torch.constant.int -2
    %int-1_5752 = torch.constant.int -1
    %4638 = torch.aten.transpose.int %227, %int-2_5751, %int-1_5752 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_5753 = torch.constant.int 4
    %int4096_5754 = torch.constant.int 4096
    %4639 = torch.prim.ListConstruct %int4_5753, %int4096_5754 : (!torch.int, !torch.int) -> !torch.list<int>
    %4640 = torch.aten.view %4637, %4639 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %4641 = torch.aten.mm %4640, %4638 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_5755 = torch.constant.int 4
    %int1_5756 = torch.constant.int 1
    %int4096_5757 = torch.constant.int 4096
    %4642 = torch.prim.ListConstruct %int4_5755, %int1_5756, %int4096_5757 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4643 = torch.aten.view %4641, %4642 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_5758 = torch.constant.int 1
    %4644 = torch.aten.add.Tensor %4471, %4643, %int1_5758 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_5759 = torch.constant.int 6
    %4645 = torch.prims.convert_element_type %4644, %int6_5759 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_5760 = torch.constant.int 2
    %4646 = torch.aten.pow.Tensor_Scalar %4645, %int2_5760 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_5761 = torch.constant.int -1
    %4647 = torch.prim.ListConstruct %int-1_5761 : (!torch.int) -> !torch.list<int>
    %true_5762 = torch.constant.bool true
    %none_5763 = torch.constant.none
    %4648 = torch.aten.mean.dim %4646, %4647, %true_5762, %none_5763 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_5764 = torch.constant.float 9.9999997473787516E-6
    %int1_5765 = torch.constant.int 1
    %4649 = torch.aten.add.Scalar %4648, %float9.999990e-06_5764, %int1_5765 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %4650 = torch.aten.rsqrt %4649 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %4651 = torch.aten.mul.Tensor %4645, %4650 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_5766 = torch.constant.int 5
    %4652 = torch.prims.convert_element_type %4651, %int5_5766 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %4653 = torch.aten.mul.Tensor %228, %4652 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_5767 = torch.constant.int 5
    %4654 = torch.prims.convert_element_type %4653, %int5_5767 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_5768 = torch.constant.int -2
    %int-1_5769 = torch.constant.int -1
    %4655 = torch.aten.transpose.int %229, %int-2_5768, %int-1_5769 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_5770 = torch.constant.int 4
    %int4096_5771 = torch.constant.int 4096
    %4656 = torch.prim.ListConstruct %int4_5770, %int4096_5771 : (!torch.int, !torch.int) -> !torch.list<int>
    %4657 = torch.aten.view %4654, %4656 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %4658 = torch.aten.mm %4657, %4655 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_5772 = torch.constant.int 4
    %int1_5773 = torch.constant.int 1
    %int14336_5774 = torch.constant.int 14336
    %4659 = torch.prim.ListConstruct %int4_5772, %int1_5773, %int14336_5774 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4660 = torch.aten.view %4658, %4659 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %4661 = torch.aten.silu %4660 : !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_5775 = torch.constant.int -2
    %int-1_5776 = torch.constant.int -1
    %4662 = torch.aten.transpose.int %230, %int-2_5775, %int-1_5776 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_5777 = torch.constant.int 4
    %int4096_5778 = torch.constant.int 4096
    %4663 = torch.prim.ListConstruct %int4_5777, %int4096_5778 : (!torch.int, !torch.int) -> !torch.list<int>
    %4664 = torch.aten.view %4654, %4663 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %4665 = torch.aten.mm %4664, %4662 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_5779 = torch.constant.int 4
    %int1_5780 = torch.constant.int 1
    %int14336_5781 = torch.constant.int 14336
    %4666 = torch.prim.ListConstruct %int4_5779, %int1_5780, %int14336_5781 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4667 = torch.aten.view %4665, %4666 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %4668 = torch.aten.mul.Tensor %4661, %4667 : !torch.vtensor<[4,1,14336],f16>, !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_5782 = torch.constant.int -2
    %int-1_5783 = torch.constant.int -1
    %4669 = torch.aten.transpose.int %231, %int-2_5782, %int-1_5783 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int4_5784 = torch.constant.int 4
    %int14336_5785 = torch.constant.int 14336
    %4670 = torch.prim.ListConstruct %int4_5784, %int14336_5785 : (!torch.int, !torch.int) -> !torch.list<int>
    %4671 = torch.aten.view %4668, %4670 : !torch.vtensor<[4,1,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,14336],f16>
    %4672 = torch.aten.mm %4671, %4669 : !torch.vtensor<[4,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_5786 = torch.constant.int 4
    %int1_5787 = torch.constant.int 1
    %int4096_5788 = torch.constant.int 4096
    %4673 = torch.prim.ListConstruct %int4_5786, %int1_5787, %int4096_5788 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4674 = torch.aten.view %4672, %4673 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_5789 = torch.constant.int 1
    %4675 = torch.aten.add.Tensor %4644, %4674, %int1_5789 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_5790 = torch.constant.int 6
    %4676 = torch.prims.convert_element_type %4675, %int6_5790 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_5791 = torch.constant.int 2
    %4677 = torch.aten.pow.Tensor_Scalar %4676, %int2_5791 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_5792 = torch.constant.int -1
    %4678 = torch.prim.ListConstruct %int-1_5792 : (!torch.int) -> !torch.list<int>
    %true_5793 = torch.constant.bool true
    %none_5794 = torch.constant.none
    %4679 = torch.aten.mean.dim %4677, %4678, %true_5793, %none_5794 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_5795 = torch.constant.float 9.9999997473787516E-6
    %int1_5796 = torch.constant.int 1
    %4680 = torch.aten.add.Scalar %4679, %float9.999990e-06_5795, %int1_5796 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %4681 = torch.aten.rsqrt %4680 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %4682 = torch.aten.mul.Tensor %4676, %4681 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_5797 = torch.constant.int 5
    %4683 = torch.prims.convert_element_type %4682, %int5_5797 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %4684 = torch.aten.mul.Tensor %232, %4683 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_5798 = torch.constant.int 5
    %4685 = torch.prims.convert_element_type %4684, %int5_5798 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_5799 = torch.constant.int -2
    %int-1_5800 = torch.constant.int -1
    %4686 = torch.aten.transpose.int %233, %int-2_5799, %int-1_5800 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_5801 = torch.constant.int 4
    %int4096_5802 = torch.constant.int 4096
    %4687 = torch.prim.ListConstruct %int4_5801, %int4096_5802 : (!torch.int, !torch.int) -> !torch.list<int>
    %4688 = torch.aten.view %4685, %4687 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %4689 = torch.aten.mm %4688, %4686 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_5803 = torch.constant.int 4
    %int1_5804 = torch.constant.int 1
    %int4096_5805 = torch.constant.int 4096
    %4690 = torch.prim.ListConstruct %int4_5803, %int1_5804, %int4096_5805 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4691 = torch.aten.view %4689, %4690 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_5806 = torch.constant.int -2
    %int-1_5807 = torch.constant.int -1
    %4692 = torch.aten.transpose.int %234, %int-2_5806, %int-1_5807 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_5808 = torch.constant.int 4
    %int4096_5809 = torch.constant.int 4096
    %4693 = torch.prim.ListConstruct %int4_5808, %int4096_5809 : (!torch.int, !torch.int) -> !torch.list<int>
    %4694 = torch.aten.view %4685, %4693 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %4695 = torch.aten.mm %4694, %4692 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_5810 = torch.constant.int 4
    %int1_5811 = torch.constant.int 1
    %int1024_5812 = torch.constant.int 1024
    %4696 = torch.prim.ListConstruct %int4_5810, %int1_5811, %int1024_5812 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4697 = torch.aten.view %4695, %4696 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int-2_5813 = torch.constant.int -2
    %int-1_5814 = torch.constant.int -1
    %4698 = torch.aten.transpose.int %235, %int-2_5813, %int-1_5814 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_5815 = torch.constant.int 4
    %int4096_5816 = torch.constant.int 4096
    %4699 = torch.prim.ListConstruct %int4_5815, %int4096_5816 : (!torch.int, !torch.int) -> !torch.list<int>
    %4700 = torch.aten.view %4685, %4699 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %4701 = torch.aten.mm %4700, %4698 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_5817 = torch.constant.int 4
    %int1_5818 = torch.constant.int 1
    %int1024_5819 = torch.constant.int 1024
    %4702 = torch.prim.ListConstruct %int4_5817, %int1_5818, %int1024_5819 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4703 = torch.aten.view %4701, %4702 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int4_5820 = torch.constant.int 4
    %int1_5821 = torch.constant.int 1
    %int32_5822 = torch.constant.int 32
    %int128_5823 = torch.constant.int 128
    %4704 = torch.prim.ListConstruct %int4_5820, %int1_5821, %int32_5822, %int128_5823 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4705 = torch.aten.view %4691, %4704 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,128],f16>
    %int4_5824 = torch.constant.int 4
    %int1_5825 = torch.constant.int 1
    %int8_5826 = torch.constant.int 8
    %int128_5827 = torch.constant.int 128
    %4706 = torch.prim.ListConstruct %int4_5824, %int1_5825, %int8_5826, %int128_5827 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4707 = torch.aten.view %4697, %4706 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int4_5828 = torch.constant.int 4
    %int1_5829 = torch.constant.int 1
    %int8_5830 = torch.constant.int 8
    %int128_5831 = torch.constant.int 128
    %4708 = torch.prim.ListConstruct %int4_5828, %int1_5829, %int8_5830, %int128_5831 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4709 = torch.aten.view %4703, %4708 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int6_5832 = torch.constant.int 6
    %4710 = torch.prims.convert_element_type %4705, %int6_5832 : !torch.vtensor<[4,1,32,128],f16>, !torch.int -> !torch.vtensor<[4,1,32,128],f32>
    %4711 = torch_c.to_builtin_tensor %4710 : !torch.vtensor<[4,1,32,128],f32> -> tensor<4x1x32x128xf32>
    %4712 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %4713 = util.call @sharktank_rotary_embedding_4_1_32_128_f32(%4711, %4712) : (tensor<4x1x32x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x32x128xf32>
    %4714 = torch_c.from_builtin_tensor %4713 : tensor<4x1x32x128xf32> -> !torch.vtensor<[4,1,32,128],f32>
    %int5_5833 = torch.constant.int 5
    %4715 = torch.prims.convert_element_type %4714, %int5_5833 : !torch.vtensor<[4,1,32,128],f32>, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int6_5834 = torch.constant.int 6
    %4716 = torch.prims.convert_element_type %4707, %int6_5834 : !torch.vtensor<[4,1,8,128],f16>, !torch.int -> !torch.vtensor<[4,1,8,128],f32>
    %4717 = torch_c.to_builtin_tensor %4716 : !torch.vtensor<[4,1,8,128],f32> -> tensor<4x1x8x128xf32>
    %4718 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %4719 = util.call @sharktank_rotary_embedding_4_1_8_128_f32(%4717, %4718) : (tensor<4x1x8x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x8x128xf32>
    %4720 = torch_c.from_builtin_tensor %4719 : tensor<4x1x8x128xf32> -> !torch.vtensor<[4,1,8,128],f32>
    %int5_5835 = torch.constant.int 5
    %4721 = torch.prims.convert_element_type %4720, %int5_5835 : !torch.vtensor<[4,1,8,128],f32>, !torch.int -> !torch.vtensor<[4,1,8,128],f16>
    %int32_5836 = torch.constant.int 32
    %4722 = torch.aten.floor_divide.Scalar %arg2, %int32_5836 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_5837 = torch.constant.int 1
    %4723 = torch.aten.unsqueeze %4722, %int1_5837 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5838 = torch.constant.int 1
    %false_5839 = torch.constant.bool false
    %4724 = torch.aten.gather %arg3, %int1_5838, %4723, %false_5839 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_5840 = torch.constant.int 32
    %4725 = torch.aten.remainder.Scalar %arg2, %int32_5840 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_5841 = torch.constant.int 1
    %4726 = torch.aten.unsqueeze %4725, %int1_5841 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_5842 = torch.constant.none
    %4727 = torch.aten.clone %236, %none_5842 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_5843 = torch.constant.int 0
    %4728 = torch.aten.unsqueeze %4727, %int0_5843 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_5844 = torch.constant.int 4
    %int1_5845 = torch.constant.int 1
    %4729 = torch.prim.ListConstruct %int4_5844, %int1_5845 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_5846 = torch.constant.int 1
    %int1_5847 = torch.constant.int 1
    %4730 = torch.prim.ListConstruct %int1_5846, %int1_5847 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_5848 = torch.constant.int 4
    %int0_5849 = torch.constant.int 0
    %cpu_5850 = torch.constant.device "cpu"
    %false_5851 = torch.constant.bool false
    %4731 = torch.aten.empty_strided %4729, %4730, %int4_5848, %int0_5849, %cpu_5850, %false_5851 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int21 = torch.constant.int 21
    %4732 = torch.aten.fill.Scalar %4731, %int21 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_5852 = torch.constant.int 4
    %int1_5853 = torch.constant.int 1
    %4733 = torch.prim.ListConstruct %int4_5852, %int1_5853 : (!torch.int, !torch.int) -> !torch.list<int>
    %4734 = torch.aten.repeat %4728, %4733 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_5854 = torch.constant.int 32
    %4735 = torch.aten.mul.Scalar %4724, %int32_5854 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5855 = torch.constant.int 1
    %4736 = torch.aten.add.Tensor %4735, %4732, %int1_5855 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_5856 = torch.constant.int 2
    %4737 = torch.aten.mul.Scalar %4736, %int2_5856 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5857 = torch.constant.int 1
    %4738 = torch.aten.add.Tensor %4737, %4734, %int1_5857 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_5858 = torch.constant.int 32
    %4739 = torch.aten.mul.Scalar %4738, %int32_5858 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5859 = torch.constant.int 1
    %4740 = torch.aten.add.Tensor %4739, %4726, %int1_5859 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_5860 = torch.constant.int 32
    %int2_5861 = torch.constant.int 2
    %int32_5862 = torch.constant.int 32
    %int8_5863 = torch.constant.int 8
    %int128_5864 = torch.constant.int 128
    %4741 = torch.prim.ListConstruct %437, %int32_5860, %int2_5861, %int32_5862, %int8_5863, %int128_5864 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4742 = torch.aten.view %4578, %4741 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4742, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_5865 = torch.constant.int 32
    %4743 = torch.aten.mul.int %437, %int32_5865 : !torch.int, !torch.int -> !torch.int
    %int2_5866 = torch.constant.int 2
    %4744 = torch.aten.mul.int %4743, %int2_5866 : !torch.int, !torch.int -> !torch.int
    %int32_5867 = torch.constant.int 32
    %4745 = torch.aten.mul.int %4744, %int32_5867 : !torch.int, !torch.int -> !torch.int
    %int8_5868 = torch.constant.int 8
    %int128_5869 = torch.constant.int 128
    %4746 = torch.prim.ListConstruct %4745, %int8_5868, %int128_5869 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4747 = torch.aten.view %4742, %4746 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %4747, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %4748 = torch.prim.ListConstruct %4740 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_5870 = torch.constant.bool false
    %4749 = torch.aten.index_put %4747, %4748, %4721, %false_5870 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %4749, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_5871 = torch.constant.int 32
    %int2_5872 = torch.constant.int 2
    %int32_5873 = torch.constant.int 32
    %int8_5874 = torch.constant.int 8
    %int128_5875 = torch.constant.int 128
    %4750 = torch.prim.ListConstruct %437, %int32_5871, %int2_5872, %int32_5873, %int8_5874, %int128_5875 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4751 = torch.aten.view %4749, %4750 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4751, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_5876 = torch.constant.int 2097152
    %4752 = torch.prim.ListConstruct %437, %int2097152_5876 : (!torch.int, !torch.int) -> !torch.list<int>
    %4753 = torch.aten.view %4751, %4752 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %4753, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_5877 = torch.constant.int 32
    %int2_5878 = torch.constant.int 2
    %int32_5879 = torch.constant.int 32
    %int8_5880 = torch.constant.int 8
    %int128_5881 = torch.constant.int 128
    %4754 = torch.prim.ListConstruct %437, %int32_5877, %int2_5878, %int32_5879, %int8_5880, %int128_5881 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4755 = torch.aten.view %4753, %4754 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4755, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int8_5882 = torch.constant.int 8
    %int128_5883 = torch.constant.int 128
    %4756 = torch.prim.ListConstruct %4745, %int8_5882, %int128_5883 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4757 = torch.aten.view %4755, %4756 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %4757, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_5884 = torch.constant.int 32
    %4758 = torch.aten.floor_divide.Scalar %arg2, %int32_5884 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_5885 = torch.constant.int 1
    %4759 = torch.aten.unsqueeze %4758, %int1_5885 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5886 = torch.constant.int 1
    %false_5887 = torch.constant.bool false
    %4760 = torch.aten.gather %arg3, %int1_5886, %4759, %false_5887 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_5888 = torch.constant.int 32
    %4761 = torch.aten.remainder.Scalar %arg2, %int32_5888 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_5889 = torch.constant.int 1
    %4762 = torch.aten.unsqueeze %4761, %int1_5889 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_5890 = torch.constant.none
    %4763 = torch.aten.clone %237, %none_5890 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_5891 = torch.constant.int 0
    %4764 = torch.aten.unsqueeze %4763, %int0_5891 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_5892 = torch.constant.int 4
    %int1_5893 = torch.constant.int 1
    %4765 = torch.prim.ListConstruct %int4_5892, %int1_5893 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_5894 = torch.constant.int 1
    %int1_5895 = torch.constant.int 1
    %4766 = torch.prim.ListConstruct %int1_5894, %int1_5895 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_5896 = torch.constant.int 4
    %int0_5897 = torch.constant.int 0
    %cpu_5898 = torch.constant.device "cpu"
    %false_5899 = torch.constant.bool false
    %4767 = torch.aten.empty_strided %4765, %4766, %int4_5896, %int0_5897, %cpu_5898, %false_5899 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int21_5900 = torch.constant.int 21
    %4768 = torch.aten.fill.Scalar %4767, %int21_5900 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_5901 = torch.constant.int 4
    %int1_5902 = torch.constant.int 1
    %4769 = torch.prim.ListConstruct %int4_5901, %int1_5902 : (!torch.int, !torch.int) -> !torch.list<int>
    %4770 = torch.aten.repeat %4764, %4769 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_5903 = torch.constant.int 32
    %4771 = torch.aten.mul.Scalar %4760, %int32_5903 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5904 = torch.constant.int 1
    %4772 = torch.aten.add.Tensor %4771, %4768, %int1_5904 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_5905 = torch.constant.int 2
    %4773 = torch.aten.mul.Scalar %4772, %int2_5905 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5906 = torch.constant.int 1
    %4774 = torch.aten.add.Tensor %4773, %4770, %int1_5906 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_5907 = torch.constant.int 32
    %4775 = torch.aten.mul.Scalar %4774, %int32_5907 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_5908 = torch.constant.int 1
    %4776 = torch.aten.add.Tensor %4775, %4762, %int1_5908 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %4777 = torch.prim.ListConstruct %4776 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_5909 = torch.constant.bool false
    %4778 = torch.aten.index_put %4757, %4777, %4709, %false_5909 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %4778, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_5910 = torch.constant.int 32
    %int2_5911 = torch.constant.int 2
    %int32_5912 = torch.constant.int 32
    %int8_5913 = torch.constant.int 8
    %int128_5914 = torch.constant.int 128
    %4779 = torch.prim.ListConstruct %437, %int32_5910, %int2_5911, %int32_5912, %int8_5913, %int128_5914 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4780 = torch.aten.view %4778, %4779 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4780, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_5915 = torch.constant.int 2097152
    %4781 = torch.prim.ListConstruct %437, %int2097152_5915 : (!torch.int, !torch.int) -> !torch.list<int>
    %4782 = torch.aten.view %4780, %4781 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %4782, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int4_5916 = torch.constant.int 4
    %4783 = torch.prim.ListConstruct %int4_5916, %358 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_5917 = torch.constant.int 1
    %4784 = torch.prim.ListConstruct %358, %int1_5917 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_5918 = torch.constant.int 4
    %int0_5919 = torch.constant.int 0
    %cpu_5920 = torch.constant.device "cpu"
    %false_5921 = torch.constant.bool false
    %4785 = torch.aten.empty_strided %4783, %4784, %int4_5918, %int0_5919, %cpu_5920, %false_5921 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4785, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int21_5922 = torch.constant.int 21
    %4786 = torch.aten.fill.Scalar %4785, %int21_5922 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4786, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int32_5923 = torch.constant.int 32
    %4787 = torch.aten.mul.Scalar %arg3, %int32_5923 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4787, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_5924 = torch.constant.int 1
    %4788 = torch.aten.add.Tensor %4787, %4786, %int1_5924 : !torch.vtensor<[4,?],si64>, !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4788, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_5925 = torch.constant.int 4
    %4789 = torch.aten.mul.int %int4_5925, %358 : !torch.int, !torch.int -> !torch.int
    %4790 = torch.prim.ListConstruct %4789 : (!torch.int) -> !torch.list<int>
    %4791 = torch.aten.view %4788, %4790 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4791, [%356], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_5926 = torch.constant.int 32
    %int2_5927 = torch.constant.int 2
    %int32_5928 = torch.constant.int 32
    %int8_5929 = torch.constant.int 8
    %int128_5930 = torch.constant.int 128
    %4792 = torch.prim.ListConstruct %437, %int32_5926, %int2_5927, %int32_5928, %int8_5929, %int128_5930 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4793 = torch.aten.view %4782, %4792 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4793, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_5931 = torch.constant.int 32
    %4794 = torch.aten.mul.int %437, %int32_5931 : !torch.int, !torch.int -> !torch.int
    %int2_5932 = torch.constant.int 2
    %int32_5933 = torch.constant.int 32
    %int8_5934 = torch.constant.int 8
    %int128_5935 = torch.constant.int 128
    %4795 = torch.prim.ListConstruct %4794, %int2_5932, %int32_5933, %int8_5934, %int128_5935 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4796 = torch.aten.view %4793, %4795 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %4796, [%357], affine_map<()[s0] -> (s0 * 32, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int0_5936 = torch.constant.int 0
    %4797 = torch.aten.index_select %4796, %int0_5936, %4791 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.int, !torch.vtensor<[?],si64> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %4797, [%356], affine_map<()[s0] -> (s0 * 4, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int4_5937 = torch.constant.int 4
    %int2_5938 = torch.constant.int 2
    %int32_5939 = torch.constant.int 32
    %int8_5940 = torch.constant.int 8
    %int128_5941 = torch.constant.int 128
    %4798 = torch.prim.ListConstruct %int4_5937, %358, %int2_5938, %int32_5939, %int8_5940, %int128_5941 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4799 = torch.aten.view %4797, %4798 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %4799, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int0_5942 = torch.constant.int 0
    %int0_5943 = torch.constant.int 0
    %int9223372036854775807_5944 = torch.constant.int 9223372036854775807
    %int1_5945 = torch.constant.int 1
    %4800 = torch.aten.slice.Tensor %4799, %int0_5942, %int0_5943, %int9223372036854775807_5944, %int1_5945 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %4800, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_5946 = torch.constant.int 1
    %int0_5947 = torch.constant.int 0
    %int9223372036854775807_5948 = torch.constant.int 9223372036854775807
    %int1_5949 = torch.constant.int 1
    %4801 = torch.aten.slice.Tensor %4800, %int1_5946, %int0_5947, %int9223372036854775807_5948, %int1_5949 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %4801, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_5950 = torch.constant.int 2
    %int0_5951 = torch.constant.int 0
    %4802 = torch.aten.select.int %4801, %int2_5950, %int0_5951 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %4802, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int32_5952 = torch.constant.int 32
    %4803 = torch.aten.mul.int %358, %int32_5952 : !torch.int, !torch.int -> !torch.int
    %int2_5953 = torch.constant.int 2
    %int0_5954 = torch.constant.int 0
    %int1_5955 = torch.constant.int 1
    %4804 = torch.aten.slice.Tensor %4802, %int2_5953, %int0_5954, %4803, %int1_5955 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %4804, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_5956 = torch.constant.int 0
    %4805 = torch.aten.clone %4804, %int0_5956 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %4805, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_5957 = torch.constant.int 1
    %4806 = torch.aten.size.int %4801, %int1_5957 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_5958 = torch.constant.int 32
    %4807 = torch.aten.mul.int %4806, %int32_5958 : !torch.int, !torch.int -> !torch.int
    %int4_5959 = torch.constant.int 4
    %int8_5960 = torch.constant.int 8
    %int128_5961 = torch.constant.int 128
    %4808 = torch.prim.ListConstruct %int4_5959, %4807, %int8_5960, %int128_5961 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4809 = torch.aten._unsafe_view %4805, %4808 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %4809, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_5962 = torch.constant.int 0
    %int0_5963 = torch.constant.int 0
    %int9223372036854775807_5964 = torch.constant.int 9223372036854775807
    %int1_5965 = torch.constant.int 1
    %4810 = torch.aten.slice.Tensor %4809, %int0_5962, %int0_5963, %int9223372036854775807_5964, %int1_5965 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %4810, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_5966 = torch.constant.int 0
    %int0_5967 = torch.constant.int 0
    %int9223372036854775807_5968 = torch.constant.int 9223372036854775807
    %int1_5969 = torch.constant.int 1
    %4811 = torch.aten.slice.Tensor %4799, %int0_5966, %int0_5967, %int9223372036854775807_5968, %int1_5969 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %4811, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_5970 = torch.constant.int 1
    %int0_5971 = torch.constant.int 0
    %int9223372036854775807_5972 = torch.constant.int 9223372036854775807
    %int1_5973 = torch.constant.int 1
    %4812 = torch.aten.slice.Tensor %4811, %int1_5970, %int0_5971, %int9223372036854775807_5972, %int1_5973 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %4812, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_5974 = torch.constant.int 2
    %int1_5975 = torch.constant.int 1
    %4813 = torch.aten.select.int %4812, %int2_5974, %int1_5975 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %4813, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int2_5976 = torch.constant.int 2
    %int0_5977 = torch.constant.int 0
    %int1_5978 = torch.constant.int 1
    %4814 = torch.aten.slice.Tensor %4813, %int2_5976, %int0_5977, %4803, %int1_5978 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %4814, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_5979 = torch.constant.int 0
    %4815 = torch.aten.clone %4814, %int0_5979 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %4815, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_5980 = torch.constant.int 1
    %4816 = torch.aten.size.int %4812, %int1_5980 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_5981 = torch.constant.int 32
    %4817 = torch.aten.mul.int %4816, %int32_5981 : !torch.int, !torch.int -> !torch.int
    %int4_5982 = torch.constant.int 4
    %int8_5983 = torch.constant.int 8
    %int128_5984 = torch.constant.int 128
    %4818 = torch.prim.ListConstruct %int4_5982, %4817, %int8_5983, %int128_5984 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4819 = torch.aten._unsafe_view %4815, %4818 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %4819, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_5985 = torch.constant.int 0
    %int0_5986 = torch.constant.int 0
    %int9223372036854775807_5987 = torch.constant.int 9223372036854775807
    %int1_5988 = torch.constant.int 1
    %4820 = torch.aten.slice.Tensor %4819, %int0_5985, %int0_5986, %int9223372036854775807_5987, %int1_5988 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %4820, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int-2_5989 = torch.constant.int -2
    %4821 = torch.aten.unsqueeze %4810, %int-2_5989 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %4821, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_5990 = torch.constant.int 1
    %4822 = torch.aten.size.int %4809, %int1_5990 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_5991 = torch.constant.int 4
    %int8_5992 = torch.constant.int 8
    %int4_5993 = torch.constant.int 4
    %int128_5994 = torch.constant.int 128
    %4823 = torch.prim.ListConstruct %int4_5991, %4822, %int8_5992, %int4_5993, %int128_5994 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_5995 = torch.constant.bool false
    %4824 = torch.aten.expand %4821, %4823, %false_5995 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4824, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_5996 = torch.constant.int 0
    %4825 = torch.aten.clone %4824, %int0_5996 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4825, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_5997 = torch.constant.int 4
    %int32_5998 = torch.constant.int 32
    %int128_5999 = torch.constant.int 128
    %4826 = torch.prim.ListConstruct %int4_5997, %4822, %int32_5998, %int128_5999 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4827 = torch.aten._unsafe_view %4825, %4826 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %4827, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_6000 = torch.constant.int -2
    %4828 = torch.aten.unsqueeze %4820, %int-2_6000 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %4828, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_6001 = torch.constant.int 1
    %4829 = torch.aten.size.int %4819, %int1_6001 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_6002 = torch.constant.int 4
    %int8_6003 = torch.constant.int 8
    %int4_6004 = torch.constant.int 4
    %int128_6005 = torch.constant.int 128
    %4830 = torch.prim.ListConstruct %int4_6002, %4829, %int8_6003, %int4_6004, %int128_6005 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_6006 = torch.constant.bool false
    %4831 = torch.aten.expand %4828, %4830, %false_6006 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4831, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_6007 = torch.constant.int 0
    %4832 = torch.aten.clone %4831, %int0_6007 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %4832, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_6008 = torch.constant.int 4
    %int32_6009 = torch.constant.int 32
    %int128_6010 = torch.constant.int 128
    %4833 = torch.prim.ListConstruct %int4_6008, %4829, %int32_6009, %int128_6010 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4834 = torch.aten._unsafe_view %4832, %4833 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %4834, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_6011 = torch.constant.int 1
    %int2_6012 = torch.constant.int 2
    %4835 = torch.aten.transpose.int %4715, %int1_6011, %int2_6012 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,128],f16>
    %int1_6013 = torch.constant.int 1
    %int2_6014 = torch.constant.int 2
    %4836 = torch.aten.transpose.int %4827, %int1_6013, %int2_6014 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %4836, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_6015 = torch.constant.int 1
    %int2_6016 = torch.constant.int 2
    %4837 = torch.aten.transpose.int %4834, %int1_6015, %int2_6016 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %4837, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_6017 = torch.constant.float 0.000000e+00
    %false_6018 = torch.constant.bool false
    %none_6019 = torch.constant.none
    %4838:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4835, %4836, %4837, %float0.000000e00_6017, %false_6018, %368, %none_6019) : (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.vtensor<[4,1,1,?],f16>, !torch.none) -> (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,1],f32>) 
    %int1_6020 = torch.constant.int 1
    %int2_6021 = torch.constant.int 2
    %4839 = torch.aten.transpose.int %4838#0, %int1_6020, %int2_6021 : !torch.vtensor<[4,32,1,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int4_6022 = torch.constant.int 4
    %int1_6023 = torch.constant.int 1
    %int4096_6024 = torch.constant.int 4096
    %4840 = torch.prim.ListConstruct %int4_6022, %int1_6023, %int4096_6024 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4841 = torch.aten.view %4839, %4840 : !torch.vtensor<[4,1,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_6025 = torch.constant.int -2
    %int-1_6026 = torch.constant.int -1
    %4842 = torch.aten.transpose.int %238, %int-2_6025, %int-1_6026 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_6027 = torch.constant.int 4
    %int4096_6028 = torch.constant.int 4096
    %4843 = torch.prim.ListConstruct %int4_6027, %int4096_6028 : (!torch.int, !torch.int) -> !torch.list<int>
    %4844 = torch.aten.view %4841, %4843 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %4845 = torch.aten.mm %4844, %4842 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_6029 = torch.constant.int 4
    %int1_6030 = torch.constant.int 1
    %int4096_6031 = torch.constant.int 4096
    %4846 = torch.prim.ListConstruct %int4_6029, %int1_6030, %int4096_6031 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4847 = torch.aten.view %4845, %4846 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_6032 = torch.constant.int 1
    %4848 = torch.aten.add.Tensor %4675, %4847, %int1_6032 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_6033 = torch.constant.int 6
    %4849 = torch.prims.convert_element_type %4848, %int6_6033 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_6034 = torch.constant.int 2
    %4850 = torch.aten.pow.Tensor_Scalar %4849, %int2_6034 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_6035 = torch.constant.int -1
    %4851 = torch.prim.ListConstruct %int-1_6035 : (!torch.int) -> !torch.list<int>
    %true_6036 = torch.constant.bool true
    %none_6037 = torch.constant.none
    %4852 = torch.aten.mean.dim %4850, %4851, %true_6036, %none_6037 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_6038 = torch.constant.float 9.9999997473787516E-6
    %int1_6039 = torch.constant.int 1
    %4853 = torch.aten.add.Scalar %4852, %float9.999990e-06_6038, %int1_6039 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %4854 = torch.aten.rsqrt %4853 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %4855 = torch.aten.mul.Tensor %4849, %4854 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_6040 = torch.constant.int 5
    %4856 = torch.prims.convert_element_type %4855, %int5_6040 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %4857 = torch.aten.mul.Tensor %239, %4856 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_6041 = torch.constant.int 5
    %4858 = torch.prims.convert_element_type %4857, %int5_6041 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_6042 = torch.constant.int -2
    %int-1_6043 = torch.constant.int -1
    %4859 = torch.aten.transpose.int %240, %int-2_6042, %int-1_6043 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_6044 = torch.constant.int 4
    %int4096_6045 = torch.constant.int 4096
    %4860 = torch.prim.ListConstruct %int4_6044, %int4096_6045 : (!torch.int, !torch.int) -> !torch.list<int>
    %4861 = torch.aten.view %4858, %4860 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %4862 = torch.aten.mm %4861, %4859 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_6046 = torch.constant.int 4
    %int1_6047 = torch.constant.int 1
    %int14336_6048 = torch.constant.int 14336
    %4863 = torch.prim.ListConstruct %int4_6046, %int1_6047, %int14336_6048 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4864 = torch.aten.view %4862, %4863 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %4865 = torch.aten.silu %4864 : !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_6049 = torch.constant.int -2
    %int-1_6050 = torch.constant.int -1
    %4866 = torch.aten.transpose.int %241, %int-2_6049, %int-1_6050 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_6051 = torch.constant.int 4
    %int4096_6052 = torch.constant.int 4096
    %4867 = torch.prim.ListConstruct %int4_6051, %int4096_6052 : (!torch.int, !torch.int) -> !torch.list<int>
    %4868 = torch.aten.view %4858, %4867 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %4869 = torch.aten.mm %4868, %4866 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_6053 = torch.constant.int 4
    %int1_6054 = torch.constant.int 1
    %int14336_6055 = torch.constant.int 14336
    %4870 = torch.prim.ListConstruct %int4_6053, %int1_6054, %int14336_6055 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4871 = torch.aten.view %4869, %4870 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %4872 = torch.aten.mul.Tensor %4865, %4871 : !torch.vtensor<[4,1,14336],f16>, !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_6056 = torch.constant.int -2
    %int-1_6057 = torch.constant.int -1
    %4873 = torch.aten.transpose.int %242, %int-2_6056, %int-1_6057 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int4_6058 = torch.constant.int 4
    %int14336_6059 = torch.constant.int 14336
    %4874 = torch.prim.ListConstruct %int4_6058, %int14336_6059 : (!torch.int, !torch.int) -> !torch.list<int>
    %4875 = torch.aten.view %4872, %4874 : !torch.vtensor<[4,1,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,14336],f16>
    %4876 = torch.aten.mm %4875, %4873 : !torch.vtensor<[4,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_6060 = torch.constant.int 4
    %int1_6061 = torch.constant.int 1
    %int4096_6062 = torch.constant.int 4096
    %4877 = torch.prim.ListConstruct %int4_6060, %int1_6061, %int4096_6062 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4878 = torch.aten.view %4876, %4877 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_6063 = torch.constant.int 1
    %4879 = torch.aten.add.Tensor %4848, %4878, %int1_6063 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_6064 = torch.constant.int 6
    %4880 = torch.prims.convert_element_type %4879, %int6_6064 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_6065 = torch.constant.int 2
    %4881 = torch.aten.pow.Tensor_Scalar %4880, %int2_6065 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_6066 = torch.constant.int -1
    %4882 = torch.prim.ListConstruct %int-1_6066 : (!torch.int) -> !torch.list<int>
    %true_6067 = torch.constant.bool true
    %none_6068 = torch.constant.none
    %4883 = torch.aten.mean.dim %4881, %4882, %true_6067, %none_6068 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_6069 = torch.constant.float 9.9999997473787516E-6
    %int1_6070 = torch.constant.int 1
    %4884 = torch.aten.add.Scalar %4883, %float9.999990e-06_6069, %int1_6070 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %4885 = torch.aten.rsqrt %4884 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %4886 = torch.aten.mul.Tensor %4880, %4885 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_6071 = torch.constant.int 5
    %4887 = torch.prims.convert_element_type %4886, %int5_6071 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %4888 = torch.aten.mul.Tensor %243, %4887 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_6072 = torch.constant.int 5
    %4889 = torch.prims.convert_element_type %4888, %int5_6072 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_6073 = torch.constant.int -2
    %int-1_6074 = torch.constant.int -1
    %4890 = torch.aten.transpose.int %244, %int-2_6073, %int-1_6074 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_6075 = torch.constant.int 4
    %int4096_6076 = torch.constant.int 4096
    %4891 = torch.prim.ListConstruct %int4_6075, %int4096_6076 : (!torch.int, !torch.int) -> !torch.list<int>
    %4892 = torch.aten.view %4889, %4891 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %4893 = torch.aten.mm %4892, %4890 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_6077 = torch.constant.int 4
    %int1_6078 = torch.constant.int 1
    %int4096_6079 = torch.constant.int 4096
    %4894 = torch.prim.ListConstruct %int4_6077, %int1_6078, %int4096_6079 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4895 = torch.aten.view %4893, %4894 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_6080 = torch.constant.int -2
    %int-1_6081 = torch.constant.int -1
    %4896 = torch.aten.transpose.int %245, %int-2_6080, %int-1_6081 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_6082 = torch.constant.int 4
    %int4096_6083 = torch.constant.int 4096
    %4897 = torch.prim.ListConstruct %int4_6082, %int4096_6083 : (!torch.int, !torch.int) -> !torch.list<int>
    %4898 = torch.aten.view %4889, %4897 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %4899 = torch.aten.mm %4898, %4896 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_6084 = torch.constant.int 4
    %int1_6085 = torch.constant.int 1
    %int1024_6086 = torch.constant.int 1024
    %4900 = torch.prim.ListConstruct %int4_6084, %int1_6085, %int1024_6086 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4901 = torch.aten.view %4899, %4900 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int-2_6087 = torch.constant.int -2
    %int-1_6088 = torch.constant.int -1
    %4902 = torch.aten.transpose.int %246, %int-2_6087, %int-1_6088 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_6089 = torch.constant.int 4
    %int4096_6090 = torch.constant.int 4096
    %4903 = torch.prim.ListConstruct %int4_6089, %int4096_6090 : (!torch.int, !torch.int) -> !torch.list<int>
    %4904 = torch.aten.view %4889, %4903 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %4905 = torch.aten.mm %4904, %4902 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_6091 = torch.constant.int 4
    %int1_6092 = torch.constant.int 1
    %int1024_6093 = torch.constant.int 1024
    %4906 = torch.prim.ListConstruct %int4_6091, %int1_6092, %int1024_6093 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4907 = torch.aten.view %4905, %4906 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int4_6094 = torch.constant.int 4
    %int1_6095 = torch.constant.int 1
    %int32_6096 = torch.constant.int 32
    %int128_6097 = torch.constant.int 128
    %4908 = torch.prim.ListConstruct %int4_6094, %int1_6095, %int32_6096, %int128_6097 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4909 = torch.aten.view %4895, %4908 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,128],f16>
    %int4_6098 = torch.constant.int 4
    %int1_6099 = torch.constant.int 1
    %int8_6100 = torch.constant.int 8
    %int128_6101 = torch.constant.int 128
    %4910 = torch.prim.ListConstruct %int4_6098, %int1_6099, %int8_6100, %int128_6101 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4911 = torch.aten.view %4901, %4910 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int4_6102 = torch.constant.int 4
    %int1_6103 = torch.constant.int 1
    %int8_6104 = torch.constant.int 8
    %int128_6105 = torch.constant.int 128
    %4912 = torch.prim.ListConstruct %int4_6102, %int1_6103, %int8_6104, %int128_6105 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4913 = torch.aten.view %4907, %4912 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int6_6106 = torch.constant.int 6
    %4914 = torch.prims.convert_element_type %4909, %int6_6106 : !torch.vtensor<[4,1,32,128],f16>, !torch.int -> !torch.vtensor<[4,1,32,128],f32>
    %4915 = torch_c.to_builtin_tensor %4914 : !torch.vtensor<[4,1,32,128],f32> -> tensor<4x1x32x128xf32>
    %4916 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %4917 = util.call @sharktank_rotary_embedding_4_1_32_128_f32(%4915, %4916) : (tensor<4x1x32x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x32x128xf32>
    %4918 = torch_c.from_builtin_tensor %4917 : tensor<4x1x32x128xf32> -> !torch.vtensor<[4,1,32,128],f32>
    %int5_6107 = torch.constant.int 5
    %4919 = torch.prims.convert_element_type %4918, %int5_6107 : !torch.vtensor<[4,1,32,128],f32>, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int6_6108 = torch.constant.int 6
    %4920 = torch.prims.convert_element_type %4911, %int6_6108 : !torch.vtensor<[4,1,8,128],f16>, !torch.int -> !torch.vtensor<[4,1,8,128],f32>
    %4921 = torch_c.to_builtin_tensor %4920 : !torch.vtensor<[4,1,8,128],f32> -> tensor<4x1x8x128xf32>
    %4922 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %4923 = util.call @sharktank_rotary_embedding_4_1_8_128_f32(%4921, %4922) : (tensor<4x1x8x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x8x128xf32>
    %4924 = torch_c.from_builtin_tensor %4923 : tensor<4x1x8x128xf32> -> !torch.vtensor<[4,1,8,128],f32>
    %int5_6109 = torch.constant.int 5
    %4925 = torch.prims.convert_element_type %4924, %int5_6109 : !torch.vtensor<[4,1,8,128],f32>, !torch.int -> !torch.vtensor<[4,1,8,128],f16>
    %int32_6110 = torch.constant.int 32
    %4926 = torch.aten.floor_divide.Scalar %arg2, %int32_6110 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_6111 = torch.constant.int 1
    %4927 = torch.aten.unsqueeze %4926, %int1_6111 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_6112 = torch.constant.int 1
    %false_6113 = torch.constant.bool false
    %4928 = torch.aten.gather %arg3, %int1_6112, %4927, %false_6113 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_6114 = torch.constant.int 32
    %4929 = torch.aten.remainder.Scalar %arg2, %int32_6114 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_6115 = torch.constant.int 1
    %4930 = torch.aten.unsqueeze %4929, %int1_6115 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_6116 = torch.constant.none
    %4931 = torch.aten.clone %247, %none_6116 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_6117 = torch.constant.int 0
    %4932 = torch.aten.unsqueeze %4931, %int0_6117 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_6118 = torch.constant.int 4
    %int1_6119 = torch.constant.int 1
    %4933 = torch.prim.ListConstruct %int4_6118, %int1_6119 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_6120 = torch.constant.int 1
    %int1_6121 = torch.constant.int 1
    %4934 = torch.prim.ListConstruct %int1_6120, %int1_6121 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_6122 = torch.constant.int 4
    %int0_6123 = torch.constant.int 0
    %cpu_6124 = torch.constant.device "cpu"
    %false_6125 = torch.constant.bool false
    %4935 = torch.aten.empty_strided %4933, %4934, %int4_6122, %int0_6123, %cpu_6124, %false_6125 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int22 = torch.constant.int 22
    %4936 = torch.aten.fill.Scalar %4935, %int22 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_6126 = torch.constant.int 4
    %int1_6127 = torch.constant.int 1
    %4937 = torch.prim.ListConstruct %int4_6126, %int1_6127 : (!torch.int, !torch.int) -> !torch.list<int>
    %4938 = torch.aten.repeat %4932, %4937 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_6128 = torch.constant.int 32
    %4939 = torch.aten.mul.Scalar %4928, %int32_6128 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_6129 = torch.constant.int 1
    %4940 = torch.aten.add.Tensor %4939, %4936, %int1_6129 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_6130 = torch.constant.int 2
    %4941 = torch.aten.mul.Scalar %4940, %int2_6130 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_6131 = torch.constant.int 1
    %4942 = torch.aten.add.Tensor %4941, %4938, %int1_6131 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_6132 = torch.constant.int 32
    %4943 = torch.aten.mul.Scalar %4942, %int32_6132 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_6133 = torch.constant.int 1
    %4944 = torch.aten.add.Tensor %4943, %4930, %int1_6133 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_6134 = torch.constant.int 32
    %int2_6135 = torch.constant.int 2
    %int32_6136 = torch.constant.int 32
    %int8_6137 = torch.constant.int 8
    %int128_6138 = torch.constant.int 128
    %4945 = torch.prim.ListConstruct %437, %int32_6134, %int2_6135, %int32_6136, %int8_6137, %int128_6138 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4946 = torch.aten.view %4782, %4945 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4946, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_6139 = torch.constant.int 32
    %4947 = torch.aten.mul.int %437, %int32_6139 : !torch.int, !torch.int -> !torch.int
    %int2_6140 = torch.constant.int 2
    %4948 = torch.aten.mul.int %4947, %int2_6140 : !torch.int, !torch.int -> !torch.int
    %int32_6141 = torch.constant.int 32
    %4949 = torch.aten.mul.int %4948, %int32_6141 : !torch.int, !torch.int -> !torch.int
    %int8_6142 = torch.constant.int 8
    %int128_6143 = torch.constant.int 128
    %4950 = torch.prim.ListConstruct %4949, %int8_6142, %int128_6143 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4951 = torch.aten.view %4946, %4950 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %4951, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %4952 = torch.prim.ListConstruct %4944 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_6144 = torch.constant.bool false
    %4953 = torch.aten.index_put %4951, %4952, %4925, %false_6144 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %4953, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_6145 = torch.constant.int 32
    %int2_6146 = torch.constant.int 2
    %int32_6147 = torch.constant.int 32
    %int8_6148 = torch.constant.int 8
    %int128_6149 = torch.constant.int 128
    %4954 = torch.prim.ListConstruct %437, %int32_6145, %int2_6146, %int32_6147, %int8_6148, %int128_6149 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4955 = torch.aten.view %4953, %4954 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4955, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_6150 = torch.constant.int 2097152
    %4956 = torch.prim.ListConstruct %437, %int2097152_6150 : (!torch.int, !torch.int) -> !torch.list<int>
    %4957 = torch.aten.view %4955, %4956 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %4957, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_6151 = torch.constant.int 32
    %int2_6152 = torch.constant.int 2
    %int32_6153 = torch.constant.int 32
    %int8_6154 = torch.constant.int 8
    %int128_6155 = torch.constant.int 128
    %4958 = torch.prim.ListConstruct %437, %int32_6151, %int2_6152, %int32_6153, %int8_6154, %int128_6155 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4959 = torch.aten.view %4957, %4958 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4959, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int8_6156 = torch.constant.int 8
    %int128_6157 = torch.constant.int 128
    %4960 = torch.prim.ListConstruct %4949, %int8_6156, %int128_6157 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4961 = torch.aten.view %4959, %4960 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %4961, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_6158 = torch.constant.int 32
    %4962 = torch.aten.floor_divide.Scalar %arg2, %int32_6158 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_6159 = torch.constant.int 1
    %4963 = torch.aten.unsqueeze %4962, %int1_6159 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_6160 = torch.constant.int 1
    %false_6161 = torch.constant.bool false
    %4964 = torch.aten.gather %arg3, %int1_6160, %4963, %false_6161 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_6162 = torch.constant.int 32
    %4965 = torch.aten.remainder.Scalar %arg2, %int32_6162 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_6163 = torch.constant.int 1
    %4966 = torch.aten.unsqueeze %4965, %int1_6163 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_6164 = torch.constant.none
    %4967 = torch.aten.clone %248, %none_6164 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_6165 = torch.constant.int 0
    %4968 = torch.aten.unsqueeze %4967, %int0_6165 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_6166 = torch.constant.int 4
    %int1_6167 = torch.constant.int 1
    %4969 = torch.prim.ListConstruct %int4_6166, %int1_6167 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_6168 = torch.constant.int 1
    %int1_6169 = torch.constant.int 1
    %4970 = torch.prim.ListConstruct %int1_6168, %int1_6169 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_6170 = torch.constant.int 4
    %int0_6171 = torch.constant.int 0
    %cpu_6172 = torch.constant.device "cpu"
    %false_6173 = torch.constant.bool false
    %4971 = torch.aten.empty_strided %4969, %4970, %int4_6170, %int0_6171, %cpu_6172, %false_6173 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int22_6174 = torch.constant.int 22
    %4972 = torch.aten.fill.Scalar %4971, %int22_6174 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_6175 = torch.constant.int 4
    %int1_6176 = torch.constant.int 1
    %4973 = torch.prim.ListConstruct %int4_6175, %int1_6176 : (!torch.int, !torch.int) -> !torch.list<int>
    %4974 = torch.aten.repeat %4968, %4973 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_6177 = torch.constant.int 32
    %4975 = torch.aten.mul.Scalar %4964, %int32_6177 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_6178 = torch.constant.int 1
    %4976 = torch.aten.add.Tensor %4975, %4972, %int1_6178 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_6179 = torch.constant.int 2
    %4977 = torch.aten.mul.Scalar %4976, %int2_6179 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_6180 = torch.constant.int 1
    %4978 = torch.aten.add.Tensor %4977, %4974, %int1_6180 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_6181 = torch.constant.int 32
    %4979 = torch.aten.mul.Scalar %4978, %int32_6181 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_6182 = torch.constant.int 1
    %4980 = torch.aten.add.Tensor %4979, %4966, %int1_6182 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %4981 = torch.prim.ListConstruct %4980 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_6183 = torch.constant.bool false
    %4982 = torch.aten.index_put %4961, %4981, %4913, %false_6183 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %4982, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_6184 = torch.constant.int 32
    %int2_6185 = torch.constant.int 2
    %int32_6186 = torch.constant.int 32
    %int8_6187 = torch.constant.int 8
    %int128_6188 = torch.constant.int 128
    %4983 = torch.prim.ListConstruct %437, %int32_6184, %int2_6185, %int32_6186, %int8_6187, %int128_6188 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4984 = torch.aten.view %4982, %4983 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4984, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_6189 = torch.constant.int 2097152
    %4985 = torch.prim.ListConstruct %437, %int2097152_6189 : (!torch.int, !torch.int) -> !torch.list<int>
    %4986 = torch.aten.view %4984, %4985 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %4986, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int4_6190 = torch.constant.int 4
    %4987 = torch.prim.ListConstruct %int4_6190, %358 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_6191 = torch.constant.int 1
    %4988 = torch.prim.ListConstruct %358, %int1_6191 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_6192 = torch.constant.int 4
    %int0_6193 = torch.constant.int 0
    %cpu_6194 = torch.constant.device "cpu"
    %false_6195 = torch.constant.bool false
    %4989 = torch.aten.empty_strided %4987, %4988, %int4_6192, %int0_6193, %cpu_6194, %false_6195 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4989, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int22_6196 = torch.constant.int 22
    %4990 = torch.aten.fill.Scalar %4989, %int22_6196 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4990, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int32_6197 = torch.constant.int 32
    %4991 = torch.aten.mul.Scalar %arg3, %int32_6197 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4991, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_6198 = torch.constant.int 1
    %4992 = torch.aten.add.Tensor %4991, %4990, %int1_6198 : !torch.vtensor<[4,?],si64>, !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %4992, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_6199 = torch.constant.int 4
    %4993 = torch.aten.mul.int %int4_6199, %358 : !torch.int, !torch.int -> !torch.int
    %4994 = torch.prim.ListConstruct %4993 : (!torch.int) -> !torch.list<int>
    %4995 = torch.aten.view %4992, %4994 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %4995, [%356], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_6200 = torch.constant.int 32
    %int2_6201 = torch.constant.int 2
    %int32_6202 = torch.constant.int 32
    %int8_6203 = torch.constant.int 8
    %int128_6204 = torch.constant.int 128
    %4996 = torch.prim.ListConstruct %437, %int32_6200, %int2_6201, %int32_6202, %int8_6203, %int128_6204 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4997 = torch.aten.view %4986, %4996 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %4997, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_6205 = torch.constant.int 32
    %4998 = torch.aten.mul.int %437, %int32_6205 : !torch.int, !torch.int -> !torch.int
    %int2_6206 = torch.constant.int 2
    %int32_6207 = torch.constant.int 32
    %int8_6208 = torch.constant.int 8
    %int128_6209 = torch.constant.int 128
    %4999 = torch.prim.ListConstruct %4998, %int2_6206, %int32_6207, %int8_6208, %int128_6209 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5000 = torch.aten.view %4997, %4999 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %5000, [%357], affine_map<()[s0] -> (s0 * 32, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int0_6210 = torch.constant.int 0
    %5001 = torch.aten.index_select %5000, %int0_6210, %4995 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.int, !torch.vtensor<[?],si64> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %5001, [%356], affine_map<()[s0] -> (s0 * 4, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int4_6211 = torch.constant.int 4
    %int2_6212 = torch.constant.int 2
    %int32_6213 = torch.constant.int 32
    %int8_6214 = torch.constant.int 8
    %int128_6215 = torch.constant.int 128
    %5002 = torch.prim.ListConstruct %int4_6211, %358, %int2_6212, %int32_6213, %int8_6214, %int128_6215 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5003 = torch.aten.view %5001, %5002 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %5003, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int0_6216 = torch.constant.int 0
    %int0_6217 = torch.constant.int 0
    %int9223372036854775807_6218 = torch.constant.int 9223372036854775807
    %int1_6219 = torch.constant.int 1
    %5004 = torch.aten.slice.Tensor %5003, %int0_6216, %int0_6217, %int9223372036854775807_6218, %int1_6219 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %5004, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_6220 = torch.constant.int 1
    %int0_6221 = torch.constant.int 0
    %int9223372036854775807_6222 = torch.constant.int 9223372036854775807
    %int1_6223 = torch.constant.int 1
    %5005 = torch.aten.slice.Tensor %5004, %int1_6220, %int0_6221, %int9223372036854775807_6222, %int1_6223 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %5005, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_6224 = torch.constant.int 2
    %int0_6225 = torch.constant.int 0
    %5006 = torch.aten.select.int %5005, %int2_6224, %int0_6225 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5006, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int32_6226 = torch.constant.int 32
    %5007 = torch.aten.mul.int %358, %int32_6226 : !torch.int, !torch.int -> !torch.int
    %int2_6227 = torch.constant.int 2
    %int0_6228 = torch.constant.int 0
    %int1_6229 = torch.constant.int 1
    %5008 = torch.aten.slice.Tensor %5006, %int2_6227, %int0_6228, %5007, %int1_6229 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5008, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_6230 = torch.constant.int 0
    %5009 = torch.aten.clone %5008, %int0_6230 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5009, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_6231 = torch.constant.int 1
    %5010 = torch.aten.size.int %5005, %int1_6231 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_6232 = torch.constant.int 32
    %5011 = torch.aten.mul.int %5010, %int32_6232 : !torch.int, !torch.int -> !torch.int
    %int4_6233 = torch.constant.int 4
    %int8_6234 = torch.constant.int 8
    %int128_6235 = torch.constant.int 128
    %5012 = torch.prim.ListConstruct %int4_6233, %5011, %int8_6234, %int128_6235 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5013 = torch.aten._unsafe_view %5009, %5012 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %5013, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_6236 = torch.constant.int 0
    %int0_6237 = torch.constant.int 0
    %int9223372036854775807_6238 = torch.constant.int 9223372036854775807
    %int1_6239 = torch.constant.int 1
    %5014 = torch.aten.slice.Tensor %5013, %int0_6236, %int0_6237, %int9223372036854775807_6238, %int1_6239 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %5014, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_6240 = torch.constant.int 0
    %int0_6241 = torch.constant.int 0
    %int9223372036854775807_6242 = torch.constant.int 9223372036854775807
    %int1_6243 = torch.constant.int 1
    %5015 = torch.aten.slice.Tensor %5003, %int0_6240, %int0_6241, %int9223372036854775807_6242, %int1_6243 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %5015, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_6244 = torch.constant.int 1
    %int0_6245 = torch.constant.int 0
    %int9223372036854775807_6246 = torch.constant.int 9223372036854775807
    %int1_6247 = torch.constant.int 1
    %5016 = torch.aten.slice.Tensor %5015, %int1_6244, %int0_6245, %int9223372036854775807_6246, %int1_6247 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %5016, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_6248 = torch.constant.int 2
    %int1_6249 = torch.constant.int 1
    %5017 = torch.aten.select.int %5016, %int2_6248, %int1_6249 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5017, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int2_6250 = torch.constant.int 2
    %int0_6251 = torch.constant.int 0
    %int1_6252 = torch.constant.int 1
    %5018 = torch.aten.slice.Tensor %5017, %int2_6250, %int0_6251, %5007, %int1_6252 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5018, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_6253 = torch.constant.int 0
    %5019 = torch.aten.clone %5018, %int0_6253 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5019, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_6254 = torch.constant.int 1
    %5020 = torch.aten.size.int %5016, %int1_6254 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_6255 = torch.constant.int 32
    %5021 = torch.aten.mul.int %5020, %int32_6255 : !torch.int, !torch.int -> !torch.int
    %int4_6256 = torch.constant.int 4
    %int8_6257 = torch.constant.int 8
    %int128_6258 = torch.constant.int 128
    %5022 = torch.prim.ListConstruct %int4_6256, %5021, %int8_6257, %int128_6258 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5023 = torch.aten._unsafe_view %5019, %5022 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %5023, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_6259 = torch.constant.int 0
    %int0_6260 = torch.constant.int 0
    %int9223372036854775807_6261 = torch.constant.int 9223372036854775807
    %int1_6262 = torch.constant.int 1
    %5024 = torch.aten.slice.Tensor %5023, %int0_6259, %int0_6260, %int9223372036854775807_6261, %int1_6262 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %5024, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int-2_6263 = torch.constant.int -2
    %5025 = torch.aten.unsqueeze %5014, %int-2_6263 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %5025, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_6264 = torch.constant.int 1
    %5026 = torch.aten.size.int %5013, %int1_6264 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_6265 = torch.constant.int 4
    %int8_6266 = torch.constant.int 8
    %int4_6267 = torch.constant.int 4
    %int128_6268 = torch.constant.int 128
    %5027 = torch.prim.ListConstruct %int4_6265, %5026, %int8_6266, %int4_6267, %int128_6268 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_6269 = torch.constant.bool false
    %5028 = torch.aten.expand %5025, %5027, %false_6269 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5028, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_6270 = torch.constant.int 0
    %5029 = torch.aten.clone %5028, %int0_6270 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5029, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_6271 = torch.constant.int 4
    %int32_6272 = torch.constant.int 32
    %int128_6273 = torch.constant.int 128
    %5030 = torch.prim.ListConstruct %int4_6271, %5026, %int32_6272, %int128_6273 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5031 = torch.aten._unsafe_view %5029, %5030 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %5031, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_6274 = torch.constant.int -2
    %5032 = torch.aten.unsqueeze %5024, %int-2_6274 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %5032, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_6275 = torch.constant.int 1
    %5033 = torch.aten.size.int %5023, %int1_6275 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_6276 = torch.constant.int 4
    %int8_6277 = torch.constant.int 8
    %int4_6278 = torch.constant.int 4
    %int128_6279 = torch.constant.int 128
    %5034 = torch.prim.ListConstruct %int4_6276, %5033, %int8_6277, %int4_6278, %int128_6279 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_6280 = torch.constant.bool false
    %5035 = torch.aten.expand %5032, %5034, %false_6280 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5035, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_6281 = torch.constant.int 0
    %5036 = torch.aten.clone %5035, %int0_6281 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5036, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_6282 = torch.constant.int 4
    %int32_6283 = torch.constant.int 32
    %int128_6284 = torch.constant.int 128
    %5037 = torch.prim.ListConstruct %int4_6282, %5033, %int32_6283, %int128_6284 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5038 = torch.aten._unsafe_view %5036, %5037 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %5038, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_6285 = torch.constant.int 1
    %int2_6286 = torch.constant.int 2
    %5039 = torch.aten.transpose.int %4919, %int1_6285, %int2_6286 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,128],f16>
    %int1_6287 = torch.constant.int 1
    %int2_6288 = torch.constant.int 2
    %5040 = torch.aten.transpose.int %5031, %int1_6287, %int2_6288 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %5040, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_6289 = torch.constant.int 1
    %int2_6290 = torch.constant.int 2
    %5041 = torch.aten.transpose.int %5038, %int1_6289, %int2_6290 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %5041, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_6291 = torch.constant.float 0.000000e+00
    %false_6292 = torch.constant.bool false
    %none_6293 = torch.constant.none
    %5042:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5039, %5040, %5041, %float0.000000e00_6291, %false_6292, %368, %none_6293) : (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.vtensor<[4,1,1,?],f16>, !torch.none) -> (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,1],f32>) 
    %int1_6294 = torch.constant.int 1
    %int2_6295 = torch.constant.int 2
    %5043 = torch.aten.transpose.int %5042#0, %int1_6294, %int2_6295 : !torch.vtensor<[4,32,1,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int4_6296 = torch.constant.int 4
    %int1_6297 = torch.constant.int 1
    %int4096_6298 = torch.constant.int 4096
    %5044 = torch.prim.ListConstruct %int4_6296, %int1_6297, %int4096_6298 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5045 = torch.aten.view %5043, %5044 : !torch.vtensor<[4,1,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_6299 = torch.constant.int -2
    %int-1_6300 = torch.constant.int -1
    %5046 = torch.aten.transpose.int %249, %int-2_6299, %int-1_6300 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_6301 = torch.constant.int 4
    %int4096_6302 = torch.constant.int 4096
    %5047 = torch.prim.ListConstruct %int4_6301, %int4096_6302 : (!torch.int, !torch.int) -> !torch.list<int>
    %5048 = torch.aten.view %5045, %5047 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %5049 = torch.aten.mm %5048, %5046 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_6303 = torch.constant.int 4
    %int1_6304 = torch.constant.int 1
    %int4096_6305 = torch.constant.int 4096
    %5050 = torch.prim.ListConstruct %int4_6303, %int1_6304, %int4096_6305 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5051 = torch.aten.view %5049, %5050 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_6306 = torch.constant.int 1
    %5052 = torch.aten.add.Tensor %4879, %5051, %int1_6306 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_6307 = torch.constant.int 6
    %5053 = torch.prims.convert_element_type %5052, %int6_6307 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_6308 = torch.constant.int 2
    %5054 = torch.aten.pow.Tensor_Scalar %5053, %int2_6308 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_6309 = torch.constant.int -1
    %5055 = torch.prim.ListConstruct %int-1_6309 : (!torch.int) -> !torch.list<int>
    %true_6310 = torch.constant.bool true
    %none_6311 = torch.constant.none
    %5056 = torch.aten.mean.dim %5054, %5055, %true_6310, %none_6311 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_6312 = torch.constant.float 9.9999997473787516E-6
    %int1_6313 = torch.constant.int 1
    %5057 = torch.aten.add.Scalar %5056, %float9.999990e-06_6312, %int1_6313 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %5058 = torch.aten.rsqrt %5057 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %5059 = torch.aten.mul.Tensor %5053, %5058 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_6314 = torch.constant.int 5
    %5060 = torch.prims.convert_element_type %5059, %int5_6314 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %5061 = torch.aten.mul.Tensor %250, %5060 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_6315 = torch.constant.int 5
    %5062 = torch.prims.convert_element_type %5061, %int5_6315 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_6316 = torch.constant.int -2
    %int-1_6317 = torch.constant.int -1
    %5063 = torch.aten.transpose.int %251, %int-2_6316, %int-1_6317 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_6318 = torch.constant.int 4
    %int4096_6319 = torch.constant.int 4096
    %5064 = torch.prim.ListConstruct %int4_6318, %int4096_6319 : (!torch.int, !torch.int) -> !torch.list<int>
    %5065 = torch.aten.view %5062, %5064 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %5066 = torch.aten.mm %5065, %5063 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_6320 = torch.constant.int 4
    %int1_6321 = torch.constant.int 1
    %int14336_6322 = torch.constant.int 14336
    %5067 = torch.prim.ListConstruct %int4_6320, %int1_6321, %int14336_6322 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5068 = torch.aten.view %5066, %5067 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %5069 = torch.aten.silu %5068 : !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_6323 = torch.constant.int -2
    %int-1_6324 = torch.constant.int -1
    %5070 = torch.aten.transpose.int %252, %int-2_6323, %int-1_6324 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_6325 = torch.constant.int 4
    %int4096_6326 = torch.constant.int 4096
    %5071 = torch.prim.ListConstruct %int4_6325, %int4096_6326 : (!torch.int, !torch.int) -> !torch.list<int>
    %5072 = torch.aten.view %5062, %5071 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %5073 = torch.aten.mm %5072, %5070 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_6327 = torch.constant.int 4
    %int1_6328 = torch.constant.int 1
    %int14336_6329 = torch.constant.int 14336
    %5074 = torch.prim.ListConstruct %int4_6327, %int1_6328, %int14336_6329 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5075 = torch.aten.view %5073, %5074 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %5076 = torch.aten.mul.Tensor %5069, %5075 : !torch.vtensor<[4,1,14336],f16>, !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_6330 = torch.constant.int -2
    %int-1_6331 = torch.constant.int -1
    %5077 = torch.aten.transpose.int %253, %int-2_6330, %int-1_6331 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int4_6332 = torch.constant.int 4
    %int14336_6333 = torch.constant.int 14336
    %5078 = torch.prim.ListConstruct %int4_6332, %int14336_6333 : (!torch.int, !torch.int) -> !torch.list<int>
    %5079 = torch.aten.view %5076, %5078 : !torch.vtensor<[4,1,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,14336],f16>
    %5080 = torch.aten.mm %5079, %5077 : !torch.vtensor<[4,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_6334 = torch.constant.int 4
    %int1_6335 = torch.constant.int 1
    %int4096_6336 = torch.constant.int 4096
    %5081 = torch.prim.ListConstruct %int4_6334, %int1_6335, %int4096_6336 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5082 = torch.aten.view %5080, %5081 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_6337 = torch.constant.int 1
    %5083 = torch.aten.add.Tensor %5052, %5082, %int1_6337 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_6338 = torch.constant.int 6
    %5084 = torch.prims.convert_element_type %5083, %int6_6338 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_6339 = torch.constant.int 2
    %5085 = torch.aten.pow.Tensor_Scalar %5084, %int2_6339 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_6340 = torch.constant.int -1
    %5086 = torch.prim.ListConstruct %int-1_6340 : (!torch.int) -> !torch.list<int>
    %true_6341 = torch.constant.bool true
    %none_6342 = torch.constant.none
    %5087 = torch.aten.mean.dim %5085, %5086, %true_6341, %none_6342 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_6343 = torch.constant.float 9.9999997473787516E-6
    %int1_6344 = torch.constant.int 1
    %5088 = torch.aten.add.Scalar %5087, %float9.999990e-06_6343, %int1_6344 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %5089 = torch.aten.rsqrt %5088 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %5090 = torch.aten.mul.Tensor %5084, %5089 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_6345 = torch.constant.int 5
    %5091 = torch.prims.convert_element_type %5090, %int5_6345 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %5092 = torch.aten.mul.Tensor %254, %5091 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_6346 = torch.constant.int 5
    %5093 = torch.prims.convert_element_type %5092, %int5_6346 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_6347 = torch.constant.int -2
    %int-1_6348 = torch.constant.int -1
    %5094 = torch.aten.transpose.int %255, %int-2_6347, %int-1_6348 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_6349 = torch.constant.int 4
    %int4096_6350 = torch.constant.int 4096
    %5095 = torch.prim.ListConstruct %int4_6349, %int4096_6350 : (!torch.int, !torch.int) -> !torch.list<int>
    %5096 = torch.aten.view %5093, %5095 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %5097 = torch.aten.mm %5096, %5094 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_6351 = torch.constant.int 4
    %int1_6352 = torch.constant.int 1
    %int4096_6353 = torch.constant.int 4096
    %5098 = torch.prim.ListConstruct %int4_6351, %int1_6352, %int4096_6353 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5099 = torch.aten.view %5097, %5098 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_6354 = torch.constant.int -2
    %int-1_6355 = torch.constant.int -1
    %5100 = torch.aten.transpose.int %256, %int-2_6354, %int-1_6355 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_6356 = torch.constant.int 4
    %int4096_6357 = torch.constant.int 4096
    %5101 = torch.prim.ListConstruct %int4_6356, %int4096_6357 : (!torch.int, !torch.int) -> !torch.list<int>
    %5102 = torch.aten.view %5093, %5101 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %5103 = torch.aten.mm %5102, %5100 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_6358 = torch.constant.int 4
    %int1_6359 = torch.constant.int 1
    %int1024_6360 = torch.constant.int 1024
    %5104 = torch.prim.ListConstruct %int4_6358, %int1_6359, %int1024_6360 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5105 = torch.aten.view %5103, %5104 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int-2_6361 = torch.constant.int -2
    %int-1_6362 = torch.constant.int -1
    %5106 = torch.aten.transpose.int %257, %int-2_6361, %int-1_6362 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_6363 = torch.constant.int 4
    %int4096_6364 = torch.constant.int 4096
    %5107 = torch.prim.ListConstruct %int4_6363, %int4096_6364 : (!torch.int, !torch.int) -> !torch.list<int>
    %5108 = torch.aten.view %5093, %5107 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %5109 = torch.aten.mm %5108, %5106 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_6365 = torch.constant.int 4
    %int1_6366 = torch.constant.int 1
    %int1024_6367 = torch.constant.int 1024
    %5110 = torch.prim.ListConstruct %int4_6365, %int1_6366, %int1024_6367 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5111 = torch.aten.view %5109, %5110 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int4_6368 = torch.constant.int 4
    %int1_6369 = torch.constant.int 1
    %int32_6370 = torch.constant.int 32
    %int128_6371 = torch.constant.int 128
    %5112 = torch.prim.ListConstruct %int4_6368, %int1_6369, %int32_6370, %int128_6371 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5113 = torch.aten.view %5099, %5112 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,128],f16>
    %int4_6372 = torch.constant.int 4
    %int1_6373 = torch.constant.int 1
    %int8_6374 = torch.constant.int 8
    %int128_6375 = torch.constant.int 128
    %5114 = torch.prim.ListConstruct %int4_6372, %int1_6373, %int8_6374, %int128_6375 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5115 = torch.aten.view %5105, %5114 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int4_6376 = torch.constant.int 4
    %int1_6377 = torch.constant.int 1
    %int8_6378 = torch.constant.int 8
    %int128_6379 = torch.constant.int 128
    %5116 = torch.prim.ListConstruct %int4_6376, %int1_6377, %int8_6378, %int128_6379 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5117 = torch.aten.view %5111, %5116 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int6_6380 = torch.constant.int 6
    %5118 = torch.prims.convert_element_type %5113, %int6_6380 : !torch.vtensor<[4,1,32,128],f16>, !torch.int -> !torch.vtensor<[4,1,32,128],f32>
    %5119 = torch_c.to_builtin_tensor %5118 : !torch.vtensor<[4,1,32,128],f32> -> tensor<4x1x32x128xf32>
    %5120 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %5121 = util.call @sharktank_rotary_embedding_4_1_32_128_f32(%5119, %5120) : (tensor<4x1x32x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x32x128xf32>
    %5122 = torch_c.from_builtin_tensor %5121 : tensor<4x1x32x128xf32> -> !torch.vtensor<[4,1,32,128],f32>
    %int5_6381 = torch.constant.int 5
    %5123 = torch.prims.convert_element_type %5122, %int5_6381 : !torch.vtensor<[4,1,32,128],f32>, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int6_6382 = torch.constant.int 6
    %5124 = torch.prims.convert_element_type %5115, %int6_6382 : !torch.vtensor<[4,1,8,128],f16>, !torch.int -> !torch.vtensor<[4,1,8,128],f32>
    %5125 = torch_c.to_builtin_tensor %5124 : !torch.vtensor<[4,1,8,128],f32> -> tensor<4x1x8x128xf32>
    %5126 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %5127 = util.call @sharktank_rotary_embedding_4_1_8_128_f32(%5125, %5126) : (tensor<4x1x8x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x8x128xf32>
    %5128 = torch_c.from_builtin_tensor %5127 : tensor<4x1x8x128xf32> -> !torch.vtensor<[4,1,8,128],f32>
    %int5_6383 = torch.constant.int 5
    %5129 = torch.prims.convert_element_type %5128, %int5_6383 : !torch.vtensor<[4,1,8,128],f32>, !torch.int -> !torch.vtensor<[4,1,8,128],f16>
    %int32_6384 = torch.constant.int 32
    %5130 = torch.aten.floor_divide.Scalar %arg2, %int32_6384 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_6385 = torch.constant.int 1
    %5131 = torch.aten.unsqueeze %5130, %int1_6385 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_6386 = torch.constant.int 1
    %false_6387 = torch.constant.bool false
    %5132 = torch.aten.gather %arg3, %int1_6386, %5131, %false_6387 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_6388 = torch.constant.int 32
    %5133 = torch.aten.remainder.Scalar %arg2, %int32_6388 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_6389 = torch.constant.int 1
    %5134 = torch.aten.unsqueeze %5133, %int1_6389 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_6390 = torch.constant.none
    %5135 = torch.aten.clone %258, %none_6390 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_6391 = torch.constant.int 0
    %5136 = torch.aten.unsqueeze %5135, %int0_6391 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_6392 = torch.constant.int 4
    %int1_6393 = torch.constant.int 1
    %5137 = torch.prim.ListConstruct %int4_6392, %int1_6393 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_6394 = torch.constant.int 1
    %int1_6395 = torch.constant.int 1
    %5138 = torch.prim.ListConstruct %int1_6394, %int1_6395 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_6396 = torch.constant.int 4
    %int0_6397 = torch.constant.int 0
    %cpu_6398 = torch.constant.device "cpu"
    %false_6399 = torch.constant.bool false
    %5139 = torch.aten.empty_strided %5137, %5138, %int4_6396, %int0_6397, %cpu_6398, %false_6399 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int23 = torch.constant.int 23
    %5140 = torch.aten.fill.Scalar %5139, %int23 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_6400 = torch.constant.int 4
    %int1_6401 = torch.constant.int 1
    %5141 = torch.prim.ListConstruct %int4_6400, %int1_6401 : (!torch.int, !torch.int) -> !torch.list<int>
    %5142 = torch.aten.repeat %5136, %5141 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_6402 = torch.constant.int 32
    %5143 = torch.aten.mul.Scalar %5132, %int32_6402 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_6403 = torch.constant.int 1
    %5144 = torch.aten.add.Tensor %5143, %5140, %int1_6403 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_6404 = torch.constant.int 2
    %5145 = torch.aten.mul.Scalar %5144, %int2_6404 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_6405 = torch.constant.int 1
    %5146 = torch.aten.add.Tensor %5145, %5142, %int1_6405 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_6406 = torch.constant.int 32
    %5147 = torch.aten.mul.Scalar %5146, %int32_6406 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_6407 = torch.constant.int 1
    %5148 = torch.aten.add.Tensor %5147, %5134, %int1_6407 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_6408 = torch.constant.int 32
    %int2_6409 = torch.constant.int 2
    %int32_6410 = torch.constant.int 32
    %int8_6411 = torch.constant.int 8
    %int128_6412 = torch.constant.int 128
    %5149 = torch.prim.ListConstruct %437, %int32_6408, %int2_6409, %int32_6410, %int8_6411, %int128_6412 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5150 = torch.aten.view %4986, %5149 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5150, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_6413 = torch.constant.int 32
    %5151 = torch.aten.mul.int %437, %int32_6413 : !torch.int, !torch.int -> !torch.int
    %int2_6414 = torch.constant.int 2
    %5152 = torch.aten.mul.int %5151, %int2_6414 : !torch.int, !torch.int -> !torch.int
    %int32_6415 = torch.constant.int 32
    %5153 = torch.aten.mul.int %5152, %int32_6415 : !torch.int, !torch.int -> !torch.int
    %int8_6416 = torch.constant.int 8
    %int128_6417 = torch.constant.int 128
    %5154 = torch.prim.ListConstruct %5153, %int8_6416, %int128_6417 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5155 = torch.aten.view %5150, %5154 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %5155, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %5156 = torch.prim.ListConstruct %5148 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_6418 = torch.constant.bool false
    %5157 = torch.aten.index_put %5155, %5156, %5129, %false_6418 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %5157, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_6419 = torch.constant.int 32
    %int2_6420 = torch.constant.int 2
    %int32_6421 = torch.constant.int 32
    %int8_6422 = torch.constant.int 8
    %int128_6423 = torch.constant.int 128
    %5158 = torch.prim.ListConstruct %437, %int32_6419, %int2_6420, %int32_6421, %int8_6422, %int128_6423 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5159 = torch.aten.view %5157, %5158 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5159, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_6424 = torch.constant.int 2097152
    %5160 = torch.prim.ListConstruct %437, %int2097152_6424 : (!torch.int, !torch.int) -> !torch.list<int>
    %5161 = torch.aten.view %5159, %5160 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %5161, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_6425 = torch.constant.int 32
    %int2_6426 = torch.constant.int 2
    %int32_6427 = torch.constant.int 32
    %int8_6428 = torch.constant.int 8
    %int128_6429 = torch.constant.int 128
    %5162 = torch.prim.ListConstruct %437, %int32_6425, %int2_6426, %int32_6427, %int8_6428, %int128_6429 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5163 = torch.aten.view %5161, %5162 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5163, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int8_6430 = torch.constant.int 8
    %int128_6431 = torch.constant.int 128
    %5164 = torch.prim.ListConstruct %5153, %int8_6430, %int128_6431 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5165 = torch.aten.view %5163, %5164 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %5165, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_6432 = torch.constant.int 32
    %5166 = torch.aten.floor_divide.Scalar %arg2, %int32_6432 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_6433 = torch.constant.int 1
    %5167 = torch.aten.unsqueeze %5166, %int1_6433 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_6434 = torch.constant.int 1
    %false_6435 = torch.constant.bool false
    %5168 = torch.aten.gather %arg3, %int1_6434, %5167, %false_6435 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_6436 = torch.constant.int 32
    %5169 = torch.aten.remainder.Scalar %arg2, %int32_6436 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_6437 = torch.constant.int 1
    %5170 = torch.aten.unsqueeze %5169, %int1_6437 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_6438 = torch.constant.none
    %5171 = torch.aten.clone %259, %none_6438 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_6439 = torch.constant.int 0
    %5172 = torch.aten.unsqueeze %5171, %int0_6439 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_6440 = torch.constant.int 4
    %int1_6441 = torch.constant.int 1
    %5173 = torch.prim.ListConstruct %int4_6440, %int1_6441 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_6442 = torch.constant.int 1
    %int1_6443 = torch.constant.int 1
    %5174 = torch.prim.ListConstruct %int1_6442, %int1_6443 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_6444 = torch.constant.int 4
    %int0_6445 = torch.constant.int 0
    %cpu_6446 = torch.constant.device "cpu"
    %false_6447 = torch.constant.bool false
    %5175 = torch.aten.empty_strided %5173, %5174, %int4_6444, %int0_6445, %cpu_6446, %false_6447 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int23_6448 = torch.constant.int 23
    %5176 = torch.aten.fill.Scalar %5175, %int23_6448 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_6449 = torch.constant.int 4
    %int1_6450 = torch.constant.int 1
    %5177 = torch.prim.ListConstruct %int4_6449, %int1_6450 : (!torch.int, !torch.int) -> !torch.list<int>
    %5178 = torch.aten.repeat %5172, %5177 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_6451 = torch.constant.int 32
    %5179 = torch.aten.mul.Scalar %5168, %int32_6451 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_6452 = torch.constant.int 1
    %5180 = torch.aten.add.Tensor %5179, %5176, %int1_6452 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_6453 = torch.constant.int 2
    %5181 = torch.aten.mul.Scalar %5180, %int2_6453 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_6454 = torch.constant.int 1
    %5182 = torch.aten.add.Tensor %5181, %5178, %int1_6454 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_6455 = torch.constant.int 32
    %5183 = torch.aten.mul.Scalar %5182, %int32_6455 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_6456 = torch.constant.int 1
    %5184 = torch.aten.add.Tensor %5183, %5170, %int1_6456 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %5185 = torch.prim.ListConstruct %5184 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_6457 = torch.constant.bool false
    %5186 = torch.aten.index_put %5165, %5185, %5117, %false_6457 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %5186, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_6458 = torch.constant.int 32
    %int2_6459 = torch.constant.int 2
    %int32_6460 = torch.constant.int 32
    %int8_6461 = torch.constant.int 8
    %int128_6462 = torch.constant.int 128
    %5187 = torch.prim.ListConstruct %437, %int32_6458, %int2_6459, %int32_6460, %int8_6461, %int128_6462 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5188 = torch.aten.view %5186, %5187 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5188, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_6463 = torch.constant.int 2097152
    %5189 = torch.prim.ListConstruct %437, %int2097152_6463 : (!torch.int, !torch.int) -> !torch.list<int>
    %5190 = torch.aten.view %5188, %5189 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %5190, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int4_6464 = torch.constant.int 4
    %5191 = torch.prim.ListConstruct %int4_6464, %358 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_6465 = torch.constant.int 1
    %5192 = torch.prim.ListConstruct %358, %int1_6465 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_6466 = torch.constant.int 4
    %int0_6467 = torch.constant.int 0
    %cpu_6468 = torch.constant.device "cpu"
    %false_6469 = torch.constant.bool false
    %5193 = torch.aten.empty_strided %5191, %5192, %int4_6466, %int0_6467, %cpu_6468, %false_6469 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5193, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int23_6470 = torch.constant.int 23
    %5194 = torch.aten.fill.Scalar %5193, %int23_6470 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5194, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int32_6471 = torch.constant.int 32
    %5195 = torch.aten.mul.Scalar %arg3, %int32_6471 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5195, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_6472 = torch.constant.int 1
    %5196 = torch.aten.add.Tensor %5195, %5194, %int1_6472 : !torch.vtensor<[4,?],si64>, !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5196, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_6473 = torch.constant.int 4
    %5197 = torch.aten.mul.int %int4_6473, %358 : !torch.int, !torch.int -> !torch.int
    %5198 = torch.prim.ListConstruct %5197 : (!torch.int) -> !torch.list<int>
    %5199 = torch.aten.view %5196, %5198 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5199, [%356], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_6474 = torch.constant.int 32
    %int2_6475 = torch.constant.int 2
    %int32_6476 = torch.constant.int 32
    %int8_6477 = torch.constant.int 8
    %int128_6478 = torch.constant.int 128
    %5200 = torch.prim.ListConstruct %437, %int32_6474, %int2_6475, %int32_6476, %int8_6477, %int128_6478 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5201 = torch.aten.view %5190, %5200 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5201, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_6479 = torch.constant.int 32
    %5202 = torch.aten.mul.int %437, %int32_6479 : !torch.int, !torch.int -> !torch.int
    %int2_6480 = torch.constant.int 2
    %int32_6481 = torch.constant.int 32
    %int8_6482 = torch.constant.int 8
    %int128_6483 = torch.constant.int 128
    %5203 = torch.prim.ListConstruct %5202, %int2_6480, %int32_6481, %int8_6482, %int128_6483 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5204 = torch.aten.view %5201, %5203 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %5204, [%357], affine_map<()[s0] -> (s0 * 32, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int0_6484 = torch.constant.int 0
    %5205 = torch.aten.index_select %5204, %int0_6484, %5199 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.int, !torch.vtensor<[?],si64> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %5205, [%356], affine_map<()[s0] -> (s0 * 4, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int4_6485 = torch.constant.int 4
    %int2_6486 = torch.constant.int 2
    %int32_6487 = torch.constant.int 32
    %int8_6488 = torch.constant.int 8
    %int128_6489 = torch.constant.int 128
    %5206 = torch.prim.ListConstruct %int4_6485, %358, %int2_6486, %int32_6487, %int8_6488, %int128_6489 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5207 = torch.aten.view %5205, %5206 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %5207, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int0_6490 = torch.constant.int 0
    %int0_6491 = torch.constant.int 0
    %int9223372036854775807_6492 = torch.constant.int 9223372036854775807
    %int1_6493 = torch.constant.int 1
    %5208 = torch.aten.slice.Tensor %5207, %int0_6490, %int0_6491, %int9223372036854775807_6492, %int1_6493 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %5208, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_6494 = torch.constant.int 1
    %int0_6495 = torch.constant.int 0
    %int9223372036854775807_6496 = torch.constant.int 9223372036854775807
    %int1_6497 = torch.constant.int 1
    %5209 = torch.aten.slice.Tensor %5208, %int1_6494, %int0_6495, %int9223372036854775807_6496, %int1_6497 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %5209, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_6498 = torch.constant.int 2
    %int0_6499 = torch.constant.int 0
    %5210 = torch.aten.select.int %5209, %int2_6498, %int0_6499 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5210, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int32_6500 = torch.constant.int 32
    %5211 = torch.aten.mul.int %358, %int32_6500 : !torch.int, !torch.int -> !torch.int
    %int2_6501 = torch.constant.int 2
    %int0_6502 = torch.constant.int 0
    %int1_6503 = torch.constant.int 1
    %5212 = torch.aten.slice.Tensor %5210, %int2_6501, %int0_6502, %5211, %int1_6503 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5212, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_6504 = torch.constant.int 0
    %5213 = torch.aten.clone %5212, %int0_6504 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5213, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_6505 = torch.constant.int 1
    %5214 = torch.aten.size.int %5209, %int1_6505 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_6506 = torch.constant.int 32
    %5215 = torch.aten.mul.int %5214, %int32_6506 : !torch.int, !torch.int -> !torch.int
    %int4_6507 = torch.constant.int 4
    %int8_6508 = torch.constant.int 8
    %int128_6509 = torch.constant.int 128
    %5216 = torch.prim.ListConstruct %int4_6507, %5215, %int8_6508, %int128_6509 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5217 = torch.aten._unsafe_view %5213, %5216 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %5217, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_6510 = torch.constant.int 0
    %int0_6511 = torch.constant.int 0
    %int9223372036854775807_6512 = torch.constant.int 9223372036854775807
    %int1_6513 = torch.constant.int 1
    %5218 = torch.aten.slice.Tensor %5217, %int0_6510, %int0_6511, %int9223372036854775807_6512, %int1_6513 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %5218, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_6514 = torch.constant.int 0
    %int0_6515 = torch.constant.int 0
    %int9223372036854775807_6516 = torch.constant.int 9223372036854775807
    %int1_6517 = torch.constant.int 1
    %5219 = torch.aten.slice.Tensor %5207, %int0_6514, %int0_6515, %int9223372036854775807_6516, %int1_6517 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %5219, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_6518 = torch.constant.int 1
    %int0_6519 = torch.constant.int 0
    %int9223372036854775807_6520 = torch.constant.int 9223372036854775807
    %int1_6521 = torch.constant.int 1
    %5220 = torch.aten.slice.Tensor %5219, %int1_6518, %int0_6519, %int9223372036854775807_6520, %int1_6521 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %5220, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_6522 = torch.constant.int 2
    %int1_6523 = torch.constant.int 1
    %5221 = torch.aten.select.int %5220, %int2_6522, %int1_6523 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5221, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int2_6524 = torch.constant.int 2
    %int0_6525 = torch.constant.int 0
    %int1_6526 = torch.constant.int 1
    %5222 = torch.aten.slice.Tensor %5221, %int2_6524, %int0_6525, %5211, %int1_6526 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5222, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_6527 = torch.constant.int 0
    %5223 = torch.aten.clone %5222, %int0_6527 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5223, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_6528 = torch.constant.int 1
    %5224 = torch.aten.size.int %5220, %int1_6528 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_6529 = torch.constant.int 32
    %5225 = torch.aten.mul.int %5224, %int32_6529 : !torch.int, !torch.int -> !torch.int
    %int4_6530 = torch.constant.int 4
    %int8_6531 = torch.constant.int 8
    %int128_6532 = torch.constant.int 128
    %5226 = torch.prim.ListConstruct %int4_6530, %5225, %int8_6531, %int128_6532 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5227 = torch.aten._unsafe_view %5223, %5226 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %5227, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_6533 = torch.constant.int 0
    %int0_6534 = torch.constant.int 0
    %int9223372036854775807_6535 = torch.constant.int 9223372036854775807
    %int1_6536 = torch.constant.int 1
    %5228 = torch.aten.slice.Tensor %5227, %int0_6533, %int0_6534, %int9223372036854775807_6535, %int1_6536 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %5228, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int-2_6537 = torch.constant.int -2
    %5229 = torch.aten.unsqueeze %5218, %int-2_6537 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %5229, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_6538 = torch.constant.int 1
    %5230 = torch.aten.size.int %5217, %int1_6538 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_6539 = torch.constant.int 4
    %int8_6540 = torch.constant.int 8
    %int4_6541 = torch.constant.int 4
    %int128_6542 = torch.constant.int 128
    %5231 = torch.prim.ListConstruct %int4_6539, %5230, %int8_6540, %int4_6541, %int128_6542 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_6543 = torch.constant.bool false
    %5232 = torch.aten.expand %5229, %5231, %false_6543 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5232, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_6544 = torch.constant.int 0
    %5233 = torch.aten.clone %5232, %int0_6544 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5233, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_6545 = torch.constant.int 4
    %int32_6546 = torch.constant.int 32
    %int128_6547 = torch.constant.int 128
    %5234 = torch.prim.ListConstruct %int4_6545, %5230, %int32_6546, %int128_6547 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5235 = torch.aten._unsafe_view %5233, %5234 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %5235, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_6548 = torch.constant.int -2
    %5236 = torch.aten.unsqueeze %5228, %int-2_6548 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %5236, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_6549 = torch.constant.int 1
    %5237 = torch.aten.size.int %5227, %int1_6549 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_6550 = torch.constant.int 4
    %int8_6551 = torch.constant.int 8
    %int4_6552 = torch.constant.int 4
    %int128_6553 = torch.constant.int 128
    %5238 = torch.prim.ListConstruct %int4_6550, %5237, %int8_6551, %int4_6552, %int128_6553 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_6554 = torch.constant.bool false
    %5239 = torch.aten.expand %5236, %5238, %false_6554 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5239, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_6555 = torch.constant.int 0
    %5240 = torch.aten.clone %5239, %int0_6555 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5240, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_6556 = torch.constant.int 4
    %int32_6557 = torch.constant.int 32
    %int128_6558 = torch.constant.int 128
    %5241 = torch.prim.ListConstruct %int4_6556, %5237, %int32_6557, %int128_6558 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5242 = torch.aten._unsafe_view %5240, %5241 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %5242, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_6559 = torch.constant.int 1
    %int2_6560 = torch.constant.int 2
    %5243 = torch.aten.transpose.int %5123, %int1_6559, %int2_6560 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,128],f16>
    %int1_6561 = torch.constant.int 1
    %int2_6562 = torch.constant.int 2
    %5244 = torch.aten.transpose.int %5235, %int1_6561, %int2_6562 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %5244, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_6563 = torch.constant.int 1
    %int2_6564 = torch.constant.int 2
    %5245 = torch.aten.transpose.int %5242, %int1_6563, %int2_6564 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %5245, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_6565 = torch.constant.float 0.000000e+00
    %false_6566 = torch.constant.bool false
    %none_6567 = torch.constant.none
    %5246:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5243, %5244, %5245, %float0.000000e00_6565, %false_6566, %368, %none_6567) : (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.vtensor<[4,1,1,?],f16>, !torch.none) -> (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,1],f32>) 
    %int1_6568 = torch.constant.int 1
    %int2_6569 = torch.constant.int 2
    %5247 = torch.aten.transpose.int %5246#0, %int1_6568, %int2_6569 : !torch.vtensor<[4,32,1,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int4_6570 = torch.constant.int 4
    %int1_6571 = torch.constant.int 1
    %int4096_6572 = torch.constant.int 4096
    %5248 = torch.prim.ListConstruct %int4_6570, %int1_6571, %int4096_6572 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5249 = torch.aten.view %5247, %5248 : !torch.vtensor<[4,1,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_6573 = torch.constant.int -2
    %int-1_6574 = torch.constant.int -1
    %5250 = torch.aten.transpose.int %260, %int-2_6573, %int-1_6574 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_6575 = torch.constant.int 4
    %int4096_6576 = torch.constant.int 4096
    %5251 = torch.prim.ListConstruct %int4_6575, %int4096_6576 : (!torch.int, !torch.int) -> !torch.list<int>
    %5252 = torch.aten.view %5249, %5251 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %5253 = torch.aten.mm %5252, %5250 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_6577 = torch.constant.int 4
    %int1_6578 = torch.constant.int 1
    %int4096_6579 = torch.constant.int 4096
    %5254 = torch.prim.ListConstruct %int4_6577, %int1_6578, %int4096_6579 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5255 = torch.aten.view %5253, %5254 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_6580 = torch.constant.int 1
    %5256 = torch.aten.add.Tensor %5083, %5255, %int1_6580 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_6581 = torch.constant.int 6
    %5257 = torch.prims.convert_element_type %5256, %int6_6581 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_6582 = torch.constant.int 2
    %5258 = torch.aten.pow.Tensor_Scalar %5257, %int2_6582 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_6583 = torch.constant.int -1
    %5259 = torch.prim.ListConstruct %int-1_6583 : (!torch.int) -> !torch.list<int>
    %true_6584 = torch.constant.bool true
    %none_6585 = torch.constant.none
    %5260 = torch.aten.mean.dim %5258, %5259, %true_6584, %none_6585 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_6586 = torch.constant.float 9.9999997473787516E-6
    %int1_6587 = torch.constant.int 1
    %5261 = torch.aten.add.Scalar %5260, %float9.999990e-06_6586, %int1_6587 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %5262 = torch.aten.rsqrt %5261 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %5263 = torch.aten.mul.Tensor %5257, %5262 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_6588 = torch.constant.int 5
    %5264 = torch.prims.convert_element_type %5263, %int5_6588 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %5265 = torch.aten.mul.Tensor %261, %5264 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_6589 = torch.constant.int 5
    %5266 = torch.prims.convert_element_type %5265, %int5_6589 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_6590 = torch.constant.int -2
    %int-1_6591 = torch.constant.int -1
    %5267 = torch.aten.transpose.int %262, %int-2_6590, %int-1_6591 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_6592 = torch.constant.int 4
    %int4096_6593 = torch.constant.int 4096
    %5268 = torch.prim.ListConstruct %int4_6592, %int4096_6593 : (!torch.int, !torch.int) -> !torch.list<int>
    %5269 = torch.aten.view %5266, %5268 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %5270 = torch.aten.mm %5269, %5267 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_6594 = torch.constant.int 4
    %int1_6595 = torch.constant.int 1
    %int14336_6596 = torch.constant.int 14336
    %5271 = torch.prim.ListConstruct %int4_6594, %int1_6595, %int14336_6596 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5272 = torch.aten.view %5270, %5271 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %5273 = torch.aten.silu %5272 : !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_6597 = torch.constant.int -2
    %int-1_6598 = torch.constant.int -1
    %5274 = torch.aten.transpose.int %263, %int-2_6597, %int-1_6598 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_6599 = torch.constant.int 4
    %int4096_6600 = torch.constant.int 4096
    %5275 = torch.prim.ListConstruct %int4_6599, %int4096_6600 : (!torch.int, !torch.int) -> !torch.list<int>
    %5276 = torch.aten.view %5266, %5275 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %5277 = torch.aten.mm %5276, %5274 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_6601 = torch.constant.int 4
    %int1_6602 = torch.constant.int 1
    %int14336_6603 = torch.constant.int 14336
    %5278 = torch.prim.ListConstruct %int4_6601, %int1_6602, %int14336_6603 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5279 = torch.aten.view %5277, %5278 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %5280 = torch.aten.mul.Tensor %5273, %5279 : !torch.vtensor<[4,1,14336],f16>, !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_6604 = torch.constant.int -2
    %int-1_6605 = torch.constant.int -1
    %5281 = torch.aten.transpose.int %264, %int-2_6604, %int-1_6605 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int4_6606 = torch.constant.int 4
    %int14336_6607 = torch.constant.int 14336
    %5282 = torch.prim.ListConstruct %int4_6606, %int14336_6607 : (!torch.int, !torch.int) -> !torch.list<int>
    %5283 = torch.aten.view %5280, %5282 : !torch.vtensor<[4,1,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,14336],f16>
    %5284 = torch.aten.mm %5283, %5281 : !torch.vtensor<[4,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_6608 = torch.constant.int 4
    %int1_6609 = torch.constant.int 1
    %int4096_6610 = torch.constant.int 4096
    %5285 = torch.prim.ListConstruct %int4_6608, %int1_6609, %int4096_6610 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5286 = torch.aten.view %5284, %5285 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_6611 = torch.constant.int 1
    %5287 = torch.aten.add.Tensor %5256, %5286, %int1_6611 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_6612 = torch.constant.int 6
    %5288 = torch.prims.convert_element_type %5287, %int6_6612 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_6613 = torch.constant.int 2
    %5289 = torch.aten.pow.Tensor_Scalar %5288, %int2_6613 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_6614 = torch.constant.int -1
    %5290 = torch.prim.ListConstruct %int-1_6614 : (!torch.int) -> !torch.list<int>
    %true_6615 = torch.constant.bool true
    %none_6616 = torch.constant.none
    %5291 = torch.aten.mean.dim %5289, %5290, %true_6615, %none_6616 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_6617 = torch.constant.float 9.9999997473787516E-6
    %int1_6618 = torch.constant.int 1
    %5292 = torch.aten.add.Scalar %5291, %float9.999990e-06_6617, %int1_6618 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %5293 = torch.aten.rsqrt %5292 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %5294 = torch.aten.mul.Tensor %5288, %5293 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_6619 = torch.constant.int 5
    %5295 = torch.prims.convert_element_type %5294, %int5_6619 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %5296 = torch.aten.mul.Tensor %265, %5295 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_6620 = torch.constant.int 5
    %5297 = torch.prims.convert_element_type %5296, %int5_6620 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_6621 = torch.constant.int -2
    %int-1_6622 = torch.constant.int -1
    %5298 = torch.aten.transpose.int %266, %int-2_6621, %int-1_6622 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_6623 = torch.constant.int 4
    %int4096_6624 = torch.constant.int 4096
    %5299 = torch.prim.ListConstruct %int4_6623, %int4096_6624 : (!torch.int, !torch.int) -> !torch.list<int>
    %5300 = torch.aten.view %5297, %5299 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %5301 = torch.aten.mm %5300, %5298 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_6625 = torch.constant.int 4
    %int1_6626 = torch.constant.int 1
    %int4096_6627 = torch.constant.int 4096
    %5302 = torch.prim.ListConstruct %int4_6625, %int1_6626, %int4096_6627 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5303 = torch.aten.view %5301, %5302 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_6628 = torch.constant.int -2
    %int-1_6629 = torch.constant.int -1
    %5304 = torch.aten.transpose.int %267, %int-2_6628, %int-1_6629 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_6630 = torch.constant.int 4
    %int4096_6631 = torch.constant.int 4096
    %5305 = torch.prim.ListConstruct %int4_6630, %int4096_6631 : (!torch.int, !torch.int) -> !torch.list<int>
    %5306 = torch.aten.view %5297, %5305 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %5307 = torch.aten.mm %5306, %5304 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_6632 = torch.constant.int 4
    %int1_6633 = torch.constant.int 1
    %int1024_6634 = torch.constant.int 1024
    %5308 = torch.prim.ListConstruct %int4_6632, %int1_6633, %int1024_6634 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5309 = torch.aten.view %5307, %5308 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int-2_6635 = torch.constant.int -2
    %int-1_6636 = torch.constant.int -1
    %5310 = torch.aten.transpose.int %268, %int-2_6635, %int-1_6636 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_6637 = torch.constant.int 4
    %int4096_6638 = torch.constant.int 4096
    %5311 = torch.prim.ListConstruct %int4_6637, %int4096_6638 : (!torch.int, !torch.int) -> !torch.list<int>
    %5312 = torch.aten.view %5297, %5311 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %5313 = torch.aten.mm %5312, %5310 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_6639 = torch.constant.int 4
    %int1_6640 = torch.constant.int 1
    %int1024_6641 = torch.constant.int 1024
    %5314 = torch.prim.ListConstruct %int4_6639, %int1_6640, %int1024_6641 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5315 = torch.aten.view %5313, %5314 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int4_6642 = torch.constant.int 4
    %int1_6643 = torch.constant.int 1
    %int32_6644 = torch.constant.int 32
    %int128_6645 = torch.constant.int 128
    %5316 = torch.prim.ListConstruct %int4_6642, %int1_6643, %int32_6644, %int128_6645 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5317 = torch.aten.view %5303, %5316 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,128],f16>
    %int4_6646 = torch.constant.int 4
    %int1_6647 = torch.constant.int 1
    %int8_6648 = torch.constant.int 8
    %int128_6649 = torch.constant.int 128
    %5318 = torch.prim.ListConstruct %int4_6646, %int1_6647, %int8_6648, %int128_6649 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5319 = torch.aten.view %5309, %5318 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int4_6650 = torch.constant.int 4
    %int1_6651 = torch.constant.int 1
    %int8_6652 = torch.constant.int 8
    %int128_6653 = torch.constant.int 128
    %5320 = torch.prim.ListConstruct %int4_6650, %int1_6651, %int8_6652, %int128_6653 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5321 = torch.aten.view %5315, %5320 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int6_6654 = torch.constant.int 6
    %5322 = torch.prims.convert_element_type %5317, %int6_6654 : !torch.vtensor<[4,1,32,128],f16>, !torch.int -> !torch.vtensor<[4,1,32,128],f32>
    %5323 = torch_c.to_builtin_tensor %5322 : !torch.vtensor<[4,1,32,128],f32> -> tensor<4x1x32x128xf32>
    %5324 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %5325 = util.call @sharktank_rotary_embedding_4_1_32_128_f32(%5323, %5324) : (tensor<4x1x32x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x32x128xf32>
    %5326 = torch_c.from_builtin_tensor %5325 : tensor<4x1x32x128xf32> -> !torch.vtensor<[4,1,32,128],f32>
    %int5_6655 = torch.constant.int 5
    %5327 = torch.prims.convert_element_type %5326, %int5_6655 : !torch.vtensor<[4,1,32,128],f32>, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int6_6656 = torch.constant.int 6
    %5328 = torch.prims.convert_element_type %5319, %int6_6656 : !torch.vtensor<[4,1,8,128],f16>, !torch.int -> !torch.vtensor<[4,1,8,128],f32>
    %5329 = torch_c.to_builtin_tensor %5328 : !torch.vtensor<[4,1,8,128],f32> -> tensor<4x1x8x128xf32>
    %5330 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %5331 = util.call @sharktank_rotary_embedding_4_1_8_128_f32(%5329, %5330) : (tensor<4x1x8x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x8x128xf32>
    %5332 = torch_c.from_builtin_tensor %5331 : tensor<4x1x8x128xf32> -> !torch.vtensor<[4,1,8,128],f32>
    %int5_6657 = torch.constant.int 5
    %5333 = torch.prims.convert_element_type %5332, %int5_6657 : !torch.vtensor<[4,1,8,128],f32>, !torch.int -> !torch.vtensor<[4,1,8,128],f16>
    %int32_6658 = torch.constant.int 32
    %5334 = torch.aten.floor_divide.Scalar %arg2, %int32_6658 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_6659 = torch.constant.int 1
    %5335 = torch.aten.unsqueeze %5334, %int1_6659 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_6660 = torch.constant.int 1
    %false_6661 = torch.constant.bool false
    %5336 = torch.aten.gather %arg3, %int1_6660, %5335, %false_6661 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_6662 = torch.constant.int 32
    %5337 = torch.aten.remainder.Scalar %arg2, %int32_6662 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_6663 = torch.constant.int 1
    %5338 = torch.aten.unsqueeze %5337, %int1_6663 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_6664 = torch.constant.none
    %5339 = torch.aten.clone %269, %none_6664 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_6665 = torch.constant.int 0
    %5340 = torch.aten.unsqueeze %5339, %int0_6665 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_6666 = torch.constant.int 4
    %int1_6667 = torch.constant.int 1
    %5341 = torch.prim.ListConstruct %int4_6666, %int1_6667 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_6668 = torch.constant.int 1
    %int1_6669 = torch.constant.int 1
    %5342 = torch.prim.ListConstruct %int1_6668, %int1_6669 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_6670 = torch.constant.int 4
    %int0_6671 = torch.constant.int 0
    %cpu_6672 = torch.constant.device "cpu"
    %false_6673 = torch.constant.bool false
    %5343 = torch.aten.empty_strided %5341, %5342, %int4_6670, %int0_6671, %cpu_6672, %false_6673 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int24 = torch.constant.int 24
    %5344 = torch.aten.fill.Scalar %5343, %int24 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_6674 = torch.constant.int 4
    %int1_6675 = torch.constant.int 1
    %5345 = torch.prim.ListConstruct %int4_6674, %int1_6675 : (!torch.int, !torch.int) -> !torch.list<int>
    %5346 = torch.aten.repeat %5340, %5345 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_6676 = torch.constant.int 32
    %5347 = torch.aten.mul.Scalar %5336, %int32_6676 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_6677 = torch.constant.int 1
    %5348 = torch.aten.add.Tensor %5347, %5344, %int1_6677 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_6678 = torch.constant.int 2
    %5349 = torch.aten.mul.Scalar %5348, %int2_6678 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_6679 = torch.constant.int 1
    %5350 = torch.aten.add.Tensor %5349, %5346, %int1_6679 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_6680 = torch.constant.int 32
    %5351 = torch.aten.mul.Scalar %5350, %int32_6680 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_6681 = torch.constant.int 1
    %5352 = torch.aten.add.Tensor %5351, %5338, %int1_6681 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_6682 = torch.constant.int 32
    %int2_6683 = torch.constant.int 2
    %int32_6684 = torch.constant.int 32
    %int8_6685 = torch.constant.int 8
    %int128_6686 = torch.constant.int 128
    %5353 = torch.prim.ListConstruct %437, %int32_6682, %int2_6683, %int32_6684, %int8_6685, %int128_6686 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5354 = torch.aten.view %5190, %5353 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5354, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_6687 = torch.constant.int 32
    %5355 = torch.aten.mul.int %437, %int32_6687 : !torch.int, !torch.int -> !torch.int
    %int2_6688 = torch.constant.int 2
    %5356 = torch.aten.mul.int %5355, %int2_6688 : !torch.int, !torch.int -> !torch.int
    %int32_6689 = torch.constant.int 32
    %5357 = torch.aten.mul.int %5356, %int32_6689 : !torch.int, !torch.int -> !torch.int
    %int8_6690 = torch.constant.int 8
    %int128_6691 = torch.constant.int 128
    %5358 = torch.prim.ListConstruct %5357, %int8_6690, %int128_6691 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5359 = torch.aten.view %5354, %5358 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %5359, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %5360 = torch.prim.ListConstruct %5352 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_6692 = torch.constant.bool false
    %5361 = torch.aten.index_put %5359, %5360, %5333, %false_6692 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %5361, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_6693 = torch.constant.int 32
    %int2_6694 = torch.constant.int 2
    %int32_6695 = torch.constant.int 32
    %int8_6696 = torch.constant.int 8
    %int128_6697 = torch.constant.int 128
    %5362 = torch.prim.ListConstruct %437, %int32_6693, %int2_6694, %int32_6695, %int8_6696, %int128_6697 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5363 = torch.aten.view %5361, %5362 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5363, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_6698 = torch.constant.int 2097152
    %5364 = torch.prim.ListConstruct %437, %int2097152_6698 : (!torch.int, !torch.int) -> !torch.list<int>
    %5365 = torch.aten.view %5363, %5364 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %5365, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_6699 = torch.constant.int 32
    %int2_6700 = torch.constant.int 2
    %int32_6701 = torch.constant.int 32
    %int8_6702 = torch.constant.int 8
    %int128_6703 = torch.constant.int 128
    %5366 = torch.prim.ListConstruct %437, %int32_6699, %int2_6700, %int32_6701, %int8_6702, %int128_6703 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5367 = torch.aten.view %5365, %5366 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5367, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int8_6704 = torch.constant.int 8
    %int128_6705 = torch.constant.int 128
    %5368 = torch.prim.ListConstruct %5357, %int8_6704, %int128_6705 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5369 = torch.aten.view %5367, %5368 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %5369, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_6706 = torch.constant.int 32
    %5370 = torch.aten.floor_divide.Scalar %arg2, %int32_6706 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_6707 = torch.constant.int 1
    %5371 = torch.aten.unsqueeze %5370, %int1_6707 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_6708 = torch.constant.int 1
    %false_6709 = torch.constant.bool false
    %5372 = torch.aten.gather %arg3, %int1_6708, %5371, %false_6709 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_6710 = torch.constant.int 32
    %5373 = torch.aten.remainder.Scalar %arg2, %int32_6710 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_6711 = torch.constant.int 1
    %5374 = torch.aten.unsqueeze %5373, %int1_6711 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_6712 = torch.constant.none
    %5375 = torch.aten.clone %270, %none_6712 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_6713 = torch.constant.int 0
    %5376 = torch.aten.unsqueeze %5375, %int0_6713 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_6714 = torch.constant.int 4
    %int1_6715 = torch.constant.int 1
    %5377 = torch.prim.ListConstruct %int4_6714, %int1_6715 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_6716 = torch.constant.int 1
    %int1_6717 = torch.constant.int 1
    %5378 = torch.prim.ListConstruct %int1_6716, %int1_6717 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_6718 = torch.constant.int 4
    %int0_6719 = torch.constant.int 0
    %cpu_6720 = torch.constant.device "cpu"
    %false_6721 = torch.constant.bool false
    %5379 = torch.aten.empty_strided %5377, %5378, %int4_6718, %int0_6719, %cpu_6720, %false_6721 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int24_6722 = torch.constant.int 24
    %5380 = torch.aten.fill.Scalar %5379, %int24_6722 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_6723 = torch.constant.int 4
    %int1_6724 = torch.constant.int 1
    %5381 = torch.prim.ListConstruct %int4_6723, %int1_6724 : (!torch.int, !torch.int) -> !torch.list<int>
    %5382 = torch.aten.repeat %5376, %5381 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_6725 = torch.constant.int 32
    %5383 = torch.aten.mul.Scalar %5372, %int32_6725 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_6726 = torch.constant.int 1
    %5384 = torch.aten.add.Tensor %5383, %5380, %int1_6726 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_6727 = torch.constant.int 2
    %5385 = torch.aten.mul.Scalar %5384, %int2_6727 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_6728 = torch.constant.int 1
    %5386 = torch.aten.add.Tensor %5385, %5382, %int1_6728 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_6729 = torch.constant.int 32
    %5387 = torch.aten.mul.Scalar %5386, %int32_6729 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_6730 = torch.constant.int 1
    %5388 = torch.aten.add.Tensor %5387, %5374, %int1_6730 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %5389 = torch.prim.ListConstruct %5388 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_6731 = torch.constant.bool false
    %5390 = torch.aten.index_put %5369, %5389, %5321, %false_6731 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %5390, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_6732 = torch.constant.int 32
    %int2_6733 = torch.constant.int 2
    %int32_6734 = torch.constant.int 32
    %int8_6735 = torch.constant.int 8
    %int128_6736 = torch.constant.int 128
    %5391 = torch.prim.ListConstruct %437, %int32_6732, %int2_6733, %int32_6734, %int8_6735, %int128_6736 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5392 = torch.aten.view %5390, %5391 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5392, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_6737 = torch.constant.int 2097152
    %5393 = torch.prim.ListConstruct %437, %int2097152_6737 : (!torch.int, !torch.int) -> !torch.list<int>
    %5394 = torch.aten.view %5392, %5393 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %5394, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int4_6738 = torch.constant.int 4
    %5395 = torch.prim.ListConstruct %int4_6738, %358 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_6739 = torch.constant.int 1
    %5396 = torch.prim.ListConstruct %358, %int1_6739 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_6740 = torch.constant.int 4
    %int0_6741 = torch.constant.int 0
    %cpu_6742 = torch.constant.device "cpu"
    %false_6743 = torch.constant.bool false
    %5397 = torch.aten.empty_strided %5395, %5396, %int4_6740, %int0_6741, %cpu_6742, %false_6743 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5397, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int24_6744 = torch.constant.int 24
    %5398 = torch.aten.fill.Scalar %5397, %int24_6744 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5398, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int32_6745 = torch.constant.int 32
    %5399 = torch.aten.mul.Scalar %arg3, %int32_6745 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5399, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_6746 = torch.constant.int 1
    %5400 = torch.aten.add.Tensor %5399, %5398, %int1_6746 : !torch.vtensor<[4,?],si64>, !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5400, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_6747 = torch.constant.int 4
    %5401 = torch.aten.mul.int %int4_6747, %358 : !torch.int, !torch.int -> !torch.int
    %5402 = torch.prim.ListConstruct %5401 : (!torch.int) -> !torch.list<int>
    %5403 = torch.aten.view %5400, %5402 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5403, [%356], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_6748 = torch.constant.int 32
    %int2_6749 = torch.constant.int 2
    %int32_6750 = torch.constant.int 32
    %int8_6751 = torch.constant.int 8
    %int128_6752 = torch.constant.int 128
    %5404 = torch.prim.ListConstruct %437, %int32_6748, %int2_6749, %int32_6750, %int8_6751, %int128_6752 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5405 = torch.aten.view %5394, %5404 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5405, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_6753 = torch.constant.int 32
    %5406 = torch.aten.mul.int %437, %int32_6753 : !torch.int, !torch.int -> !torch.int
    %int2_6754 = torch.constant.int 2
    %int32_6755 = torch.constant.int 32
    %int8_6756 = torch.constant.int 8
    %int128_6757 = torch.constant.int 128
    %5407 = torch.prim.ListConstruct %5406, %int2_6754, %int32_6755, %int8_6756, %int128_6757 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5408 = torch.aten.view %5405, %5407 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %5408, [%357], affine_map<()[s0] -> (s0 * 32, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int0_6758 = torch.constant.int 0
    %5409 = torch.aten.index_select %5408, %int0_6758, %5403 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.int, !torch.vtensor<[?],si64> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %5409, [%356], affine_map<()[s0] -> (s0 * 4, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int4_6759 = torch.constant.int 4
    %int2_6760 = torch.constant.int 2
    %int32_6761 = torch.constant.int 32
    %int8_6762 = torch.constant.int 8
    %int128_6763 = torch.constant.int 128
    %5410 = torch.prim.ListConstruct %int4_6759, %358, %int2_6760, %int32_6761, %int8_6762, %int128_6763 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5411 = torch.aten.view %5409, %5410 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %5411, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int0_6764 = torch.constant.int 0
    %int0_6765 = torch.constant.int 0
    %int9223372036854775807_6766 = torch.constant.int 9223372036854775807
    %int1_6767 = torch.constant.int 1
    %5412 = torch.aten.slice.Tensor %5411, %int0_6764, %int0_6765, %int9223372036854775807_6766, %int1_6767 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %5412, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_6768 = torch.constant.int 1
    %int0_6769 = torch.constant.int 0
    %int9223372036854775807_6770 = torch.constant.int 9223372036854775807
    %int1_6771 = torch.constant.int 1
    %5413 = torch.aten.slice.Tensor %5412, %int1_6768, %int0_6769, %int9223372036854775807_6770, %int1_6771 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %5413, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_6772 = torch.constant.int 2
    %int0_6773 = torch.constant.int 0
    %5414 = torch.aten.select.int %5413, %int2_6772, %int0_6773 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5414, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int32_6774 = torch.constant.int 32
    %5415 = torch.aten.mul.int %358, %int32_6774 : !torch.int, !torch.int -> !torch.int
    %int2_6775 = torch.constant.int 2
    %int0_6776 = torch.constant.int 0
    %int1_6777 = torch.constant.int 1
    %5416 = torch.aten.slice.Tensor %5414, %int2_6775, %int0_6776, %5415, %int1_6777 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5416, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_6778 = torch.constant.int 0
    %5417 = torch.aten.clone %5416, %int0_6778 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5417, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_6779 = torch.constant.int 1
    %5418 = torch.aten.size.int %5413, %int1_6779 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_6780 = torch.constant.int 32
    %5419 = torch.aten.mul.int %5418, %int32_6780 : !torch.int, !torch.int -> !torch.int
    %int4_6781 = torch.constant.int 4
    %int8_6782 = torch.constant.int 8
    %int128_6783 = torch.constant.int 128
    %5420 = torch.prim.ListConstruct %int4_6781, %5419, %int8_6782, %int128_6783 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5421 = torch.aten._unsafe_view %5417, %5420 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %5421, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_6784 = torch.constant.int 0
    %int0_6785 = torch.constant.int 0
    %int9223372036854775807_6786 = torch.constant.int 9223372036854775807
    %int1_6787 = torch.constant.int 1
    %5422 = torch.aten.slice.Tensor %5421, %int0_6784, %int0_6785, %int9223372036854775807_6786, %int1_6787 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %5422, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_6788 = torch.constant.int 0
    %int0_6789 = torch.constant.int 0
    %int9223372036854775807_6790 = torch.constant.int 9223372036854775807
    %int1_6791 = torch.constant.int 1
    %5423 = torch.aten.slice.Tensor %5411, %int0_6788, %int0_6789, %int9223372036854775807_6790, %int1_6791 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %5423, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_6792 = torch.constant.int 1
    %int0_6793 = torch.constant.int 0
    %int9223372036854775807_6794 = torch.constant.int 9223372036854775807
    %int1_6795 = torch.constant.int 1
    %5424 = torch.aten.slice.Tensor %5423, %int1_6792, %int0_6793, %int9223372036854775807_6794, %int1_6795 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %5424, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_6796 = torch.constant.int 2
    %int1_6797 = torch.constant.int 1
    %5425 = torch.aten.select.int %5424, %int2_6796, %int1_6797 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5425, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int2_6798 = torch.constant.int 2
    %int0_6799 = torch.constant.int 0
    %int1_6800 = torch.constant.int 1
    %5426 = torch.aten.slice.Tensor %5425, %int2_6798, %int0_6799, %5415, %int1_6800 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5426, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_6801 = torch.constant.int 0
    %5427 = torch.aten.clone %5426, %int0_6801 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5427, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_6802 = torch.constant.int 1
    %5428 = torch.aten.size.int %5424, %int1_6802 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_6803 = torch.constant.int 32
    %5429 = torch.aten.mul.int %5428, %int32_6803 : !torch.int, !torch.int -> !torch.int
    %int4_6804 = torch.constant.int 4
    %int8_6805 = torch.constant.int 8
    %int128_6806 = torch.constant.int 128
    %5430 = torch.prim.ListConstruct %int4_6804, %5429, %int8_6805, %int128_6806 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5431 = torch.aten._unsafe_view %5427, %5430 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %5431, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_6807 = torch.constant.int 0
    %int0_6808 = torch.constant.int 0
    %int9223372036854775807_6809 = torch.constant.int 9223372036854775807
    %int1_6810 = torch.constant.int 1
    %5432 = torch.aten.slice.Tensor %5431, %int0_6807, %int0_6808, %int9223372036854775807_6809, %int1_6810 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %5432, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int-2_6811 = torch.constant.int -2
    %5433 = torch.aten.unsqueeze %5422, %int-2_6811 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %5433, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_6812 = torch.constant.int 1
    %5434 = torch.aten.size.int %5421, %int1_6812 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_6813 = torch.constant.int 4
    %int8_6814 = torch.constant.int 8
    %int4_6815 = torch.constant.int 4
    %int128_6816 = torch.constant.int 128
    %5435 = torch.prim.ListConstruct %int4_6813, %5434, %int8_6814, %int4_6815, %int128_6816 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_6817 = torch.constant.bool false
    %5436 = torch.aten.expand %5433, %5435, %false_6817 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5436, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_6818 = torch.constant.int 0
    %5437 = torch.aten.clone %5436, %int0_6818 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5437, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_6819 = torch.constant.int 4
    %int32_6820 = torch.constant.int 32
    %int128_6821 = torch.constant.int 128
    %5438 = torch.prim.ListConstruct %int4_6819, %5434, %int32_6820, %int128_6821 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5439 = torch.aten._unsafe_view %5437, %5438 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %5439, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_6822 = torch.constant.int -2
    %5440 = torch.aten.unsqueeze %5432, %int-2_6822 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %5440, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_6823 = torch.constant.int 1
    %5441 = torch.aten.size.int %5431, %int1_6823 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_6824 = torch.constant.int 4
    %int8_6825 = torch.constant.int 8
    %int4_6826 = torch.constant.int 4
    %int128_6827 = torch.constant.int 128
    %5442 = torch.prim.ListConstruct %int4_6824, %5441, %int8_6825, %int4_6826, %int128_6827 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_6828 = torch.constant.bool false
    %5443 = torch.aten.expand %5440, %5442, %false_6828 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5443, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_6829 = torch.constant.int 0
    %5444 = torch.aten.clone %5443, %int0_6829 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5444, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_6830 = torch.constant.int 4
    %int32_6831 = torch.constant.int 32
    %int128_6832 = torch.constant.int 128
    %5445 = torch.prim.ListConstruct %int4_6830, %5441, %int32_6831, %int128_6832 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5446 = torch.aten._unsafe_view %5444, %5445 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %5446, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_6833 = torch.constant.int 1
    %int2_6834 = torch.constant.int 2
    %5447 = torch.aten.transpose.int %5327, %int1_6833, %int2_6834 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,128],f16>
    %int1_6835 = torch.constant.int 1
    %int2_6836 = torch.constant.int 2
    %5448 = torch.aten.transpose.int %5439, %int1_6835, %int2_6836 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %5448, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_6837 = torch.constant.int 1
    %int2_6838 = torch.constant.int 2
    %5449 = torch.aten.transpose.int %5446, %int1_6837, %int2_6838 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %5449, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_6839 = torch.constant.float 0.000000e+00
    %false_6840 = torch.constant.bool false
    %none_6841 = torch.constant.none
    %5450:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5447, %5448, %5449, %float0.000000e00_6839, %false_6840, %368, %none_6841) : (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.vtensor<[4,1,1,?],f16>, !torch.none) -> (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,1],f32>) 
    %int1_6842 = torch.constant.int 1
    %int2_6843 = torch.constant.int 2
    %5451 = torch.aten.transpose.int %5450#0, %int1_6842, %int2_6843 : !torch.vtensor<[4,32,1,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int4_6844 = torch.constant.int 4
    %int1_6845 = torch.constant.int 1
    %int4096_6846 = torch.constant.int 4096
    %5452 = torch.prim.ListConstruct %int4_6844, %int1_6845, %int4096_6846 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5453 = torch.aten.view %5451, %5452 : !torch.vtensor<[4,1,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_6847 = torch.constant.int -2
    %int-1_6848 = torch.constant.int -1
    %5454 = torch.aten.transpose.int %271, %int-2_6847, %int-1_6848 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_6849 = torch.constant.int 4
    %int4096_6850 = torch.constant.int 4096
    %5455 = torch.prim.ListConstruct %int4_6849, %int4096_6850 : (!torch.int, !torch.int) -> !torch.list<int>
    %5456 = torch.aten.view %5453, %5455 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %5457 = torch.aten.mm %5456, %5454 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_6851 = torch.constant.int 4
    %int1_6852 = torch.constant.int 1
    %int4096_6853 = torch.constant.int 4096
    %5458 = torch.prim.ListConstruct %int4_6851, %int1_6852, %int4096_6853 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5459 = torch.aten.view %5457, %5458 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_6854 = torch.constant.int 1
    %5460 = torch.aten.add.Tensor %5287, %5459, %int1_6854 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_6855 = torch.constant.int 6
    %5461 = torch.prims.convert_element_type %5460, %int6_6855 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_6856 = torch.constant.int 2
    %5462 = torch.aten.pow.Tensor_Scalar %5461, %int2_6856 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_6857 = torch.constant.int -1
    %5463 = torch.prim.ListConstruct %int-1_6857 : (!torch.int) -> !torch.list<int>
    %true_6858 = torch.constant.bool true
    %none_6859 = torch.constant.none
    %5464 = torch.aten.mean.dim %5462, %5463, %true_6858, %none_6859 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_6860 = torch.constant.float 9.9999997473787516E-6
    %int1_6861 = torch.constant.int 1
    %5465 = torch.aten.add.Scalar %5464, %float9.999990e-06_6860, %int1_6861 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %5466 = torch.aten.rsqrt %5465 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %5467 = torch.aten.mul.Tensor %5461, %5466 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_6862 = torch.constant.int 5
    %5468 = torch.prims.convert_element_type %5467, %int5_6862 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %5469 = torch.aten.mul.Tensor %272, %5468 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_6863 = torch.constant.int 5
    %5470 = torch.prims.convert_element_type %5469, %int5_6863 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_6864 = torch.constant.int -2
    %int-1_6865 = torch.constant.int -1
    %5471 = torch.aten.transpose.int %273, %int-2_6864, %int-1_6865 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_6866 = torch.constant.int 4
    %int4096_6867 = torch.constant.int 4096
    %5472 = torch.prim.ListConstruct %int4_6866, %int4096_6867 : (!torch.int, !torch.int) -> !torch.list<int>
    %5473 = torch.aten.view %5470, %5472 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %5474 = torch.aten.mm %5473, %5471 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_6868 = torch.constant.int 4
    %int1_6869 = torch.constant.int 1
    %int14336_6870 = torch.constant.int 14336
    %5475 = torch.prim.ListConstruct %int4_6868, %int1_6869, %int14336_6870 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5476 = torch.aten.view %5474, %5475 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %5477 = torch.aten.silu %5476 : !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_6871 = torch.constant.int -2
    %int-1_6872 = torch.constant.int -1
    %5478 = torch.aten.transpose.int %274, %int-2_6871, %int-1_6872 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_6873 = torch.constant.int 4
    %int4096_6874 = torch.constant.int 4096
    %5479 = torch.prim.ListConstruct %int4_6873, %int4096_6874 : (!torch.int, !torch.int) -> !torch.list<int>
    %5480 = torch.aten.view %5470, %5479 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %5481 = torch.aten.mm %5480, %5478 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_6875 = torch.constant.int 4
    %int1_6876 = torch.constant.int 1
    %int14336_6877 = torch.constant.int 14336
    %5482 = torch.prim.ListConstruct %int4_6875, %int1_6876, %int14336_6877 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5483 = torch.aten.view %5481, %5482 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %5484 = torch.aten.mul.Tensor %5477, %5483 : !torch.vtensor<[4,1,14336],f16>, !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_6878 = torch.constant.int -2
    %int-1_6879 = torch.constant.int -1
    %5485 = torch.aten.transpose.int %275, %int-2_6878, %int-1_6879 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int4_6880 = torch.constant.int 4
    %int14336_6881 = torch.constant.int 14336
    %5486 = torch.prim.ListConstruct %int4_6880, %int14336_6881 : (!torch.int, !torch.int) -> !torch.list<int>
    %5487 = torch.aten.view %5484, %5486 : !torch.vtensor<[4,1,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,14336],f16>
    %5488 = torch.aten.mm %5487, %5485 : !torch.vtensor<[4,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_6882 = torch.constant.int 4
    %int1_6883 = torch.constant.int 1
    %int4096_6884 = torch.constant.int 4096
    %5489 = torch.prim.ListConstruct %int4_6882, %int1_6883, %int4096_6884 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5490 = torch.aten.view %5488, %5489 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_6885 = torch.constant.int 1
    %5491 = torch.aten.add.Tensor %5460, %5490, %int1_6885 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_6886 = torch.constant.int 6
    %5492 = torch.prims.convert_element_type %5491, %int6_6886 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_6887 = torch.constant.int 2
    %5493 = torch.aten.pow.Tensor_Scalar %5492, %int2_6887 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_6888 = torch.constant.int -1
    %5494 = torch.prim.ListConstruct %int-1_6888 : (!torch.int) -> !torch.list<int>
    %true_6889 = torch.constant.bool true
    %none_6890 = torch.constant.none
    %5495 = torch.aten.mean.dim %5493, %5494, %true_6889, %none_6890 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_6891 = torch.constant.float 9.9999997473787516E-6
    %int1_6892 = torch.constant.int 1
    %5496 = torch.aten.add.Scalar %5495, %float9.999990e-06_6891, %int1_6892 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %5497 = torch.aten.rsqrt %5496 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %5498 = torch.aten.mul.Tensor %5492, %5497 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_6893 = torch.constant.int 5
    %5499 = torch.prims.convert_element_type %5498, %int5_6893 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %5500 = torch.aten.mul.Tensor %276, %5499 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_6894 = torch.constant.int 5
    %5501 = torch.prims.convert_element_type %5500, %int5_6894 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_6895 = torch.constant.int -2
    %int-1_6896 = torch.constant.int -1
    %5502 = torch.aten.transpose.int %277, %int-2_6895, %int-1_6896 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_6897 = torch.constant.int 4
    %int4096_6898 = torch.constant.int 4096
    %5503 = torch.prim.ListConstruct %int4_6897, %int4096_6898 : (!torch.int, !torch.int) -> !torch.list<int>
    %5504 = torch.aten.view %5501, %5503 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %5505 = torch.aten.mm %5504, %5502 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_6899 = torch.constant.int 4
    %int1_6900 = torch.constant.int 1
    %int4096_6901 = torch.constant.int 4096
    %5506 = torch.prim.ListConstruct %int4_6899, %int1_6900, %int4096_6901 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5507 = torch.aten.view %5505, %5506 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_6902 = torch.constant.int -2
    %int-1_6903 = torch.constant.int -1
    %5508 = torch.aten.transpose.int %278, %int-2_6902, %int-1_6903 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_6904 = torch.constant.int 4
    %int4096_6905 = torch.constant.int 4096
    %5509 = torch.prim.ListConstruct %int4_6904, %int4096_6905 : (!torch.int, !torch.int) -> !torch.list<int>
    %5510 = torch.aten.view %5501, %5509 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %5511 = torch.aten.mm %5510, %5508 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_6906 = torch.constant.int 4
    %int1_6907 = torch.constant.int 1
    %int1024_6908 = torch.constant.int 1024
    %5512 = torch.prim.ListConstruct %int4_6906, %int1_6907, %int1024_6908 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5513 = torch.aten.view %5511, %5512 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int-2_6909 = torch.constant.int -2
    %int-1_6910 = torch.constant.int -1
    %5514 = torch.aten.transpose.int %279, %int-2_6909, %int-1_6910 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_6911 = torch.constant.int 4
    %int4096_6912 = torch.constant.int 4096
    %5515 = torch.prim.ListConstruct %int4_6911, %int4096_6912 : (!torch.int, !torch.int) -> !torch.list<int>
    %5516 = torch.aten.view %5501, %5515 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %5517 = torch.aten.mm %5516, %5514 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_6913 = torch.constant.int 4
    %int1_6914 = torch.constant.int 1
    %int1024_6915 = torch.constant.int 1024
    %5518 = torch.prim.ListConstruct %int4_6913, %int1_6914, %int1024_6915 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5519 = torch.aten.view %5517, %5518 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int4_6916 = torch.constant.int 4
    %int1_6917 = torch.constant.int 1
    %int32_6918 = torch.constant.int 32
    %int128_6919 = torch.constant.int 128
    %5520 = torch.prim.ListConstruct %int4_6916, %int1_6917, %int32_6918, %int128_6919 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5521 = torch.aten.view %5507, %5520 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,128],f16>
    %int4_6920 = torch.constant.int 4
    %int1_6921 = torch.constant.int 1
    %int8_6922 = torch.constant.int 8
    %int128_6923 = torch.constant.int 128
    %5522 = torch.prim.ListConstruct %int4_6920, %int1_6921, %int8_6922, %int128_6923 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5523 = torch.aten.view %5513, %5522 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int4_6924 = torch.constant.int 4
    %int1_6925 = torch.constant.int 1
    %int8_6926 = torch.constant.int 8
    %int128_6927 = torch.constant.int 128
    %5524 = torch.prim.ListConstruct %int4_6924, %int1_6925, %int8_6926, %int128_6927 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5525 = torch.aten.view %5519, %5524 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int6_6928 = torch.constant.int 6
    %5526 = torch.prims.convert_element_type %5521, %int6_6928 : !torch.vtensor<[4,1,32,128],f16>, !torch.int -> !torch.vtensor<[4,1,32,128],f32>
    %5527 = torch_c.to_builtin_tensor %5526 : !torch.vtensor<[4,1,32,128],f32> -> tensor<4x1x32x128xf32>
    %5528 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %5529 = util.call @sharktank_rotary_embedding_4_1_32_128_f32(%5527, %5528) : (tensor<4x1x32x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x32x128xf32>
    %5530 = torch_c.from_builtin_tensor %5529 : tensor<4x1x32x128xf32> -> !torch.vtensor<[4,1,32,128],f32>
    %int5_6929 = torch.constant.int 5
    %5531 = torch.prims.convert_element_type %5530, %int5_6929 : !torch.vtensor<[4,1,32,128],f32>, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int6_6930 = torch.constant.int 6
    %5532 = torch.prims.convert_element_type %5523, %int6_6930 : !torch.vtensor<[4,1,8,128],f16>, !torch.int -> !torch.vtensor<[4,1,8,128],f32>
    %5533 = torch_c.to_builtin_tensor %5532 : !torch.vtensor<[4,1,8,128],f32> -> tensor<4x1x8x128xf32>
    %5534 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %5535 = util.call @sharktank_rotary_embedding_4_1_8_128_f32(%5533, %5534) : (tensor<4x1x8x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x8x128xf32>
    %5536 = torch_c.from_builtin_tensor %5535 : tensor<4x1x8x128xf32> -> !torch.vtensor<[4,1,8,128],f32>
    %int5_6931 = torch.constant.int 5
    %5537 = torch.prims.convert_element_type %5536, %int5_6931 : !torch.vtensor<[4,1,8,128],f32>, !torch.int -> !torch.vtensor<[4,1,8,128],f16>
    %int32_6932 = torch.constant.int 32
    %5538 = torch.aten.floor_divide.Scalar %arg2, %int32_6932 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_6933 = torch.constant.int 1
    %5539 = torch.aten.unsqueeze %5538, %int1_6933 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_6934 = torch.constant.int 1
    %false_6935 = torch.constant.bool false
    %5540 = torch.aten.gather %arg3, %int1_6934, %5539, %false_6935 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_6936 = torch.constant.int 32
    %5541 = torch.aten.remainder.Scalar %arg2, %int32_6936 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_6937 = torch.constant.int 1
    %5542 = torch.aten.unsqueeze %5541, %int1_6937 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_6938 = torch.constant.none
    %5543 = torch.aten.clone %280, %none_6938 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_6939 = torch.constant.int 0
    %5544 = torch.aten.unsqueeze %5543, %int0_6939 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_6940 = torch.constant.int 4
    %int1_6941 = torch.constant.int 1
    %5545 = torch.prim.ListConstruct %int4_6940, %int1_6941 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_6942 = torch.constant.int 1
    %int1_6943 = torch.constant.int 1
    %5546 = torch.prim.ListConstruct %int1_6942, %int1_6943 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_6944 = torch.constant.int 4
    %int0_6945 = torch.constant.int 0
    %cpu_6946 = torch.constant.device "cpu"
    %false_6947 = torch.constant.bool false
    %5547 = torch.aten.empty_strided %5545, %5546, %int4_6944, %int0_6945, %cpu_6946, %false_6947 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int25 = torch.constant.int 25
    %5548 = torch.aten.fill.Scalar %5547, %int25 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_6948 = torch.constant.int 4
    %int1_6949 = torch.constant.int 1
    %5549 = torch.prim.ListConstruct %int4_6948, %int1_6949 : (!torch.int, !torch.int) -> !torch.list<int>
    %5550 = torch.aten.repeat %5544, %5549 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_6950 = torch.constant.int 32
    %5551 = torch.aten.mul.Scalar %5540, %int32_6950 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_6951 = torch.constant.int 1
    %5552 = torch.aten.add.Tensor %5551, %5548, %int1_6951 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_6952 = torch.constant.int 2
    %5553 = torch.aten.mul.Scalar %5552, %int2_6952 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_6953 = torch.constant.int 1
    %5554 = torch.aten.add.Tensor %5553, %5550, %int1_6953 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_6954 = torch.constant.int 32
    %5555 = torch.aten.mul.Scalar %5554, %int32_6954 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_6955 = torch.constant.int 1
    %5556 = torch.aten.add.Tensor %5555, %5542, %int1_6955 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_6956 = torch.constant.int 32
    %int2_6957 = torch.constant.int 2
    %int32_6958 = torch.constant.int 32
    %int8_6959 = torch.constant.int 8
    %int128_6960 = torch.constant.int 128
    %5557 = torch.prim.ListConstruct %437, %int32_6956, %int2_6957, %int32_6958, %int8_6959, %int128_6960 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5558 = torch.aten.view %5394, %5557 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5558, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_6961 = torch.constant.int 32
    %5559 = torch.aten.mul.int %437, %int32_6961 : !torch.int, !torch.int -> !torch.int
    %int2_6962 = torch.constant.int 2
    %5560 = torch.aten.mul.int %5559, %int2_6962 : !torch.int, !torch.int -> !torch.int
    %int32_6963 = torch.constant.int 32
    %5561 = torch.aten.mul.int %5560, %int32_6963 : !torch.int, !torch.int -> !torch.int
    %int8_6964 = torch.constant.int 8
    %int128_6965 = torch.constant.int 128
    %5562 = torch.prim.ListConstruct %5561, %int8_6964, %int128_6965 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5563 = torch.aten.view %5558, %5562 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %5563, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %5564 = torch.prim.ListConstruct %5556 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_6966 = torch.constant.bool false
    %5565 = torch.aten.index_put %5563, %5564, %5537, %false_6966 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %5565, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_6967 = torch.constant.int 32
    %int2_6968 = torch.constant.int 2
    %int32_6969 = torch.constant.int 32
    %int8_6970 = torch.constant.int 8
    %int128_6971 = torch.constant.int 128
    %5566 = torch.prim.ListConstruct %437, %int32_6967, %int2_6968, %int32_6969, %int8_6970, %int128_6971 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5567 = torch.aten.view %5565, %5566 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5567, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_6972 = torch.constant.int 2097152
    %5568 = torch.prim.ListConstruct %437, %int2097152_6972 : (!torch.int, !torch.int) -> !torch.list<int>
    %5569 = torch.aten.view %5567, %5568 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %5569, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_6973 = torch.constant.int 32
    %int2_6974 = torch.constant.int 2
    %int32_6975 = torch.constant.int 32
    %int8_6976 = torch.constant.int 8
    %int128_6977 = torch.constant.int 128
    %5570 = torch.prim.ListConstruct %437, %int32_6973, %int2_6974, %int32_6975, %int8_6976, %int128_6977 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5571 = torch.aten.view %5569, %5570 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5571, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int8_6978 = torch.constant.int 8
    %int128_6979 = torch.constant.int 128
    %5572 = torch.prim.ListConstruct %5561, %int8_6978, %int128_6979 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5573 = torch.aten.view %5571, %5572 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %5573, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_6980 = torch.constant.int 32
    %5574 = torch.aten.floor_divide.Scalar %arg2, %int32_6980 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_6981 = torch.constant.int 1
    %5575 = torch.aten.unsqueeze %5574, %int1_6981 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_6982 = torch.constant.int 1
    %false_6983 = torch.constant.bool false
    %5576 = torch.aten.gather %arg3, %int1_6982, %5575, %false_6983 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_6984 = torch.constant.int 32
    %5577 = torch.aten.remainder.Scalar %arg2, %int32_6984 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_6985 = torch.constant.int 1
    %5578 = torch.aten.unsqueeze %5577, %int1_6985 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_6986 = torch.constant.none
    %5579 = torch.aten.clone %281, %none_6986 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_6987 = torch.constant.int 0
    %5580 = torch.aten.unsqueeze %5579, %int0_6987 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_6988 = torch.constant.int 4
    %int1_6989 = torch.constant.int 1
    %5581 = torch.prim.ListConstruct %int4_6988, %int1_6989 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_6990 = torch.constant.int 1
    %int1_6991 = torch.constant.int 1
    %5582 = torch.prim.ListConstruct %int1_6990, %int1_6991 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_6992 = torch.constant.int 4
    %int0_6993 = torch.constant.int 0
    %cpu_6994 = torch.constant.device "cpu"
    %false_6995 = torch.constant.bool false
    %5583 = torch.aten.empty_strided %5581, %5582, %int4_6992, %int0_6993, %cpu_6994, %false_6995 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int25_6996 = torch.constant.int 25
    %5584 = torch.aten.fill.Scalar %5583, %int25_6996 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_6997 = torch.constant.int 4
    %int1_6998 = torch.constant.int 1
    %5585 = torch.prim.ListConstruct %int4_6997, %int1_6998 : (!torch.int, !torch.int) -> !torch.list<int>
    %5586 = torch.aten.repeat %5580, %5585 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_6999 = torch.constant.int 32
    %5587 = torch.aten.mul.Scalar %5576, %int32_6999 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_7000 = torch.constant.int 1
    %5588 = torch.aten.add.Tensor %5587, %5584, %int1_7000 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_7001 = torch.constant.int 2
    %5589 = torch.aten.mul.Scalar %5588, %int2_7001 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_7002 = torch.constant.int 1
    %5590 = torch.aten.add.Tensor %5589, %5586, %int1_7002 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_7003 = torch.constant.int 32
    %5591 = torch.aten.mul.Scalar %5590, %int32_7003 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_7004 = torch.constant.int 1
    %5592 = torch.aten.add.Tensor %5591, %5578, %int1_7004 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %5593 = torch.prim.ListConstruct %5592 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_7005 = torch.constant.bool false
    %5594 = torch.aten.index_put %5573, %5593, %5525, %false_7005 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %5594, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_7006 = torch.constant.int 32
    %int2_7007 = torch.constant.int 2
    %int32_7008 = torch.constant.int 32
    %int8_7009 = torch.constant.int 8
    %int128_7010 = torch.constant.int 128
    %5595 = torch.prim.ListConstruct %437, %int32_7006, %int2_7007, %int32_7008, %int8_7009, %int128_7010 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5596 = torch.aten.view %5594, %5595 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5596, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_7011 = torch.constant.int 2097152
    %5597 = torch.prim.ListConstruct %437, %int2097152_7011 : (!torch.int, !torch.int) -> !torch.list<int>
    %5598 = torch.aten.view %5596, %5597 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %5598, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int4_7012 = torch.constant.int 4
    %5599 = torch.prim.ListConstruct %int4_7012, %358 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_7013 = torch.constant.int 1
    %5600 = torch.prim.ListConstruct %358, %int1_7013 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_7014 = torch.constant.int 4
    %int0_7015 = torch.constant.int 0
    %cpu_7016 = torch.constant.device "cpu"
    %false_7017 = torch.constant.bool false
    %5601 = torch.aten.empty_strided %5599, %5600, %int4_7014, %int0_7015, %cpu_7016, %false_7017 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5601, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int25_7018 = torch.constant.int 25
    %5602 = torch.aten.fill.Scalar %5601, %int25_7018 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5602, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int32_7019 = torch.constant.int 32
    %5603 = torch.aten.mul.Scalar %arg3, %int32_7019 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5603, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_7020 = torch.constant.int 1
    %5604 = torch.aten.add.Tensor %5603, %5602, %int1_7020 : !torch.vtensor<[4,?],si64>, !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5604, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_7021 = torch.constant.int 4
    %5605 = torch.aten.mul.int %int4_7021, %358 : !torch.int, !torch.int -> !torch.int
    %5606 = torch.prim.ListConstruct %5605 : (!torch.int) -> !torch.list<int>
    %5607 = torch.aten.view %5604, %5606 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5607, [%356], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_7022 = torch.constant.int 32
    %int2_7023 = torch.constant.int 2
    %int32_7024 = torch.constant.int 32
    %int8_7025 = torch.constant.int 8
    %int128_7026 = torch.constant.int 128
    %5608 = torch.prim.ListConstruct %437, %int32_7022, %int2_7023, %int32_7024, %int8_7025, %int128_7026 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5609 = torch.aten.view %5598, %5608 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5609, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_7027 = torch.constant.int 32
    %5610 = torch.aten.mul.int %437, %int32_7027 : !torch.int, !torch.int -> !torch.int
    %int2_7028 = torch.constant.int 2
    %int32_7029 = torch.constant.int 32
    %int8_7030 = torch.constant.int 8
    %int128_7031 = torch.constant.int 128
    %5611 = torch.prim.ListConstruct %5610, %int2_7028, %int32_7029, %int8_7030, %int128_7031 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5612 = torch.aten.view %5609, %5611 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %5612, [%357], affine_map<()[s0] -> (s0 * 32, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int0_7032 = torch.constant.int 0
    %5613 = torch.aten.index_select %5612, %int0_7032, %5607 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.int, !torch.vtensor<[?],si64> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %5613, [%356], affine_map<()[s0] -> (s0 * 4, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int4_7033 = torch.constant.int 4
    %int2_7034 = torch.constant.int 2
    %int32_7035 = torch.constant.int 32
    %int8_7036 = torch.constant.int 8
    %int128_7037 = torch.constant.int 128
    %5614 = torch.prim.ListConstruct %int4_7033, %358, %int2_7034, %int32_7035, %int8_7036, %int128_7037 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5615 = torch.aten.view %5613, %5614 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %5615, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int0_7038 = torch.constant.int 0
    %int0_7039 = torch.constant.int 0
    %int9223372036854775807_7040 = torch.constant.int 9223372036854775807
    %int1_7041 = torch.constant.int 1
    %5616 = torch.aten.slice.Tensor %5615, %int0_7038, %int0_7039, %int9223372036854775807_7040, %int1_7041 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %5616, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_7042 = torch.constant.int 1
    %int0_7043 = torch.constant.int 0
    %int9223372036854775807_7044 = torch.constant.int 9223372036854775807
    %int1_7045 = torch.constant.int 1
    %5617 = torch.aten.slice.Tensor %5616, %int1_7042, %int0_7043, %int9223372036854775807_7044, %int1_7045 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %5617, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_7046 = torch.constant.int 2
    %int0_7047 = torch.constant.int 0
    %5618 = torch.aten.select.int %5617, %int2_7046, %int0_7047 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5618, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int32_7048 = torch.constant.int 32
    %5619 = torch.aten.mul.int %358, %int32_7048 : !torch.int, !torch.int -> !torch.int
    %int2_7049 = torch.constant.int 2
    %int0_7050 = torch.constant.int 0
    %int1_7051 = torch.constant.int 1
    %5620 = torch.aten.slice.Tensor %5618, %int2_7049, %int0_7050, %5619, %int1_7051 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5620, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_7052 = torch.constant.int 0
    %5621 = torch.aten.clone %5620, %int0_7052 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5621, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_7053 = torch.constant.int 1
    %5622 = torch.aten.size.int %5617, %int1_7053 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_7054 = torch.constant.int 32
    %5623 = torch.aten.mul.int %5622, %int32_7054 : !torch.int, !torch.int -> !torch.int
    %int4_7055 = torch.constant.int 4
    %int8_7056 = torch.constant.int 8
    %int128_7057 = torch.constant.int 128
    %5624 = torch.prim.ListConstruct %int4_7055, %5623, %int8_7056, %int128_7057 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5625 = torch.aten._unsafe_view %5621, %5624 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %5625, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_7058 = torch.constant.int 0
    %int0_7059 = torch.constant.int 0
    %int9223372036854775807_7060 = torch.constant.int 9223372036854775807
    %int1_7061 = torch.constant.int 1
    %5626 = torch.aten.slice.Tensor %5625, %int0_7058, %int0_7059, %int9223372036854775807_7060, %int1_7061 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %5626, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_7062 = torch.constant.int 0
    %int0_7063 = torch.constant.int 0
    %int9223372036854775807_7064 = torch.constant.int 9223372036854775807
    %int1_7065 = torch.constant.int 1
    %5627 = torch.aten.slice.Tensor %5615, %int0_7062, %int0_7063, %int9223372036854775807_7064, %int1_7065 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %5627, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_7066 = torch.constant.int 1
    %int0_7067 = torch.constant.int 0
    %int9223372036854775807_7068 = torch.constant.int 9223372036854775807
    %int1_7069 = torch.constant.int 1
    %5628 = torch.aten.slice.Tensor %5627, %int1_7066, %int0_7067, %int9223372036854775807_7068, %int1_7069 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %5628, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_7070 = torch.constant.int 2
    %int1_7071 = torch.constant.int 1
    %5629 = torch.aten.select.int %5628, %int2_7070, %int1_7071 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5629, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int2_7072 = torch.constant.int 2
    %int0_7073 = torch.constant.int 0
    %int1_7074 = torch.constant.int 1
    %5630 = torch.aten.slice.Tensor %5629, %int2_7072, %int0_7073, %5619, %int1_7074 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5630, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_7075 = torch.constant.int 0
    %5631 = torch.aten.clone %5630, %int0_7075 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5631, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_7076 = torch.constant.int 1
    %5632 = torch.aten.size.int %5628, %int1_7076 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_7077 = torch.constant.int 32
    %5633 = torch.aten.mul.int %5632, %int32_7077 : !torch.int, !torch.int -> !torch.int
    %int4_7078 = torch.constant.int 4
    %int8_7079 = torch.constant.int 8
    %int128_7080 = torch.constant.int 128
    %5634 = torch.prim.ListConstruct %int4_7078, %5633, %int8_7079, %int128_7080 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5635 = torch.aten._unsafe_view %5631, %5634 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %5635, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_7081 = torch.constant.int 0
    %int0_7082 = torch.constant.int 0
    %int9223372036854775807_7083 = torch.constant.int 9223372036854775807
    %int1_7084 = torch.constant.int 1
    %5636 = torch.aten.slice.Tensor %5635, %int0_7081, %int0_7082, %int9223372036854775807_7083, %int1_7084 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %5636, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int-2_7085 = torch.constant.int -2
    %5637 = torch.aten.unsqueeze %5626, %int-2_7085 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %5637, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_7086 = torch.constant.int 1
    %5638 = torch.aten.size.int %5625, %int1_7086 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_7087 = torch.constant.int 4
    %int8_7088 = torch.constant.int 8
    %int4_7089 = torch.constant.int 4
    %int128_7090 = torch.constant.int 128
    %5639 = torch.prim.ListConstruct %int4_7087, %5638, %int8_7088, %int4_7089, %int128_7090 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_7091 = torch.constant.bool false
    %5640 = torch.aten.expand %5637, %5639, %false_7091 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5640, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_7092 = torch.constant.int 0
    %5641 = torch.aten.clone %5640, %int0_7092 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5641, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_7093 = torch.constant.int 4
    %int32_7094 = torch.constant.int 32
    %int128_7095 = torch.constant.int 128
    %5642 = torch.prim.ListConstruct %int4_7093, %5638, %int32_7094, %int128_7095 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5643 = torch.aten._unsafe_view %5641, %5642 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %5643, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_7096 = torch.constant.int -2
    %5644 = torch.aten.unsqueeze %5636, %int-2_7096 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %5644, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_7097 = torch.constant.int 1
    %5645 = torch.aten.size.int %5635, %int1_7097 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_7098 = torch.constant.int 4
    %int8_7099 = torch.constant.int 8
    %int4_7100 = torch.constant.int 4
    %int128_7101 = torch.constant.int 128
    %5646 = torch.prim.ListConstruct %int4_7098, %5645, %int8_7099, %int4_7100, %int128_7101 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_7102 = torch.constant.bool false
    %5647 = torch.aten.expand %5644, %5646, %false_7102 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5647, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_7103 = torch.constant.int 0
    %5648 = torch.aten.clone %5647, %int0_7103 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5648, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_7104 = torch.constant.int 4
    %int32_7105 = torch.constant.int 32
    %int128_7106 = torch.constant.int 128
    %5649 = torch.prim.ListConstruct %int4_7104, %5645, %int32_7105, %int128_7106 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5650 = torch.aten._unsafe_view %5648, %5649 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %5650, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_7107 = torch.constant.int 1
    %int2_7108 = torch.constant.int 2
    %5651 = torch.aten.transpose.int %5531, %int1_7107, %int2_7108 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,128],f16>
    %int1_7109 = torch.constant.int 1
    %int2_7110 = torch.constant.int 2
    %5652 = torch.aten.transpose.int %5643, %int1_7109, %int2_7110 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %5652, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_7111 = torch.constant.int 1
    %int2_7112 = torch.constant.int 2
    %5653 = torch.aten.transpose.int %5650, %int1_7111, %int2_7112 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %5653, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_7113 = torch.constant.float 0.000000e+00
    %false_7114 = torch.constant.bool false
    %none_7115 = torch.constant.none
    %5654:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5651, %5652, %5653, %float0.000000e00_7113, %false_7114, %368, %none_7115) : (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.vtensor<[4,1,1,?],f16>, !torch.none) -> (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,1],f32>) 
    %int1_7116 = torch.constant.int 1
    %int2_7117 = torch.constant.int 2
    %5655 = torch.aten.transpose.int %5654#0, %int1_7116, %int2_7117 : !torch.vtensor<[4,32,1,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int4_7118 = torch.constant.int 4
    %int1_7119 = torch.constant.int 1
    %int4096_7120 = torch.constant.int 4096
    %5656 = torch.prim.ListConstruct %int4_7118, %int1_7119, %int4096_7120 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5657 = torch.aten.view %5655, %5656 : !torch.vtensor<[4,1,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_7121 = torch.constant.int -2
    %int-1_7122 = torch.constant.int -1
    %5658 = torch.aten.transpose.int %282, %int-2_7121, %int-1_7122 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_7123 = torch.constant.int 4
    %int4096_7124 = torch.constant.int 4096
    %5659 = torch.prim.ListConstruct %int4_7123, %int4096_7124 : (!torch.int, !torch.int) -> !torch.list<int>
    %5660 = torch.aten.view %5657, %5659 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %5661 = torch.aten.mm %5660, %5658 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_7125 = torch.constant.int 4
    %int1_7126 = torch.constant.int 1
    %int4096_7127 = torch.constant.int 4096
    %5662 = torch.prim.ListConstruct %int4_7125, %int1_7126, %int4096_7127 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5663 = torch.aten.view %5661, %5662 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_7128 = torch.constant.int 1
    %5664 = torch.aten.add.Tensor %5491, %5663, %int1_7128 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_7129 = torch.constant.int 6
    %5665 = torch.prims.convert_element_type %5664, %int6_7129 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_7130 = torch.constant.int 2
    %5666 = torch.aten.pow.Tensor_Scalar %5665, %int2_7130 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_7131 = torch.constant.int -1
    %5667 = torch.prim.ListConstruct %int-1_7131 : (!torch.int) -> !torch.list<int>
    %true_7132 = torch.constant.bool true
    %none_7133 = torch.constant.none
    %5668 = torch.aten.mean.dim %5666, %5667, %true_7132, %none_7133 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_7134 = torch.constant.float 9.9999997473787516E-6
    %int1_7135 = torch.constant.int 1
    %5669 = torch.aten.add.Scalar %5668, %float9.999990e-06_7134, %int1_7135 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %5670 = torch.aten.rsqrt %5669 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %5671 = torch.aten.mul.Tensor %5665, %5670 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_7136 = torch.constant.int 5
    %5672 = torch.prims.convert_element_type %5671, %int5_7136 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %5673 = torch.aten.mul.Tensor %283, %5672 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_7137 = torch.constant.int 5
    %5674 = torch.prims.convert_element_type %5673, %int5_7137 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_7138 = torch.constant.int -2
    %int-1_7139 = torch.constant.int -1
    %5675 = torch.aten.transpose.int %284, %int-2_7138, %int-1_7139 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_7140 = torch.constant.int 4
    %int4096_7141 = torch.constant.int 4096
    %5676 = torch.prim.ListConstruct %int4_7140, %int4096_7141 : (!torch.int, !torch.int) -> !torch.list<int>
    %5677 = torch.aten.view %5674, %5676 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %5678 = torch.aten.mm %5677, %5675 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_7142 = torch.constant.int 4
    %int1_7143 = torch.constant.int 1
    %int14336_7144 = torch.constant.int 14336
    %5679 = torch.prim.ListConstruct %int4_7142, %int1_7143, %int14336_7144 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5680 = torch.aten.view %5678, %5679 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %5681 = torch.aten.silu %5680 : !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_7145 = torch.constant.int -2
    %int-1_7146 = torch.constant.int -1
    %5682 = torch.aten.transpose.int %285, %int-2_7145, %int-1_7146 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_7147 = torch.constant.int 4
    %int4096_7148 = torch.constant.int 4096
    %5683 = torch.prim.ListConstruct %int4_7147, %int4096_7148 : (!torch.int, !torch.int) -> !torch.list<int>
    %5684 = torch.aten.view %5674, %5683 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %5685 = torch.aten.mm %5684, %5682 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_7149 = torch.constant.int 4
    %int1_7150 = torch.constant.int 1
    %int14336_7151 = torch.constant.int 14336
    %5686 = torch.prim.ListConstruct %int4_7149, %int1_7150, %int14336_7151 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5687 = torch.aten.view %5685, %5686 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %5688 = torch.aten.mul.Tensor %5681, %5687 : !torch.vtensor<[4,1,14336],f16>, !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_7152 = torch.constant.int -2
    %int-1_7153 = torch.constant.int -1
    %5689 = torch.aten.transpose.int %286, %int-2_7152, %int-1_7153 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int4_7154 = torch.constant.int 4
    %int14336_7155 = torch.constant.int 14336
    %5690 = torch.prim.ListConstruct %int4_7154, %int14336_7155 : (!torch.int, !torch.int) -> !torch.list<int>
    %5691 = torch.aten.view %5688, %5690 : !torch.vtensor<[4,1,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,14336],f16>
    %5692 = torch.aten.mm %5691, %5689 : !torch.vtensor<[4,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_7156 = torch.constant.int 4
    %int1_7157 = torch.constant.int 1
    %int4096_7158 = torch.constant.int 4096
    %5693 = torch.prim.ListConstruct %int4_7156, %int1_7157, %int4096_7158 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5694 = torch.aten.view %5692, %5693 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_7159 = torch.constant.int 1
    %5695 = torch.aten.add.Tensor %5664, %5694, %int1_7159 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_7160 = torch.constant.int 6
    %5696 = torch.prims.convert_element_type %5695, %int6_7160 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_7161 = torch.constant.int 2
    %5697 = torch.aten.pow.Tensor_Scalar %5696, %int2_7161 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_7162 = torch.constant.int -1
    %5698 = torch.prim.ListConstruct %int-1_7162 : (!torch.int) -> !torch.list<int>
    %true_7163 = torch.constant.bool true
    %none_7164 = torch.constant.none
    %5699 = torch.aten.mean.dim %5697, %5698, %true_7163, %none_7164 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_7165 = torch.constant.float 9.9999997473787516E-6
    %int1_7166 = torch.constant.int 1
    %5700 = torch.aten.add.Scalar %5699, %float9.999990e-06_7165, %int1_7166 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %5701 = torch.aten.rsqrt %5700 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %5702 = torch.aten.mul.Tensor %5696, %5701 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_7167 = torch.constant.int 5
    %5703 = torch.prims.convert_element_type %5702, %int5_7167 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %5704 = torch.aten.mul.Tensor %287, %5703 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_7168 = torch.constant.int 5
    %5705 = torch.prims.convert_element_type %5704, %int5_7168 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_7169 = torch.constant.int -2
    %int-1_7170 = torch.constant.int -1
    %5706 = torch.aten.transpose.int %288, %int-2_7169, %int-1_7170 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_7171 = torch.constant.int 4
    %int4096_7172 = torch.constant.int 4096
    %5707 = torch.prim.ListConstruct %int4_7171, %int4096_7172 : (!torch.int, !torch.int) -> !torch.list<int>
    %5708 = torch.aten.view %5705, %5707 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %5709 = torch.aten.mm %5708, %5706 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_7173 = torch.constant.int 4
    %int1_7174 = torch.constant.int 1
    %int4096_7175 = torch.constant.int 4096
    %5710 = torch.prim.ListConstruct %int4_7173, %int1_7174, %int4096_7175 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5711 = torch.aten.view %5709, %5710 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_7176 = torch.constant.int -2
    %int-1_7177 = torch.constant.int -1
    %5712 = torch.aten.transpose.int %289, %int-2_7176, %int-1_7177 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_7178 = torch.constant.int 4
    %int4096_7179 = torch.constant.int 4096
    %5713 = torch.prim.ListConstruct %int4_7178, %int4096_7179 : (!torch.int, !torch.int) -> !torch.list<int>
    %5714 = torch.aten.view %5705, %5713 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %5715 = torch.aten.mm %5714, %5712 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_7180 = torch.constant.int 4
    %int1_7181 = torch.constant.int 1
    %int1024_7182 = torch.constant.int 1024
    %5716 = torch.prim.ListConstruct %int4_7180, %int1_7181, %int1024_7182 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5717 = torch.aten.view %5715, %5716 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int-2_7183 = torch.constant.int -2
    %int-1_7184 = torch.constant.int -1
    %5718 = torch.aten.transpose.int %290, %int-2_7183, %int-1_7184 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_7185 = torch.constant.int 4
    %int4096_7186 = torch.constant.int 4096
    %5719 = torch.prim.ListConstruct %int4_7185, %int4096_7186 : (!torch.int, !torch.int) -> !torch.list<int>
    %5720 = torch.aten.view %5705, %5719 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %5721 = torch.aten.mm %5720, %5718 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_7187 = torch.constant.int 4
    %int1_7188 = torch.constant.int 1
    %int1024_7189 = torch.constant.int 1024
    %5722 = torch.prim.ListConstruct %int4_7187, %int1_7188, %int1024_7189 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5723 = torch.aten.view %5721, %5722 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int4_7190 = torch.constant.int 4
    %int1_7191 = torch.constant.int 1
    %int32_7192 = torch.constant.int 32
    %int128_7193 = torch.constant.int 128
    %5724 = torch.prim.ListConstruct %int4_7190, %int1_7191, %int32_7192, %int128_7193 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5725 = torch.aten.view %5711, %5724 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,128],f16>
    %int4_7194 = torch.constant.int 4
    %int1_7195 = torch.constant.int 1
    %int8_7196 = torch.constant.int 8
    %int128_7197 = torch.constant.int 128
    %5726 = torch.prim.ListConstruct %int4_7194, %int1_7195, %int8_7196, %int128_7197 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5727 = torch.aten.view %5717, %5726 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int4_7198 = torch.constant.int 4
    %int1_7199 = torch.constant.int 1
    %int8_7200 = torch.constant.int 8
    %int128_7201 = torch.constant.int 128
    %5728 = torch.prim.ListConstruct %int4_7198, %int1_7199, %int8_7200, %int128_7201 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5729 = torch.aten.view %5723, %5728 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int6_7202 = torch.constant.int 6
    %5730 = torch.prims.convert_element_type %5725, %int6_7202 : !torch.vtensor<[4,1,32,128],f16>, !torch.int -> !torch.vtensor<[4,1,32,128],f32>
    %5731 = torch_c.to_builtin_tensor %5730 : !torch.vtensor<[4,1,32,128],f32> -> tensor<4x1x32x128xf32>
    %5732 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %5733 = util.call @sharktank_rotary_embedding_4_1_32_128_f32(%5731, %5732) : (tensor<4x1x32x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x32x128xf32>
    %5734 = torch_c.from_builtin_tensor %5733 : tensor<4x1x32x128xf32> -> !torch.vtensor<[4,1,32,128],f32>
    %int5_7203 = torch.constant.int 5
    %5735 = torch.prims.convert_element_type %5734, %int5_7203 : !torch.vtensor<[4,1,32,128],f32>, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int6_7204 = torch.constant.int 6
    %5736 = torch.prims.convert_element_type %5727, %int6_7204 : !torch.vtensor<[4,1,8,128],f16>, !torch.int -> !torch.vtensor<[4,1,8,128],f32>
    %5737 = torch_c.to_builtin_tensor %5736 : !torch.vtensor<[4,1,8,128],f32> -> tensor<4x1x8x128xf32>
    %5738 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %5739 = util.call @sharktank_rotary_embedding_4_1_8_128_f32(%5737, %5738) : (tensor<4x1x8x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x8x128xf32>
    %5740 = torch_c.from_builtin_tensor %5739 : tensor<4x1x8x128xf32> -> !torch.vtensor<[4,1,8,128],f32>
    %int5_7205 = torch.constant.int 5
    %5741 = torch.prims.convert_element_type %5740, %int5_7205 : !torch.vtensor<[4,1,8,128],f32>, !torch.int -> !torch.vtensor<[4,1,8,128],f16>
    %int32_7206 = torch.constant.int 32
    %5742 = torch.aten.floor_divide.Scalar %arg2, %int32_7206 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_7207 = torch.constant.int 1
    %5743 = torch.aten.unsqueeze %5742, %int1_7207 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_7208 = torch.constant.int 1
    %false_7209 = torch.constant.bool false
    %5744 = torch.aten.gather %arg3, %int1_7208, %5743, %false_7209 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_7210 = torch.constant.int 32
    %5745 = torch.aten.remainder.Scalar %arg2, %int32_7210 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_7211 = torch.constant.int 1
    %5746 = torch.aten.unsqueeze %5745, %int1_7211 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_7212 = torch.constant.none
    %5747 = torch.aten.clone %291, %none_7212 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_7213 = torch.constant.int 0
    %5748 = torch.aten.unsqueeze %5747, %int0_7213 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_7214 = torch.constant.int 4
    %int1_7215 = torch.constant.int 1
    %5749 = torch.prim.ListConstruct %int4_7214, %int1_7215 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_7216 = torch.constant.int 1
    %int1_7217 = torch.constant.int 1
    %5750 = torch.prim.ListConstruct %int1_7216, %int1_7217 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_7218 = torch.constant.int 4
    %int0_7219 = torch.constant.int 0
    %cpu_7220 = torch.constant.device "cpu"
    %false_7221 = torch.constant.bool false
    %5751 = torch.aten.empty_strided %5749, %5750, %int4_7218, %int0_7219, %cpu_7220, %false_7221 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int26 = torch.constant.int 26
    %5752 = torch.aten.fill.Scalar %5751, %int26 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_7222 = torch.constant.int 4
    %int1_7223 = torch.constant.int 1
    %5753 = torch.prim.ListConstruct %int4_7222, %int1_7223 : (!torch.int, !torch.int) -> !torch.list<int>
    %5754 = torch.aten.repeat %5748, %5753 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_7224 = torch.constant.int 32
    %5755 = torch.aten.mul.Scalar %5744, %int32_7224 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_7225 = torch.constant.int 1
    %5756 = torch.aten.add.Tensor %5755, %5752, %int1_7225 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_7226 = torch.constant.int 2
    %5757 = torch.aten.mul.Scalar %5756, %int2_7226 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_7227 = torch.constant.int 1
    %5758 = torch.aten.add.Tensor %5757, %5754, %int1_7227 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_7228 = torch.constant.int 32
    %5759 = torch.aten.mul.Scalar %5758, %int32_7228 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_7229 = torch.constant.int 1
    %5760 = torch.aten.add.Tensor %5759, %5746, %int1_7229 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_7230 = torch.constant.int 32
    %int2_7231 = torch.constant.int 2
    %int32_7232 = torch.constant.int 32
    %int8_7233 = torch.constant.int 8
    %int128_7234 = torch.constant.int 128
    %5761 = torch.prim.ListConstruct %437, %int32_7230, %int2_7231, %int32_7232, %int8_7233, %int128_7234 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5762 = torch.aten.view %5598, %5761 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5762, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_7235 = torch.constant.int 32
    %5763 = torch.aten.mul.int %437, %int32_7235 : !torch.int, !torch.int -> !torch.int
    %int2_7236 = torch.constant.int 2
    %5764 = torch.aten.mul.int %5763, %int2_7236 : !torch.int, !torch.int -> !torch.int
    %int32_7237 = torch.constant.int 32
    %5765 = torch.aten.mul.int %5764, %int32_7237 : !torch.int, !torch.int -> !torch.int
    %int8_7238 = torch.constant.int 8
    %int128_7239 = torch.constant.int 128
    %5766 = torch.prim.ListConstruct %5765, %int8_7238, %int128_7239 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5767 = torch.aten.view %5762, %5766 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %5767, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %5768 = torch.prim.ListConstruct %5760 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_7240 = torch.constant.bool false
    %5769 = torch.aten.index_put %5767, %5768, %5741, %false_7240 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %5769, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_7241 = torch.constant.int 32
    %int2_7242 = torch.constant.int 2
    %int32_7243 = torch.constant.int 32
    %int8_7244 = torch.constant.int 8
    %int128_7245 = torch.constant.int 128
    %5770 = torch.prim.ListConstruct %437, %int32_7241, %int2_7242, %int32_7243, %int8_7244, %int128_7245 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5771 = torch.aten.view %5769, %5770 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5771, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_7246 = torch.constant.int 2097152
    %5772 = torch.prim.ListConstruct %437, %int2097152_7246 : (!torch.int, !torch.int) -> !torch.list<int>
    %5773 = torch.aten.view %5771, %5772 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %5773, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_7247 = torch.constant.int 32
    %int2_7248 = torch.constant.int 2
    %int32_7249 = torch.constant.int 32
    %int8_7250 = torch.constant.int 8
    %int128_7251 = torch.constant.int 128
    %5774 = torch.prim.ListConstruct %437, %int32_7247, %int2_7248, %int32_7249, %int8_7250, %int128_7251 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5775 = torch.aten.view %5773, %5774 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5775, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int8_7252 = torch.constant.int 8
    %int128_7253 = torch.constant.int 128
    %5776 = torch.prim.ListConstruct %5765, %int8_7252, %int128_7253 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5777 = torch.aten.view %5775, %5776 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %5777, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_7254 = torch.constant.int 32
    %5778 = torch.aten.floor_divide.Scalar %arg2, %int32_7254 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_7255 = torch.constant.int 1
    %5779 = torch.aten.unsqueeze %5778, %int1_7255 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_7256 = torch.constant.int 1
    %false_7257 = torch.constant.bool false
    %5780 = torch.aten.gather %arg3, %int1_7256, %5779, %false_7257 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_7258 = torch.constant.int 32
    %5781 = torch.aten.remainder.Scalar %arg2, %int32_7258 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_7259 = torch.constant.int 1
    %5782 = torch.aten.unsqueeze %5781, %int1_7259 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_7260 = torch.constant.none
    %5783 = torch.aten.clone %292, %none_7260 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_7261 = torch.constant.int 0
    %5784 = torch.aten.unsqueeze %5783, %int0_7261 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_7262 = torch.constant.int 4
    %int1_7263 = torch.constant.int 1
    %5785 = torch.prim.ListConstruct %int4_7262, %int1_7263 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_7264 = torch.constant.int 1
    %int1_7265 = torch.constant.int 1
    %5786 = torch.prim.ListConstruct %int1_7264, %int1_7265 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_7266 = torch.constant.int 4
    %int0_7267 = torch.constant.int 0
    %cpu_7268 = torch.constant.device "cpu"
    %false_7269 = torch.constant.bool false
    %5787 = torch.aten.empty_strided %5785, %5786, %int4_7266, %int0_7267, %cpu_7268, %false_7269 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int26_7270 = torch.constant.int 26
    %5788 = torch.aten.fill.Scalar %5787, %int26_7270 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_7271 = torch.constant.int 4
    %int1_7272 = torch.constant.int 1
    %5789 = torch.prim.ListConstruct %int4_7271, %int1_7272 : (!torch.int, !torch.int) -> !torch.list<int>
    %5790 = torch.aten.repeat %5784, %5789 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_7273 = torch.constant.int 32
    %5791 = torch.aten.mul.Scalar %5780, %int32_7273 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_7274 = torch.constant.int 1
    %5792 = torch.aten.add.Tensor %5791, %5788, %int1_7274 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_7275 = torch.constant.int 2
    %5793 = torch.aten.mul.Scalar %5792, %int2_7275 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_7276 = torch.constant.int 1
    %5794 = torch.aten.add.Tensor %5793, %5790, %int1_7276 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_7277 = torch.constant.int 32
    %5795 = torch.aten.mul.Scalar %5794, %int32_7277 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_7278 = torch.constant.int 1
    %5796 = torch.aten.add.Tensor %5795, %5782, %int1_7278 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %5797 = torch.prim.ListConstruct %5796 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_7279 = torch.constant.bool false
    %5798 = torch.aten.index_put %5777, %5797, %5729, %false_7279 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %5798, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_7280 = torch.constant.int 32
    %int2_7281 = torch.constant.int 2
    %int32_7282 = torch.constant.int 32
    %int8_7283 = torch.constant.int 8
    %int128_7284 = torch.constant.int 128
    %5799 = torch.prim.ListConstruct %437, %int32_7280, %int2_7281, %int32_7282, %int8_7283, %int128_7284 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5800 = torch.aten.view %5798, %5799 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5800, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_7285 = torch.constant.int 2097152
    %5801 = torch.prim.ListConstruct %437, %int2097152_7285 : (!torch.int, !torch.int) -> !torch.list<int>
    %5802 = torch.aten.view %5800, %5801 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %5802, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int4_7286 = torch.constant.int 4
    %5803 = torch.prim.ListConstruct %int4_7286, %358 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_7287 = torch.constant.int 1
    %5804 = torch.prim.ListConstruct %358, %int1_7287 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_7288 = torch.constant.int 4
    %int0_7289 = torch.constant.int 0
    %cpu_7290 = torch.constant.device "cpu"
    %false_7291 = torch.constant.bool false
    %5805 = torch.aten.empty_strided %5803, %5804, %int4_7288, %int0_7289, %cpu_7290, %false_7291 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5805, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int26_7292 = torch.constant.int 26
    %5806 = torch.aten.fill.Scalar %5805, %int26_7292 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5806, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int32_7293 = torch.constant.int 32
    %5807 = torch.aten.mul.Scalar %arg3, %int32_7293 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5807, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_7294 = torch.constant.int 1
    %5808 = torch.aten.add.Tensor %5807, %5806, %int1_7294 : !torch.vtensor<[4,?],si64>, !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %5808, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_7295 = torch.constant.int 4
    %5809 = torch.aten.mul.int %int4_7295, %358 : !torch.int, !torch.int -> !torch.int
    %5810 = torch.prim.ListConstruct %5809 : (!torch.int) -> !torch.list<int>
    %5811 = torch.aten.view %5808, %5810 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %5811, [%356], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_7296 = torch.constant.int 32
    %int2_7297 = torch.constant.int 2
    %int32_7298 = torch.constant.int 32
    %int8_7299 = torch.constant.int 8
    %int128_7300 = torch.constant.int 128
    %5812 = torch.prim.ListConstruct %437, %int32_7296, %int2_7297, %int32_7298, %int8_7299, %int128_7300 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5813 = torch.aten.view %5802, %5812 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5813, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_7301 = torch.constant.int 32
    %5814 = torch.aten.mul.int %437, %int32_7301 : !torch.int, !torch.int -> !torch.int
    %int2_7302 = torch.constant.int 2
    %int32_7303 = torch.constant.int 32
    %int8_7304 = torch.constant.int 8
    %int128_7305 = torch.constant.int 128
    %5815 = torch.prim.ListConstruct %5814, %int2_7302, %int32_7303, %int8_7304, %int128_7305 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5816 = torch.aten.view %5813, %5815 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %5816, [%357], affine_map<()[s0] -> (s0 * 32, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int0_7306 = torch.constant.int 0
    %5817 = torch.aten.index_select %5816, %int0_7306, %5811 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.int, !torch.vtensor<[?],si64> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %5817, [%356], affine_map<()[s0] -> (s0 * 4, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int4_7307 = torch.constant.int 4
    %int2_7308 = torch.constant.int 2
    %int32_7309 = torch.constant.int 32
    %int8_7310 = torch.constant.int 8
    %int128_7311 = torch.constant.int 128
    %5818 = torch.prim.ListConstruct %int4_7307, %358, %int2_7308, %int32_7309, %int8_7310, %int128_7311 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5819 = torch.aten.view %5817, %5818 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %5819, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int0_7312 = torch.constant.int 0
    %int0_7313 = torch.constant.int 0
    %int9223372036854775807_7314 = torch.constant.int 9223372036854775807
    %int1_7315 = torch.constant.int 1
    %5820 = torch.aten.slice.Tensor %5819, %int0_7312, %int0_7313, %int9223372036854775807_7314, %int1_7315 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %5820, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_7316 = torch.constant.int 1
    %int0_7317 = torch.constant.int 0
    %int9223372036854775807_7318 = torch.constant.int 9223372036854775807
    %int1_7319 = torch.constant.int 1
    %5821 = torch.aten.slice.Tensor %5820, %int1_7316, %int0_7317, %int9223372036854775807_7318, %int1_7319 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %5821, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_7320 = torch.constant.int 2
    %int0_7321 = torch.constant.int 0
    %5822 = torch.aten.select.int %5821, %int2_7320, %int0_7321 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5822, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int32_7322 = torch.constant.int 32
    %5823 = torch.aten.mul.int %358, %int32_7322 : !torch.int, !torch.int -> !torch.int
    %int2_7323 = torch.constant.int 2
    %int0_7324 = torch.constant.int 0
    %int1_7325 = torch.constant.int 1
    %5824 = torch.aten.slice.Tensor %5822, %int2_7323, %int0_7324, %5823, %int1_7325 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5824, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_7326 = torch.constant.int 0
    %5825 = torch.aten.clone %5824, %int0_7326 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5825, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_7327 = torch.constant.int 1
    %5826 = torch.aten.size.int %5821, %int1_7327 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_7328 = torch.constant.int 32
    %5827 = torch.aten.mul.int %5826, %int32_7328 : !torch.int, !torch.int -> !torch.int
    %int4_7329 = torch.constant.int 4
    %int8_7330 = torch.constant.int 8
    %int128_7331 = torch.constant.int 128
    %5828 = torch.prim.ListConstruct %int4_7329, %5827, %int8_7330, %int128_7331 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5829 = torch.aten._unsafe_view %5825, %5828 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %5829, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_7332 = torch.constant.int 0
    %int0_7333 = torch.constant.int 0
    %int9223372036854775807_7334 = torch.constant.int 9223372036854775807
    %int1_7335 = torch.constant.int 1
    %5830 = torch.aten.slice.Tensor %5829, %int0_7332, %int0_7333, %int9223372036854775807_7334, %int1_7335 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %5830, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_7336 = torch.constant.int 0
    %int0_7337 = torch.constant.int 0
    %int9223372036854775807_7338 = torch.constant.int 9223372036854775807
    %int1_7339 = torch.constant.int 1
    %5831 = torch.aten.slice.Tensor %5819, %int0_7336, %int0_7337, %int9223372036854775807_7338, %int1_7339 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %5831, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_7340 = torch.constant.int 1
    %int0_7341 = torch.constant.int 0
    %int9223372036854775807_7342 = torch.constant.int 9223372036854775807
    %int1_7343 = torch.constant.int 1
    %5832 = torch.aten.slice.Tensor %5831, %int1_7340, %int0_7341, %int9223372036854775807_7342, %int1_7343 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %5832, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_7344 = torch.constant.int 2
    %int1_7345 = torch.constant.int 1
    %5833 = torch.aten.select.int %5832, %int2_7344, %int1_7345 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5833, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int2_7346 = torch.constant.int 2
    %int0_7347 = torch.constant.int 0
    %int1_7348 = torch.constant.int 1
    %5834 = torch.aten.slice.Tensor %5833, %int2_7346, %int0_7347, %5823, %int1_7348 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5834, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_7349 = torch.constant.int 0
    %5835 = torch.aten.clone %5834, %int0_7349 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %5835, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_7350 = torch.constant.int 1
    %5836 = torch.aten.size.int %5832, %int1_7350 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_7351 = torch.constant.int 32
    %5837 = torch.aten.mul.int %5836, %int32_7351 : !torch.int, !torch.int -> !torch.int
    %int4_7352 = torch.constant.int 4
    %int8_7353 = torch.constant.int 8
    %int128_7354 = torch.constant.int 128
    %5838 = torch.prim.ListConstruct %int4_7352, %5837, %int8_7353, %int128_7354 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5839 = torch.aten._unsafe_view %5835, %5838 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %5839, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_7355 = torch.constant.int 0
    %int0_7356 = torch.constant.int 0
    %int9223372036854775807_7357 = torch.constant.int 9223372036854775807
    %int1_7358 = torch.constant.int 1
    %5840 = torch.aten.slice.Tensor %5839, %int0_7355, %int0_7356, %int9223372036854775807_7357, %int1_7358 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %5840, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int-2_7359 = torch.constant.int -2
    %5841 = torch.aten.unsqueeze %5830, %int-2_7359 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %5841, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_7360 = torch.constant.int 1
    %5842 = torch.aten.size.int %5829, %int1_7360 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_7361 = torch.constant.int 4
    %int8_7362 = torch.constant.int 8
    %int4_7363 = torch.constant.int 4
    %int128_7364 = torch.constant.int 128
    %5843 = torch.prim.ListConstruct %int4_7361, %5842, %int8_7362, %int4_7363, %int128_7364 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_7365 = torch.constant.bool false
    %5844 = torch.aten.expand %5841, %5843, %false_7365 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5844, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_7366 = torch.constant.int 0
    %5845 = torch.aten.clone %5844, %int0_7366 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5845, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_7367 = torch.constant.int 4
    %int32_7368 = torch.constant.int 32
    %int128_7369 = torch.constant.int 128
    %5846 = torch.prim.ListConstruct %int4_7367, %5842, %int32_7368, %int128_7369 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5847 = torch.aten._unsafe_view %5845, %5846 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %5847, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_7370 = torch.constant.int -2
    %5848 = torch.aten.unsqueeze %5840, %int-2_7370 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %5848, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_7371 = torch.constant.int 1
    %5849 = torch.aten.size.int %5839, %int1_7371 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_7372 = torch.constant.int 4
    %int8_7373 = torch.constant.int 8
    %int4_7374 = torch.constant.int 4
    %int128_7375 = torch.constant.int 128
    %5850 = torch.prim.ListConstruct %int4_7372, %5849, %int8_7373, %int4_7374, %int128_7375 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_7376 = torch.constant.bool false
    %5851 = torch.aten.expand %5848, %5850, %false_7376 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5851, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_7377 = torch.constant.int 0
    %5852 = torch.aten.clone %5851, %int0_7377 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %5852, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_7378 = torch.constant.int 4
    %int32_7379 = torch.constant.int 32
    %int128_7380 = torch.constant.int 128
    %5853 = torch.prim.ListConstruct %int4_7378, %5849, %int32_7379, %int128_7380 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5854 = torch.aten._unsafe_view %5852, %5853 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %5854, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_7381 = torch.constant.int 1
    %int2_7382 = torch.constant.int 2
    %5855 = torch.aten.transpose.int %5735, %int1_7381, %int2_7382 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,128],f16>
    %int1_7383 = torch.constant.int 1
    %int2_7384 = torch.constant.int 2
    %5856 = torch.aten.transpose.int %5847, %int1_7383, %int2_7384 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %5856, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_7385 = torch.constant.int 1
    %int2_7386 = torch.constant.int 2
    %5857 = torch.aten.transpose.int %5854, %int1_7385, %int2_7386 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %5857, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_7387 = torch.constant.float 0.000000e+00
    %false_7388 = torch.constant.bool false
    %none_7389 = torch.constant.none
    %5858:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5855, %5856, %5857, %float0.000000e00_7387, %false_7388, %368, %none_7389) : (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.vtensor<[4,1,1,?],f16>, !torch.none) -> (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,1],f32>) 
    %int1_7390 = torch.constant.int 1
    %int2_7391 = torch.constant.int 2
    %5859 = torch.aten.transpose.int %5858#0, %int1_7390, %int2_7391 : !torch.vtensor<[4,32,1,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int4_7392 = torch.constant.int 4
    %int1_7393 = torch.constant.int 1
    %int4096_7394 = torch.constant.int 4096
    %5860 = torch.prim.ListConstruct %int4_7392, %int1_7393, %int4096_7394 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5861 = torch.aten.view %5859, %5860 : !torch.vtensor<[4,1,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_7395 = torch.constant.int -2
    %int-1_7396 = torch.constant.int -1
    %5862 = torch.aten.transpose.int %293, %int-2_7395, %int-1_7396 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_7397 = torch.constant.int 4
    %int4096_7398 = torch.constant.int 4096
    %5863 = torch.prim.ListConstruct %int4_7397, %int4096_7398 : (!torch.int, !torch.int) -> !torch.list<int>
    %5864 = torch.aten.view %5861, %5863 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %5865 = torch.aten.mm %5864, %5862 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_7399 = torch.constant.int 4
    %int1_7400 = torch.constant.int 1
    %int4096_7401 = torch.constant.int 4096
    %5866 = torch.prim.ListConstruct %int4_7399, %int1_7400, %int4096_7401 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5867 = torch.aten.view %5865, %5866 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_7402 = torch.constant.int 1
    %5868 = torch.aten.add.Tensor %5695, %5867, %int1_7402 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_7403 = torch.constant.int 6
    %5869 = torch.prims.convert_element_type %5868, %int6_7403 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_7404 = torch.constant.int 2
    %5870 = torch.aten.pow.Tensor_Scalar %5869, %int2_7404 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_7405 = torch.constant.int -1
    %5871 = torch.prim.ListConstruct %int-1_7405 : (!torch.int) -> !torch.list<int>
    %true_7406 = torch.constant.bool true
    %none_7407 = torch.constant.none
    %5872 = torch.aten.mean.dim %5870, %5871, %true_7406, %none_7407 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_7408 = torch.constant.float 9.9999997473787516E-6
    %int1_7409 = torch.constant.int 1
    %5873 = torch.aten.add.Scalar %5872, %float9.999990e-06_7408, %int1_7409 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %5874 = torch.aten.rsqrt %5873 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %5875 = torch.aten.mul.Tensor %5869, %5874 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_7410 = torch.constant.int 5
    %5876 = torch.prims.convert_element_type %5875, %int5_7410 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %5877 = torch.aten.mul.Tensor %294, %5876 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_7411 = torch.constant.int 5
    %5878 = torch.prims.convert_element_type %5877, %int5_7411 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_7412 = torch.constant.int -2
    %int-1_7413 = torch.constant.int -1
    %5879 = torch.aten.transpose.int %295, %int-2_7412, %int-1_7413 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_7414 = torch.constant.int 4
    %int4096_7415 = torch.constant.int 4096
    %5880 = torch.prim.ListConstruct %int4_7414, %int4096_7415 : (!torch.int, !torch.int) -> !torch.list<int>
    %5881 = torch.aten.view %5878, %5880 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %5882 = torch.aten.mm %5881, %5879 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_7416 = torch.constant.int 4
    %int1_7417 = torch.constant.int 1
    %int14336_7418 = torch.constant.int 14336
    %5883 = torch.prim.ListConstruct %int4_7416, %int1_7417, %int14336_7418 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5884 = torch.aten.view %5882, %5883 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %5885 = torch.aten.silu %5884 : !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_7419 = torch.constant.int -2
    %int-1_7420 = torch.constant.int -1
    %5886 = torch.aten.transpose.int %296, %int-2_7419, %int-1_7420 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_7421 = torch.constant.int 4
    %int4096_7422 = torch.constant.int 4096
    %5887 = torch.prim.ListConstruct %int4_7421, %int4096_7422 : (!torch.int, !torch.int) -> !torch.list<int>
    %5888 = torch.aten.view %5878, %5887 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %5889 = torch.aten.mm %5888, %5886 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_7423 = torch.constant.int 4
    %int1_7424 = torch.constant.int 1
    %int14336_7425 = torch.constant.int 14336
    %5890 = torch.prim.ListConstruct %int4_7423, %int1_7424, %int14336_7425 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5891 = torch.aten.view %5889, %5890 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %5892 = torch.aten.mul.Tensor %5885, %5891 : !torch.vtensor<[4,1,14336],f16>, !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_7426 = torch.constant.int -2
    %int-1_7427 = torch.constant.int -1
    %5893 = torch.aten.transpose.int %297, %int-2_7426, %int-1_7427 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int4_7428 = torch.constant.int 4
    %int14336_7429 = torch.constant.int 14336
    %5894 = torch.prim.ListConstruct %int4_7428, %int14336_7429 : (!torch.int, !torch.int) -> !torch.list<int>
    %5895 = torch.aten.view %5892, %5894 : !torch.vtensor<[4,1,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,14336],f16>
    %5896 = torch.aten.mm %5895, %5893 : !torch.vtensor<[4,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_7430 = torch.constant.int 4
    %int1_7431 = torch.constant.int 1
    %int4096_7432 = torch.constant.int 4096
    %5897 = torch.prim.ListConstruct %int4_7430, %int1_7431, %int4096_7432 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5898 = torch.aten.view %5896, %5897 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_7433 = torch.constant.int 1
    %5899 = torch.aten.add.Tensor %5868, %5898, %int1_7433 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_7434 = torch.constant.int 6
    %5900 = torch.prims.convert_element_type %5899, %int6_7434 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_7435 = torch.constant.int 2
    %5901 = torch.aten.pow.Tensor_Scalar %5900, %int2_7435 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_7436 = torch.constant.int -1
    %5902 = torch.prim.ListConstruct %int-1_7436 : (!torch.int) -> !torch.list<int>
    %true_7437 = torch.constant.bool true
    %none_7438 = torch.constant.none
    %5903 = torch.aten.mean.dim %5901, %5902, %true_7437, %none_7438 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_7439 = torch.constant.float 9.9999997473787516E-6
    %int1_7440 = torch.constant.int 1
    %5904 = torch.aten.add.Scalar %5903, %float9.999990e-06_7439, %int1_7440 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %5905 = torch.aten.rsqrt %5904 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %5906 = torch.aten.mul.Tensor %5900, %5905 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_7441 = torch.constant.int 5
    %5907 = torch.prims.convert_element_type %5906, %int5_7441 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %5908 = torch.aten.mul.Tensor %298, %5907 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_7442 = torch.constant.int 5
    %5909 = torch.prims.convert_element_type %5908, %int5_7442 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_7443 = torch.constant.int -2
    %int-1_7444 = torch.constant.int -1
    %5910 = torch.aten.transpose.int %299, %int-2_7443, %int-1_7444 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_7445 = torch.constant.int 4
    %int4096_7446 = torch.constant.int 4096
    %5911 = torch.prim.ListConstruct %int4_7445, %int4096_7446 : (!torch.int, !torch.int) -> !torch.list<int>
    %5912 = torch.aten.view %5909, %5911 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %5913 = torch.aten.mm %5912, %5910 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_7447 = torch.constant.int 4
    %int1_7448 = torch.constant.int 1
    %int4096_7449 = torch.constant.int 4096
    %5914 = torch.prim.ListConstruct %int4_7447, %int1_7448, %int4096_7449 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5915 = torch.aten.view %5913, %5914 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_7450 = torch.constant.int -2
    %int-1_7451 = torch.constant.int -1
    %5916 = torch.aten.transpose.int %300, %int-2_7450, %int-1_7451 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_7452 = torch.constant.int 4
    %int4096_7453 = torch.constant.int 4096
    %5917 = torch.prim.ListConstruct %int4_7452, %int4096_7453 : (!torch.int, !torch.int) -> !torch.list<int>
    %5918 = torch.aten.view %5909, %5917 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %5919 = torch.aten.mm %5918, %5916 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_7454 = torch.constant.int 4
    %int1_7455 = torch.constant.int 1
    %int1024_7456 = torch.constant.int 1024
    %5920 = torch.prim.ListConstruct %int4_7454, %int1_7455, %int1024_7456 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5921 = torch.aten.view %5919, %5920 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int-2_7457 = torch.constant.int -2
    %int-1_7458 = torch.constant.int -1
    %5922 = torch.aten.transpose.int %301, %int-2_7457, %int-1_7458 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_7459 = torch.constant.int 4
    %int4096_7460 = torch.constant.int 4096
    %5923 = torch.prim.ListConstruct %int4_7459, %int4096_7460 : (!torch.int, !torch.int) -> !torch.list<int>
    %5924 = torch.aten.view %5909, %5923 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %5925 = torch.aten.mm %5924, %5922 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_7461 = torch.constant.int 4
    %int1_7462 = torch.constant.int 1
    %int1024_7463 = torch.constant.int 1024
    %5926 = torch.prim.ListConstruct %int4_7461, %int1_7462, %int1024_7463 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5927 = torch.aten.view %5925, %5926 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int4_7464 = torch.constant.int 4
    %int1_7465 = torch.constant.int 1
    %int32_7466 = torch.constant.int 32
    %int128_7467 = torch.constant.int 128
    %5928 = torch.prim.ListConstruct %int4_7464, %int1_7465, %int32_7466, %int128_7467 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5929 = torch.aten.view %5915, %5928 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,128],f16>
    %int4_7468 = torch.constant.int 4
    %int1_7469 = torch.constant.int 1
    %int8_7470 = torch.constant.int 8
    %int128_7471 = torch.constant.int 128
    %5930 = torch.prim.ListConstruct %int4_7468, %int1_7469, %int8_7470, %int128_7471 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5931 = torch.aten.view %5921, %5930 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int4_7472 = torch.constant.int 4
    %int1_7473 = torch.constant.int 1
    %int8_7474 = torch.constant.int 8
    %int128_7475 = torch.constant.int 128
    %5932 = torch.prim.ListConstruct %int4_7472, %int1_7473, %int8_7474, %int128_7475 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5933 = torch.aten.view %5927, %5932 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int6_7476 = torch.constant.int 6
    %5934 = torch.prims.convert_element_type %5929, %int6_7476 : !torch.vtensor<[4,1,32,128],f16>, !torch.int -> !torch.vtensor<[4,1,32,128],f32>
    %5935 = torch_c.to_builtin_tensor %5934 : !torch.vtensor<[4,1,32,128],f32> -> tensor<4x1x32x128xf32>
    %5936 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %5937 = util.call @sharktank_rotary_embedding_4_1_32_128_f32(%5935, %5936) : (tensor<4x1x32x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x32x128xf32>
    %5938 = torch_c.from_builtin_tensor %5937 : tensor<4x1x32x128xf32> -> !torch.vtensor<[4,1,32,128],f32>
    %int5_7477 = torch.constant.int 5
    %5939 = torch.prims.convert_element_type %5938, %int5_7477 : !torch.vtensor<[4,1,32,128],f32>, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int6_7478 = torch.constant.int 6
    %5940 = torch.prims.convert_element_type %5931, %int6_7478 : !torch.vtensor<[4,1,8,128],f16>, !torch.int -> !torch.vtensor<[4,1,8,128],f32>
    %5941 = torch_c.to_builtin_tensor %5940 : !torch.vtensor<[4,1,8,128],f32> -> tensor<4x1x8x128xf32>
    %5942 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %5943 = util.call @sharktank_rotary_embedding_4_1_8_128_f32(%5941, %5942) : (tensor<4x1x8x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x8x128xf32>
    %5944 = torch_c.from_builtin_tensor %5943 : tensor<4x1x8x128xf32> -> !torch.vtensor<[4,1,8,128],f32>
    %int5_7479 = torch.constant.int 5
    %5945 = torch.prims.convert_element_type %5944, %int5_7479 : !torch.vtensor<[4,1,8,128],f32>, !torch.int -> !torch.vtensor<[4,1,8,128],f16>
    %int32_7480 = torch.constant.int 32
    %5946 = torch.aten.floor_divide.Scalar %arg2, %int32_7480 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_7481 = torch.constant.int 1
    %5947 = torch.aten.unsqueeze %5946, %int1_7481 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_7482 = torch.constant.int 1
    %false_7483 = torch.constant.bool false
    %5948 = torch.aten.gather %arg3, %int1_7482, %5947, %false_7483 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_7484 = torch.constant.int 32
    %5949 = torch.aten.remainder.Scalar %arg2, %int32_7484 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_7485 = torch.constant.int 1
    %5950 = torch.aten.unsqueeze %5949, %int1_7485 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_7486 = torch.constant.none
    %5951 = torch.aten.clone %302, %none_7486 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_7487 = torch.constant.int 0
    %5952 = torch.aten.unsqueeze %5951, %int0_7487 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_7488 = torch.constant.int 4
    %int1_7489 = torch.constant.int 1
    %5953 = torch.prim.ListConstruct %int4_7488, %int1_7489 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_7490 = torch.constant.int 1
    %int1_7491 = torch.constant.int 1
    %5954 = torch.prim.ListConstruct %int1_7490, %int1_7491 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_7492 = torch.constant.int 4
    %int0_7493 = torch.constant.int 0
    %cpu_7494 = torch.constant.device "cpu"
    %false_7495 = torch.constant.bool false
    %5955 = torch.aten.empty_strided %5953, %5954, %int4_7492, %int0_7493, %cpu_7494, %false_7495 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int27 = torch.constant.int 27
    %5956 = torch.aten.fill.Scalar %5955, %int27 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_7496 = torch.constant.int 4
    %int1_7497 = torch.constant.int 1
    %5957 = torch.prim.ListConstruct %int4_7496, %int1_7497 : (!torch.int, !torch.int) -> !torch.list<int>
    %5958 = torch.aten.repeat %5952, %5957 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_7498 = torch.constant.int 32
    %5959 = torch.aten.mul.Scalar %5948, %int32_7498 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_7499 = torch.constant.int 1
    %5960 = torch.aten.add.Tensor %5959, %5956, %int1_7499 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_7500 = torch.constant.int 2
    %5961 = torch.aten.mul.Scalar %5960, %int2_7500 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_7501 = torch.constant.int 1
    %5962 = torch.aten.add.Tensor %5961, %5958, %int1_7501 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_7502 = torch.constant.int 32
    %5963 = torch.aten.mul.Scalar %5962, %int32_7502 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_7503 = torch.constant.int 1
    %5964 = torch.aten.add.Tensor %5963, %5950, %int1_7503 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_7504 = torch.constant.int 32
    %int2_7505 = torch.constant.int 2
    %int32_7506 = torch.constant.int 32
    %int8_7507 = torch.constant.int 8
    %int128_7508 = torch.constant.int 128
    %5965 = torch.prim.ListConstruct %437, %int32_7504, %int2_7505, %int32_7506, %int8_7507, %int128_7508 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5966 = torch.aten.view %5802, %5965 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5966, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_7509 = torch.constant.int 32
    %5967 = torch.aten.mul.int %437, %int32_7509 : !torch.int, !torch.int -> !torch.int
    %int2_7510 = torch.constant.int 2
    %5968 = torch.aten.mul.int %5967, %int2_7510 : !torch.int, !torch.int -> !torch.int
    %int32_7511 = torch.constant.int 32
    %5969 = torch.aten.mul.int %5968, %int32_7511 : !torch.int, !torch.int -> !torch.int
    %int8_7512 = torch.constant.int 8
    %int128_7513 = torch.constant.int 128
    %5970 = torch.prim.ListConstruct %5969, %int8_7512, %int128_7513 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5971 = torch.aten.view %5966, %5970 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %5971, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %5972 = torch.prim.ListConstruct %5964 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_7514 = torch.constant.bool false
    %5973 = torch.aten.index_put %5971, %5972, %5945, %false_7514 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %5973, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_7515 = torch.constant.int 32
    %int2_7516 = torch.constant.int 2
    %int32_7517 = torch.constant.int 32
    %int8_7518 = torch.constant.int 8
    %int128_7519 = torch.constant.int 128
    %5974 = torch.prim.ListConstruct %437, %int32_7515, %int2_7516, %int32_7517, %int8_7518, %int128_7519 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5975 = torch.aten.view %5973, %5974 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5975, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_7520 = torch.constant.int 2097152
    %5976 = torch.prim.ListConstruct %437, %int2097152_7520 : (!torch.int, !torch.int) -> !torch.list<int>
    %5977 = torch.aten.view %5975, %5976 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %5977, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_7521 = torch.constant.int 32
    %int2_7522 = torch.constant.int 2
    %int32_7523 = torch.constant.int 32
    %int8_7524 = torch.constant.int 8
    %int128_7525 = torch.constant.int 128
    %5978 = torch.prim.ListConstruct %437, %int32_7521, %int2_7522, %int32_7523, %int8_7524, %int128_7525 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5979 = torch.aten.view %5977, %5978 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %5979, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int8_7526 = torch.constant.int 8
    %int128_7527 = torch.constant.int 128
    %5980 = torch.prim.ListConstruct %5969, %int8_7526, %int128_7527 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5981 = torch.aten.view %5979, %5980 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %5981, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_7528 = torch.constant.int 32
    %5982 = torch.aten.floor_divide.Scalar %arg2, %int32_7528 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_7529 = torch.constant.int 1
    %5983 = torch.aten.unsqueeze %5982, %int1_7529 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_7530 = torch.constant.int 1
    %false_7531 = torch.constant.bool false
    %5984 = torch.aten.gather %arg3, %int1_7530, %5983, %false_7531 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_7532 = torch.constant.int 32
    %5985 = torch.aten.remainder.Scalar %arg2, %int32_7532 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_7533 = torch.constant.int 1
    %5986 = torch.aten.unsqueeze %5985, %int1_7533 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_7534 = torch.constant.none
    %5987 = torch.aten.clone %303, %none_7534 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_7535 = torch.constant.int 0
    %5988 = torch.aten.unsqueeze %5987, %int0_7535 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_7536 = torch.constant.int 4
    %int1_7537 = torch.constant.int 1
    %5989 = torch.prim.ListConstruct %int4_7536, %int1_7537 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_7538 = torch.constant.int 1
    %int1_7539 = torch.constant.int 1
    %5990 = torch.prim.ListConstruct %int1_7538, %int1_7539 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_7540 = torch.constant.int 4
    %int0_7541 = torch.constant.int 0
    %cpu_7542 = torch.constant.device "cpu"
    %false_7543 = torch.constant.bool false
    %5991 = torch.aten.empty_strided %5989, %5990, %int4_7540, %int0_7541, %cpu_7542, %false_7543 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int27_7544 = torch.constant.int 27
    %5992 = torch.aten.fill.Scalar %5991, %int27_7544 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_7545 = torch.constant.int 4
    %int1_7546 = torch.constant.int 1
    %5993 = torch.prim.ListConstruct %int4_7545, %int1_7546 : (!torch.int, !torch.int) -> !torch.list<int>
    %5994 = torch.aten.repeat %5988, %5993 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_7547 = torch.constant.int 32
    %5995 = torch.aten.mul.Scalar %5984, %int32_7547 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_7548 = torch.constant.int 1
    %5996 = torch.aten.add.Tensor %5995, %5992, %int1_7548 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_7549 = torch.constant.int 2
    %5997 = torch.aten.mul.Scalar %5996, %int2_7549 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_7550 = torch.constant.int 1
    %5998 = torch.aten.add.Tensor %5997, %5994, %int1_7550 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_7551 = torch.constant.int 32
    %5999 = torch.aten.mul.Scalar %5998, %int32_7551 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_7552 = torch.constant.int 1
    %6000 = torch.aten.add.Tensor %5999, %5986, %int1_7552 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %6001 = torch.prim.ListConstruct %6000 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_7553 = torch.constant.bool false
    %6002 = torch.aten.index_put %5981, %6001, %5933, %false_7553 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %6002, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_7554 = torch.constant.int 32
    %int2_7555 = torch.constant.int 2
    %int32_7556 = torch.constant.int 32
    %int8_7557 = torch.constant.int 8
    %int128_7558 = torch.constant.int 128
    %6003 = torch.prim.ListConstruct %437, %int32_7554, %int2_7555, %int32_7556, %int8_7557, %int128_7558 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6004 = torch.aten.view %6002, %6003 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %6004, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_7559 = torch.constant.int 2097152
    %6005 = torch.prim.ListConstruct %437, %int2097152_7559 : (!torch.int, !torch.int) -> !torch.list<int>
    %6006 = torch.aten.view %6004, %6005 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %6006, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int4_7560 = torch.constant.int 4
    %6007 = torch.prim.ListConstruct %int4_7560, %358 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_7561 = torch.constant.int 1
    %6008 = torch.prim.ListConstruct %358, %int1_7561 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_7562 = torch.constant.int 4
    %int0_7563 = torch.constant.int 0
    %cpu_7564 = torch.constant.device "cpu"
    %false_7565 = torch.constant.bool false
    %6009 = torch.aten.empty_strided %6007, %6008, %int4_7562, %int0_7563, %cpu_7564, %false_7565 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6009, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int27_7566 = torch.constant.int 27
    %6010 = torch.aten.fill.Scalar %6009, %int27_7566 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6010, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int32_7567 = torch.constant.int 32
    %6011 = torch.aten.mul.Scalar %arg3, %int32_7567 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6011, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_7568 = torch.constant.int 1
    %6012 = torch.aten.add.Tensor %6011, %6010, %int1_7568 : !torch.vtensor<[4,?],si64>, !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6012, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_7569 = torch.constant.int 4
    %6013 = torch.aten.mul.int %int4_7569, %358 : !torch.int, !torch.int -> !torch.int
    %6014 = torch.prim.ListConstruct %6013 : (!torch.int) -> !torch.list<int>
    %6015 = torch.aten.view %6012, %6014 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %6015, [%356], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_7570 = torch.constant.int 32
    %int2_7571 = torch.constant.int 2
    %int32_7572 = torch.constant.int 32
    %int8_7573 = torch.constant.int 8
    %int128_7574 = torch.constant.int 128
    %6016 = torch.prim.ListConstruct %437, %int32_7570, %int2_7571, %int32_7572, %int8_7573, %int128_7574 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6017 = torch.aten.view %6006, %6016 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %6017, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_7575 = torch.constant.int 32
    %6018 = torch.aten.mul.int %437, %int32_7575 : !torch.int, !torch.int -> !torch.int
    %int2_7576 = torch.constant.int 2
    %int32_7577 = torch.constant.int 32
    %int8_7578 = torch.constant.int 8
    %int128_7579 = torch.constant.int 128
    %6019 = torch.prim.ListConstruct %6018, %int2_7576, %int32_7577, %int8_7578, %int128_7579 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6020 = torch.aten.view %6017, %6019 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %6020, [%357], affine_map<()[s0] -> (s0 * 32, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int0_7580 = torch.constant.int 0
    %6021 = torch.aten.index_select %6020, %int0_7580, %6015 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.int, !torch.vtensor<[?],si64> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %6021, [%356], affine_map<()[s0] -> (s0 * 4, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int4_7581 = torch.constant.int 4
    %int2_7582 = torch.constant.int 2
    %int32_7583 = torch.constant.int 32
    %int8_7584 = torch.constant.int 8
    %int128_7585 = torch.constant.int 128
    %6022 = torch.prim.ListConstruct %int4_7581, %358, %int2_7582, %int32_7583, %int8_7584, %int128_7585 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6023 = torch.aten.view %6021, %6022 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %6023, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int0_7586 = torch.constant.int 0
    %int0_7587 = torch.constant.int 0
    %int9223372036854775807_7588 = torch.constant.int 9223372036854775807
    %int1_7589 = torch.constant.int 1
    %6024 = torch.aten.slice.Tensor %6023, %int0_7586, %int0_7587, %int9223372036854775807_7588, %int1_7589 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %6024, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_7590 = torch.constant.int 1
    %int0_7591 = torch.constant.int 0
    %int9223372036854775807_7592 = torch.constant.int 9223372036854775807
    %int1_7593 = torch.constant.int 1
    %6025 = torch.aten.slice.Tensor %6024, %int1_7590, %int0_7591, %int9223372036854775807_7592, %int1_7593 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %6025, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_7594 = torch.constant.int 2
    %int0_7595 = torch.constant.int 0
    %6026 = torch.aten.select.int %6025, %int2_7594, %int0_7595 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6026, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int32_7596 = torch.constant.int 32
    %6027 = torch.aten.mul.int %358, %int32_7596 : !torch.int, !torch.int -> !torch.int
    %int2_7597 = torch.constant.int 2
    %int0_7598 = torch.constant.int 0
    %int1_7599 = torch.constant.int 1
    %6028 = torch.aten.slice.Tensor %6026, %int2_7597, %int0_7598, %6027, %int1_7599 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6028, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_7600 = torch.constant.int 0
    %6029 = torch.aten.clone %6028, %int0_7600 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6029, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_7601 = torch.constant.int 1
    %6030 = torch.aten.size.int %6025, %int1_7601 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_7602 = torch.constant.int 32
    %6031 = torch.aten.mul.int %6030, %int32_7602 : !torch.int, !torch.int -> !torch.int
    %int4_7603 = torch.constant.int 4
    %int8_7604 = torch.constant.int 8
    %int128_7605 = torch.constant.int 128
    %6032 = torch.prim.ListConstruct %int4_7603, %6031, %int8_7604, %int128_7605 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6033 = torch.aten._unsafe_view %6029, %6032 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %6033, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_7606 = torch.constant.int 0
    %int0_7607 = torch.constant.int 0
    %int9223372036854775807_7608 = torch.constant.int 9223372036854775807
    %int1_7609 = torch.constant.int 1
    %6034 = torch.aten.slice.Tensor %6033, %int0_7606, %int0_7607, %int9223372036854775807_7608, %int1_7609 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %6034, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_7610 = torch.constant.int 0
    %int0_7611 = torch.constant.int 0
    %int9223372036854775807_7612 = torch.constant.int 9223372036854775807
    %int1_7613 = torch.constant.int 1
    %6035 = torch.aten.slice.Tensor %6023, %int0_7610, %int0_7611, %int9223372036854775807_7612, %int1_7613 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %6035, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_7614 = torch.constant.int 1
    %int0_7615 = torch.constant.int 0
    %int9223372036854775807_7616 = torch.constant.int 9223372036854775807
    %int1_7617 = torch.constant.int 1
    %6036 = torch.aten.slice.Tensor %6035, %int1_7614, %int0_7615, %int9223372036854775807_7616, %int1_7617 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %6036, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_7618 = torch.constant.int 2
    %int1_7619 = torch.constant.int 1
    %6037 = torch.aten.select.int %6036, %int2_7618, %int1_7619 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6037, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int2_7620 = torch.constant.int 2
    %int0_7621 = torch.constant.int 0
    %int1_7622 = torch.constant.int 1
    %6038 = torch.aten.slice.Tensor %6037, %int2_7620, %int0_7621, %6027, %int1_7622 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6038, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_7623 = torch.constant.int 0
    %6039 = torch.aten.clone %6038, %int0_7623 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6039, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_7624 = torch.constant.int 1
    %6040 = torch.aten.size.int %6036, %int1_7624 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_7625 = torch.constant.int 32
    %6041 = torch.aten.mul.int %6040, %int32_7625 : !torch.int, !torch.int -> !torch.int
    %int4_7626 = torch.constant.int 4
    %int8_7627 = torch.constant.int 8
    %int128_7628 = torch.constant.int 128
    %6042 = torch.prim.ListConstruct %int4_7626, %6041, %int8_7627, %int128_7628 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6043 = torch.aten._unsafe_view %6039, %6042 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %6043, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_7629 = torch.constant.int 0
    %int0_7630 = torch.constant.int 0
    %int9223372036854775807_7631 = torch.constant.int 9223372036854775807
    %int1_7632 = torch.constant.int 1
    %6044 = torch.aten.slice.Tensor %6043, %int0_7629, %int0_7630, %int9223372036854775807_7631, %int1_7632 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %6044, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int-2_7633 = torch.constant.int -2
    %6045 = torch.aten.unsqueeze %6034, %int-2_7633 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %6045, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_7634 = torch.constant.int 1
    %6046 = torch.aten.size.int %6033, %int1_7634 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_7635 = torch.constant.int 4
    %int8_7636 = torch.constant.int 8
    %int4_7637 = torch.constant.int 4
    %int128_7638 = torch.constant.int 128
    %6047 = torch.prim.ListConstruct %int4_7635, %6046, %int8_7636, %int4_7637, %int128_7638 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_7639 = torch.constant.bool false
    %6048 = torch.aten.expand %6045, %6047, %false_7639 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %6048, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_7640 = torch.constant.int 0
    %6049 = torch.aten.clone %6048, %int0_7640 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %6049, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_7641 = torch.constant.int 4
    %int32_7642 = torch.constant.int 32
    %int128_7643 = torch.constant.int 128
    %6050 = torch.prim.ListConstruct %int4_7641, %6046, %int32_7642, %int128_7643 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6051 = torch.aten._unsafe_view %6049, %6050 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %6051, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_7644 = torch.constant.int -2
    %6052 = torch.aten.unsqueeze %6044, %int-2_7644 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %6052, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_7645 = torch.constant.int 1
    %6053 = torch.aten.size.int %6043, %int1_7645 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_7646 = torch.constant.int 4
    %int8_7647 = torch.constant.int 8
    %int4_7648 = torch.constant.int 4
    %int128_7649 = torch.constant.int 128
    %6054 = torch.prim.ListConstruct %int4_7646, %6053, %int8_7647, %int4_7648, %int128_7649 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_7650 = torch.constant.bool false
    %6055 = torch.aten.expand %6052, %6054, %false_7650 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %6055, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_7651 = torch.constant.int 0
    %6056 = torch.aten.clone %6055, %int0_7651 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %6056, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_7652 = torch.constant.int 4
    %int32_7653 = torch.constant.int 32
    %int128_7654 = torch.constant.int 128
    %6057 = torch.prim.ListConstruct %int4_7652, %6053, %int32_7653, %int128_7654 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6058 = torch.aten._unsafe_view %6056, %6057 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %6058, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_7655 = torch.constant.int 1
    %int2_7656 = torch.constant.int 2
    %6059 = torch.aten.transpose.int %5939, %int1_7655, %int2_7656 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,128],f16>
    %int1_7657 = torch.constant.int 1
    %int2_7658 = torch.constant.int 2
    %6060 = torch.aten.transpose.int %6051, %int1_7657, %int2_7658 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %6060, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_7659 = torch.constant.int 1
    %int2_7660 = torch.constant.int 2
    %6061 = torch.aten.transpose.int %6058, %int1_7659, %int2_7660 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %6061, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_7661 = torch.constant.float 0.000000e+00
    %false_7662 = torch.constant.bool false
    %none_7663 = torch.constant.none
    %6062:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6059, %6060, %6061, %float0.000000e00_7661, %false_7662, %368, %none_7663) : (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.vtensor<[4,1,1,?],f16>, !torch.none) -> (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,1],f32>) 
    %int1_7664 = torch.constant.int 1
    %int2_7665 = torch.constant.int 2
    %6063 = torch.aten.transpose.int %6062#0, %int1_7664, %int2_7665 : !torch.vtensor<[4,32,1,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int4_7666 = torch.constant.int 4
    %int1_7667 = torch.constant.int 1
    %int4096_7668 = torch.constant.int 4096
    %6064 = torch.prim.ListConstruct %int4_7666, %int1_7667, %int4096_7668 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6065 = torch.aten.view %6063, %6064 : !torch.vtensor<[4,1,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_7669 = torch.constant.int -2
    %int-1_7670 = torch.constant.int -1
    %6066 = torch.aten.transpose.int %304, %int-2_7669, %int-1_7670 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_7671 = torch.constant.int 4
    %int4096_7672 = torch.constant.int 4096
    %6067 = torch.prim.ListConstruct %int4_7671, %int4096_7672 : (!torch.int, !torch.int) -> !torch.list<int>
    %6068 = torch.aten.view %6065, %6067 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %6069 = torch.aten.mm %6068, %6066 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_7673 = torch.constant.int 4
    %int1_7674 = torch.constant.int 1
    %int4096_7675 = torch.constant.int 4096
    %6070 = torch.prim.ListConstruct %int4_7673, %int1_7674, %int4096_7675 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6071 = torch.aten.view %6069, %6070 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_7676 = torch.constant.int 1
    %6072 = torch.aten.add.Tensor %5899, %6071, %int1_7676 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_7677 = torch.constant.int 6
    %6073 = torch.prims.convert_element_type %6072, %int6_7677 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_7678 = torch.constant.int 2
    %6074 = torch.aten.pow.Tensor_Scalar %6073, %int2_7678 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_7679 = torch.constant.int -1
    %6075 = torch.prim.ListConstruct %int-1_7679 : (!torch.int) -> !torch.list<int>
    %true_7680 = torch.constant.bool true
    %none_7681 = torch.constant.none
    %6076 = torch.aten.mean.dim %6074, %6075, %true_7680, %none_7681 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_7682 = torch.constant.float 9.9999997473787516E-6
    %int1_7683 = torch.constant.int 1
    %6077 = torch.aten.add.Scalar %6076, %float9.999990e-06_7682, %int1_7683 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %6078 = torch.aten.rsqrt %6077 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %6079 = torch.aten.mul.Tensor %6073, %6078 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_7684 = torch.constant.int 5
    %6080 = torch.prims.convert_element_type %6079, %int5_7684 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %6081 = torch.aten.mul.Tensor %305, %6080 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_7685 = torch.constant.int 5
    %6082 = torch.prims.convert_element_type %6081, %int5_7685 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_7686 = torch.constant.int -2
    %int-1_7687 = torch.constant.int -1
    %6083 = torch.aten.transpose.int %306, %int-2_7686, %int-1_7687 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_7688 = torch.constant.int 4
    %int4096_7689 = torch.constant.int 4096
    %6084 = torch.prim.ListConstruct %int4_7688, %int4096_7689 : (!torch.int, !torch.int) -> !torch.list<int>
    %6085 = torch.aten.view %6082, %6084 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %6086 = torch.aten.mm %6085, %6083 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_7690 = torch.constant.int 4
    %int1_7691 = torch.constant.int 1
    %int14336_7692 = torch.constant.int 14336
    %6087 = torch.prim.ListConstruct %int4_7690, %int1_7691, %int14336_7692 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6088 = torch.aten.view %6086, %6087 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %6089 = torch.aten.silu %6088 : !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_7693 = torch.constant.int -2
    %int-1_7694 = torch.constant.int -1
    %6090 = torch.aten.transpose.int %307, %int-2_7693, %int-1_7694 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_7695 = torch.constant.int 4
    %int4096_7696 = torch.constant.int 4096
    %6091 = torch.prim.ListConstruct %int4_7695, %int4096_7696 : (!torch.int, !torch.int) -> !torch.list<int>
    %6092 = torch.aten.view %6082, %6091 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %6093 = torch.aten.mm %6092, %6090 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_7697 = torch.constant.int 4
    %int1_7698 = torch.constant.int 1
    %int14336_7699 = torch.constant.int 14336
    %6094 = torch.prim.ListConstruct %int4_7697, %int1_7698, %int14336_7699 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6095 = torch.aten.view %6093, %6094 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %6096 = torch.aten.mul.Tensor %6089, %6095 : !torch.vtensor<[4,1,14336],f16>, !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_7700 = torch.constant.int -2
    %int-1_7701 = torch.constant.int -1
    %6097 = torch.aten.transpose.int %308, %int-2_7700, %int-1_7701 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int4_7702 = torch.constant.int 4
    %int14336_7703 = torch.constant.int 14336
    %6098 = torch.prim.ListConstruct %int4_7702, %int14336_7703 : (!torch.int, !torch.int) -> !torch.list<int>
    %6099 = torch.aten.view %6096, %6098 : !torch.vtensor<[4,1,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,14336],f16>
    %6100 = torch.aten.mm %6099, %6097 : !torch.vtensor<[4,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_7704 = torch.constant.int 4
    %int1_7705 = torch.constant.int 1
    %int4096_7706 = torch.constant.int 4096
    %6101 = torch.prim.ListConstruct %int4_7704, %int1_7705, %int4096_7706 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6102 = torch.aten.view %6100, %6101 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_7707 = torch.constant.int 1
    %6103 = torch.aten.add.Tensor %6072, %6102, %int1_7707 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_7708 = torch.constant.int 6
    %6104 = torch.prims.convert_element_type %6103, %int6_7708 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_7709 = torch.constant.int 2
    %6105 = torch.aten.pow.Tensor_Scalar %6104, %int2_7709 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_7710 = torch.constant.int -1
    %6106 = torch.prim.ListConstruct %int-1_7710 : (!torch.int) -> !torch.list<int>
    %true_7711 = torch.constant.bool true
    %none_7712 = torch.constant.none
    %6107 = torch.aten.mean.dim %6105, %6106, %true_7711, %none_7712 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_7713 = torch.constant.float 9.9999997473787516E-6
    %int1_7714 = torch.constant.int 1
    %6108 = torch.aten.add.Scalar %6107, %float9.999990e-06_7713, %int1_7714 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %6109 = torch.aten.rsqrt %6108 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %6110 = torch.aten.mul.Tensor %6104, %6109 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_7715 = torch.constant.int 5
    %6111 = torch.prims.convert_element_type %6110, %int5_7715 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %6112 = torch.aten.mul.Tensor %309, %6111 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_7716 = torch.constant.int 5
    %6113 = torch.prims.convert_element_type %6112, %int5_7716 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_7717 = torch.constant.int -2
    %int-1_7718 = torch.constant.int -1
    %6114 = torch.aten.transpose.int %310, %int-2_7717, %int-1_7718 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_7719 = torch.constant.int 4
    %int4096_7720 = torch.constant.int 4096
    %6115 = torch.prim.ListConstruct %int4_7719, %int4096_7720 : (!torch.int, !torch.int) -> !torch.list<int>
    %6116 = torch.aten.view %6113, %6115 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %6117 = torch.aten.mm %6116, %6114 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_7721 = torch.constant.int 4
    %int1_7722 = torch.constant.int 1
    %int4096_7723 = torch.constant.int 4096
    %6118 = torch.prim.ListConstruct %int4_7721, %int1_7722, %int4096_7723 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6119 = torch.aten.view %6117, %6118 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_7724 = torch.constant.int -2
    %int-1_7725 = torch.constant.int -1
    %6120 = torch.aten.transpose.int %311, %int-2_7724, %int-1_7725 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_7726 = torch.constant.int 4
    %int4096_7727 = torch.constant.int 4096
    %6121 = torch.prim.ListConstruct %int4_7726, %int4096_7727 : (!torch.int, !torch.int) -> !torch.list<int>
    %6122 = torch.aten.view %6113, %6121 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %6123 = torch.aten.mm %6122, %6120 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_7728 = torch.constant.int 4
    %int1_7729 = torch.constant.int 1
    %int1024_7730 = torch.constant.int 1024
    %6124 = torch.prim.ListConstruct %int4_7728, %int1_7729, %int1024_7730 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6125 = torch.aten.view %6123, %6124 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int-2_7731 = torch.constant.int -2
    %int-1_7732 = torch.constant.int -1
    %6126 = torch.aten.transpose.int %312, %int-2_7731, %int-1_7732 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_7733 = torch.constant.int 4
    %int4096_7734 = torch.constant.int 4096
    %6127 = torch.prim.ListConstruct %int4_7733, %int4096_7734 : (!torch.int, !torch.int) -> !torch.list<int>
    %6128 = torch.aten.view %6113, %6127 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %6129 = torch.aten.mm %6128, %6126 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_7735 = torch.constant.int 4
    %int1_7736 = torch.constant.int 1
    %int1024_7737 = torch.constant.int 1024
    %6130 = torch.prim.ListConstruct %int4_7735, %int1_7736, %int1024_7737 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6131 = torch.aten.view %6129, %6130 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int4_7738 = torch.constant.int 4
    %int1_7739 = torch.constant.int 1
    %int32_7740 = torch.constant.int 32
    %int128_7741 = torch.constant.int 128
    %6132 = torch.prim.ListConstruct %int4_7738, %int1_7739, %int32_7740, %int128_7741 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6133 = torch.aten.view %6119, %6132 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,128],f16>
    %int4_7742 = torch.constant.int 4
    %int1_7743 = torch.constant.int 1
    %int8_7744 = torch.constant.int 8
    %int128_7745 = torch.constant.int 128
    %6134 = torch.prim.ListConstruct %int4_7742, %int1_7743, %int8_7744, %int128_7745 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6135 = torch.aten.view %6125, %6134 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int4_7746 = torch.constant.int 4
    %int1_7747 = torch.constant.int 1
    %int8_7748 = torch.constant.int 8
    %int128_7749 = torch.constant.int 128
    %6136 = torch.prim.ListConstruct %int4_7746, %int1_7747, %int8_7748, %int128_7749 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6137 = torch.aten.view %6131, %6136 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int6_7750 = torch.constant.int 6
    %6138 = torch.prims.convert_element_type %6133, %int6_7750 : !torch.vtensor<[4,1,32,128],f16>, !torch.int -> !torch.vtensor<[4,1,32,128],f32>
    %6139 = torch_c.to_builtin_tensor %6138 : !torch.vtensor<[4,1,32,128],f32> -> tensor<4x1x32x128xf32>
    %6140 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %6141 = util.call @sharktank_rotary_embedding_4_1_32_128_f32(%6139, %6140) : (tensor<4x1x32x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x32x128xf32>
    %6142 = torch_c.from_builtin_tensor %6141 : tensor<4x1x32x128xf32> -> !torch.vtensor<[4,1,32,128],f32>
    %int5_7751 = torch.constant.int 5
    %6143 = torch.prims.convert_element_type %6142, %int5_7751 : !torch.vtensor<[4,1,32,128],f32>, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int6_7752 = torch.constant.int 6
    %6144 = torch.prims.convert_element_type %6135, %int6_7752 : !torch.vtensor<[4,1,8,128],f16>, !torch.int -> !torch.vtensor<[4,1,8,128],f32>
    %6145 = torch_c.to_builtin_tensor %6144 : !torch.vtensor<[4,1,8,128],f32> -> tensor<4x1x8x128xf32>
    %6146 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %6147 = util.call @sharktank_rotary_embedding_4_1_8_128_f32(%6145, %6146) : (tensor<4x1x8x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x8x128xf32>
    %6148 = torch_c.from_builtin_tensor %6147 : tensor<4x1x8x128xf32> -> !torch.vtensor<[4,1,8,128],f32>
    %int5_7753 = torch.constant.int 5
    %6149 = torch.prims.convert_element_type %6148, %int5_7753 : !torch.vtensor<[4,1,8,128],f32>, !torch.int -> !torch.vtensor<[4,1,8,128],f16>
    %int32_7754 = torch.constant.int 32
    %6150 = torch.aten.floor_divide.Scalar %arg2, %int32_7754 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_7755 = torch.constant.int 1
    %6151 = torch.aten.unsqueeze %6150, %int1_7755 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_7756 = torch.constant.int 1
    %false_7757 = torch.constant.bool false
    %6152 = torch.aten.gather %arg3, %int1_7756, %6151, %false_7757 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_7758 = torch.constant.int 32
    %6153 = torch.aten.remainder.Scalar %arg2, %int32_7758 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_7759 = torch.constant.int 1
    %6154 = torch.aten.unsqueeze %6153, %int1_7759 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_7760 = torch.constant.none
    %6155 = torch.aten.clone %313, %none_7760 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_7761 = torch.constant.int 0
    %6156 = torch.aten.unsqueeze %6155, %int0_7761 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_7762 = torch.constant.int 4
    %int1_7763 = torch.constant.int 1
    %6157 = torch.prim.ListConstruct %int4_7762, %int1_7763 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_7764 = torch.constant.int 1
    %int1_7765 = torch.constant.int 1
    %6158 = torch.prim.ListConstruct %int1_7764, %int1_7765 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_7766 = torch.constant.int 4
    %int0_7767 = torch.constant.int 0
    %cpu_7768 = torch.constant.device "cpu"
    %false_7769 = torch.constant.bool false
    %6159 = torch.aten.empty_strided %6157, %6158, %int4_7766, %int0_7767, %cpu_7768, %false_7769 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int28 = torch.constant.int 28
    %6160 = torch.aten.fill.Scalar %6159, %int28 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_7770 = torch.constant.int 4
    %int1_7771 = torch.constant.int 1
    %6161 = torch.prim.ListConstruct %int4_7770, %int1_7771 : (!torch.int, !torch.int) -> !torch.list<int>
    %6162 = torch.aten.repeat %6156, %6161 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_7772 = torch.constant.int 32
    %6163 = torch.aten.mul.Scalar %6152, %int32_7772 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_7773 = torch.constant.int 1
    %6164 = torch.aten.add.Tensor %6163, %6160, %int1_7773 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_7774 = torch.constant.int 2
    %6165 = torch.aten.mul.Scalar %6164, %int2_7774 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_7775 = torch.constant.int 1
    %6166 = torch.aten.add.Tensor %6165, %6162, %int1_7775 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_7776 = torch.constant.int 32
    %6167 = torch.aten.mul.Scalar %6166, %int32_7776 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_7777 = torch.constant.int 1
    %6168 = torch.aten.add.Tensor %6167, %6154, %int1_7777 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_7778 = torch.constant.int 32
    %int2_7779 = torch.constant.int 2
    %int32_7780 = torch.constant.int 32
    %int8_7781 = torch.constant.int 8
    %int128_7782 = torch.constant.int 128
    %6169 = torch.prim.ListConstruct %437, %int32_7778, %int2_7779, %int32_7780, %int8_7781, %int128_7782 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6170 = torch.aten.view %6006, %6169 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %6170, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_7783 = torch.constant.int 32
    %6171 = torch.aten.mul.int %437, %int32_7783 : !torch.int, !torch.int -> !torch.int
    %int2_7784 = torch.constant.int 2
    %6172 = torch.aten.mul.int %6171, %int2_7784 : !torch.int, !torch.int -> !torch.int
    %int32_7785 = torch.constant.int 32
    %6173 = torch.aten.mul.int %6172, %int32_7785 : !torch.int, !torch.int -> !torch.int
    %int8_7786 = torch.constant.int 8
    %int128_7787 = torch.constant.int 128
    %6174 = torch.prim.ListConstruct %6173, %int8_7786, %int128_7787 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6175 = torch.aten.view %6170, %6174 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %6175, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %6176 = torch.prim.ListConstruct %6168 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_7788 = torch.constant.bool false
    %6177 = torch.aten.index_put %6175, %6176, %6149, %false_7788 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %6177, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_7789 = torch.constant.int 32
    %int2_7790 = torch.constant.int 2
    %int32_7791 = torch.constant.int 32
    %int8_7792 = torch.constant.int 8
    %int128_7793 = torch.constant.int 128
    %6178 = torch.prim.ListConstruct %437, %int32_7789, %int2_7790, %int32_7791, %int8_7792, %int128_7793 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6179 = torch.aten.view %6177, %6178 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %6179, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_7794 = torch.constant.int 2097152
    %6180 = torch.prim.ListConstruct %437, %int2097152_7794 : (!torch.int, !torch.int) -> !torch.list<int>
    %6181 = torch.aten.view %6179, %6180 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %6181, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_7795 = torch.constant.int 32
    %int2_7796 = torch.constant.int 2
    %int32_7797 = torch.constant.int 32
    %int8_7798 = torch.constant.int 8
    %int128_7799 = torch.constant.int 128
    %6182 = torch.prim.ListConstruct %437, %int32_7795, %int2_7796, %int32_7797, %int8_7798, %int128_7799 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6183 = torch.aten.view %6181, %6182 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %6183, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int8_7800 = torch.constant.int 8
    %int128_7801 = torch.constant.int 128
    %6184 = torch.prim.ListConstruct %6173, %int8_7800, %int128_7801 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6185 = torch.aten.view %6183, %6184 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %6185, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_7802 = torch.constant.int 32
    %6186 = torch.aten.floor_divide.Scalar %arg2, %int32_7802 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_7803 = torch.constant.int 1
    %6187 = torch.aten.unsqueeze %6186, %int1_7803 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_7804 = torch.constant.int 1
    %false_7805 = torch.constant.bool false
    %6188 = torch.aten.gather %arg3, %int1_7804, %6187, %false_7805 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_7806 = torch.constant.int 32
    %6189 = torch.aten.remainder.Scalar %arg2, %int32_7806 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_7807 = torch.constant.int 1
    %6190 = torch.aten.unsqueeze %6189, %int1_7807 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_7808 = torch.constant.none
    %6191 = torch.aten.clone %314, %none_7808 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_7809 = torch.constant.int 0
    %6192 = torch.aten.unsqueeze %6191, %int0_7809 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_7810 = torch.constant.int 4
    %int1_7811 = torch.constant.int 1
    %6193 = torch.prim.ListConstruct %int4_7810, %int1_7811 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_7812 = torch.constant.int 1
    %int1_7813 = torch.constant.int 1
    %6194 = torch.prim.ListConstruct %int1_7812, %int1_7813 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_7814 = torch.constant.int 4
    %int0_7815 = torch.constant.int 0
    %cpu_7816 = torch.constant.device "cpu"
    %false_7817 = torch.constant.bool false
    %6195 = torch.aten.empty_strided %6193, %6194, %int4_7814, %int0_7815, %cpu_7816, %false_7817 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int28_7818 = torch.constant.int 28
    %6196 = torch.aten.fill.Scalar %6195, %int28_7818 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_7819 = torch.constant.int 4
    %int1_7820 = torch.constant.int 1
    %6197 = torch.prim.ListConstruct %int4_7819, %int1_7820 : (!torch.int, !torch.int) -> !torch.list<int>
    %6198 = torch.aten.repeat %6192, %6197 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_7821 = torch.constant.int 32
    %6199 = torch.aten.mul.Scalar %6188, %int32_7821 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_7822 = torch.constant.int 1
    %6200 = torch.aten.add.Tensor %6199, %6196, %int1_7822 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_7823 = torch.constant.int 2
    %6201 = torch.aten.mul.Scalar %6200, %int2_7823 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_7824 = torch.constant.int 1
    %6202 = torch.aten.add.Tensor %6201, %6198, %int1_7824 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_7825 = torch.constant.int 32
    %6203 = torch.aten.mul.Scalar %6202, %int32_7825 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_7826 = torch.constant.int 1
    %6204 = torch.aten.add.Tensor %6203, %6190, %int1_7826 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %6205 = torch.prim.ListConstruct %6204 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_7827 = torch.constant.bool false
    %6206 = torch.aten.index_put %6185, %6205, %6137, %false_7827 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %6206, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_7828 = torch.constant.int 32
    %int2_7829 = torch.constant.int 2
    %int32_7830 = torch.constant.int 32
    %int8_7831 = torch.constant.int 8
    %int128_7832 = torch.constant.int 128
    %6207 = torch.prim.ListConstruct %437, %int32_7828, %int2_7829, %int32_7830, %int8_7831, %int128_7832 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6208 = torch.aten.view %6206, %6207 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %6208, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_7833 = torch.constant.int 2097152
    %6209 = torch.prim.ListConstruct %437, %int2097152_7833 : (!torch.int, !torch.int) -> !torch.list<int>
    %6210 = torch.aten.view %6208, %6209 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %6210, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int4_7834 = torch.constant.int 4
    %6211 = torch.prim.ListConstruct %int4_7834, %358 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_7835 = torch.constant.int 1
    %6212 = torch.prim.ListConstruct %358, %int1_7835 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_7836 = torch.constant.int 4
    %int0_7837 = torch.constant.int 0
    %cpu_7838 = torch.constant.device "cpu"
    %false_7839 = torch.constant.bool false
    %6213 = torch.aten.empty_strided %6211, %6212, %int4_7836, %int0_7837, %cpu_7838, %false_7839 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6213, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int28_7840 = torch.constant.int 28
    %6214 = torch.aten.fill.Scalar %6213, %int28_7840 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6214, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int32_7841 = torch.constant.int 32
    %6215 = torch.aten.mul.Scalar %arg3, %int32_7841 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6215, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_7842 = torch.constant.int 1
    %6216 = torch.aten.add.Tensor %6215, %6214, %int1_7842 : !torch.vtensor<[4,?],si64>, !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6216, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_7843 = torch.constant.int 4
    %6217 = torch.aten.mul.int %int4_7843, %358 : !torch.int, !torch.int -> !torch.int
    %6218 = torch.prim.ListConstruct %6217 : (!torch.int) -> !torch.list<int>
    %6219 = torch.aten.view %6216, %6218 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %6219, [%356], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_7844 = torch.constant.int 32
    %int2_7845 = torch.constant.int 2
    %int32_7846 = torch.constant.int 32
    %int8_7847 = torch.constant.int 8
    %int128_7848 = torch.constant.int 128
    %6220 = torch.prim.ListConstruct %437, %int32_7844, %int2_7845, %int32_7846, %int8_7847, %int128_7848 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6221 = torch.aten.view %6210, %6220 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %6221, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_7849 = torch.constant.int 32
    %6222 = torch.aten.mul.int %437, %int32_7849 : !torch.int, !torch.int -> !torch.int
    %int2_7850 = torch.constant.int 2
    %int32_7851 = torch.constant.int 32
    %int8_7852 = torch.constant.int 8
    %int128_7853 = torch.constant.int 128
    %6223 = torch.prim.ListConstruct %6222, %int2_7850, %int32_7851, %int8_7852, %int128_7853 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6224 = torch.aten.view %6221, %6223 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %6224, [%357], affine_map<()[s0] -> (s0 * 32, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int0_7854 = torch.constant.int 0
    %6225 = torch.aten.index_select %6224, %int0_7854, %6219 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.int, !torch.vtensor<[?],si64> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %6225, [%356], affine_map<()[s0] -> (s0 * 4, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int4_7855 = torch.constant.int 4
    %int2_7856 = torch.constant.int 2
    %int32_7857 = torch.constant.int 32
    %int8_7858 = torch.constant.int 8
    %int128_7859 = torch.constant.int 128
    %6226 = torch.prim.ListConstruct %int4_7855, %358, %int2_7856, %int32_7857, %int8_7858, %int128_7859 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6227 = torch.aten.view %6225, %6226 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %6227, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int0_7860 = torch.constant.int 0
    %int0_7861 = torch.constant.int 0
    %int9223372036854775807_7862 = torch.constant.int 9223372036854775807
    %int1_7863 = torch.constant.int 1
    %6228 = torch.aten.slice.Tensor %6227, %int0_7860, %int0_7861, %int9223372036854775807_7862, %int1_7863 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %6228, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_7864 = torch.constant.int 1
    %int0_7865 = torch.constant.int 0
    %int9223372036854775807_7866 = torch.constant.int 9223372036854775807
    %int1_7867 = torch.constant.int 1
    %6229 = torch.aten.slice.Tensor %6228, %int1_7864, %int0_7865, %int9223372036854775807_7866, %int1_7867 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %6229, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_7868 = torch.constant.int 2
    %int0_7869 = torch.constant.int 0
    %6230 = torch.aten.select.int %6229, %int2_7868, %int0_7869 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6230, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int32_7870 = torch.constant.int 32
    %6231 = torch.aten.mul.int %358, %int32_7870 : !torch.int, !torch.int -> !torch.int
    %int2_7871 = torch.constant.int 2
    %int0_7872 = torch.constant.int 0
    %int1_7873 = torch.constant.int 1
    %6232 = torch.aten.slice.Tensor %6230, %int2_7871, %int0_7872, %6231, %int1_7873 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6232, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_7874 = torch.constant.int 0
    %6233 = torch.aten.clone %6232, %int0_7874 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6233, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_7875 = torch.constant.int 1
    %6234 = torch.aten.size.int %6229, %int1_7875 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_7876 = torch.constant.int 32
    %6235 = torch.aten.mul.int %6234, %int32_7876 : !torch.int, !torch.int -> !torch.int
    %int4_7877 = torch.constant.int 4
    %int8_7878 = torch.constant.int 8
    %int128_7879 = torch.constant.int 128
    %6236 = torch.prim.ListConstruct %int4_7877, %6235, %int8_7878, %int128_7879 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6237 = torch.aten._unsafe_view %6233, %6236 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %6237, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_7880 = torch.constant.int 0
    %int0_7881 = torch.constant.int 0
    %int9223372036854775807_7882 = torch.constant.int 9223372036854775807
    %int1_7883 = torch.constant.int 1
    %6238 = torch.aten.slice.Tensor %6237, %int0_7880, %int0_7881, %int9223372036854775807_7882, %int1_7883 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %6238, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_7884 = torch.constant.int 0
    %int0_7885 = torch.constant.int 0
    %int9223372036854775807_7886 = torch.constant.int 9223372036854775807
    %int1_7887 = torch.constant.int 1
    %6239 = torch.aten.slice.Tensor %6227, %int0_7884, %int0_7885, %int9223372036854775807_7886, %int1_7887 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %6239, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_7888 = torch.constant.int 1
    %int0_7889 = torch.constant.int 0
    %int9223372036854775807_7890 = torch.constant.int 9223372036854775807
    %int1_7891 = torch.constant.int 1
    %6240 = torch.aten.slice.Tensor %6239, %int1_7888, %int0_7889, %int9223372036854775807_7890, %int1_7891 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %6240, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_7892 = torch.constant.int 2
    %int1_7893 = torch.constant.int 1
    %6241 = torch.aten.select.int %6240, %int2_7892, %int1_7893 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6241, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int2_7894 = torch.constant.int 2
    %int0_7895 = torch.constant.int 0
    %int1_7896 = torch.constant.int 1
    %6242 = torch.aten.slice.Tensor %6241, %int2_7894, %int0_7895, %6231, %int1_7896 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6242, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_7897 = torch.constant.int 0
    %6243 = torch.aten.clone %6242, %int0_7897 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6243, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_7898 = torch.constant.int 1
    %6244 = torch.aten.size.int %6240, %int1_7898 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_7899 = torch.constant.int 32
    %6245 = torch.aten.mul.int %6244, %int32_7899 : !torch.int, !torch.int -> !torch.int
    %int4_7900 = torch.constant.int 4
    %int8_7901 = torch.constant.int 8
    %int128_7902 = torch.constant.int 128
    %6246 = torch.prim.ListConstruct %int4_7900, %6245, %int8_7901, %int128_7902 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6247 = torch.aten._unsafe_view %6243, %6246 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %6247, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_7903 = torch.constant.int 0
    %int0_7904 = torch.constant.int 0
    %int9223372036854775807_7905 = torch.constant.int 9223372036854775807
    %int1_7906 = torch.constant.int 1
    %6248 = torch.aten.slice.Tensor %6247, %int0_7903, %int0_7904, %int9223372036854775807_7905, %int1_7906 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %6248, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int-2_7907 = torch.constant.int -2
    %6249 = torch.aten.unsqueeze %6238, %int-2_7907 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %6249, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_7908 = torch.constant.int 1
    %6250 = torch.aten.size.int %6237, %int1_7908 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_7909 = torch.constant.int 4
    %int8_7910 = torch.constant.int 8
    %int4_7911 = torch.constant.int 4
    %int128_7912 = torch.constant.int 128
    %6251 = torch.prim.ListConstruct %int4_7909, %6250, %int8_7910, %int4_7911, %int128_7912 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_7913 = torch.constant.bool false
    %6252 = torch.aten.expand %6249, %6251, %false_7913 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %6252, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_7914 = torch.constant.int 0
    %6253 = torch.aten.clone %6252, %int0_7914 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %6253, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_7915 = torch.constant.int 4
    %int32_7916 = torch.constant.int 32
    %int128_7917 = torch.constant.int 128
    %6254 = torch.prim.ListConstruct %int4_7915, %6250, %int32_7916, %int128_7917 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6255 = torch.aten._unsafe_view %6253, %6254 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %6255, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_7918 = torch.constant.int -2
    %6256 = torch.aten.unsqueeze %6248, %int-2_7918 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %6256, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_7919 = torch.constant.int 1
    %6257 = torch.aten.size.int %6247, %int1_7919 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_7920 = torch.constant.int 4
    %int8_7921 = torch.constant.int 8
    %int4_7922 = torch.constant.int 4
    %int128_7923 = torch.constant.int 128
    %6258 = torch.prim.ListConstruct %int4_7920, %6257, %int8_7921, %int4_7922, %int128_7923 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_7924 = torch.constant.bool false
    %6259 = torch.aten.expand %6256, %6258, %false_7924 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %6259, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_7925 = torch.constant.int 0
    %6260 = torch.aten.clone %6259, %int0_7925 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %6260, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_7926 = torch.constant.int 4
    %int32_7927 = torch.constant.int 32
    %int128_7928 = torch.constant.int 128
    %6261 = torch.prim.ListConstruct %int4_7926, %6257, %int32_7927, %int128_7928 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6262 = torch.aten._unsafe_view %6260, %6261 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %6262, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_7929 = torch.constant.int 1
    %int2_7930 = torch.constant.int 2
    %6263 = torch.aten.transpose.int %6143, %int1_7929, %int2_7930 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,128],f16>
    %int1_7931 = torch.constant.int 1
    %int2_7932 = torch.constant.int 2
    %6264 = torch.aten.transpose.int %6255, %int1_7931, %int2_7932 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %6264, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_7933 = torch.constant.int 1
    %int2_7934 = torch.constant.int 2
    %6265 = torch.aten.transpose.int %6262, %int1_7933, %int2_7934 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %6265, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_7935 = torch.constant.float 0.000000e+00
    %false_7936 = torch.constant.bool false
    %none_7937 = torch.constant.none
    %6266:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6263, %6264, %6265, %float0.000000e00_7935, %false_7936, %368, %none_7937) : (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.vtensor<[4,1,1,?],f16>, !torch.none) -> (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,1],f32>) 
    %int1_7938 = torch.constant.int 1
    %int2_7939 = torch.constant.int 2
    %6267 = torch.aten.transpose.int %6266#0, %int1_7938, %int2_7939 : !torch.vtensor<[4,32,1,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int4_7940 = torch.constant.int 4
    %int1_7941 = torch.constant.int 1
    %int4096_7942 = torch.constant.int 4096
    %6268 = torch.prim.ListConstruct %int4_7940, %int1_7941, %int4096_7942 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6269 = torch.aten.view %6267, %6268 : !torch.vtensor<[4,1,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_7943 = torch.constant.int -2
    %int-1_7944 = torch.constant.int -1
    %6270 = torch.aten.transpose.int %315, %int-2_7943, %int-1_7944 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_7945 = torch.constant.int 4
    %int4096_7946 = torch.constant.int 4096
    %6271 = torch.prim.ListConstruct %int4_7945, %int4096_7946 : (!torch.int, !torch.int) -> !torch.list<int>
    %6272 = torch.aten.view %6269, %6271 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %6273 = torch.aten.mm %6272, %6270 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_7947 = torch.constant.int 4
    %int1_7948 = torch.constant.int 1
    %int4096_7949 = torch.constant.int 4096
    %6274 = torch.prim.ListConstruct %int4_7947, %int1_7948, %int4096_7949 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6275 = torch.aten.view %6273, %6274 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_7950 = torch.constant.int 1
    %6276 = torch.aten.add.Tensor %6103, %6275, %int1_7950 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_7951 = torch.constant.int 6
    %6277 = torch.prims.convert_element_type %6276, %int6_7951 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_7952 = torch.constant.int 2
    %6278 = torch.aten.pow.Tensor_Scalar %6277, %int2_7952 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_7953 = torch.constant.int -1
    %6279 = torch.prim.ListConstruct %int-1_7953 : (!torch.int) -> !torch.list<int>
    %true_7954 = torch.constant.bool true
    %none_7955 = torch.constant.none
    %6280 = torch.aten.mean.dim %6278, %6279, %true_7954, %none_7955 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_7956 = torch.constant.float 9.9999997473787516E-6
    %int1_7957 = torch.constant.int 1
    %6281 = torch.aten.add.Scalar %6280, %float9.999990e-06_7956, %int1_7957 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %6282 = torch.aten.rsqrt %6281 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %6283 = torch.aten.mul.Tensor %6277, %6282 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_7958 = torch.constant.int 5
    %6284 = torch.prims.convert_element_type %6283, %int5_7958 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %6285 = torch.aten.mul.Tensor %316, %6284 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_7959 = torch.constant.int 5
    %6286 = torch.prims.convert_element_type %6285, %int5_7959 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_7960 = torch.constant.int -2
    %int-1_7961 = torch.constant.int -1
    %6287 = torch.aten.transpose.int %317, %int-2_7960, %int-1_7961 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_7962 = torch.constant.int 4
    %int4096_7963 = torch.constant.int 4096
    %6288 = torch.prim.ListConstruct %int4_7962, %int4096_7963 : (!torch.int, !torch.int) -> !torch.list<int>
    %6289 = torch.aten.view %6286, %6288 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %6290 = torch.aten.mm %6289, %6287 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_7964 = torch.constant.int 4
    %int1_7965 = torch.constant.int 1
    %int14336_7966 = torch.constant.int 14336
    %6291 = torch.prim.ListConstruct %int4_7964, %int1_7965, %int14336_7966 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6292 = torch.aten.view %6290, %6291 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %6293 = torch.aten.silu %6292 : !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_7967 = torch.constant.int -2
    %int-1_7968 = torch.constant.int -1
    %6294 = torch.aten.transpose.int %318, %int-2_7967, %int-1_7968 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_7969 = torch.constant.int 4
    %int4096_7970 = torch.constant.int 4096
    %6295 = torch.prim.ListConstruct %int4_7969, %int4096_7970 : (!torch.int, !torch.int) -> !torch.list<int>
    %6296 = torch.aten.view %6286, %6295 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %6297 = torch.aten.mm %6296, %6294 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_7971 = torch.constant.int 4
    %int1_7972 = torch.constant.int 1
    %int14336_7973 = torch.constant.int 14336
    %6298 = torch.prim.ListConstruct %int4_7971, %int1_7972, %int14336_7973 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6299 = torch.aten.view %6297, %6298 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %6300 = torch.aten.mul.Tensor %6293, %6299 : !torch.vtensor<[4,1,14336],f16>, !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_7974 = torch.constant.int -2
    %int-1_7975 = torch.constant.int -1
    %6301 = torch.aten.transpose.int %319, %int-2_7974, %int-1_7975 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int4_7976 = torch.constant.int 4
    %int14336_7977 = torch.constant.int 14336
    %6302 = torch.prim.ListConstruct %int4_7976, %int14336_7977 : (!torch.int, !torch.int) -> !torch.list<int>
    %6303 = torch.aten.view %6300, %6302 : !torch.vtensor<[4,1,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,14336],f16>
    %6304 = torch.aten.mm %6303, %6301 : !torch.vtensor<[4,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_7978 = torch.constant.int 4
    %int1_7979 = torch.constant.int 1
    %int4096_7980 = torch.constant.int 4096
    %6305 = torch.prim.ListConstruct %int4_7978, %int1_7979, %int4096_7980 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6306 = torch.aten.view %6304, %6305 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_7981 = torch.constant.int 1
    %6307 = torch.aten.add.Tensor %6276, %6306, %int1_7981 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_7982 = torch.constant.int 6
    %6308 = torch.prims.convert_element_type %6307, %int6_7982 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_7983 = torch.constant.int 2
    %6309 = torch.aten.pow.Tensor_Scalar %6308, %int2_7983 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_7984 = torch.constant.int -1
    %6310 = torch.prim.ListConstruct %int-1_7984 : (!torch.int) -> !torch.list<int>
    %true_7985 = torch.constant.bool true
    %none_7986 = torch.constant.none
    %6311 = torch.aten.mean.dim %6309, %6310, %true_7985, %none_7986 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_7987 = torch.constant.float 9.9999997473787516E-6
    %int1_7988 = torch.constant.int 1
    %6312 = torch.aten.add.Scalar %6311, %float9.999990e-06_7987, %int1_7988 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %6313 = torch.aten.rsqrt %6312 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %6314 = torch.aten.mul.Tensor %6308, %6313 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_7989 = torch.constant.int 5
    %6315 = torch.prims.convert_element_type %6314, %int5_7989 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %6316 = torch.aten.mul.Tensor %320, %6315 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_7990 = torch.constant.int 5
    %6317 = torch.prims.convert_element_type %6316, %int5_7990 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_7991 = torch.constant.int -2
    %int-1_7992 = torch.constant.int -1
    %6318 = torch.aten.transpose.int %321, %int-2_7991, %int-1_7992 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_7993 = torch.constant.int 4
    %int4096_7994 = torch.constant.int 4096
    %6319 = torch.prim.ListConstruct %int4_7993, %int4096_7994 : (!torch.int, !torch.int) -> !torch.list<int>
    %6320 = torch.aten.view %6317, %6319 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %6321 = torch.aten.mm %6320, %6318 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_7995 = torch.constant.int 4
    %int1_7996 = torch.constant.int 1
    %int4096_7997 = torch.constant.int 4096
    %6322 = torch.prim.ListConstruct %int4_7995, %int1_7996, %int4096_7997 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6323 = torch.aten.view %6321, %6322 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_7998 = torch.constant.int -2
    %int-1_7999 = torch.constant.int -1
    %6324 = torch.aten.transpose.int %322, %int-2_7998, %int-1_7999 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_8000 = torch.constant.int 4
    %int4096_8001 = torch.constant.int 4096
    %6325 = torch.prim.ListConstruct %int4_8000, %int4096_8001 : (!torch.int, !torch.int) -> !torch.list<int>
    %6326 = torch.aten.view %6317, %6325 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %6327 = torch.aten.mm %6326, %6324 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_8002 = torch.constant.int 4
    %int1_8003 = torch.constant.int 1
    %int1024_8004 = torch.constant.int 1024
    %6328 = torch.prim.ListConstruct %int4_8002, %int1_8003, %int1024_8004 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6329 = torch.aten.view %6327, %6328 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int-2_8005 = torch.constant.int -2
    %int-1_8006 = torch.constant.int -1
    %6330 = torch.aten.transpose.int %323, %int-2_8005, %int-1_8006 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_8007 = torch.constant.int 4
    %int4096_8008 = torch.constant.int 4096
    %6331 = torch.prim.ListConstruct %int4_8007, %int4096_8008 : (!torch.int, !torch.int) -> !torch.list<int>
    %6332 = torch.aten.view %6317, %6331 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %6333 = torch.aten.mm %6332, %6330 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_8009 = torch.constant.int 4
    %int1_8010 = torch.constant.int 1
    %int1024_8011 = torch.constant.int 1024
    %6334 = torch.prim.ListConstruct %int4_8009, %int1_8010, %int1024_8011 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6335 = torch.aten.view %6333, %6334 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int4_8012 = torch.constant.int 4
    %int1_8013 = torch.constant.int 1
    %int32_8014 = torch.constant.int 32
    %int128_8015 = torch.constant.int 128
    %6336 = torch.prim.ListConstruct %int4_8012, %int1_8013, %int32_8014, %int128_8015 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6337 = torch.aten.view %6323, %6336 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,128],f16>
    %int4_8016 = torch.constant.int 4
    %int1_8017 = torch.constant.int 1
    %int8_8018 = torch.constant.int 8
    %int128_8019 = torch.constant.int 128
    %6338 = torch.prim.ListConstruct %int4_8016, %int1_8017, %int8_8018, %int128_8019 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6339 = torch.aten.view %6329, %6338 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int4_8020 = torch.constant.int 4
    %int1_8021 = torch.constant.int 1
    %int8_8022 = torch.constant.int 8
    %int128_8023 = torch.constant.int 128
    %6340 = torch.prim.ListConstruct %int4_8020, %int1_8021, %int8_8022, %int128_8023 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6341 = torch.aten.view %6335, %6340 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int6_8024 = torch.constant.int 6
    %6342 = torch.prims.convert_element_type %6337, %int6_8024 : !torch.vtensor<[4,1,32,128],f16>, !torch.int -> !torch.vtensor<[4,1,32,128],f32>
    %6343 = torch_c.to_builtin_tensor %6342 : !torch.vtensor<[4,1,32,128],f32> -> tensor<4x1x32x128xf32>
    %6344 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %6345 = util.call @sharktank_rotary_embedding_4_1_32_128_f32(%6343, %6344) : (tensor<4x1x32x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x32x128xf32>
    %6346 = torch_c.from_builtin_tensor %6345 : tensor<4x1x32x128xf32> -> !torch.vtensor<[4,1,32,128],f32>
    %int5_8025 = torch.constant.int 5
    %6347 = torch.prims.convert_element_type %6346, %int5_8025 : !torch.vtensor<[4,1,32,128],f32>, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int6_8026 = torch.constant.int 6
    %6348 = torch.prims.convert_element_type %6339, %int6_8026 : !torch.vtensor<[4,1,8,128],f16>, !torch.int -> !torch.vtensor<[4,1,8,128],f32>
    %6349 = torch_c.to_builtin_tensor %6348 : !torch.vtensor<[4,1,8,128],f32> -> tensor<4x1x8x128xf32>
    %6350 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %6351 = util.call @sharktank_rotary_embedding_4_1_8_128_f32(%6349, %6350) : (tensor<4x1x8x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x8x128xf32>
    %6352 = torch_c.from_builtin_tensor %6351 : tensor<4x1x8x128xf32> -> !torch.vtensor<[4,1,8,128],f32>
    %int5_8027 = torch.constant.int 5
    %6353 = torch.prims.convert_element_type %6352, %int5_8027 : !torch.vtensor<[4,1,8,128],f32>, !torch.int -> !torch.vtensor<[4,1,8,128],f16>
    %int32_8028 = torch.constant.int 32
    %6354 = torch.aten.floor_divide.Scalar %arg2, %int32_8028 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_8029 = torch.constant.int 1
    %6355 = torch.aten.unsqueeze %6354, %int1_8029 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_8030 = torch.constant.int 1
    %false_8031 = torch.constant.bool false
    %6356 = torch.aten.gather %arg3, %int1_8030, %6355, %false_8031 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_8032 = torch.constant.int 32
    %6357 = torch.aten.remainder.Scalar %arg2, %int32_8032 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_8033 = torch.constant.int 1
    %6358 = torch.aten.unsqueeze %6357, %int1_8033 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_8034 = torch.constant.none
    %6359 = torch.aten.clone %324, %none_8034 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_8035 = torch.constant.int 0
    %6360 = torch.aten.unsqueeze %6359, %int0_8035 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_8036 = torch.constant.int 4
    %int1_8037 = torch.constant.int 1
    %6361 = torch.prim.ListConstruct %int4_8036, %int1_8037 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_8038 = torch.constant.int 1
    %int1_8039 = torch.constant.int 1
    %6362 = torch.prim.ListConstruct %int1_8038, %int1_8039 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_8040 = torch.constant.int 4
    %int0_8041 = torch.constant.int 0
    %cpu_8042 = torch.constant.device "cpu"
    %false_8043 = torch.constant.bool false
    %6363 = torch.aten.empty_strided %6361, %6362, %int4_8040, %int0_8041, %cpu_8042, %false_8043 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int29 = torch.constant.int 29
    %6364 = torch.aten.fill.Scalar %6363, %int29 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_8044 = torch.constant.int 4
    %int1_8045 = torch.constant.int 1
    %6365 = torch.prim.ListConstruct %int4_8044, %int1_8045 : (!torch.int, !torch.int) -> !torch.list<int>
    %6366 = torch.aten.repeat %6360, %6365 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_8046 = torch.constant.int 32
    %6367 = torch.aten.mul.Scalar %6356, %int32_8046 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_8047 = torch.constant.int 1
    %6368 = torch.aten.add.Tensor %6367, %6364, %int1_8047 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_8048 = torch.constant.int 2
    %6369 = torch.aten.mul.Scalar %6368, %int2_8048 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_8049 = torch.constant.int 1
    %6370 = torch.aten.add.Tensor %6369, %6366, %int1_8049 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_8050 = torch.constant.int 32
    %6371 = torch.aten.mul.Scalar %6370, %int32_8050 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_8051 = torch.constant.int 1
    %6372 = torch.aten.add.Tensor %6371, %6358, %int1_8051 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_8052 = torch.constant.int 32
    %int2_8053 = torch.constant.int 2
    %int32_8054 = torch.constant.int 32
    %int8_8055 = torch.constant.int 8
    %int128_8056 = torch.constant.int 128
    %6373 = torch.prim.ListConstruct %437, %int32_8052, %int2_8053, %int32_8054, %int8_8055, %int128_8056 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6374 = torch.aten.view %6210, %6373 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %6374, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_8057 = torch.constant.int 32
    %6375 = torch.aten.mul.int %437, %int32_8057 : !torch.int, !torch.int -> !torch.int
    %int2_8058 = torch.constant.int 2
    %6376 = torch.aten.mul.int %6375, %int2_8058 : !torch.int, !torch.int -> !torch.int
    %int32_8059 = torch.constant.int 32
    %6377 = torch.aten.mul.int %6376, %int32_8059 : !torch.int, !torch.int -> !torch.int
    %int8_8060 = torch.constant.int 8
    %int128_8061 = torch.constant.int 128
    %6378 = torch.prim.ListConstruct %6377, %int8_8060, %int128_8061 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6379 = torch.aten.view %6374, %6378 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %6379, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %6380 = torch.prim.ListConstruct %6372 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_8062 = torch.constant.bool false
    %6381 = torch.aten.index_put %6379, %6380, %6353, %false_8062 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %6381, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_8063 = torch.constant.int 32
    %int2_8064 = torch.constant.int 2
    %int32_8065 = torch.constant.int 32
    %int8_8066 = torch.constant.int 8
    %int128_8067 = torch.constant.int 128
    %6382 = torch.prim.ListConstruct %437, %int32_8063, %int2_8064, %int32_8065, %int8_8066, %int128_8067 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6383 = torch.aten.view %6381, %6382 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %6383, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_8068 = torch.constant.int 2097152
    %6384 = torch.prim.ListConstruct %437, %int2097152_8068 : (!torch.int, !torch.int) -> !torch.list<int>
    %6385 = torch.aten.view %6383, %6384 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %6385, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_8069 = torch.constant.int 32
    %int2_8070 = torch.constant.int 2
    %int32_8071 = torch.constant.int 32
    %int8_8072 = torch.constant.int 8
    %int128_8073 = torch.constant.int 128
    %6386 = torch.prim.ListConstruct %437, %int32_8069, %int2_8070, %int32_8071, %int8_8072, %int128_8073 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6387 = torch.aten.view %6385, %6386 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %6387, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int8_8074 = torch.constant.int 8
    %int128_8075 = torch.constant.int 128
    %6388 = torch.prim.ListConstruct %6377, %int8_8074, %int128_8075 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6389 = torch.aten.view %6387, %6388 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %6389, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_8076 = torch.constant.int 32
    %6390 = torch.aten.floor_divide.Scalar %arg2, %int32_8076 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_8077 = torch.constant.int 1
    %6391 = torch.aten.unsqueeze %6390, %int1_8077 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_8078 = torch.constant.int 1
    %false_8079 = torch.constant.bool false
    %6392 = torch.aten.gather %arg3, %int1_8078, %6391, %false_8079 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_8080 = torch.constant.int 32
    %6393 = torch.aten.remainder.Scalar %arg2, %int32_8080 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_8081 = torch.constant.int 1
    %6394 = torch.aten.unsqueeze %6393, %int1_8081 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_8082 = torch.constant.none
    %6395 = torch.aten.clone %325, %none_8082 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_8083 = torch.constant.int 0
    %6396 = torch.aten.unsqueeze %6395, %int0_8083 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_8084 = torch.constant.int 4
    %int1_8085 = torch.constant.int 1
    %6397 = torch.prim.ListConstruct %int4_8084, %int1_8085 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_8086 = torch.constant.int 1
    %int1_8087 = torch.constant.int 1
    %6398 = torch.prim.ListConstruct %int1_8086, %int1_8087 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_8088 = torch.constant.int 4
    %int0_8089 = torch.constant.int 0
    %cpu_8090 = torch.constant.device "cpu"
    %false_8091 = torch.constant.bool false
    %6399 = torch.aten.empty_strided %6397, %6398, %int4_8088, %int0_8089, %cpu_8090, %false_8091 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int29_8092 = torch.constant.int 29
    %6400 = torch.aten.fill.Scalar %6399, %int29_8092 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_8093 = torch.constant.int 4
    %int1_8094 = torch.constant.int 1
    %6401 = torch.prim.ListConstruct %int4_8093, %int1_8094 : (!torch.int, !torch.int) -> !torch.list<int>
    %6402 = torch.aten.repeat %6396, %6401 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_8095 = torch.constant.int 32
    %6403 = torch.aten.mul.Scalar %6392, %int32_8095 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_8096 = torch.constant.int 1
    %6404 = torch.aten.add.Tensor %6403, %6400, %int1_8096 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_8097 = torch.constant.int 2
    %6405 = torch.aten.mul.Scalar %6404, %int2_8097 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_8098 = torch.constant.int 1
    %6406 = torch.aten.add.Tensor %6405, %6402, %int1_8098 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_8099 = torch.constant.int 32
    %6407 = torch.aten.mul.Scalar %6406, %int32_8099 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_8100 = torch.constant.int 1
    %6408 = torch.aten.add.Tensor %6407, %6394, %int1_8100 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %6409 = torch.prim.ListConstruct %6408 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_8101 = torch.constant.bool false
    %6410 = torch.aten.index_put %6389, %6409, %6341, %false_8101 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %6410, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_8102 = torch.constant.int 32
    %int2_8103 = torch.constant.int 2
    %int32_8104 = torch.constant.int 32
    %int8_8105 = torch.constant.int 8
    %int128_8106 = torch.constant.int 128
    %6411 = torch.prim.ListConstruct %437, %int32_8102, %int2_8103, %int32_8104, %int8_8105, %int128_8106 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6412 = torch.aten.view %6410, %6411 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %6412, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_8107 = torch.constant.int 2097152
    %6413 = torch.prim.ListConstruct %437, %int2097152_8107 : (!torch.int, !torch.int) -> !torch.list<int>
    %6414 = torch.aten.view %6412, %6413 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %6414, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int4_8108 = torch.constant.int 4
    %6415 = torch.prim.ListConstruct %int4_8108, %358 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_8109 = torch.constant.int 1
    %6416 = torch.prim.ListConstruct %358, %int1_8109 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_8110 = torch.constant.int 4
    %int0_8111 = torch.constant.int 0
    %cpu_8112 = torch.constant.device "cpu"
    %false_8113 = torch.constant.bool false
    %6417 = torch.aten.empty_strided %6415, %6416, %int4_8110, %int0_8111, %cpu_8112, %false_8113 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6417, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int29_8114 = torch.constant.int 29
    %6418 = torch.aten.fill.Scalar %6417, %int29_8114 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6418, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int32_8115 = torch.constant.int 32
    %6419 = torch.aten.mul.Scalar %arg3, %int32_8115 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6419, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_8116 = torch.constant.int 1
    %6420 = torch.aten.add.Tensor %6419, %6418, %int1_8116 : !torch.vtensor<[4,?],si64>, !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6420, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_8117 = torch.constant.int 4
    %6421 = torch.aten.mul.int %int4_8117, %358 : !torch.int, !torch.int -> !torch.int
    %6422 = torch.prim.ListConstruct %6421 : (!torch.int) -> !torch.list<int>
    %6423 = torch.aten.view %6420, %6422 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %6423, [%356], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_8118 = torch.constant.int 32
    %int2_8119 = torch.constant.int 2
    %int32_8120 = torch.constant.int 32
    %int8_8121 = torch.constant.int 8
    %int128_8122 = torch.constant.int 128
    %6424 = torch.prim.ListConstruct %437, %int32_8118, %int2_8119, %int32_8120, %int8_8121, %int128_8122 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6425 = torch.aten.view %6414, %6424 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %6425, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_8123 = torch.constant.int 32
    %6426 = torch.aten.mul.int %437, %int32_8123 : !torch.int, !torch.int -> !torch.int
    %int2_8124 = torch.constant.int 2
    %int32_8125 = torch.constant.int 32
    %int8_8126 = torch.constant.int 8
    %int128_8127 = torch.constant.int 128
    %6427 = torch.prim.ListConstruct %6426, %int2_8124, %int32_8125, %int8_8126, %int128_8127 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6428 = torch.aten.view %6425, %6427 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %6428, [%357], affine_map<()[s0] -> (s0 * 32, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int0_8128 = torch.constant.int 0
    %6429 = torch.aten.index_select %6428, %int0_8128, %6423 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.int, !torch.vtensor<[?],si64> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %6429, [%356], affine_map<()[s0] -> (s0 * 4, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int4_8129 = torch.constant.int 4
    %int2_8130 = torch.constant.int 2
    %int32_8131 = torch.constant.int 32
    %int8_8132 = torch.constant.int 8
    %int128_8133 = torch.constant.int 128
    %6430 = torch.prim.ListConstruct %int4_8129, %358, %int2_8130, %int32_8131, %int8_8132, %int128_8133 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6431 = torch.aten.view %6429, %6430 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %6431, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int0_8134 = torch.constant.int 0
    %int0_8135 = torch.constant.int 0
    %int9223372036854775807_8136 = torch.constant.int 9223372036854775807
    %int1_8137 = torch.constant.int 1
    %6432 = torch.aten.slice.Tensor %6431, %int0_8134, %int0_8135, %int9223372036854775807_8136, %int1_8137 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %6432, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_8138 = torch.constant.int 1
    %int0_8139 = torch.constant.int 0
    %int9223372036854775807_8140 = torch.constant.int 9223372036854775807
    %int1_8141 = torch.constant.int 1
    %6433 = torch.aten.slice.Tensor %6432, %int1_8138, %int0_8139, %int9223372036854775807_8140, %int1_8141 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %6433, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_8142 = torch.constant.int 2
    %int0_8143 = torch.constant.int 0
    %6434 = torch.aten.select.int %6433, %int2_8142, %int0_8143 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6434, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int32_8144 = torch.constant.int 32
    %6435 = torch.aten.mul.int %358, %int32_8144 : !torch.int, !torch.int -> !torch.int
    %int2_8145 = torch.constant.int 2
    %int0_8146 = torch.constant.int 0
    %int1_8147 = torch.constant.int 1
    %6436 = torch.aten.slice.Tensor %6434, %int2_8145, %int0_8146, %6435, %int1_8147 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6436, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_8148 = torch.constant.int 0
    %6437 = torch.aten.clone %6436, %int0_8148 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6437, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_8149 = torch.constant.int 1
    %6438 = torch.aten.size.int %6433, %int1_8149 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_8150 = torch.constant.int 32
    %6439 = torch.aten.mul.int %6438, %int32_8150 : !torch.int, !torch.int -> !torch.int
    %int4_8151 = torch.constant.int 4
    %int8_8152 = torch.constant.int 8
    %int128_8153 = torch.constant.int 128
    %6440 = torch.prim.ListConstruct %int4_8151, %6439, %int8_8152, %int128_8153 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6441 = torch.aten._unsafe_view %6437, %6440 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %6441, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_8154 = torch.constant.int 0
    %int0_8155 = torch.constant.int 0
    %int9223372036854775807_8156 = torch.constant.int 9223372036854775807
    %int1_8157 = torch.constant.int 1
    %6442 = torch.aten.slice.Tensor %6441, %int0_8154, %int0_8155, %int9223372036854775807_8156, %int1_8157 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %6442, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_8158 = torch.constant.int 0
    %int0_8159 = torch.constant.int 0
    %int9223372036854775807_8160 = torch.constant.int 9223372036854775807
    %int1_8161 = torch.constant.int 1
    %6443 = torch.aten.slice.Tensor %6431, %int0_8158, %int0_8159, %int9223372036854775807_8160, %int1_8161 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %6443, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_8162 = torch.constant.int 1
    %int0_8163 = torch.constant.int 0
    %int9223372036854775807_8164 = torch.constant.int 9223372036854775807
    %int1_8165 = torch.constant.int 1
    %6444 = torch.aten.slice.Tensor %6443, %int1_8162, %int0_8163, %int9223372036854775807_8164, %int1_8165 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %6444, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_8166 = torch.constant.int 2
    %int1_8167 = torch.constant.int 1
    %6445 = torch.aten.select.int %6444, %int2_8166, %int1_8167 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6445, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int2_8168 = torch.constant.int 2
    %int0_8169 = torch.constant.int 0
    %int1_8170 = torch.constant.int 1
    %6446 = torch.aten.slice.Tensor %6445, %int2_8168, %int0_8169, %6435, %int1_8170 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6446, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_8171 = torch.constant.int 0
    %6447 = torch.aten.clone %6446, %int0_8171 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6447, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_8172 = torch.constant.int 1
    %6448 = torch.aten.size.int %6444, %int1_8172 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_8173 = torch.constant.int 32
    %6449 = torch.aten.mul.int %6448, %int32_8173 : !torch.int, !torch.int -> !torch.int
    %int4_8174 = torch.constant.int 4
    %int8_8175 = torch.constant.int 8
    %int128_8176 = torch.constant.int 128
    %6450 = torch.prim.ListConstruct %int4_8174, %6449, %int8_8175, %int128_8176 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6451 = torch.aten._unsafe_view %6447, %6450 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %6451, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_8177 = torch.constant.int 0
    %int0_8178 = torch.constant.int 0
    %int9223372036854775807_8179 = torch.constant.int 9223372036854775807
    %int1_8180 = torch.constant.int 1
    %6452 = torch.aten.slice.Tensor %6451, %int0_8177, %int0_8178, %int9223372036854775807_8179, %int1_8180 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %6452, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int-2_8181 = torch.constant.int -2
    %6453 = torch.aten.unsqueeze %6442, %int-2_8181 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %6453, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_8182 = torch.constant.int 1
    %6454 = torch.aten.size.int %6441, %int1_8182 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_8183 = torch.constant.int 4
    %int8_8184 = torch.constant.int 8
    %int4_8185 = torch.constant.int 4
    %int128_8186 = torch.constant.int 128
    %6455 = torch.prim.ListConstruct %int4_8183, %6454, %int8_8184, %int4_8185, %int128_8186 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_8187 = torch.constant.bool false
    %6456 = torch.aten.expand %6453, %6455, %false_8187 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %6456, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_8188 = torch.constant.int 0
    %6457 = torch.aten.clone %6456, %int0_8188 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %6457, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_8189 = torch.constant.int 4
    %int32_8190 = torch.constant.int 32
    %int128_8191 = torch.constant.int 128
    %6458 = torch.prim.ListConstruct %int4_8189, %6454, %int32_8190, %int128_8191 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6459 = torch.aten._unsafe_view %6457, %6458 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %6459, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_8192 = torch.constant.int -2
    %6460 = torch.aten.unsqueeze %6452, %int-2_8192 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %6460, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_8193 = torch.constant.int 1
    %6461 = torch.aten.size.int %6451, %int1_8193 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_8194 = torch.constant.int 4
    %int8_8195 = torch.constant.int 8
    %int4_8196 = torch.constant.int 4
    %int128_8197 = torch.constant.int 128
    %6462 = torch.prim.ListConstruct %int4_8194, %6461, %int8_8195, %int4_8196, %int128_8197 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_8198 = torch.constant.bool false
    %6463 = torch.aten.expand %6460, %6462, %false_8198 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %6463, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_8199 = torch.constant.int 0
    %6464 = torch.aten.clone %6463, %int0_8199 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %6464, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_8200 = torch.constant.int 4
    %int32_8201 = torch.constant.int 32
    %int128_8202 = torch.constant.int 128
    %6465 = torch.prim.ListConstruct %int4_8200, %6461, %int32_8201, %int128_8202 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6466 = torch.aten._unsafe_view %6464, %6465 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %6466, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_8203 = torch.constant.int 1
    %int2_8204 = torch.constant.int 2
    %6467 = torch.aten.transpose.int %6347, %int1_8203, %int2_8204 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,128],f16>
    %int1_8205 = torch.constant.int 1
    %int2_8206 = torch.constant.int 2
    %6468 = torch.aten.transpose.int %6459, %int1_8205, %int2_8206 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %6468, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_8207 = torch.constant.int 1
    %int2_8208 = torch.constant.int 2
    %6469 = torch.aten.transpose.int %6466, %int1_8207, %int2_8208 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %6469, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_8209 = torch.constant.float 0.000000e+00
    %false_8210 = torch.constant.bool false
    %none_8211 = torch.constant.none
    %6470:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6467, %6468, %6469, %float0.000000e00_8209, %false_8210, %368, %none_8211) : (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.vtensor<[4,1,1,?],f16>, !torch.none) -> (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,1],f32>) 
    %int1_8212 = torch.constant.int 1
    %int2_8213 = torch.constant.int 2
    %6471 = torch.aten.transpose.int %6470#0, %int1_8212, %int2_8213 : !torch.vtensor<[4,32,1,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int4_8214 = torch.constant.int 4
    %int1_8215 = torch.constant.int 1
    %int4096_8216 = torch.constant.int 4096
    %6472 = torch.prim.ListConstruct %int4_8214, %int1_8215, %int4096_8216 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6473 = torch.aten.view %6471, %6472 : !torch.vtensor<[4,1,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_8217 = torch.constant.int -2
    %int-1_8218 = torch.constant.int -1
    %6474 = torch.aten.transpose.int %326, %int-2_8217, %int-1_8218 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_8219 = torch.constant.int 4
    %int4096_8220 = torch.constant.int 4096
    %6475 = torch.prim.ListConstruct %int4_8219, %int4096_8220 : (!torch.int, !torch.int) -> !torch.list<int>
    %6476 = torch.aten.view %6473, %6475 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %6477 = torch.aten.mm %6476, %6474 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_8221 = torch.constant.int 4
    %int1_8222 = torch.constant.int 1
    %int4096_8223 = torch.constant.int 4096
    %6478 = torch.prim.ListConstruct %int4_8221, %int1_8222, %int4096_8223 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6479 = torch.aten.view %6477, %6478 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_8224 = torch.constant.int 1
    %6480 = torch.aten.add.Tensor %6307, %6479, %int1_8224 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_8225 = torch.constant.int 6
    %6481 = torch.prims.convert_element_type %6480, %int6_8225 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_8226 = torch.constant.int 2
    %6482 = torch.aten.pow.Tensor_Scalar %6481, %int2_8226 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_8227 = torch.constant.int -1
    %6483 = torch.prim.ListConstruct %int-1_8227 : (!torch.int) -> !torch.list<int>
    %true_8228 = torch.constant.bool true
    %none_8229 = torch.constant.none
    %6484 = torch.aten.mean.dim %6482, %6483, %true_8228, %none_8229 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_8230 = torch.constant.float 9.9999997473787516E-6
    %int1_8231 = torch.constant.int 1
    %6485 = torch.aten.add.Scalar %6484, %float9.999990e-06_8230, %int1_8231 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %6486 = torch.aten.rsqrt %6485 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %6487 = torch.aten.mul.Tensor %6481, %6486 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_8232 = torch.constant.int 5
    %6488 = torch.prims.convert_element_type %6487, %int5_8232 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %6489 = torch.aten.mul.Tensor %327, %6488 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_8233 = torch.constant.int 5
    %6490 = torch.prims.convert_element_type %6489, %int5_8233 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_8234 = torch.constant.int -2
    %int-1_8235 = torch.constant.int -1
    %6491 = torch.aten.transpose.int %328, %int-2_8234, %int-1_8235 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_8236 = torch.constant.int 4
    %int4096_8237 = torch.constant.int 4096
    %6492 = torch.prim.ListConstruct %int4_8236, %int4096_8237 : (!torch.int, !torch.int) -> !torch.list<int>
    %6493 = torch.aten.view %6490, %6492 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %6494 = torch.aten.mm %6493, %6491 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_8238 = torch.constant.int 4
    %int1_8239 = torch.constant.int 1
    %int14336_8240 = torch.constant.int 14336
    %6495 = torch.prim.ListConstruct %int4_8238, %int1_8239, %int14336_8240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6496 = torch.aten.view %6494, %6495 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %6497 = torch.aten.silu %6496 : !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_8241 = torch.constant.int -2
    %int-1_8242 = torch.constant.int -1
    %6498 = torch.aten.transpose.int %329, %int-2_8241, %int-1_8242 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_8243 = torch.constant.int 4
    %int4096_8244 = torch.constant.int 4096
    %6499 = torch.prim.ListConstruct %int4_8243, %int4096_8244 : (!torch.int, !torch.int) -> !torch.list<int>
    %6500 = torch.aten.view %6490, %6499 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %6501 = torch.aten.mm %6500, %6498 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_8245 = torch.constant.int 4
    %int1_8246 = torch.constant.int 1
    %int14336_8247 = torch.constant.int 14336
    %6502 = torch.prim.ListConstruct %int4_8245, %int1_8246, %int14336_8247 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6503 = torch.aten.view %6501, %6502 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %6504 = torch.aten.mul.Tensor %6497, %6503 : !torch.vtensor<[4,1,14336],f16>, !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_8248 = torch.constant.int -2
    %int-1_8249 = torch.constant.int -1
    %6505 = torch.aten.transpose.int %330, %int-2_8248, %int-1_8249 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int4_8250 = torch.constant.int 4
    %int14336_8251 = torch.constant.int 14336
    %6506 = torch.prim.ListConstruct %int4_8250, %int14336_8251 : (!torch.int, !torch.int) -> !torch.list<int>
    %6507 = torch.aten.view %6504, %6506 : !torch.vtensor<[4,1,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,14336],f16>
    %6508 = torch.aten.mm %6507, %6505 : !torch.vtensor<[4,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_8252 = torch.constant.int 4
    %int1_8253 = torch.constant.int 1
    %int4096_8254 = torch.constant.int 4096
    %6509 = torch.prim.ListConstruct %int4_8252, %int1_8253, %int4096_8254 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6510 = torch.aten.view %6508, %6509 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_8255 = torch.constant.int 1
    %6511 = torch.aten.add.Tensor %6480, %6510, %int1_8255 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_8256 = torch.constant.int 6
    %6512 = torch.prims.convert_element_type %6511, %int6_8256 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_8257 = torch.constant.int 2
    %6513 = torch.aten.pow.Tensor_Scalar %6512, %int2_8257 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_8258 = torch.constant.int -1
    %6514 = torch.prim.ListConstruct %int-1_8258 : (!torch.int) -> !torch.list<int>
    %true_8259 = torch.constant.bool true
    %none_8260 = torch.constant.none
    %6515 = torch.aten.mean.dim %6513, %6514, %true_8259, %none_8260 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_8261 = torch.constant.float 9.9999997473787516E-6
    %int1_8262 = torch.constant.int 1
    %6516 = torch.aten.add.Scalar %6515, %float9.999990e-06_8261, %int1_8262 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %6517 = torch.aten.rsqrt %6516 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %6518 = torch.aten.mul.Tensor %6512, %6517 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_8263 = torch.constant.int 5
    %6519 = torch.prims.convert_element_type %6518, %int5_8263 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %6520 = torch.aten.mul.Tensor %331, %6519 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_8264 = torch.constant.int 5
    %6521 = torch.prims.convert_element_type %6520, %int5_8264 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_8265 = torch.constant.int -2
    %int-1_8266 = torch.constant.int -1
    %6522 = torch.aten.transpose.int %332, %int-2_8265, %int-1_8266 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_8267 = torch.constant.int 4
    %int4096_8268 = torch.constant.int 4096
    %6523 = torch.prim.ListConstruct %int4_8267, %int4096_8268 : (!torch.int, !torch.int) -> !torch.list<int>
    %6524 = torch.aten.view %6521, %6523 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %6525 = torch.aten.mm %6524, %6522 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_8269 = torch.constant.int 4
    %int1_8270 = torch.constant.int 1
    %int4096_8271 = torch.constant.int 4096
    %6526 = torch.prim.ListConstruct %int4_8269, %int1_8270, %int4096_8271 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6527 = torch.aten.view %6525, %6526 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_8272 = torch.constant.int -2
    %int-1_8273 = torch.constant.int -1
    %6528 = torch.aten.transpose.int %333, %int-2_8272, %int-1_8273 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_8274 = torch.constant.int 4
    %int4096_8275 = torch.constant.int 4096
    %6529 = torch.prim.ListConstruct %int4_8274, %int4096_8275 : (!torch.int, !torch.int) -> !torch.list<int>
    %6530 = torch.aten.view %6521, %6529 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %6531 = torch.aten.mm %6530, %6528 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_8276 = torch.constant.int 4
    %int1_8277 = torch.constant.int 1
    %int1024_8278 = torch.constant.int 1024
    %6532 = torch.prim.ListConstruct %int4_8276, %int1_8277, %int1024_8278 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6533 = torch.aten.view %6531, %6532 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int-2_8279 = torch.constant.int -2
    %int-1_8280 = torch.constant.int -1
    %6534 = torch.aten.transpose.int %334, %int-2_8279, %int-1_8280 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_8281 = torch.constant.int 4
    %int4096_8282 = torch.constant.int 4096
    %6535 = torch.prim.ListConstruct %int4_8281, %int4096_8282 : (!torch.int, !torch.int) -> !torch.list<int>
    %6536 = torch.aten.view %6521, %6535 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %6537 = torch.aten.mm %6536, %6534 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_8283 = torch.constant.int 4
    %int1_8284 = torch.constant.int 1
    %int1024_8285 = torch.constant.int 1024
    %6538 = torch.prim.ListConstruct %int4_8283, %int1_8284, %int1024_8285 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6539 = torch.aten.view %6537, %6538 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int4_8286 = torch.constant.int 4
    %int1_8287 = torch.constant.int 1
    %int32_8288 = torch.constant.int 32
    %int128_8289 = torch.constant.int 128
    %6540 = torch.prim.ListConstruct %int4_8286, %int1_8287, %int32_8288, %int128_8289 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6541 = torch.aten.view %6527, %6540 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,128],f16>
    %int4_8290 = torch.constant.int 4
    %int1_8291 = torch.constant.int 1
    %int8_8292 = torch.constant.int 8
    %int128_8293 = torch.constant.int 128
    %6542 = torch.prim.ListConstruct %int4_8290, %int1_8291, %int8_8292, %int128_8293 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6543 = torch.aten.view %6533, %6542 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int4_8294 = torch.constant.int 4
    %int1_8295 = torch.constant.int 1
    %int8_8296 = torch.constant.int 8
    %int128_8297 = torch.constant.int 128
    %6544 = torch.prim.ListConstruct %int4_8294, %int1_8295, %int8_8296, %int128_8297 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6545 = torch.aten.view %6539, %6544 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int6_8298 = torch.constant.int 6
    %6546 = torch.prims.convert_element_type %6541, %int6_8298 : !torch.vtensor<[4,1,32,128],f16>, !torch.int -> !torch.vtensor<[4,1,32,128],f32>
    %6547 = torch_c.to_builtin_tensor %6546 : !torch.vtensor<[4,1,32,128],f32> -> tensor<4x1x32x128xf32>
    %6548 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %6549 = util.call @sharktank_rotary_embedding_4_1_32_128_f32(%6547, %6548) : (tensor<4x1x32x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x32x128xf32>
    %6550 = torch_c.from_builtin_tensor %6549 : tensor<4x1x32x128xf32> -> !torch.vtensor<[4,1,32,128],f32>
    %int5_8299 = torch.constant.int 5
    %6551 = torch.prims.convert_element_type %6550, %int5_8299 : !torch.vtensor<[4,1,32,128],f32>, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int6_8300 = torch.constant.int 6
    %6552 = torch.prims.convert_element_type %6543, %int6_8300 : !torch.vtensor<[4,1,8,128],f16>, !torch.int -> !torch.vtensor<[4,1,8,128],f32>
    %6553 = torch_c.to_builtin_tensor %6552 : !torch.vtensor<[4,1,8,128],f32> -> tensor<4x1x8x128xf32>
    %6554 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %6555 = util.call @sharktank_rotary_embedding_4_1_8_128_f32(%6553, %6554) : (tensor<4x1x8x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x8x128xf32>
    %6556 = torch_c.from_builtin_tensor %6555 : tensor<4x1x8x128xf32> -> !torch.vtensor<[4,1,8,128],f32>
    %int5_8301 = torch.constant.int 5
    %6557 = torch.prims.convert_element_type %6556, %int5_8301 : !torch.vtensor<[4,1,8,128],f32>, !torch.int -> !torch.vtensor<[4,1,8,128],f16>
    %int32_8302 = torch.constant.int 32
    %6558 = torch.aten.floor_divide.Scalar %arg2, %int32_8302 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_8303 = torch.constant.int 1
    %6559 = torch.aten.unsqueeze %6558, %int1_8303 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_8304 = torch.constant.int 1
    %false_8305 = torch.constant.bool false
    %6560 = torch.aten.gather %arg3, %int1_8304, %6559, %false_8305 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_8306 = torch.constant.int 32
    %6561 = torch.aten.remainder.Scalar %arg2, %int32_8306 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_8307 = torch.constant.int 1
    %6562 = torch.aten.unsqueeze %6561, %int1_8307 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_8308 = torch.constant.none
    %6563 = torch.aten.clone %335, %none_8308 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_8309 = torch.constant.int 0
    %6564 = torch.aten.unsqueeze %6563, %int0_8309 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_8310 = torch.constant.int 4
    %int1_8311 = torch.constant.int 1
    %6565 = torch.prim.ListConstruct %int4_8310, %int1_8311 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_8312 = torch.constant.int 1
    %int1_8313 = torch.constant.int 1
    %6566 = torch.prim.ListConstruct %int1_8312, %int1_8313 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_8314 = torch.constant.int 4
    %int0_8315 = torch.constant.int 0
    %cpu_8316 = torch.constant.device "cpu"
    %false_8317 = torch.constant.bool false
    %6567 = torch.aten.empty_strided %6565, %6566, %int4_8314, %int0_8315, %cpu_8316, %false_8317 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int30 = torch.constant.int 30
    %6568 = torch.aten.fill.Scalar %6567, %int30 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_8318 = torch.constant.int 4
    %int1_8319 = torch.constant.int 1
    %6569 = torch.prim.ListConstruct %int4_8318, %int1_8319 : (!torch.int, !torch.int) -> !torch.list<int>
    %6570 = torch.aten.repeat %6564, %6569 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_8320 = torch.constant.int 32
    %6571 = torch.aten.mul.Scalar %6560, %int32_8320 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_8321 = torch.constant.int 1
    %6572 = torch.aten.add.Tensor %6571, %6568, %int1_8321 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_8322 = torch.constant.int 2
    %6573 = torch.aten.mul.Scalar %6572, %int2_8322 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_8323 = torch.constant.int 1
    %6574 = torch.aten.add.Tensor %6573, %6570, %int1_8323 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_8324 = torch.constant.int 32
    %6575 = torch.aten.mul.Scalar %6574, %int32_8324 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_8325 = torch.constant.int 1
    %6576 = torch.aten.add.Tensor %6575, %6562, %int1_8325 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_8326 = torch.constant.int 32
    %int2_8327 = torch.constant.int 2
    %int32_8328 = torch.constant.int 32
    %int8_8329 = torch.constant.int 8
    %int128_8330 = torch.constant.int 128
    %6577 = torch.prim.ListConstruct %437, %int32_8326, %int2_8327, %int32_8328, %int8_8329, %int128_8330 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6578 = torch.aten.view %6414, %6577 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %6578, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_8331 = torch.constant.int 32
    %6579 = torch.aten.mul.int %437, %int32_8331 : !torch.int, !torch.int -> !torch.int
    %int2_8332 = torch.constant.int 2
    %6580 = torch.aten.mul.int %6579, %int2_8332 : !torch.int, !torch.int -> !torch.int
    %int32_8333 = torch.constant.int 32
    %6581 = torch.aten.mul.int %6580, %int32_8333 : !torch.int, !torch.int -> !torch.int
    %int8_8334 = torch.constant.int 8
    %int128_8335 = torch.constant.int 128
    %6582 = torch.prim.ListConstruct %6581, %int8_8334, %int128_8335 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6583 = torch.aten.view %6578, %6582 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %6583, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %6584 = torch.prim.ListConstruct %6576 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_8336 = torch.constant.bool false
    %6585 = torch.aten.index_put %6583, %6584, %6557, %false_8336 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %6585, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_8337 = torch.constant.int 32
    %int2_8338 = torch.constant.int 2
    %int32_8339 = torch.constant.int 32
    %int8_8340 = torch.constant.int 8
    %int128_8341 = torch.constant.int 128
    %6586 = torch.prim.ListConstruct %437, %int32_8337, %int2_8338, %int32_8339, %int8_8340, %int128_8341 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6587 = torch.aten.view %6585, %6586 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %6587, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_8342 = torch.constant.int 2097152
    %6588 = torch.prim.ListConstruct %437, %int2097152_8342 : (!torch.int, !torch.int) -> !torch.list<int>
    %6589 = torch.aten.view %6587, %6588 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %6589, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_8343 = torch.constant.int 32
    %int2_8344 = torch.constant.int 2
    %int32_8345 = torch.constant.int 32
    %int8_8346 = torch.constant.int 8
    %int128_8347 = torch.constant.int 128
    %6590 = torch.prim.ListConstruct %437, %int32_8343, %int2_8344, %int32_8345, %int8_8346, %int128_8347 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6591 = torch.aten.view %6589, %6590 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %6591, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int8_8348 = torch.constant.int 8
    %int128_8349 = torch.constant.int 128
    %6592 = torch.prim.ListConstruct %6581, %int8_8348, %int128_8349 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6593 = torch.aten.view %6591, %6592 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %6593, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_8350 = torch.constant.int 32
    %6594 = torch.aten.floor_divide.Scalar %arg2, %int32_8350 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_8351 = torch.constant.int 1
    %6595 = torch.aten.unsqueeze %6594, %int1_8351 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_8352 = torch.constant.int 1
    %false_8353 = torch.constant.bool false
    %6596 = torch.aten.gather %arg3, %int1_8352, %6595, %false_8353 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_8354 = torch.constant.int 32
    %6597 = torch.aten.remainder.Scalar %arg2, %int32_8354 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_8355 = torch.constant.int 1
    %6598 = torch.aten.unsqueeze %6597, %int1_8355 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_8356 = torch.constant.none
    %6599 = torch.aten.clone %336, %none_8356 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_8357 = torch.constant.int 0
    %6600 = torch.aten.unsqueeze %6599, %int0_8357 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_8358 = torch.constant.int 4
    %int1_8359 = torch.constant.int 1
    %6601 = torch.prim.ListConstruct %int4_8358, %int1_8359 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_8360 = torch.constant.int 1
    %int1_8361 = torch.constant.int 1
    %6602 = torch.prim.ListConstruct %int1_8360, %int1_8361 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_8362 = torch.constant.int 4
    %int0_8363 = torch.constant.int 0
    %cpu_8364 = torch.constant.device "cpu"
    %false_8365 = torch.constant.bool false
    %6603 = torch.aten.empty_strided %6601, %6602, %int4_8362, %int0_8363, %cpu_8364, %false_8365 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int30_8366 = torch.constant.int 30
    %6604 = torch.aten.fill.Scalar %6603, %int30_8366 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_8367 = torch.constant.int 4
    %int1_8368 = torch.constant.int 1
    %6605 = torch.prim.ListConstruct %int4_8367, %int1_8368 : (!torch.int, !torch.int) -> !torch.list<int>
    %6606 = torch.aten.repeat %6600, %6605 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_8369 = torch.constant.int 32
    %6607 = torch.aten.mul.Scalar %6596, %int32_8369 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_8370 = torch.constant.int 1
    %6608 = torch.aten.add.Tensor %6607, %6604, %int1_8370 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_8371 = torch.constant.int 2
    %6609 = torch.aten.mul.Scalar %6608, %int2_8371 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_8372 = torch.constant.int 1
    %6610 = torch.aten.add.Tensor %6609, %6606, %int1_8372 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_8373 = torch.constant.int 32
    %6611 = torch.aten.mul.Scalar %6610, %int32_8373 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_8374 = torch.constant.int 1
    %6612 = torch.aten.add.Tensor %6611, %6598, %int1_8374 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %6613 = torch.prim.ListConstruct %6612 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_8375 = torch.constant.bool false
    %6614 = torch.aten.index_put %6593, %6613, %6545, %false_8375 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %6614, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_8376 = torch.constant.int 32
    %int2_8377 = torch.constant.int 2
    %int32_8378 = torch.constant.int 32
    %int8_8379 = torch.constant.int 8
    %int128_8380 = torch.constant.int 128
    %6615 = torch.prim.ListConstruct %437, %int32_8376, %int2_8377, %int32_8378, %int8_8379, %int128_8380 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6616 = torch.aten.view %6614, %6615 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %6616, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_8381 = torch.constant.int 2097152
    %6617 = torch.prim.ListConstruct %437, %int2097152_8381 : (!torch.int, !torch.int) -> !torch.list<int>
    %6618 = torch.aten.view %6616, %6617 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %6618, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int4_8382 = torch.constant.int 4
    %6619 = torch.prim.ListConstruct %int4_8382, %358 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_8383 = torch.constant.int 1
    %6620 = torch.prim.ListConstruct %358, %int1_8383 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_8384 = torch.constant.int 4
    %int0_8385 = torch.constant.int 0
    %cpu_8386 = torch.constant.device "cpu"
    %false_8387 = torch.constant.bool false
    %6621 = torch.aten.empty_strided %6619, %6620, %int4_8384, %int0_8385, %cpu_8386, %false_8387 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6621, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int30_8388 = torch.constant.int 30
    %6622 = torch.aten.fill.Scalar %6621, %int30_8388 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6622, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int32_8389 = torch.constant.int 32
    %6623 = torch.aten.mul.Scalar %arg3, %int32_8389 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6623, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_8390 = torch.constant.int 1
    %6624 = torch.aten.add.Tensor %6623, %6622, %int1_8390 : !torch.vtensor<[4,?],si64>, !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6624, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_8391 = torch.constant.int 4
    %6625 = torch.aten.mul.int %int4_8391, %358 : !torch.int, !torch.int -> !torch.int
    %6626 = torch.prim.ListConstruct %6625 : (!torch.int) -> !torch.list<int>
    %6627 = torch.aten.view %6624, %6626 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %6627, [%356], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_8392 = torch.constant.int 32
    %int2_8393 = torch.constant.int 2
    %int32_8394 = torch.constant.int 32
    %int8_8395 = torch.constant.int 8
    %int128_8396 = torch.constant.int 128
    %6628 = torch.prim.ListConstruct %437, %int32_8392, %int2_8393, %int32_8394, %int8_8395, %int128_8396 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6629 = torch.aten.view %6618, %6628 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %6629, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_8397 = torch.constant.int 32
    %6630 = torch.aten.mul.int %437, %int32_8397 : !torch.int, !torch.int -> !torch.int
    %int2_8398 = torch.constant.int 2
    %int32_8399 = torch.constant.int 32
    %int8_8400 = torch.constant.int 8
    %int128_8401 = torch.constant.int 128
    %6631 = torch.prim.ListConstruct %6630, %int2_8398, %int32_8399, %int8_8400, %int128_8401 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6632 = torch.aten.view %6629, %6631 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %6632, [%357], affine_map<()[s0] -> (s0 * 32, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int0_8402 = torch.constant.int 0
    %6633 = torch.aten.index_select %6632, %int0_8402, %6627 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.int, !torch.vtensor<[?],si64> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %6633, [%356], affine_map<()[s0] -> (s0 * 4, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int4_8403 = torch.constant.int 4
    %int2_8404 = torch.constant.int 2
    %int32_8405 = torch.constant.int 32
    %int8_8406 = torch.constant.int 8
    %int128_8407 = torch.constant.int 128
    %6634 = torch.prim.ListConstruct %int4_8403, %358, %int2_8404, %int32_8405, %int8_8406, %int128_8407 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6635 = torch.aten.view %6633, %6634 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %6635, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int0_8408 = torch.constant.int 0
    %int0_8409 = torch.constant.int 0
    %int9223372036854775807_8410 = torch.constant.int 9223372036854775807
    %int1_8411 = torch.constant.int 1
    %6636 = torch.aten.slice.Tensor %6635, %int0_8408, %int0_8409, %int9223372036854775807_8410, %int1_8411 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %6636, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_8412 = torch.constant.int 1
    %int0_8413 = torch.constant.int 0
    %int9223372036854775807_8414 = torch.constant.int 9223372036854775807
    %int1_8415 = torch.constant.int 1
    %6637 = torch.aten.slice.Tensor %6636, %int1_8412, %int0_8413, %int9223372036854775807_8414, %int1_8415 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %6637, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_8416 = torch.constant.int 2
    %int0_8417 = torch.constant.int 0
    %6638 = torch.aten.select.int %6637, %int2_8416, %int0_8417 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6638, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int32_8418 = torch.constant.int 32
    %6639 = torch.aten.mul.int %358, %int32_8418 : !torch.int, !torch.int -> !torch.int
    %int2_8419 = torch.constant.int 2
    %int0_8420 = torch.constant.int 0
    %int1_8421 = torch.constant.int 1
    %6640 = torch.aten.slice.Tensor %6638, %int2_8419, %int0_8420, %6639, %int1_8421 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6640, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_8422 = torch.constant.int 0
    %6641 = torch.aten.clone %6640, %int0_8422 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6641, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_8423 = torch.constant.int 1
    %6642 = torch.aten.size.int %6637, %int1_8423 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_8424 = torch.constant.int 32
    %6643 = torch.aten.mul.int %6642, %int32_8424 : !torch.int, !torch.int -> !torch.int
    %int4_8425 = torch.constant.int 4
    %int8_8426 = torch.constant.int 8
    %int128_8427 = torch.constant.int 128
    %6644 = torch.prim.ListConstruct %int4_8425, %6643, %int8_8426, %int128_8427 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6645 = torch.aten._unsafe_view %6641, %6644 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %6645, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_8428 = torch.constant.int 0
    %int0_8429 = torch.constant.int 0
    %int9223372036854775807_8430 = torch.constant.int 9223372036854775807
    %int1_8431 = torch.constant.int 1
    %6646 = torch.aten.slice.Tensor %6645, %int0_8428, %int0_8429, %int9223372036854775807_8430, %int1_8431 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %6646, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_8432 = torch.constant.int 0
    %int0_8433 = torch.constant.int 0
    %int9223372036854775807_8434 = torch.constant.int 9223372036854775807
    %int1_8435 = torch.constant.int 1
    %6647 = torch.aten.slice.Tensor %6635, %int0_8432, %int0_8433, %int9223372036854775807_8434, %int1_8435 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %6647, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_8436 = torch.constant.int 1
    %int0_8437 = torch.constant.int 0
    %int9223372036854775807_8438 = torch.constant.int 9223372036854775807
    %int1_8439 = torch.constant.int 1
    %6648 = torch.aten.slice.Tensor %6647, %int1_8436, %int0_8437, %int9223372036854775807_8438, %int1_8439 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %6648, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_8440 = torch.constant.int 2
    %int1_8441 = torch.constant.int 1
    %6649 = torch.aten.select.int %6648, %int2_8440, %int1_8441 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6649, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int2_8442 = torch.constant.int 2
    %int0_8443 = torch.constant.int 0
    %int1_8444 = torch.constant.int 1
    %6650 = torch.aten.slice.Tensor %6649, %int2_8442, %int0_8443, %6639, %int1_8444 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6650, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_8445 = torch.constant.int 0
    %6651 = torch.aten.clone %6650, %int0_8445 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6651, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_8446 = torch.constant.int 1
    %6652 = torch.aten.size.int %6648, %int1_8446 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_8447 = torch.constant.int 32
    %6653 = torch.aten.mul.int %6652, %int32_8447 : !torch.int, !torch.int -> !torch.int
    %int4_8448 = torch.constant.int 4
    %int8_8449 = torch.constant.int 8
    %int128_8450 = torch.constant.int 128
    %6654 = torch.prim.ListConstruct %int4_8448, %6653, %int8_8449, %int128_8450 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6655 = torch.aten._unsafe_view %6651, %6654 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %6655, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_8451 = torch.constant.int 0
    %int0_8452 = torch.constant.int 0
    %int9223372036854775807_8453 = torch.constant.int 9223372036854775807
    %int1_8454 = torch.constant.int 1
    %6656 = torch.aten.slice.Tensor %6655, %int0_8451, %int0_8452, %int9223372036854775807_8453, %int1_8454 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %6656, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int-2_8455 = torch.constant.int -2
    %6657 = torch.aten.unsqueeze %6646, %int-2_8455 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %6657, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_8456 = torch.constant.int 1
    %6658 = torch.aten.size.int %6645, %int1_8456 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_8457 = torch.constant.int 4
    %int8_8458 = torch.constant.int 8
    %int4_8459 = torch.constant.int 4
    %int128_8460 = torch.constant.int 128
    %6659 = torch.prim.ListConstruct %int4_8457, %6658, %int8_8458, %int4_8459, %int128_8460 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_8461 = torch.constant.bool false
    %6660 = torch.aten.expand %6657, %6659, %false_8461 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %6660, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_8462 = torch.constant.int 0
    %6661 = torch.aten.clone %6660, %int0_8462 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %6661, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_8463 = torch.constant.int 4
    %int32_8464 = torch.constant.int 32
    %int128_8465 = torch.constant.int 128
    %6662 = torch.prim.ListConstruct %int4_8463, %6658, %int32_8464, %int128_8465 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6663 = torch.aten._unsafe_view %6661, %6662 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %6663, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_8466 = torch.constant.int -2
    %6664 = torch.aten.unsqueeze %6656, %int-2_8466 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %6664, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_8467 = torch.constant.int 1
    %6665 = torch.aten.size.int %6655, %int1_8467 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_8468 = torch.constant.int 4
    %int8_8469 = torch.constant.int 8
    %int4_8470 = torch.constant.int 4
    %int128_8471 = torch.constant.int 128
    %6666 = torch.prim.ListConstruct %int4_8468, %6665, %int8_8469, %int4_8470, %int128_8471 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_8472 = torch.constant.bool false
    %6667 = torch.aten.expand %6664, %6666, %false_8472 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %6667, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_8473 = torch.constant.int 0
    %6668 = torch.aten.clone %6667, %int0_8473 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %6668, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_8474 = torch.constant.int 4
    %int32_8475 = torch.constant.int 32
    %int128_8476 = torch.constant.int 128
    %6669 = torch.prim.ListConstruct %int4_8474, %6665, %int32_8475, %int128_8476 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6670 = torch.aten._unsafe_view %6668, %6669 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %6670, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_8477 = torch.constant.int 1
    %int2_8478 = torch.constant.int 2
    %6671 = torch.aten.transpose.int %6551, %int1_8477, %int2_8478 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,128],f16>
    %int1_8479 = torch.constant.int 1
    %int2_8480 = torch.constant.int 2
    %6672 = torch.aten.transpose.int %6663, %int1_8479, %int2_8480 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %6672, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_8481 = torch.constant.int 1
    %int2_8482 = torch.constant.int 2
    %6673 = torch.aten.transpose.int %6670, %int1_8481, %int2_8482 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %6673, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_8483 = torch.constant.float 0.000000e+00
    %false_8484 = torch.constant.bool false
    %none_8485 = torch.constant.none
    %6674:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6671, %6672, %6673, %float0.000000e00_8483, %false_8484, %368, %none_8485) : (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.vtensor<[4,1,1,?],f16>, !torch.none) -> (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,1],f32>) 
    %int1_8486 = torch.constant.int 1
    %int2_8487 = torch.constant.int 2
    %6675 = torch.aten.transpose.int %6674#0, %int1_8486, %int2_8487 : !torch.vtensor<[4,32,1,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int4_8488 = torch.constant.int 4
    %int1_8489 = torch.constant.int 1
    %int4096_8490 = torch.constant.int 4096
    %6676 = torch.prim.ListConstruct %int4_8488, %int1_8489, %int4096_8490 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6677 = torch.aten.view %6675, %6676 : !torch.vtensor<[4,1,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_8491 = torch.constant.int -2
    %int-1_8492 = torch.constant.int -1
    %6678 = torch.aten.transpose.int %337, %int-2_8491, %int-1_8492 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_8493 = torch.constant.int 4
    %int4096_8494 = torch.constant.int 4096
    %6679 = torch.prim.ListConstruct %int4_8493, %int4096_8494 : (!torch.int, !torch.int) -> !torch.list<int>
    %6680 = torch.aten.view %6677, %6679 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %6681 = torch.aten.mm %6680, %6678 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_8495 = torch.constant.int 4
    %int1_8496 = torch.constant.int 1
    %int4096_8497 = torch.constant.int 4096
    %6682 = torch.prim.ListConstruct %int4_8495, %int1_8496, %int4096_8497 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6683 = torch.aten.view %6681, %6682 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_8498 = torch.constant.int 1
    %6684 = torch.aten.add.Tensor %6511, %6683, %int1_8498 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_8499 = torch.constant.int 6
    %6685 = torch.prims.convert_element_type %6684, %int6_8499 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_8500 = torch.constant.int 2
    %6686 = torch.aten.pow.Tensor_Scalar %6685, %int2_8500 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_8501 = torch.constant.int -1
    %6687 = torch.prim.ListConstruct %int-1_8501 : (!torch.int) -> !torch.list<int>
    %true_8502 = torch.constant.bool true
    %none_8503 = torch.constant.none
    %6688 = torch.aten.mean.dim %6686, %6687, %true_8502, %none_8503 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_8504 = torch.constant.float 9.9999997473787516E-6
    %int1_8505 = torch.constant.int 1
    %6689 = torch.aten.add.Scalar %6688, %float9.999990e-06_8504, %int1_8505 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %6690 = torch.aten.rsqrt %6689 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %6691 = torch.aten.mul.Tensor %6685, %6690 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_8506 = torch.constant.int 5
    %6692 = torch.prims.convert_element_type %6691, %int5_8506 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %6693 = torch.aten.mul.Tensor %338, %6692 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_8507 = torch.constant.int 5
    %6694 = torch.prims.convert_element_type %6693, %int5_8507 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_8508 = torch.constant.int -2
    %int-1_8509 = torch.constant.int -1
    %6695 = torch.aten.transpose.int %339, %int-2_8508, %int-1_8509 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_8510 = torch.constant.int 4
    %int4096_8511 = torch.constant.int 4096
    %6696 = torch.prim.ListConstruct %int4_8510, %int4096_8511 : (!torch.int, !torch.int) -> !torch.list<int>
    %6697 = torch.aten.view %6694, %6696 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %6698 = torch.aten.mm %6697, %6695 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_8512 = torch.constant.int 4
    %int1_8513 = torch.constant.int 1
    %int14336_8514 = torch.constant.int 14336
    %6699 = torch.prim.ListConstruct %int4_8512, %int1_8513, %int14336_8514 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6700 = torch.aten.view %6698, %6699 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %6701 = torch.aten.silu %6700 : !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_8515 = torch.constant.int -2
    %int-1_8516 = torch.constant.int -1
    %6702 = torch.aten.transpose.int %340, %int-2_8515, %int-1_8516 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_8517 = torch.constant.int 4
    %int4096_8518 = torch.constant.int 4096
    %6703 = torch.prim.ListConstruct %int4_8517, %int4096_8518 : (!torch.int, !torch.int) -> !torch.list<int>
    %6704 = torch.aten.view %6694, %6703 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %6705 = torch.aten.mm %6704, %6702 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_8519 = torch.constant.int 4
    %int1_8520 = torch.constant.int 1
    %int14336_8521 = torch.constant.int 14336
    %6706 = torch.prim.ListConstruct %int4_8519, %int1_8520, %int14336_8521 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6707 = torch.aten.view %6705, %6706 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %6708 = torch.aten.mul.Tensor %6701, %6707 : !torch.vtensor<[4,1,14336],f16>, !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_8522 = torch.constant.int -2
    %int-1_8523 = torch.constant.int -1
    %6709 = torch.aten.transpose.int %341, %int-2_8522, %int-1_8523 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int4_8524 = torch.constant.int 4
    %int14336_8525 = torch.constant.int 14336
    %6710 = torch.prim.ListConstruct %int4_8524, %int14336_8525 : (!torch.int, !torch.int) -> !torch.list<int>
    %6711 = torch.aten.view %6708, %6710 : !torch.vtensor<[4,1,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,14336],f16>
    %6712 = torch.aten.mm %6711, %6709 : !torch.vtensor<[4,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_8526 = torch.constant.int 4
    %int1_8527 = torch.constant.int 1
    %int4096_8528 = torch.constant.int 4096
    %6713 = torch.prim.ListConstruct %int4_8526, %int1_8527, %int4096_8528 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6714 = torch.aten.view %6712, %6713 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_8529 = torch.constant.int 1
    %6715 = torch.aten.add.Tensor %6684, %6714, %int1_8529 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_8530 = torch.constant.int 6
    %6716 = torch.prims.convert_element_type %6715, %int6_8530 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_8531 = torch.constant.int 2
    %6717 = torch.aten.pow.Tensor_Scalar %6716, %int2_8531 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_8532 = torch.constant.int -1
    %6718 = torch.prim.ListConstruct %int-1_8532 : (!torch.int) -> !torch.list<int>
    %true_8533 = torch.constant.bool true
    %none_8534 = torch.constant.none
    %6719 = torch.aten.mean.dim %6717, %6718, %true_8533, %none_8534 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_8535 = torch.constant.float 9.9999997473787516E-6
    %int1_8536 = torch.constant.int 1
    %6720 = torch.aten.add.Scalar %6719, %float9.999990e-06_8535, %int1_8536 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %6721 = torch.aten.rsqrt %6720 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %6722 = torch.aten.mul.Tensor %6716, %6721 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_8537 = torch.constant.int 5
    %6723 = torch.prims.convert_element_type %6722, %int5_8537 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %6724 = torch.aten.mul.Tensor %342, %6723 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_8538 = torch.constant.int 5
    %6725 = torch.prims.convert_element_type %6724, %int5_8538 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_8539 = torch.constant.int -2
    %int-1_8540 = torch.constant.int -1
    %6726 = torch.aten.transpose.int %343, %int-2_8539, %int-1_8540 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_8541 = torch.constant.int 4
    %int4096_8542 = torch.constant.int 4096
    %6727 = torch.prim.ListConstruct %int4_8541, %int4096_8542 : (!torch.int, !torch.int) -> !torch.list<int>
    %6728 = torch.aten.view %6725, %6727 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %6729 = torch.aten.mm %6728, %6726 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_8543 = torch.constant.int 4
    %int1_8544 = torch.constant.int 1
    %int4096_8545 = torch.constant.int 4096
    %6730 = torch.prim.ListConstruct %int4_8543, %int1_8544, %int4096_8545 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6731 = torch.aten.view %6729, %6730 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_8546 = torch.constant.int -2
    %int-1_8547 = torch.constant.int -1
    %6732 = torch.aten.transpose.int %344, %int-2_8546, %int-1_8547 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_8548 = torch.constant.int 4
    %int4096_8549 = torch.constant.int 4096
    %6733 = torch.prim.ListConstruct %int4_8548, %int4096_8549 : (!torch.int, !torch.int) -> !torch.list<int>
    %6734 = torch.aten.view %6725, %6733 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %6735 = torch.aten.mm %6734, %6732 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_8550 = torch.constant.int 4
    %int1_8551 = torch.constant.int 1
    %int1024_8552 = torch.constant.int 1024
    %6736 = torch.prim.ListConstruct %int4_8550, %int1_8551, %int1024_8552 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6737 = torch.aten.view %6735, %6736 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int-2_8553 = torch.constant.int -2
    %int-1_8554 = torch.constant.int -1
    %6738 = torch.aten.transpose.int %345, %int-2_8553, %int-1_8554 : !torch.vtensor<[1024,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,1024],f16>
    %int4_8555 = torch.constant.int 4
    %int4096_8556 = torch.constant.int 4096
    %6739 = torch.prim.ListConstruct %int4_8555, %int4096_8556 : (!torch.int, !torch.int) -> !torch.list<int>
    %6740 = torch.aten.view %6725, %6739 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %6741 = torch.aten.mm %6740, %6738 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,1024],f16> -> !torch.vtensor<[4,1024],f16>
    %int4_8557 = torch.constant.int 4
    %int1_8558 = torch.constant.int 1
    %int1024_8559 = torch.constant.int 1024
    %6742 = torch.prim.ListConstruct %int4_8557, %int1_8558, %int1024_8559 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6743 = torch.aten.view %6741, %6742 : !torch.vtensor<[4,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,1024],f16>
    %int4_8560 = torch.constant.int 4
    %int1_8561 = torch.constant.int 1
    %int32_8562 = torch.constant.int 32
    %int128_8563 = torch.constant.int 128
    %6744 = torch.prim.ListConstruct %int4_8560, %int1_8561, %int32_8562, %int128_8563 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6745 = torch.aten.view %6731, %6744 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,32,128],f16>
    %int4_8564 = torch.constant.int 4
    %int1_8565 = torch.constant.int 1
    %int8_8566 = torch.constant.int 8
    %int128_8567 = torch.constant.int 128
    %6746 = torch.prim.ListConstruct %int4_8564, %int1_8565, %int8_8566, %int128_8567 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6747 = torch.aten.view %6737, %6746 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int4_8568 = torch.constant.int 4
    %int1_8569 = torch.constant.int 1
    %int8_8570 = torch.constant.int 8
    %int128_8571 = torch.constant.int 128
    %6748 = torch.prim.ListConstruct %int4_8568, %int1_8569, %int8_8570, %int128_8571 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6749 = torch.aten.view %6743, %6748 : !torch.vtensor<[4,1,1024],f16>, !torch.list<int> -> !torch.vtensor<[4,1,8,128],f16>
    %int6_8572 = torch.constant.int 6
    %6750 = torch.prims.convert_element_type %6745, %int6_8572 : !torch.vtensor<[4,1,32,128],f16>, !torch.int -> !torch.vtensor<[4,1,32,128],f32>
    %6751 = torch_c.to_builtin_tensor %6750 : !torch.vtensor<[4,1,32,128],f32> -> tensor<4x1x32x128xf32>
    %6752 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %6753 = util.call @sharktank_rotary_embedding_4_1_32_128_f32(%6751, %6752) : (tensor<4x1x32x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x32x128xf32>
    %6754 = torch_c.from_builtin_tensor %6753 : tensor<4x1x32x128xf32> -> !torch.vtensor<[4,1,32,128],f32>
    %int5_8573 = torch.constant.int 5
    %6755 = torch.prims.convert_element_type %6754, %int5_8573 : !torch.vtensor<[4,1,32,128],f32>, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int6_8574 = torch.constant.int 6
    %6756 = torch.prims.convert_element_type %6747, %int6_8574 : !torch.vtensor<[4,1,8,128],f16>, !torch.int -> !torch.vtensor<[4,1,8,128],f32>
    %6757 = torch_c.to_builtin_tensor %6756 : !torch.vtensor<[4,1,8,128],f32> -> tensor<4x1x8x128xf32>
    %6758 = torch_c.to_builtin_tensor %389 : !torch.vtensor<[4,1,128],f32> -> tensor<4x1x128xf32>
    %6759 = util.call @sharktank_rotary_embedding_4_1_8_128_f32(%6757, %6758) : (tensor<4x1x8x128xf32>, tensor<4x1x128xf32>) -> tensor<4x1x8x128xf32>
    %6760 = torch_c.from_builtin_tensor %6759 : tensor<4x1x8x128xf32> -> !torch.vtensor<[4,1,8,128],f32>
    %int5_8575 = torch.constant.int 5
    %6761 = torch.prims.convert_element_type %6760, %int5_8575 : !torch.vtensor<[4,1,8,128],f32>, !torch.int -> !torch.vtensor<[4,1,8,128],f16>
    %int32_8576 = torch.constant.int 32
    %6762 = torch.aten.floor_divide.Scalar %arg2, %int32_8576 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_8577 = torch.constant.int 1
    %6763 = torch.aten.unsqueeze %6762, %int1_8577 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_8578 = torch.constant.int 1
    %false_8579 = torch.constant.bool false
    %6764 = torch.aten.gather %arg3, %int1_8578, %6763, %false_8579 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_8580 = torch.constant.int 32
    %6765 = torch.aten.remainder.Scalar %arg2, %int32_8580 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_8581 = torch.constant.int 1
    %6766 = torch.aten.unsqueeze %6765, %int1_8581 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_8582 = torch.constant.none
    %6767 = torch.aten.clone %346, %none_8582 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_8583 = torch.constant.int 0
    %6768 = torch.aten.unsqueeze %6767, %int0_8583 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_8584 = torch.constant.int 4
    %int1_8585 = torch.constant.int 1
    %6769 = torch.prim.ListConstruct %int4_8584, %int1_8585 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_8586 = torch.constant.int 1
    %int1_8587 = torch.constant.int 1
    %6770 = torch.prim.ListConstruct %int1_8586, %int1_8587 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_8588 = torch.constant.int 4
    %int0_8589 = torch.constant.int 0
    %cpu_8590 = torch.constant.device "cpu"
    %false_8591 = torch.constant.bool false
    %6771 = torch.aten.empty_strided %6769, %6770, %int4_8588, %int0_8589, %cpu_8590, %false_8591 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int31 = torch.constant.int 31
    %6772 = torch.aten.fill.Scalar %6771, %int31 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_8592 = torch.constant.int 4
    %int1_8593 = torch.constant.int 1
    %6773 = torch.prim.ListConstruct %int4_8592, %int1_8593 : (!torch.int, !torch.int) -> !torch.list<int>
    %6774 = torch.aten.repeat %6768, %6773 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_8594 = torch.constant.int 32
    %6775 = torch.aten.mul.Scalar %6764, %int32_8594 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_8595 = torch.constant.int 1
    %6776 = torch.aten.add.Tensor %6775, %6772, %int1_8595 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_8596 = torch.constant.int 2
    %6777 = torch.aten.mul.Scalar %6776, %int2_8596 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_8597 = torch.constant.int 1
    %6778 = torch.aten.add.Tensor %6777, %6774, %int1_8597 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_8598 = torch.constant.int 32
    %6779 = torch.aten.mul.Scalar %6778, %int32_8598 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_8599 = torch.constant.int 1
    %6780 = torch.aten.add.Tensor %6779, %6766, %int1_8599 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_8600 = torch.constant.int 32
    %int2_8601 = torch.constant.int 2
    %int32_8602 = torch.constant.int 32
    %int8_8603 = torch.constant.int 8
    %int128_8604 = torch.constant.int 128
    %6781 = torch.prim.ListConstruct %437, %int32_8600, %int2_8601, %int32_8602, %int8_8603, %int128_8604 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6782 = torch.aten.view %6618, %6781 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %6782, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_8605 = torch.constant.int 32
    %6783 = torch.aten.mul.int %437, %int32_8605 : !torch.int, !torch.int -> !torch.int
    %int2_8606 = torch.constant.int 2
    %6784 = torch.aten.mul.int %6783, %int2_8606 : !torch.int, !torch.int -> !torch.int
    %int32_8607 = torch.constant.int 32
    %6785 = torch.aten.mul.int %6784, %int32_8607 : !torch.int, !torch.int -> !torch.int
    %int8_8608 = torch.constant.int 8
    %int128_8609 = torch.constant.int 128
    %6786 = torch.prim.ListConstruct %6785, %int8_8608, %int128_8609 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6787 = torch.aten.view %6782, %6786 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %6787, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %6788 = torch.prim.ListConstruct %6780 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_8610 = torch.constant.bool false
    %6789 = torch.aten.index_put %6787, %6788, %6761, %false_8610 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %6789, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_8611 = torch.constant.int 32
    %int2_8612 = torch.constant.int 2
    %int32_8613 = torch.constant.int 32
    %int8_8614 = torch.constant.int 8
    %int128_8615 = torch.constant.int 128
    %6790 = torch.prim.ListConstruct %437, %int32_8611, %int2_8612, %int32_8613, %int8_8614, %int128_8615 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6791 = torch.aten.view %6789, %6790 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %6791, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_8616 = torch.constant.int 2097152
    %6792 = torch.prim.ListConstruct %437, %int2097152_8616 : (!torch.int, !torch.int) -> !torch.list<int>
    %6793 = torch.aten.view %6791, %6792 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.bind_symbolic_shape %6793, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int32_8617 = torch.constant.int 32
    %int2_8618 = torch.constant.int 2
    %int32_8619 = torch.constant.int 32
    %int8_8620 = torch.constant.int 8
    %int128_8621 = torch.constant.int 128
    %6794 = torch.prim.ListConstruct %437, %int32_8617, %int2_8618, %int32_8619, %int8_8620, %int128_8621 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6795 = torch.aten.view %6793, %6794 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %6795, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int8_8622 = torch.constant.int 8
    %int128_8623 = torch.constant.int 128
    %6796 = torch.prim.ListConstruct %6785, %int8_8622, %int128_8623 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6797 = torch.aten.view %6795, %6796 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %6797, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_8624 = torch.constant.int 32
    %6798 = torch.aten.floor_divide.Scalar %arg2, %int32_8624 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_8625 = torch.constant.int 1
    %6799 = torch.aten.unsqueeze %6798, %int1_8625 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_8626 = torch.constant.int 1
    %false_8627 = torch.constant.bool false
    %6800 = torch.aten.gather %arg3, %int1_8626, %6799, %false_8627 : !torch.vtensor<[4,?],si64>, !torch.int, !torch.vtensor<[4,1],si64>, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int32_8628 = torch.constant.int 32
    %6801 = torch.aten.remainder.Scalar %arg2, %int32_8628 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4],si64>
    %int1_8629 = torch.constant.int 1
    %6802 = torch.aten.unsqueeze %6801, %int1_8629 : !torch.vtensor<[4],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %none_8630 = torch.constant.none
    %6803 = torch.aten.clone %347, %none_8630 : !torch.vtensor<[],si64>, !torch.none -> !torch.vtensor<[],si64>
    %int0_8631 = torch.constant.int 0
    %6804 = torch.aten.unsqueeze %6803, %int0_8631 : !torch.vtensor<[],si64>, !torch.int -> !torch.vtensor<[1],si64>
    %int4_8632 = torch.constant.int 4
    %int1_8633 = torch.constant.int 1
    %6805 = torch.prim.ListConstruct %int4_8632, %int1_8633 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_8634 = torch.constant.int 1
    %int1_8635 = torch.constant.int 1
    %6806 = torch.prim.ListConstruct %int1_8634, %int1_8635 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_8636 = torch.constant.int 4
    %int0_8637 = torch.constant.int 0
    %cpu_8638 = torch.constant.device "cpu"
    %false_8639 = torch.constant.bool false
    %6807 = torch.aten.empty_strided %6805, %6806, %int4_8636, %int0_8637, %cpu_8638, %false_8639 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,1],si64>
    %int31_8640 = torch.constant.int 31
    %6808 = torch.aten.fill.Scalar %6807, %int31_8640 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int4_8641 = torch.constant.int 4
    %int1_8642 = torch.constant.int 1
    %6809 = torch.prim.ListConstruct %int4_8641, %int1_8642 : (!torch.int, !torch.int) -> !torch.list<int>
    %6810 = torch.aten.repeat %6804, %6809 : !torch.vtensor<[1],si64>, !torch.list<int> -> !torch.vtensor<[4,1],si64>
    %int32_8643 = torch.constant.int 32
    %6811 = torch.aten.mul.Scalar %6800, %int32_8643 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_8644 = torch.constant.int 1
    %6812 = torch.aten.add.Tensor %6811, %6808, %int1_8644 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int2_8645 = torch.constant.int 2
    %6813 = torch.aten.mul.Scalar %6812, %int2_8645 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_8646 = torch.constant.int 1
    %6814 = torch.aten.add.Tensor %6813, %6810, %int1_8646 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int32_8647 = torch.constant.int 32
    %6815 = torch.aten.mul.Scalar %6814, %int32_8647 : !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %int1_8648 = torch.constant.int 1
    %6816 = torch.aten.add.Tensor %6815, %6802, %int1_8648 : !torch.vtensor<[4,1],si64>, !torch.vtensor<[4,1],si64>, !torch.int -> !torch.vtensor<[4,1],si64>
    %6817 = torch.prim.ListConstruct %6816 : (!torch.vtensor<[4,1],si64>) -> !torch.list<optional<vtensor>>
    %false_8649 = torch.constant.bool false
    %6818 = torch.aten.index_put %6797, %6817, %6749, %false_8649 : !torch.vtensor<[?,8,128],f16>, !torch.list<optional<vtensor>>, !torch.vtensor<[4,1,8,128],f16>, !torch.bool -> !torch.vtensor<[?,8,128],f16>
    torch.bind_symbolic_shape %6818, [%357], affine_map<()[s0] -> (s0 * 2048, 8, 128)> : !torch.vtensor<[?,8,128],f16>
    %int32_8650 = torch.constant.int 32
    %int2_8651 = torch.constant.int 2
    %int32_8652 = torch.constant.int 32
    %int8_8653 = torch.constant.int 8
    %int128_8654 = torch.constant.int 128
    %6819 = torch.prim.ListConstruct %437, %int32_8650, %int2_8651, %int32_8652, %int8_8653, %int128_8654 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6820 = torch.aten.view %6818, %6819 : !torch.vtensor<[?,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %6820, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int2097152_8655 = torch.constant.int 2097152
    %6821 = torch.prim.ListConstruct %437, %int2097152_8655 : (!torch.int, !torch.int) -> !torch.list<int>
    %6822 = torch.aten.view %6820, %6821 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2097152],f16>
    torch.overwrite.tensor.contents %6822 overwrites %arg4 : !torch.vtensor<[?,2097152],f16>, !torch.tensor<[?,2097152],f16>
    torch.bind_symbolic_shape %6822, [%357], affine_map<()[s0] -> (s0, 2097152)> : !torch.vtensor<[?,2097152],f16>
    %int4_8656 = torch.constant.int 4
    %6823 = torch.prim.ListConstruct %int4_8656, %358 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_8657 = torch.constant.int 1
    %6824 = torch.prim.ListConstruct %358, %int1_8657 : (!torch.int, !torch.int) -> !torch.list<int>
    %int4_8658 = torch.constant.int 4
    %int0_8659 = torch.constant.int 0
    %cpu_8660 = torch.constant.device "cpu"
    %false_8661 = torch.constant.bool false
    %6825 = torch.aten.empty_strided %6823, %6824, %int4_8658, %int0_8659, %cpu_8660, %false_8661 : !torch.list<int>, !torch.list<int>, !torch.int, !torch.int, !torch.Device, !torch.bool -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6825, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int31_8662 = torch.constant.int 31
    %6826 = torch.aten.fill.Scalar %6825, %int31_8662 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6826, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int32_8663 = torch.constant.int 32
    %6827 = torch.aten.mul.Scalar %arg3, %int32_8663 : !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6827, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int1_8664 = torch.constant.int 1
    %6828 = torch.aten.add.Tensor %6827, %6826, %int1_8664 : !torch.vtensor<[4,?],si64>, !torch.vtensor<[4,?],si64>, !torch.int -> !torch.vtensor<[4,?],si64>
    torch.bind_symbolic_shape %6828, [%356], affine_map<()[s0] -> (4, s0)> : !torch.vtensor<[4,?],si64>
    %int4_8665 = torch.constant.int 4
    %6829 = torch.aten.mul.int %int4_8665, %358 : !torch.int, !torch.int -> !torch.int
    %6830 = torch.prim.ListConstruct %6829 : (!torch.int) -> !torch.list<int>
    %6831 = torch.aten.view %6828, %6830 : !torch.vtensor<[4,?],si64>, !torch.list<int> -> !torch.vtensor<[?],si64>
    torch.bind_symbolic_shape %6831, [%356], affine_map<()[s0] -> (s0 * 4)> : !torch.vtensor<[?],si64>
    %int32_8666 = torch.constant.int 32
    %int2_8667 = torch.constant.int 2
    %int32_8668 = torch.constant.int 32
    %int8_8669 = torch.constant.int 8
    %int128_8670 = torch.constant.int 128
    %6832 = torch.prim.ListConstruct %437, %int32_8666, %int2_8667, %int32_8668, %int8_8669, %int128_8670 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6833 = torch.aten.view %6822, %6832 : !torch.vtensor<[?,2097152],f16>, !torch.list<int> -> !torch.vtensor<[?,32,2,32,8,128],f16>
    torch.bind_symbolic_shape %6833, [%357], affine_map<()[s0] -> (s0, 32, 2, 32, 8, 128)> : !torch.vtensor<[?,32,2,32,8,128],f16>
    %int32_8671 = torch.constant.int 32
    %6834 = torch.aten.mul.int %437, %int32_8671 : !torch.int, !torch.int -> !torch.int
    %int2_8672 = torch.constant.int 2
    %int32_8673 = torch.constant.int 32
    %int8_8674 = torch.constant.int 8
    %int128_8675 = torch.constant.int 128
    %6835 = torch.prim.ListConstruct %6834, %int2_8672, %int32_8673, %int8_8674, %int128_8675 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6836 = torch.aten.view %6833, %6835 : !torch.vtensor<[?,32,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %6836, [%357], affine_map<()[s0] -> (s0 * 32, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int0_8676 = torch.constant.int 0
    %6837 = torch.aten.index_select %6836, %int0_8676, %6831 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.int, !torch.vtensor<[?],si64> -> !torch.vtensor<[?,2,32,8,128],f16>
    torch.bind_symbolic_shape %6837, [%356], affine_map<()[s0] -> (s0 * 4, 2, 32, 8, 128)> : !torch.vtensor<[?,2,32,8,128],f16>
    %int4_8677 = torch.constant.int 4
    %int2_8678 = torch.constant.int 2
    %int32_8679 = torch.constant.int 32
    %int8_8680 = torch.constant.int 8
    %int128_8681 = torch.constant.int 128
    %6838 = torch.prim.ListConstruct %int4_8677, %358, %int2_8678, %int32_8679, %int8_8680, %int128_8681 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6839 = torch.aten.view %6837, %6838 : !torch.vtensor<[?,2,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %6839, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int0_8682 = torch.constant.int 0
    %int0_8683 = torch.constant.int 0
    %int9223372036854775807_8684 = torch.constant.int 9223372036854775807
    %int1_8685 = torch.constant.int 1
    %6840 = torch.aten.slice.Tensor %6839, %int0_8682, %int0_8683, %int9223372036854775807_8684, %int1_8685 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %6840, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_8686 = torch.constant.int 1
    %int0_8687 = torch.constant.int 0
    %int9223372036854775807_8688 = torch.constant.int 9223372036854775807
    %int1_8689 = torch.constant.int 1
    %6841 = torch.aten.slice.Tensor %6840, %int1_8686, %int0_8687, %int9223372036854775807_8688, %int1_8689 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %6841, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_8690 = torch.constant.int 2
    %int0_8691 = torch.constant.int 0
    %6842 = torch.aten.select.int %6841, %int2_8690, %int0_8691 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6842, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int32_8692 = torch.constant.int 32
    %6843 = torch.aten.mul.int %358, %int32_8692 : !torch.int, !torch.int -> !torch.int
    %int2_8693 = torch.constant.int 2
    %int0_8694 = torch.constant.int 0
    %int1_8695 = torch.constant.int 1
    %6844 = torch.aten.slice.Tensor %6842, %int2_8693, %int0_8694, %6843, %int1_8695 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6844, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_8696 = torch.constant.int 0
    %6845 = torch.aten.clone %6844, %int0_8696 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6845, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_8697 = torch.constant.int 1
    %6846 = torch.aten.size.int %6841, %int1_8697 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_8698 = torch.constant.int 32
    %6847 = torch.aten.mul.int %6846, %int32_8698 : !torch.int, !torch.int -> !torch.int
    %int4_8699 = torch.constant.int 4
    %int8_8700 = torch.constant.int 8
    %int128_8701 = torch.constant.int 128
    %6848 = torch.prim.ListConstruct %int4_8699, %6847, %int8_8700, %int128_8701 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6849 = torch.aten._unsafe_view %6845, %6848 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %6849, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_8702 = torch.constant.int 0
    %int0_8703 = torch.constant.int 0
    %int9223372036854775807_8704 = torch.constant.int 9223372036854775807
    %int1_8705 = torch.constant.int 1
    %6850 = torch.aten.slice.Tensor %6849, %int0_8702, %int0_8703, %int9223372036854775807_8704, %int1_8705 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %6850, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_8706 = torch.constant.int 0
    %int0_8707 = torch.constant.int 0
    %int9223372036854775807_8708 = torch.constant.int 9223372036854775807
    %int1_8709 = torch.constant.int 1
    %6851 = torch.aten.slice.Tensor %6839, %int0_8706, %int0_8707, %int9223372036854775807_8708, %int1_8709 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %6851, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int1_8710 = torch.constant.int 1
    %int0_8711 = torch.constant.int 0
    %int9223372036854775807_8712 = torch.constant.int 9223372036854775807
    %int1_8713 = torch.constant.int 1
    %6852 = torch.aten.slice.Tensor %6851, %int1_8710, %int0_8711, %int9223372036854775807_8712, %int1_8713 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,2,32,8,128],f16>
    torch.bind_symbolic_shape %6852, [%356], affine_map<()[s0] -> (4, s0, 2, 32, 8, 128)> : !torch.vtensor<[4,?,2,32,8,128],f16>
    %int2_8714 = torch.constant.int 2
    %int1_8715 = torch.constant.int 1
    %6853 = torch.aten.select.int %6852, %int2_8714, %int1_8715 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6853, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int2_8716 = torch.constant.int 2
    %int0_8717 = torch.constant.int 0
    %int1_8718 = torch.constant.int 1
    %6854 = torch.aten.slice.Tensor %6853, %int2_8716, %int0_8717, %6843, %int1_8718 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6854, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int0_8719 = torch.constant.int 0
    %6855 = torch.aten.clone %6854, %int0_8719 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,32,8,128],f16>
    torch.bind_symbolic_shape %6855, [%356], affine_map<()[s0] -> (4, s0, 32, 8, 128)> : !torch.vtensor<[4,?,32,8,128],f16>
    %int1_8720 = torch.constant.int 1
    %6856 = torch.aten.size.int %6852, %int1_8720 : !torch.vtensor<[4,?,2,32,8,128],f16>, !torch.int -> !torch.int
    %int32_8721 = torch.constant.int 32
    %6857 = torch.aten.mul.int %6856, %int32_8721 : !torch.int, !torch.int -> !torch.int
    %int4_8722 = torch.constant.int 4
    %int8_8723 = torch.constant.int 8
    %int128_8724 = torch.constant.int 128
    %6858 = torch.prim.ListConstruct %int4_8722, %6857, %int8_8723, %int128_8724 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6859 = torch.aten._unsafe_view %6855, %6858 : !torch.vtensor<[4,?,32,8,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %6859, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int0_8725 = torch.constant.int 0
    %int0_8726 = torch.constant.int 0
    %int9223372036854775807_8727 = torch.constant.int 9223372036854775807
    %int1_8728 = torch.constant.int 1
    %6860 = torch.aten.slice.Tensor %6859, %int0_8725, %int0_8726, %int9223372036854775807_8727, %int1_8728 : !torch.vtensor<[4,?,8,128],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[4,?,8,128],f16>
    torch.bind_symbolic_shape %6860, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 128)> : !torch.vtensor<[4,?,8,128],f16>
    %int-2_8729 = torch.constant.int -2
    %6861 = torch.aten.unsqueeze %6850, %int-2_8729 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %6861, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_8730 = torch.constant.int 1
    %6862 = torch.aten.size.int %6849, %int1_8730 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_8731 = torch.constant.int 4
    %int8_8732 = torch.constant.int 8
    %int4_8733 = torch.constant.int 4
    %int128_8734 = torch.constant.int 128
    %6863 = torch.prim.ListConstruct %int4_8731, %6862, %int8_8732, %int4_8733, %int128_8734 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_8735 = torch.constant.bool false
    %6864 = torch.aten.expand %6861, %6863, %false_8735 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %6864, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_8736 = torch.constant.int 0
    %6865 = torch.aten.clone %6864, %int0_8736 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %6865, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_8737 = torch.constant.int 4
    %int32_8738 = torch.constant.int 32
    %int128_8739 = torch.constant.int 128
    %6866 = torch.prim.ListConstruct %int4_8737, %6862, %int32_8738, %int128_8739 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6867 = torch.aten._unsafe_view %6865, %6866 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %6867, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int-2_8740 = torch.constant.int -2
    %6868 = torch.aten.unsqueeze %6860, %int-2_8740 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,1,128],f16>
    torch.bind_symbolic_shape %6868, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 1, 128)> : !torch.vtensor<[4,?,8,1,128],f16>
    %int1_8741 = torch.constant.int 1
    %6869 = torch.aten.size.int %6859, %int1_8741 : !torch.vtensor<[4,?,8,128],f16>, !torch.int -> !torch.int
    %int4_8742 = torch.constant.int 4
    %int8_8743 = torch.constant.int 8
    %int4_8744 = torch.constant.int 4
    %int128_8745 = torch.constant.int 128
    %6870 = torch.prim.ListConstruct %int4_8742, %6869, %int8_8743, %int4_8744, %int128_8745 : (!torch.int, !torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %false_8746 = torch.constant.bool false
    %6871 = torch.aten.expand %6868, %6870, %false_8746 : !torch.vtensor<[4,?,8,1,128],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %6871, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int0_8747 = torch.constant.int 0
    %6872 = torch.aten.clone %6871, %int0_8747 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.int -> !torch.vtensor<[4,?,8,4,128],f16>
    torch.bind_symbolic_shape %6872, [%356], affine_map<()[s0] -> (4, s0 * 32, 8, 4, 128)> : !torch.vtensor<[4,?,8,4,128],f16>
    %int4_8748 = torch.constant.int 4
    %int32_8749 = torch.constant.int 32
    %int128_8750 = torch.constant.int 128
    %6873 = torch.prim.ListConstruct %int4_8748, %6869, %int32_8749, %int128_8750 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6874 = torch.aten._unsafe_view %6872, %6873 : !torch.vtensor<[4,?,8,4,128],f16>, !torch.list<int> -> !torch.vtensor<[4,?,32,128],f16>
    torch.bind_symbolic_shape %6874, [%356], affine_map<()[s0] -> (4, s0 * 32, 32, 128)> : !torch.vtensor<[4,?,32,128],f16>
    %int1_8751 = torch.constant.int 1
    %int2_8752 = torch.constant.int 2
    %6875 = torch.aten.transpose.int %6755, %int1_8751, %int2_8752 : !torch.vtensor<[4,1,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,1,128],f16>
    %int1_8753 = torch.constant.int 1
    %int2_8754 = torch.constant.int 2
    %6876 = torch.aten.transpose.int %6867, %int1_8753, %int2_8754 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %6876, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %int1_8755 = torch.constant.int 1
    %int2_8756 = torch.constant.int 2
    %6877 = torch.aten.transpose.int %6874, %int1_8755, %int2_8756 : !torch.vtensor<[4,?,32,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,32,?,128],f16>
    torch.bind_symbolic_shape %6877, [%356], affine_map<()[s0] -> (4, 32, s0 * 32, 128)> : !torch.vtensor<[4,32,?,128],f16>
    %float0.000000e00_8757 = torch.constant.float 0.000000e+00
    %false_8758 = torch.constant.bool false
    %none_8759 = torch.constant.none
    %6878:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6875, %6876, %6877, %float0.000000e00_8757, %false_8758, %368, %none_8759) : (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.vtensor<[4,32,?,128],f16>, !torch.float, !torch.bool, !torch.vtensor<[4,1,1,?],f16>, !torch.none) -> (!torch.vtensor<[4,32,1,128],f16>, !torch.vtensor<[4,32,1],f32>) 
    %int1_8760 = torch.constant.int 1
    %int2_8761 = torch.constant.int 2
    %6879 = torch.aten.transpose.int %6878#0, %int1_8760, %int2_8761 : !torch.vtensor<[4,32,1,128],f16>, !torch.int, !torch.int -> !torch.vtensor<[4,1,32,128],f16>
    %int4_8762 = torch.constant.int 4
    %int1_8763 = torch.constant.int 1
    %int4096_8764 = torch.constant.int 4096
    %6880 = torch.prim.ListConstruct %int4_8762, %int1_8763, %int4096_8764 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6881 = torch.aten.view %6879, %6880 : !torch.vtensor<[4,1,32,128],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int-2_8765 = torch.constant.int -2
    %int-1_8766 = torch.constant.int -1
    %6882 = torch.aten.transpose.int %348, %int-2_8765, %int-1_8766 : !torch.vtensor<[4096,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,4096],f16>
    %int4_8767 = torch.constant.int 4
    %int4096_8768 = torch.constant.int 4096
    %6883 = torch.prim.ListConstruct %int4_8767, %int4096_8768 : (!torch.int, !torch.int) -> !torch.list<int>
    %6884 = torch.aten.view %6881, %6883 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %6885 = torch.aten.mm %6884, %6882 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_8769 = torch.constant.int 4
    %int1_8770 = torch.constant.int 1
    %int4096_8771 = torch.constant.int 4096
    %6886 = torch.prim.ListConstruct %int4_8769, %int1_8770, %int4096_8771 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6887 = torch.aten.view %6885, %6886 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_8772 = torch.constant.int 1
    %6888 = torch.aten.add.Tensor %6715, %6887, %int1_8772 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_8773 = torch.constant.int 6
    %6889 = torch.prims.convert_element_type %6888, %int6_8773 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_8774 = torch.constant.int 2
    %6890 = torch.aten.pow.Tensor_Scalar %6889, %int2_8774 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_8775 = torch.constant.int -1
    %6891 = torch.prim.ListConstruct %int-1_8775 : (!torch.int) -> !torch.list<int>
    %true_8776 = torch.constant.bool true
    %none_8777 = torch.constant.none
    %6892 = torch.aten.mean.dim %6890, %6891, %true_8776, %none_8777 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_8778 = torch.constant.float 9.9999997473787516E-6
    %int1_8779 = torch.constant.int 1
    %6893 = torch.aten.add.Scalar %6892, %float9.999990e-06_8778, %int1_8779 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %6894 = torch.aten.rsqrt %6893 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %6895 = torch.aten.mul.Tensor %6889, %6894 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_8780 = torch.constant.int 5
    %6896 = torch.prims.convert_element_type %6895, %int5_8780 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %6897 = torch.aten.mul.Tensor %349, %6896 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_8781 = torch.constant.int 5
    %6898 = torch.prims.convert_element_type %6897, %int5_8781 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_8782 = torch.constant.int -2
    %int-1_8783 = torch.constant.int -1
    %6899 = torch.aten.transpose.int %350, %int-2_8782, %int-1_8783 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_8784 = torch.constant.int 4
    %int4096_8785 = torch.constant.int 4096
    %6900 = torch.prim.ListConstruct %int4_8784, %int4096_8785 : (!torch.int, !torch.int) -> !torch.list<int>
    %6901 = torch.aten.view %6898, %6900 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %6902 = torch.aten.mm %6901, %6899 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_8786 = torch.constant.int 4
    %int1_8787 = torch.constant.int 1
    %int14336_8788 = torch.constant.int 14336
    %6903 = torch.prim.ListConstruct %int4_8786, %int1_8787, %int14336_8788 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6904 = torch.aten.view %6902, %6903 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %6905 = torch.aten.silu %6904 : !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_8789 = torch.constant.int -2
    %int-1_8790 = torch.constant.int -1
    %6906 = torch.aten.transpose.int %351, %int-2_8789, %int-1_8790 : !torch.vtensor<[14336,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,14336],f16>
    %int4_8791 = torch.constant.int 4
    %int4096_8792 = torch.constant.int 4096
    %6907 = torch.prim.ListConstruct %int4_8791, %int4096_8792 : (!torch.int, !torch.int) -> !torch.list<int>
    %6908 = torch.aten.view %6898, %6907 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %6909 = torch.aten.mm %6908, %6906 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,14336],f16> -> !torch.vtensor<[4,14336],f16>
    %int4_8793 = torch.constant.int 4
    %int1_8794 = torch.constant.int 1
    %int14336_8795 = torch.constant.int 14336
    %6910 = torch.prim.ListConstruct %int4_8793, %int1_8794, %int14336_8795 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6911 = torch.aten.view %6909, %6910 : !torch.vtensor<[4,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,1,14336],f16>
    %6912 = torch.aten.mul.Tensor %6905, %6911 : !torch.vtensor<[4,1,14336],f16>, !torch.vtensor<[4,1,14336],f16> -> !torch.vtensor<[4,1,14336],f16>
    %int-2_8796 = torch.constant.int -2
    %int-1_8797 = torch.constant.int -1
    %6913 = torch.aten.transpose.int %352, %int-2_8796, %int-1_8797 : !torch.vtensor<[4096,14336],f16>, !torch.int, !torch.int -> !torch.vtensor<[14336,4096],f16>
    %int4_8798 = torch.constant.int 4
    %int14336_8799 = torch.constant.int 14336
    %6914 = torch.prim.ListConstruct %int4_8798, %int14336_8799 : (!torch.int, !torch.int) -> !torch.list<int>
    %6915 = torch.aten.view %6912, %6914 : !torch.vtensor<[4,1,14336],f16>, !torch.list<int> -> !torch.vtensor<[4,14336],f16>
    %6916 = torch.aten.mm %6915, %6913 : !torch.vtensor<[4,14336],f16>, !torch.vtensor<[14336,4096],f16> -> !torch.vtensor<[4,4096],f16>
    %int4_8800 = torch.constant.int 4
    %int1_8801 = torch.constant.int 1
    %int4096_8802 = torch.constant.int 4096
    %6917 = torch.prim.ListConstruct %int4_8800, %int1_8801, %int4096_8802 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6918 = torch.aten.view %6916, %6917 : !torch.vtensor<[4,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,1,4096],f16>
    %int1_8803 = torch.constant.int 1
    %6919 = torch.aten.add.Tensor %6888, %6918, %int1_8803 : !torch.vtensor<[4,1,4096],f16>, !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int6_8804 = torch.constant.int 6
    %6920 = torch.prims.convert_element_type %6919, %int6_8804 : !torch.vtensor<[4,1,4096],f16>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int2_8805 = torch.constant.int 2
    %6921 = torch.aten.pow.Tensor_Scalar %6920, %int2_8805 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f32>
    %int-1_8806 = torch.constant.int -1
    %6922 = torch.prim.ListConstruct %int-1_8806 : (!torch.int) -> !torch.list<int>
    %true_8807 = torch.constant.bool true
    %none_8808 = torch.constant.none
    %6923 = torch.aten.mean.dim %6921, %6922, %true_8807, %none_8808 : !torch.vtensor<[4,1,4096],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[4,1,1],f32>
    %float9.999990e-06_8809 = torch.constant.float 9.9999997473787516E-6
    %int1_8810 = torch.constant.int 1
    %6924 = torch.aten.add.Scalar %6923, %float9.999990e-06_8809, %int1_8810 : !torch.vtensor<[4,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[4,1,1],f32>
    %6925 = torch.aten.rsqrt %6924 : !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,1],f32>
    %6926 = torch.aten.mul.Tensor %6920, %6925 : !torch.vtensor<[4,1,4096],f32>, !torch.vtensor<[4,1,1],f32> -> !torch.vtensor<[4,1,4096],f32>
    %int5_8811 = torch.constant.int 5
    %6927 = torch.prims.convert_element_type %6926, %int5_8811 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %6928 = torch.aten.mul.Tensor %353, %6927 : !torch.vtensor<[4096],f32>, !torch.vtensor<[4,1,4096],f16> -> !torch.vtensor<[4,1,4096],f32>
    %int5_8812 = torch.constant.int 5
    %6929 = torch.prims.convert_element_type %6928, %int5_8812 : !torch.vtensor<[4,1,4096],f32>, !torch.int -> !torch.vtensor<[4,1,4096],f16>
    %int-2_8813 = torch.constant.int -2
    %int-1_8814 = torch.constant.int -1
    %6930 = torch.aten.transpose.int %354, %int-2_8813, %int-1_8814 : !torch.vtensor<[128256,4096],f16>, !torch.int, !torch.int -> !torch.vtensor<[4096,128256],f16>
    %int4_8815 = torch.constant.int 4
    %int4096_8816 = torch.constant.int 4096
    %6931 = torch.prim.ListConstruct %int4_8815, %int4096_8816 : (!torch.int, !torch.int) -> !torch.list<int>
    %6932 = torch.aten.view %6929, %6931 : !torch.vtensor<[4,1,4096],f16>, !torch.list<int> -> !torch.vtensor<[4,4096],f16>
    %6933 = torch.aten.mm %6932, %6930 : !torch.vtensor<[4,4096],f16>, !torch.vtensor<[4096,128256],f16> -> !torch.vtensor<[4,128256],f16>
    %int4_8817 = torch.constant.int 4
    %int1_8818 = torch.constant.int 1
    %int128256 = torch.constant.int 128256
    %6934 = torch.prim.ListConstruct %int4_8817, %int1_8818, %int128256 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6935 = torch.aten.view %6933, %6934 : !torch.vtensor<[4,128256],f16>, !torch.list<int> -> !torch.vtensor<[4,1,128256],f16>
    return %6935 : !torch.vtensor<[4,1,128256],f16>
  }
  util.func private @sharktank_rotary_embedding_4_D_32_128_f32(%arg0: tensor<4x?x32x128xf32>, %arg1: tensor<4x?x128xf32>) -> tensor<4x?x32x128xf32> {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %dim = tensor.dim %arg0, %c0 : tensor<4x?x32x128xf32>
    %dim_0 = tensor.dim %arg0, %c1 : tensor<4x?x32x128xf32>
    %dim_1 = tensor.dim %arg0, %c2 : tensor<4x?x32x128xf32>
    %dim_2 = tensor.dim %arg0, %c3 : tensor<4x?x32x128xf32>
    %0 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %cast = tensor.cast %0 : tensor<?x?x?x?xf32> to tensor<4x?x32x128xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg1 : tensor<4x?x128xf32>) outs(%cast : tensor<4x?x32x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      %2 = linalg.index 0 : index
      %3 = linalg.index 1 : index
      %4 = linalg.index 2 : index
      %5 = linalg.index 3 : index
      %6 = arith.divui %5, %c2 : index
      %7 = arith.remui %5, %c2 : index
      %8 = math.cos %in : f32
      %9 = math.sin %in : f32
      %10 = arith.muli %6, %c2 : index
      %11 = arith.addi %10, %c1 : index
      %extracted = tensor.extract %arg0[%2, %3, %4, %10] : tensor<4x?x32x128xf32>
      %extracted_3 = tensor.extract %arg0[%2, %3, %4, %11] : tensor<4x?x32x128xf32>
      %12 = arith.cmpi eq, %7, %c0 : index
      %13 = arith.mulf %extracted, %8 : f32
      %14 = arith.mulf %extracted_3, %9 : f32
      %15 = arith.subf %13, %14 : f32
      %16 = arith.mulf %extracted_3, %8 : f32
      %17 = arith.mulf %extracted, %9 : f32
      %18 = arith.addf %16, %17 : f32
      %19 = arith.select %12, %15, %18 : f32
      linalg.yield %19 : f32
    } -> tensor<4x?x32x128xf32>
    util.return %1 : tensor<4x?x32x128xf32>
  }
  util.func private @sharktank_rotary_embedding_4_D_8_128_f32(%arg0: tensor<4x?x8x128xf32>, %arg1: tensor<4x?x128xf32>) -> tensor<4x?x8x128xf32> {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %dim = tensor.dim %arg0, %c0 : tensor<4x?x8x128xf32>
    %dim_0 = tensor.dim %arg0, %c1 : tensor<4x?x8x128xf32>
    %dim_1 = tensor.dim %arg0, %c2 : tensor<4x?x8x128xf32>
    %dim_2 = tensor.dim %arg0, %c3 : tensor<4x?x8x128xf32>
    %0 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %cast = tensor.cast %0 : tensor<?x?x?x?xf32> to tensor<4x?x8x128xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg1 : tensor<4x?x128xf32>) outs(%cast : tensor<4x?x8x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      %2 = linalg.index 0 : index
      %3 = linalg.index 1 : index
      %4 = linalg.index 2 : index
      %5 = linalg.index 3 : index
      %6 = arith.divui %5, %c2 : index
      %7 = arith.remui %5, %c2 : index
      %8 = math.cos %in : f32
      %9 = math.sin %in : f32
      %10 = arith.muli %6, %c2 : index
      %11 = arith.addi %10, %c1 : index
      %extracted = tensor.extract %arg0[%2, %3, %4, %10] : tensor<4x?x8x128xf32>
      %extracted_3 = tensor.extract %arg0[%2, %3, %4, %11] : tensor<4x?x8x128xf32>
      %12 = arith.cmpi eq, %7, %c0 : index
      %13 = arith.mulf %extracted, %8 : f32
      %14 = arith.mulf %extracted_3, %9 : f32
      %15 = arith.subf %13, %14 : f32
      %16 = arith.mulf %extracted_3, %8 : f32
      %17 = arith.mulf %extracted, %9 : f32
      %18 = arith.addf %16, %17 : f32
      %19 = arith.select %12, %15, %18 : f32
      linalg.yield %19 : f32
    } -> tensor<4x?x8x128xf32>
    util.return %1 : tensor<4x?x8x128xf32>
  }
  util.func private @sharktank_rotary_embedding_4_1_32_128_f32(%arg0: tensor<4x1x32x128xf32>, %arg1: tensor<4x1x128xf32>) -> tensor<4x1x32x128xf32> {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %dim = tensor.dim %arg0, %c0 : tensor<4x1x32x128xf32>
    %dim_0 = tensor.dim %arg0, %c1 : tensor<4x1x32x128xf32>
    %dim_1 = tensor.dim %arg0, %c2 : tensor<4x1x32x128xf32>
    %dim_2 = tensor.dim %arg0, %c3 : tensor<4x1x32x128xf32>
    %0 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %cast = tensor.cast %0 : tensor<?x?x?x?xf32> to tensor<4x1x32x128xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg1 : tensor<4x1x128xf32>) outs(%cast : tensor<4x1x32x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      %2 = linalg.index 0 : index
      %3 = linalg.index 1 : index
      %4 = linalg.index 2 : index
      %5 = linalg.index 3 : index
      %6 = arith.divui %5, %c2 : index
      %7 = arith.remui %5, %c2 : index
      %8 = math.cos %in : f32
      %9 = math.sin %in : f32
      %10 = arith.muli %6, %c2 : index
      %11 = arith.addi %10, %c1 : index
      %extracted = tensor.extract %arg0[%2, %3, %4, %10] : tensor<4x1x32x128xf32>
      %extracted_3 = tensor.extract %arg0[%2, %3, %4, %11] : tensor<4x1x32x128xf32>
      %12 = arith.cmpi eq, %7, %c0 : index
      %13 = arith.mulf %extracted, %8 : f32
      %14 = arith.mulf %extracted_3, %9 : f32
      %15 = arith.subf %13, %14 : f32
      %16 = arith.mulf %extracted_3, %8 : f32
      %17 = arith.mulf %extracted, %9 : f32
      %18 = arith.addf %16, %17 : f32
      %19 = arith.select %12, %15, %18 : f32
      linalg.yield %19 : f32
    } -> tensor<4x1x32x128xf32>
    util.return %1 : tensor<4x1x32x128xf32>
  }
  util.func private @sharktank_rotary_embedding_4_1_8_128_f32(%arg0: tensor<4x1x8x128xf32>, %arg1: tensor<4x1x128xf32>) -> tensor<4x1x8x128xf32> {
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    %c2 = arith.constant 2 : index
    %c3 = arith.constant 3 : index
    %dim = tensor.dim %arg0, %c0 : tensor<4x1x8x128xf32>
    %dim_0 = tensor.dim %arg0, %c1 : tensor<4x1x8x128xf32>
    %dim_1 = tensor.dim %arg0, %c2 : tensor<4x1x8x128xf32>
    %dim_2 = tensor.dim %arg0, %c3 : tensor<4x1x8x128xf32>
    %0 = tensor.empty(%dim, %dim_0, %dim_1, %dim_2) : tensor<?x?x?x?xf32>
    %cast = tensor.cast %0 : tensor<?x?x?x?xf32> to tensor<4x1x8x128xf32>
    %1 = linalg.generic {indexing_maps = [#map, #map1], iterator_types = ["parallel", "parallel", "parallel", "parallel"]} ins(%arg1 : tensor<4x1x128xf32>) outs(%cast : tensor<4x1x8x128xf32>) {
    ^bb0(%in: f32, %out: f32):
      %2 = linalg.index 0 : index
      %3 = linalg.index 1 : index
      %4 = linalg.index 2 : index
      %5 = linalg.index 3 : index
      %6 = arith.divui %5, %c2 : index
      %7 = arith.remui %5, %c2 : index
      %8 = math.cos %in : f32
      %9 = math.sin %in : f32
      %10 = arith.muli %6, %c2 : index
      %11 = arith.addi %10, %c1 : index
      %extracted = tensor.extract %arg0[%2, %3, %4, %10] : tensor<4x1x8x128xf32>
      %extracted_3 = tensor.extract %arg0[%2, %3, %4, %11] : tensor<4x1x8x128xf32>
      %12 = arith.cmpi eq, %7, %c0 : index
      %13 = arith.mulf %extracted, %8 : f32
      %14 = arith.mulf %extracted_3, %9 : f32
      %15 = arith.subf %13, %14 : f32
      %16 = arith.mulf %extracted_3, %8 : f32
      %17 = arith.mulf %extracted, %9 : f32
      %18 = arith.addf %16, %17 : f32
      %19 = arith.select %12, %15, %18 : f32
      linalg.yield %19 : f32
    } -> tensor<4x1x8x128xf32>
    util.return %1 : tensor<4x1x8x128xf32>
  }
}
